{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI output results and speed comparaison\n",
    "## 1. Continuous variable\n",
    "### 1.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 11:32:16.163552: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 11:32:16.163609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 11:32:16.164438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-22 11:32:16.640619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "newPath = os.path.dirname(os.path.abspath(\"\"))\n",
    "if newPath not in sys.path:\n",
    "    sys.path.append(newPath)\n",
    "#from src.main import*\n",
    "import time as tm\n",
    "from src.main_jax import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 2.2279 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/arviz/data/base.py:221: UserWarning: More chains (2000) than draws (4). Passed array should have shape (chains, draws, *shape)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Model m4.3\n",
    "d = pd.read_csv('../data/Howell1.csv', sep=';')\n",
    "d = d[d.age > 18]\n",
    "#self.df[\"weight.per.g\"].pipe(lambda x: (x - x.mean()) / x.std())\n",
    "d.weight = d.weight - d.weight.mean()\n",
    "d.age = d.age - d.age.mean()\n",
    "formula = dict(main1 = 'height ~ Normal(mu,sigma)',\n",
    "            likelihood = 'mu ~ alpha + beta * weight',\n",
    "            prior1 = 'sigma ~ Uniform(0,50)',\n",
    "            prior2 = 'alpha ~ Normal(178,20)',\n",
    "            prior3 = 'beta ~ Normal(0,1)')    \n",
    "\n",
    "start = tm.time()\n",
    "m4_3 = model(formula, df = d, float = 32)\n",
    "m4_3.fit(observed_data = dict(height = d.height.astype('float32').values))   \n",
    "end = tm.time()        \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m stan_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  vector[346] height;\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 32\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43md\u001b[49m\u001b[38;5;241m.\u001b[39mheight\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m: d\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     34\u001b[0m }\n\u001b[1;32m     35\u001b[0m start \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     36\u001b[0m stan_model \u001b[38;5;241m=\u001b[39m stan\u001b[38;5;241m.\u001b[39mbuild(stan_code, data \u001b[38;5;241m=\u001b[39m data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "  vector[346] height;\n",
    "  vector[346] weight;\n",
    "}\n",
    "parameters{\n",
    "  real a;\n",
    "  real<lower=0> b;\n",
    "  real<lower=0,upper=50> sigma;\n",
    "}\n",
    "model{\n",
    "  vector[346] mu;\n",
    "  sigma ~ uniform( 0 , 50 );\n",
    "  b ~ lognormal( 0 , 1 );\n",
    "  a ~ normal( 178 , 20 );\n",
    "  for ( i in 1:346 ) {\n",
    "    mu[i] = a + b* weight[i] ;\n",
    "  }\n",
    "  height ~ normal( mu , sigma );  \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'height': d.height.values,\n",
    "  'weight': d.weight.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)  \n",
    "df = fit.to_frame()\n",
    "end = tm.time()  \n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Output comparaison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma[0]</th>\n",
       "      <td>5.141531</td>\n",
       "      <td>5.144897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.905336</td>\n",
       "      <td>0.904733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>154.653732</td>\n",
       "      <td>154.649662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfp      pystan\n",
       "sigma[0]    5.141531    5.144897\n",
       "beta[0]     0.905336    0.904733\n",
       "alpha[0]  154.653732  154.649662"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m4_3.summary(round_to = 'none')['mean'],\n",
    "        \"pystan\": [df.sigma.mean(),  df.b.mean(), df.a.mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical variable\n",
    "### 2.1. Speed comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 1.5272 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/arviz/data/base.py:221: UserWarning: More chains (2000) than draws (4). Passed array should have shape (chains, draws, *shape)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "m5_9 = model()\n",
    "m5_9.import_csv('../data/milk.csv', sep = ';')\n",
    "m5_9.df[\"K\"] = m5_9.df[\"kcal.per.g\"].pipe(lambda x: (x - x.mean()) / x.std())\n",
    "m5_9.index(cols = \"clade\")\n",
    "formula = dict(main = 'K ~ Normal(mu,sigma)',\n",
    "            likelihood = 'mu ~ alpha[index_clade]',\n",
    "            prior1 = 'alpha~ Normal(0,0.5)',\n",
    "            prior2 = 'sigma ~ Exponential(1)') \n",
    "\n",
    "m5_9.f = formula\n",
    "\n",
    "start = tm.time()\n",
    "m5_9.build_model()\n",
    "m5_9.fit(observed_data = dict(K = m5_9.df.K.astype('float32').values))\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 11.8s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 6e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 6e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 6e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 6e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/httpstan_wgftl1d9/model_utzoleag.stan', line 17, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 12.1463 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[29] K;\n",
    "    array[29] int clade_id;\n",
    "}\n",
    "parameters{\n",
    "    vector[4] a;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[29] mu;\n",
    "    sigma ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 0.5 );\n",
    "    for ( i in 1:29 ) {\n",
    "        mu[i] = a[clade_id[i]];\n",
    "    }\n",
    "    K ~ normal( mu , sigma );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'clade_id': m5_9.df.index_clade.values+1,\n",
    "  'K': m5_9.df.K.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma[0]</th>\n",
       "      <td>0.799599</td>\n",
       "      <td>0.801640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>-0.461480</td>\n",
       "      <td>-0.459873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>0.351941</td>\n",
       "      <td>0.353296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.636981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[3]</th>\n",
       "      <td>-0.549415</td>\n",
       "      <td>-0.547616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan\n",
       "sigma[0]  0.799599  0.801640\n",
       "alpha[0] -0.461480 -0.459873\n",
       "alpha[1]  0.351941  0.353296\n",
       "alpha[2]  0.636618  0.636981\n",
       "alpha[3] -0.549415 -0.547616"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m5_9.summary(round_to = 'none')['mean'],\n",
    "        \"pystan\": [df.sigma.mean(),  df['a.1'].mean(), df['a.2'].mean(), df['a.3'].mean(), df['a.4'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous interactions terms (model 8.3)\n",
    "### 3.1. Speed comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 1.5952 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/arviz/data/base.py:221: UserWarning: More chains (2000) than draws (4). Passed array should have shape (chains, draws, *shape)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('../data/tulips.csv', sep = ';')\n",
    "d[\"blooms_std\"] = d.blooms / d.blooms.max()\n",
    "d[\"water_cent\"] = d.water - d.water.mean()\n",
    "d[\"shade_cent\"] = d.shade - d.shade.mean()\n",
    "\n",
    "formula = dict(\n",
    "            main = 'blooms_std ~ Normal( mu , sigma ) ',\n",
    "            likelihood ='mu ~ a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent' ,\n",
    "            prior1 = 'a ~ Normal( 0.5 , 0.25 ) ',\n",
    "            prior2 = 'bw ~ Normal( 0 , 0.25 ) ',\n",
    "            prior3 = 'bs ~ Normal( 0 , 0.25 ) ',\n",
    "            prior4 = 'bws ~ Normal( 0 , 0.25 ) ',\n",
    "            prior5 = 'sigma ~ Exponential( 1 )',\n",
    "            )\n",
    "start = tm.time()\n",
    "m8_5 = model(formula, d)\n",
    "m8_5.fit(observed_data = dict(blooms_std =d.blooms_std.astype('float32').values))\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 11.7s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 3.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 2.8e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 12.1231 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[27] blooms_std;\n",
    "    array[27] int shade_cent;\n",
    "    array[27] int water_cent;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real bw;\n",
    "    real bs;\n",
    "    real bws;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[27] mu;\n",
    "    sigma ~ exponential( 1 );\n",
    "    bws ~ normal( 0 , 0.25 );\n",
    "    bs ~ normal( 0 , 0.25 );\n",
    "    bw ~ normal( 0 , 0.25 );\n",
    "    a ~ normal( 0.5 , 0.25 );\n",
    "    for ( i in 1:27 ) {\n",
    "        mu[i] = a + bw * water_cent[i] + bs * shade_cent[i] + bws * water_cent[i] * shade_cent[i];\n",
    "    }\n",
    "\n",
    "    \n",
    "    blooms_std ~ normal( mu , sigma );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'blooms_std' : d[\"blooms_std\"].values,\n",
    "    \"water_cent\": d[\"water_cent\"].values.astype(int),\n",
    "    \"shade_cent\": d[\"shade_cent\"].values.astype(int),\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma[0]</th>\n",
       "      <td>0.142805</td>\n",
       "      <td>0.142327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bws[0]</th>\n",
       "      <td>-0.142726</td>\n",
       "      <td>-0.143176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bw[0]</th>\n",
       "      <td>0.205038</td>\n",
       "      <td>0.206099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bs[0]</th>\n",
       "      <td>-0.112257</td>\n",
       "      <td>-0.112393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>0.358838</td>\n",
       "      <td>0.358277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan\n",
       "sigma[0]  0.142805  0.142327\n",
       "bws[0]   -0.142726 -0.143176\n",
       "bw[0]     0.205038  0.206099\n",
       "bs[0]    -0.112257 -0.112393\n",
       "a[0]      0.358838  0.358277"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m8_5.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df.sigma.mean(),  df['bws'].mean(), df['bw'].mean(), df['bs'].mean(), df['a'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binomial model\n",
    "### 4.1. Speed comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 1.4346 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/arviz/data/base.py:221: UserWarning: More chains (2000) than draws (4). Passed array should have shape (chains, draws, *shape)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('../data/chimpanzees.csv', sep = ';')\n",
    "d[\"treatment\"] = 1 + d.prosoc_left + 2 * d.condition\n",
    "d[\"side\"] = d.prosoc_left  # right 0, left 1\n",
    "d[\"cond\"] = d.condition  # no partner 0, partner 1\n",
    "\n",
    "d_aggregated = (\n",
    "    d.groupby([\"treatment\", \"actor\", \"side\", \"cond\"])[\"pulled_left\"].sum().reset_index()\n",
    ")\n",
    "d_aggregated.rename(columns={\"pulled_left\": \"left_pulls\"}, inplace=True)\n",
    "d_aggregated[\"actor_id\"] = d_aggregated[\"actor\"].values - 1\n",
    "\n",
    "formula = dict(\n",
    "    main = 'pulled_left ~ Binomial( 1 , logits = p )' ,\n",
    "    likelihood = 'p ~ a' ,\n",
    "    prior1 = 'a ~ Normal( 0 , 10 )'\n",
    ")\n",
    "start = tm.time()\n",
    "m11_1 = model(formula, d)\n",
    "m11_1.fit(observed_data = dict(pulled_left =d.pulled_left.astype('float32').values))\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 10.2s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 5e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 5e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 5e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 5e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 10.5593 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    a ~ normal( 0 , 10 );\n",
    "    p = a;\n",
    "    p = inv_logit(p);\n",
    "    pulled_left ~ binomial( 1 , p );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : d[\"pulled_left\"].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>0.323161</td>\n",
       "      <td>0.321119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tfp    pystan\n",
       "a[0]  0.323161  0.321119"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m11_1.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df.a.mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Binomial with indices\n",
    "### 5.1.Speed comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:`_` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "BI took: 5.1156 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.main import*\n",
    "d = pd.read_csv('../data/chimpanzees.csv', sep = ';')\n",
    "d.actor = d.actor - 1\n",
    "d[\"treatment\"] = d.prosoc_left + 2 * d.condition\n",
    "d[[\"actor\", \"prosoc_left\", \"condition\", \"treatment\"]]\n",
    "\n",
    "formula = dict(\n",
    "    main = 'pulled_left ~ Binomial(1 , p )' ,\n",
    "    likelihood = 'p ~ a[actor] + b[treatment]' ,\n",
    "    prior1 = 'a ~ Normal(0, 1.5)',\n",
    "    prior2 = 'b ~ Normal(0, 0.5)'\n",
    ")\n",
    "\n",
    "start = tm.time()\n",
    "m11_4 = model(formula, d, float = 32)\n",
    "m11_4.fit(observed_data = dict(pulled_left =d.pulled_left.astype('float32').values))\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 11.8s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling:  79% (7900/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 5.6e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.8e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.9e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 6.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.61 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 15.5431 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "    array[504] int treatment;\n",
    "    array[504] int actor;\n",
    "}\n",
    "parameters{\n",
    "    vector[7] a;\n",
    "    vector[4] b;\n",
    "}\n",
    "model{\n",
    "    vector[504] p;\n",
    "    b ~ normal( 0 , 0.5 );\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    for ( i in 1:504 ) {\n",
    "        p[i] = a[actor[i]] + b[treatment[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    pulled_left ~ binomial( 1 , p );\n",
    "}\n",
    "generated quantities{\n",
    "    vector[504] log_lik;\n",
    "    vector[504] p;\n",
    "    for ( i in 1:504 ) {\n",
    "        p[i] = a[actor[i]] + b[treatment[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    for ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : d[\"pulled_left\"].values.astype(int),\n",
    "    'treatment' : d[\"treatment\"].values.astype(int) +1,\n",
    "    'actor' : d[\"actor\"].values.astype(int) +1\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>-0.041349</td>\n",
       "      <td>-0.031648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[1]</th>\n",
       "      <td>0.480509</td>\n",
       "      <td>0.490967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[2]</th>\n",
       "      <td>-0.356620</td>\n",
       "      <td>-0.375165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[3]</th>\n",
       "      <td>0.375912</td>\n",
       "      <td>0.376894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>-0.468963</td>\n",
       "      <td>-0.455304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>3.874392</td>\n",
       "      <td>3.887521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[2]</th>\n",
       "      <td>-0.752882</td>\n",
       "      <td>-0.757069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[3]</th>\n",
       "      <td>-0.749913</td>\n",
       "      <td>-0.758169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[4]</th>\n",
       "      <td>-0.468166</td>\n",
       "      <td>-0.459580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[5]</th>\n",
       "      <td>0.461863</td>\n",
       "      <td>0.471230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[6]</th>\n",
       "      <td>1.954741</td>\n",
       "      <td>1.946828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tfp    pystan\n",
       "b[0] -0.041349 -0.031648\n",
       "b[1]  0.480509  0.490967\n",
       "b[2] -0.356620 -0.375165\n",
       "b[3]  0.375912  0.376894\n",
       "a[0] -0.468963 -0.455304\n",
       "a[1]  3.874392  3.887521\n",
       "a[2] -0.752882 -0.757069\n",
       "a[3] -0.749913 -0.758169\n",
       "a[4] -0.468166 -0.459580\n",
       "a[5]  0.461863  0.471230\n",
       "a[6]  1.954741  1.946828"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m11_4.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['b.1'].mean(), df['b.2'].mean(), df['b.3'].mean(), df['b.4'].mean(),\n",
    "                   df['a.1'].mean(), df['a.2'].mean(), df['a.3'].mean(), df['a.4'].mean(), df['a.5'].mean(), df['a.6'].mean(), df['a.7'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Poisson\n",
    "### 6.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:`_` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "BI took: 4.6995 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.main import*\n",
    "d = pd.read_csv('../data/Kline.csv', sep = ';')\n",
    "d[\"P\"] = d.population.pipe(np.log).pipe(lambda x: (x - x.mean()) / x.std())\n",
    "d[\"cid\"] = (d.contact == \"high\").astype(int)\n",
    "d['pLog'] = tf.math.log(d.P).numpy()\n",
    "formula = dict(main = 'total_tools ~ Poisson(log_rate = lambda)',\n",
    "               likelihood = 'lambda ~ alpha[cid] + beta[cid]*P',\n",
    "               prior1 = 'alpha ~ Normal(3,0.5)',\n",
    "               prior2 = 'beta ~ Normal(0,0.2)')\n",
    "start = tm.time()\n",
    "m11_10 = model(formula, d)\n",
    "m11_10.fit(observed_data = dict(total_tools =d.total_tools.astype('float32').values))\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 13.1s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 9e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 13.4400 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[10] int T;\n",
    "    vector[10] P;\n",
    "    array[10] int cid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    vector[2] b;\n",
    "}\n",
    "model{\n",
    "    vector[10] lambda;\n",
    "    b ~ normal( 0 , 0.2 );\n",
    "    a ~ normal( 3 , 0.5 );\n",
    "    for ( i in 1:10 ) {\n",
    "       lambda[i] = a[cid[i]] + b[cid[i]] * P[i];\n",
    "       lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    T ~ poisson( lambda );\n",
    "}\n",
    "generated quantities{\n",
    "    vector[10] log_lik;\n",
    "    vector[10] lambda;\n",
    "    for ( i in 1:10 ) {\n",
    "        lambda[i] = a[cid[i]] + b[cid[i]] * P[i];\n",
    "        lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    for ( i in 1:10 ) log_lik[i] = poisson_lpmf( T[i] | lambda[i]);\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'T' : d[\"total_tools\"].values.astype(int),\n",
    "    'P' : d[\"P\"].values.astype(float),\n",
    "    'cid' : d[\"cid\"].values.astype(int) +1\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Output comparaison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.377608</td>\n",
       "      <td>0.377975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1]</th>\n",
       "      <td>0.186534</td>\n",
       "      <td>0.190583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>3.318159</td>\n",
       "      <td>3.319431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>3.615976</td>\n",
       "      <td>3.610163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan\n",
       "beta[0]   0.377608  0.377975\n",
       "beta[1]   0.186534  0.190583\n",
       "alpha[0]  3.318159  3.319431\n",
       "alpha[1]  3.615976  3.610163"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m11_10.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['b.1'].mean(), df['b.2'].mean(), \n",
    "                   df['a.1'].mean(), df['a.2'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Negative binomial (PB estimation)\n",
    "### 7.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:`_` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "BI took: 4.6588 seconds\n"
     ]
    }
   ],
   "source": [
    "num_days = 30\n",
    "y = tfd.Poisson(rate=1.5).sample((num_days,))\n",
    "num_weeks = 4\n",
    "y_new = tfd.Poisson(rate=0.5 * 7).sample((num_weeks,))\n",
    "y_all = np.concatenate([y, y_new])\n",
    "exposure = np.concatenate([np.repeat(1, 30), np.repeat(7, 4)])\n",
    "monastery = np.concatenate([np.repeat(0, 30), np.repeat(1, 4)])\n",
    "d = pd.DataFrame.from_dict(dict(y=y_all, days=exposure, monastery=monastery))\n",
    "d[\"log_days\"] = d.days.pipe(np.log)\n",
    "\n",
    "formula = dict(main = 'y ~ Poisson(rate = lambda)',\n",
    "               likelihood = 'lambda ~ tf.exp(log_days + alpha +  beta * monastery)',\n",
    "               prior1 = 'alpha ~ Normal(0,1)',\n",
    "               prior2 = 'beta ~ Normal(0,1)')\n",
    "start = tm.time()\n",
    "m11_12 = model(formula, d, float=32)\n",
    "m11_12.fit(observed_data = dict(y =d.y.astype('float32').values))\n",
    "end = tm.time()\n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 11.0s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 7e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 11.4282 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[34] int days;\n",
    "    array[34] int y;\n",
    "    array[34] int monastery;\n",
    "    vector[34] log_days;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real b;\n",
    "}\n",
    "model{\n",
    "    vector[34] lambda;\n",
    "    b ~ normal( 0 , 1 );\n",
    "    a ~ normal( 0 , 1 );\n",
    "    for ( i in 1:34 ) {\n",
    "        lambda[i] = log_days[i] + a + b * monastery[i];\n",
    "        lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    \n",
    "    y ~ poisson( lambda );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'days' : d[\"days\"].values.astype(int),\n",
    "    'y' : d[\"y\"].values.astype(int),\n",
    "    'monastery' : d[\"monastery\"].values.astype(int) +1,\n",
    "    'log_days' : d[\"log_days\"].values.astype(float),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output comparaison (PB estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>-0.979285</td>\n",
       "      <td>-0.866921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>0.389058</td>\n",
       "      <td>1.210285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan\n",
       "beta[0]  -0.979285 -0.866921\n",
       "alpha[0]  0.389058  1.210285"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m11_12.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['b'].mean(), df['a'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multinomial\n",
    "TODO : This model can't account for indices in the formula nor varying intercepts of varying effects.\n",
    "### 8.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate career choices among 500 individuals\n",
    "N = 500  # number of individuals\n",
    "income = np.array([1, 2, 5])  # expected income of each career\n",
    "score = 0.5 * income  # scores for each career, based on income\n",
    "\n",
    "# next line converts scores to probabilities\n",
    "p = tf.nn.softmax(score)\n",
    "\n",
    "# now simulate choice\n",
    "# outcome career holds event type values, not counts\n",
    "career = np.repeat(np.nan, N)  # empty vector of choices for each individual\n",
    "\n",
    "# sample chosen career for each individual\n",
    "for i in range(N):\n",
    "    career[i] = tfd.Categorical(probs=p).sample()\n",
    "\n",
    "career = career.astype(int)\n",
    "result = [income[index] for index in career]\n",
    "data = {'career': career, 'income': result}\n",
    "d = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 14:32:43.869962: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 14:32:43.870024: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 14:32:43.870038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 14:32:43.870218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 14:32:43.870231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-15 14:32:43.870248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 14:32:43.870262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 5679 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 OEM, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': \"tfd.Sample(tfd.Normal(0,1, name = 'prior1'), sample_shape = 3)\",\n",
       " 'beta': \"tfd.Sample(tfd.Normal(0,0.5, name = 'prior2'), sample_shape = 1)\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = dict(main = 'y ~ Poisson(log_rate = p)',\n",
    "               likelihood = 'p ~ alpha[income] + beta * income',\n",
    "               prior1 = 'alpha ~ Normal(0,1)',\n",
    "               prior2 = 'beta ~ Normal(0, 0.5)')\n",
    "m11_13 = model(formula, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.nn.softmax(tf.stack([alpha[0]+beta*1.0,alpha[1]+beta*2.0,0 +beta*2.0], axis=1))\n",
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:`_` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:`__1` is not a valid node name. Accepted names conform to Regex /re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\\\\\\\/>-]*$')/\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function run_modelH at 0x7fa09725eef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "BI took: 4.6478 seconds\n"
     ]
    }
   ],
   "source": [
    "formula = dict(main = 'y ~ Categorical(probs = p, cat = income)',\n",
    "               likelihood = 'p ~ alpha[income] + beta * income',\n",
    "               prior1 = 'alpha ~ Normal(0, 1)',\n",
    "               prior2 = 'beta ~ HalfNormal(0.5)'\n",
    "               )\n",
    "start = tm.time()   \n",
    "m11_13 = model(formula, d)\n",
    "m11_13.fit(observed_data = dict(y =d.career.astype('float32').values), num_chains=4)\n",
    "end = tm.time()\n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-617' coro=<HttpstanClient.post() done, defined at /home/sosa/.local/lib/python3.10/site-packages/stan/common.py:50> exception=ServerDisconnectedError('Server disconnected')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/stan/model.py\", line 519, in build\n",
      "    return asyncio.run(go())\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/nest_asyncio.py\", line 116, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 469, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/stan/common.py\", line 51, in post\n",
      "    async with self.session.post(f\"{self.base_url}{path}\", json=json) as resp:\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/aiohttp/client.py\", line 1194, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/aiohttp/client.py\", line 605, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/aiohttp/client_reqrep.py\", line 966, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"/home/sosa/.local/lib/python3.10/site-packages/aiohttp/streams.py\", line 622, in read\n",
      "    await self._waiter\n",
      "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [a, b]\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [12000/12000 00:06&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Sampling 4 chains for 2_000 tune and 1_000 draw iterations (8_000 + 4_000 draws total) took 7 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "with pm.Model() as m11_13_pm:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.0, shape=2)  # intercepts\n",
    "    b = pm.HalfNormal(\"b\", 0.5)  # association of income with choice\n",
    "\n",
    "    s0 = a[0] + b * income[0]\n",
    "    s1 = a[1] + b * income[1]\n",
    "    s2 = 0.0 + b * income[2]  # pivoting the intercept for the third category\n",
    "    s = pm.math.stack([s0, s1, s2])\n",
    "\n",
    "    p_ = pm.math.softmax(s)\n",
    "    career_obs = pm.Categorical(\"career\", p=p_, observed=career)\n",
    "\n",
    "    trace_11_13 = pm.sample(tune=2000, target_accept=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 12.2s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 1.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 9e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 9e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 12.7902 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    int N; // number of individuals\n",
    "    int K; // number of possible careers\n",
    "    array[N] int career; // outcome\n",
    "    vector[K] career_income;\n",
    "}\n",
    "parameters{\n",
    "    vector[K-1] a; // intercepts\n",
    "    real<lower=0> b; // association of income with choice\n",
    "}\n",
    "model{\n",
    "    vector[K] p;\n",
    "    vector[K] s;\n",
    "    a ~ normal( 0 , 1 );\n",
    "    b ~ normal( 0 , 0.5 );\n",
    "    s[1] = a[1] + b*career_income[1];\n",
    "    s[2] = a[2] + b*career_income[2];\n",
    "    s[3] = 0; // pivot\n",
    "    p = softmax( s );\n",
    "    career ~ categorical( p );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'N' : 500,\n",
    "    'K' : 3,\n",
    "    'career' : d[\"career\"].values.astype(int) + 1,\n",
    "    'career_income' : d[\"income\"].unique().astype(int).tolist(),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_212934/3955049630.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"pymc\": [tmp[2], tmp[0], tmp[1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "      <th>pymc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.572273</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>-1.634189</td>\n",
       "      <td>-2.454034</td>\n",
       "      <td>-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>-1.395898</td>\n",
       "      <td>-1.498306</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan  pymc\n",
       "beta[0]   0.572273  0.052870  0.46\n",
       "alpha[0] -1.634189 -2.454034 -0.42\n",
       "alpha[1] -1.395898 -1.498306 -0.05"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = az.summary(trace_11_13, round_to=2)['mean']\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m11_13.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['b'].mean(), df['a.1'].mean(), df['a.2'].mean()],\n",
    "        \"pymc\": [tmp[2], tmp[0], tmp[1]]\n",
    "    })\n",
    "# It seems that beta should be 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Beta binomial (PB estimation)\n",
    "### 9.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function run_modelH at 0x7fa09725eef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "BI took: 5.6142 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.main import*\n",
    "d = pd.read_csv('../data/UCBadmit.csv', sep = ';')\n",
    "d[\"gid\"] = (d[\"applicant.gender\"] != \"male\").astype(int)\n",
    "len(d.applications)\n",
    "formula = dict(main = 'y ~ BetaBinomial(applications, concentration1 = pbar*theta, concentration0 = (1 - pbar) * theta)',\n",
    "               likelihood = 'pbar ~ sigmoid(alpha[gid])',\n",
    "               likelihood2 = 'theta ~ phi + 2.0',\n",
    "               prior1 = 'alpha ~ Normal(0.,1.5)',\n",
    "               prior2 = 'phi ~ Exponential(1)'\n",
    "               )\n",
    "start = tm.time()\n",
    "m12_1 = model(formula, d)\n",
    "m12_1.fit(observed_data = dict(y =d.admit.astype('float32').values))\n",
    "end = tm.time()\n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 12.4s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 9e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[1] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[2] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[1] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[1] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 8e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[1] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[2] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 7e-06 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: beta_binomial_lpmf: Second prior sample size parameter[1] is 0, but must be positive finite! (in '/tmp/httpstan_bhvvarns/model_aokd6ccs.stan', line 23, column 4 to column 57)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 12.8920 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[12] int N;\n",
    "    array[12] int A;\n",
    "    array[12] int gid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    real<lower=0> phi;\n",
    "}\n",
    "transformed parameters{\n",
    "    real theta;\n",
    "    theta = phi + 2;\n",
    "}\n",
    "model{\n",
    "    vector[12] pbar;\n",
    "    phi ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    for ( i in 1:12 ) {\n",
    "        pbar[i] = a[gid[i]];\n",
    "        pbar[i] = inv_logit(pbar[i]);\n",
    "    }\n",
    "    A ~ beta_binomial( N , pbar*theta , (1-pbar)*theta );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'A' : d[\"admit\"].values.astype(int),\n",
    "    'N' : d[\"applications\"].values.astype(int),\n",
    "    'gid' : d[\"gid\"].values.astype(int) +1,\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Output comparaison (PB estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>phi[0]</th>\n",
       "      <td>-0.820031</td>\n",
       "      <td>1.025307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>0.535661</td>\n",
       "      <td>-0.446708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>0.320808</td>\n",
       "      <td>-0.321521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfp    pystan\n",
       "phi[0]   -0.820031  1.025307\n",
       "alpha[0]  0.535661 -0.446708\n",
       "alpha[1]  0.320808 -0.321521"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m12_1.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['phi'].mean(), df['a.1'].mean(), df['a.2'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Negative-binomial (WIP)\n",
    "### 10.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.main import*\n",
    "#d = pd.read_csv('../data/Kline.csv', sep = ';')\n",
    "#formula = dict(main = 'y ~ BetaBinomial(12,  concentration1 = pbar/phi, concentration0 = g_rate)',\n",
    "#               likelihood1 = 'pbar ~ tf.exp(alpha[gid])*tf.math.pow(beta[gid])/gamma',\n",
    "#               likelihood2 = 'g_rate ~ 1/phi',\n",
    "#               prior1 = 'alpha ~ Normal(1,1)',\n",
    "#               prior2 = 'beta ~ Exponential(1)',\n",
    "#               prior3 = 'gamma ~ Exponential(1)',\n",
    "#               prior4 = 'phi ~ Exponential(1)',\n",
    "#               )\n",
    "#start = tm.time()\n",
    "#m12_1 = model(formula, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "#\n",
    "#tf.random.set_seed(42)\n",
    "#\n",
    "## Define the model\n",
    "#def model():\n",
    "#    m = {}\n",
    "#    m['alpha'] = tfp.distributions.Sample(tfp.distributions.Normal(1, 1), sample_shape=2)\n",
    "#    m['beta'] = tfp.distributions.Sample(tfp.distributions.Exponential(1), sample_shape=2)\n",
    "#    m['gamma'] = tfp.distributions.Sample(tfp.distributions.Exponential(1), sample_shape=1)\n",
    "#    m['phi'] = tfp.distributions.Sample(tfp.distributions.Exponential(1), sample_shape=1)\n",
    "#    \n",
    "#    # Define the distribution 'y' which depends on alpha, beta, gamma, and phi\n",
    "#    def y_dist(phi, alpha, beta, gamma):\n",
    "#        concentration1 = ((tf.exp(alpha) * beta) / gamma) / phi\n",
    "#        concentration0 = 1. / phi\n",
    "#        return tfp.distributions.Independent(\n",
    "#            tfp.distributions.BetaBinomial(12, concentration1=concentration1, concentration0=concentration0),\n",
    "#            reinterpreted_batch_ndims=1)\n",
    "#\n",
    "#    m['y'] = y_dist\n",
    "#    return tfp.distributions.JointDistributionNamedAutoBatched(m)\n",
    "#\n",
    "## Sample from the model\n",
    "#sampled_values = model().sample(alpha=tf.random.normal((2,)), beta=tfp.distributions.Exponential(rate=1.).sample((2,)), gamma=tfp.distributions.Exponential(rate=1.).sample((1,)), phi=tfp.distributions.Exponential(rate=1.).sample((1,)))\n",
    "#\n",
    "#print(sampled_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = d\n",
    "#m = {}\n",
    "#m['alpha']= tfd.Sample(tfd.Normal(1,1, name = 'prior1'), sample_shape = 2)\n",
    "#m['beta']= tfd.Sample(tfd.Exponential(1, name = 'prior2'), sample_shape = 2)\n",
    "#m['gamma']= tfd.Sample(tfd.Exponential(1, name = 'prior3'), sample_shape = 1)\n",
    "#m['phi']= tfd.Sample(tfd.Exponential(1, name = 'prior4'), sample_shape = 1)\n",
    "#m['y'] = lambda phi, alpha, beta, gamma : tfd.Independent(\n",
    "#    tfd.BetaBinomial(12,\n",
    "#                     concentration1=(\n",
    "#                         tf.exp(tf.squeeze(tf.gather(alpha,tf.cast(df.gid.astype('float32').values, dtype=tf.int32), axis = -1)))\n",
    "#                         *( tf.math.powtf.squeeze(tf.gather(beta,tf.cast(df.gid.astype('float32').values, dtype=tf.int32), axis = -1)))\n",
    "#                         /gamma)/phi,\n",
    "#                     concentration0=(1/phi), name ='main'), reinterpreted_batch_ndims=1)\n",
    "#m = tfd.JointDistributionNamedAutoBatched(m)\n",
    "#m.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zero inflated outcomes (PB estimation)\n",
    "### 11.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"JointDistributionNamedAutoBatched/log_prob/add_2:0\", shape=(1,), dtype=float32)\n",
      "BI took: 4.9368 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "# Define parameters\n",
    "prob_drink = 0.2  # 20% of days\n",
    "rate_work = 1     # average 1 manuscript per day\n",
    "\n",
    "# sample one year of production\n",
    "N = 365\n",
    "\n",
    "np.random.seed(365)\n",
    "drink = np.random.binomial(1, prob_drink, N)\n",
    "y = (1 - drink) * np.random.poisson(rate_work, N)\n",
    "d = pd.DataFrame(y)\n",
    "formula = dict(main = 'y ~ ZeroInflatedNegativeBinomial(total_count = 365, inflated_loc_logits = p, logits = AL)',\n",
    "               likelihood = \"p ~ ap\",\n",
    "               likelihood2 = \"AL ~ tf.math.log(al)\",\n",
    "               prior1 = 'ap ~ Normal(-1.5 , 1)',\n",
    "               prior2 = 'al ~ Normal(1,0.5)'\n",
    "               )\n",
    "\n",
    "start = tm.time()\n",
    "m12_3 = model()\n",
    "m12_3 = model(formula)       \n",
    "m12_3.fit(observed_data = dict(y = d.iloc[:,0].astype('float32').values))\n",
    "end = tm.time()\n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 10.2s, done.Sampling:   0%\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 9.5e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 7.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 7.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 9.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 11.5214 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[365] int y;\n",
    "}\n",
    "parameters{\n",
    "    real ap;\n",
    "    real al;\n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    real lambda;\n",
    "    al ~ normal( 1 , 0.5 );\n",
    "    ap ~ normal( -1.5 , 1 );\n",
    "    lambda = al;\n",
    "    lambda = exp(lambda);\n",
    "    p = ap;\n",
    "    p = inv_logit(p);\n",
    "    for ( i in 1:365 ) {\n",
    "        if ( y[i]==0 )\n",
    "            target += log_mix( p , 0 , poisson_lpmf(0|lambda) );\n",
    "        if ( y[i] > 0 )\n",
    "            target += log1m( p ) + poisson_lpmf(y[i] | lambda );\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'y' :d.iloc[:,0].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2. Output comparaison (PB estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap[0]</th>\n",
       "      <td>-1.965017</td>\n",
       "      <td>-1.356490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al[0]</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.109806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfp    pystan\n",
       "ap[0] -1.965017 -1.356490\n",
       "al[0]  0.001720  0.109806"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m12_3.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['ap'].mean(), df['al'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Varying interceps\n",
    "### 12.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 4.8834 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/arviz/data/base.py:221: UserWarning: More chains (2000) than draws (4). Passed array should have shape (chains, draws, *shape)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('../data/reedfrogs.csv', sep = ';')\n",
    "d[\"tank\"] = np.arange(d.shape[0])\n",
    "formula = dict(main = 'y ~ Binomial(total_count = density, logits = p)',\n",
    "               likelihood = 'p ~ alpha[tank]', \n",
    "               prior = 'alpha ~ Normal(a_bar, sigma)',\n",
    "               prior1 = 'a_bar ~ Normal(0.,1.5)',\n",
    "               prior2 = 'sigma ~ Exponential(1)'\n",
    "               )\n",
    "\n",
    "start = tm.time()   \n",
    "m13_2 = model(formula, d, float=32)\n",
    "m13_2.fit(observed_data = dict(y =d.surv.astype('float32').values), num_chains= 4)\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Sampling:   0%\n",
      "Sampling:   0% (1/10000)\n",
      "Sampling:  25% (2501/10000)\n",
      "Sampling:  50% (5001/10000)\n",
      "Sampling:  75% (7501/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.002889 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.89 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.00289 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.9 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/httpstan_ixx0of3a/model_nkw3miai.stan', line 16, column 4 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002913 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 29.13 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000678 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 6.78 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 1.9976 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[48] int N;\n",
    "    array[48] int S;\n",
    "    array[48] int tank;\n",
    "}\n",
    "parameters{\n",
    "    vector[48] a;\n",
    "    real a_bar;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[48] p;\n",
    "    sigma ~ exponential( 1 );\n",
    "    a_bar ~ normal( 0 , 1.5 );\n",
    "    a ~ normal( a_bar , sigma );\n",
    "    for ( i in 1:48 ) {\n",
    "        p[i] = a[tank[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    S ~ binomial( N , p );\n",
    "}\n",
    "generated quantities{\n",
    "    vector[48] log_lik;\n",
    "    vector[48] p;\n",
    "    for ( i in 1:48 ) {\n",
    "        p[i] = a[tank[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    for ( i in 1:48 ) log_lik[i] = binomial_lpmf( S[i] | N[i] , p[i] );    \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'S' : d['surv'].values.astype(int),\n",
    "    'N' : d['density'].values.astype(int),\n",
    "    'tank' : d['tank'].values.astype(int)+1,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma[0]</th>\n",
       "      <td>1.626218</td>\n",
       "      <td>1.620453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_bar[0]</th>\n",
       "      <td>1.351135</td>\n",
       "      <td>1.348769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 0]</th>\n",
       "      <td>2.148302</td>\n",
       "      <td>2.140860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 1]</th>\n",
       "      <td>3.182271</td>\n",
       "      <td>3.077484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 2]</th>\n",
       "      <td>1.007853</td>\n",
       "      <td>0.997670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 3]</th>\n",
       "      <td>3.174410</td>\n",
       "      <td>3.075111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 4]</th>\n",
       "      <td>2.165270</td>\n",
       "      <td>2.146803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 5]</th>\n",
       "      <td>2.177249</td>\n",
       "      <td>2.135257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 6]</th>\n",
       "      <td>3.050784</td>\n",
       "      <td>3.081528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 7]</th>\n",
       "      <td>2.126648</td>\n",
       "      <td>2.135861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 8]</th>\n",
       "      <td>-0.182801</td>\n",
       "      <td>-0.178863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 9]</th>\n",
       "      <td>2.149825</td>\n",
       "      <td>2.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 10]</th>\n",
       "      <td>1.017289</td>\n",
       "      <td>0.998232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 11]</th>\n",
       "      <td>0.587782</td>\n",
       "      <td>0.582025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 12]</th>\n",
       "      <td>1.023714</td>\n",
       "      <td>1.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 13]</th>\n",
       "      <td>0.212295</td>\n",
       "      <td>0.192227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 14]</th>\n",
       "      <td>2.093900</td>\n",
       "      <td>2.131494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 15]</th>\n",
       "      <td>2.170717</td>\n",
       "      <td>2.141905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 16]</th>\n",
       "      <td>2.919388</td>\n",
       "      <td>2.911370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 17]</th>\n",
       "      <td>2.387000</td>\n",
       "      <td>2.399580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 18]</th>\n",
       "      <td>2.006687</td>\n",
       "      <td>2.006390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 19]</th>\n",
       "      <td>3.695935</td>\n",
       "      <td>3.693251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 20]</th>\n",
       "      <td>2.401524</td>\n",
       "      <td>2.400074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 21]</th>\n",
       "      <td>2.427003</td>\n",
       "      <td>2.390040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 22]</th>\n",
       "      <td>2.392018</td>\n",
       "      <td>2.402681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 23]</th>\n",
       "      <td>1.684565</td>\n",
       "      <td>1.700469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 24]</th>\n",
       "      <td>-1.002580</td>\n",
       "      <td>-0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 25]</th>\n",
       "      <td>0.157274</td>\n",
       "      <td>0.166003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 26]</th>\n",
       "      <td>-1.444577</td>\n",
       "      <td>-1.424604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 27]</th>\n",
       "      <td>-0.474610</td>\n",
       "      <td>-0.466732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 28]</th>\n",
       "      <td>0.158350</td>\n",
       "      <td>0.157696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 29]</th>\n",
       "      <td>1.426129</td>\n",
       "      <td>1.444401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 30]</th>\n",
       "      <td>-0.628595</td>\n",
       "      <td>-0.633593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 31]</th>\n",
       "      <td>-0.303776</td>\n",
       "      <td>-0.315833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 32]</th>\n",
       "      <td>3.200550</td>\n",
       "      <td>3.181673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 33]</th>\n",
       "      <td>2.691208</td>\n",
       "      <td>2.713592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 34]</th>\n",
       "      <td>2.711707</td>\n",
       "      <td>2.710875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 35]</th>\n",
       "      <td>2.052027</td>\n",
       "      <td>2.067803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 36]</th>\n",
       "      <td>2.068563</td>\n",
       "      <td>2.060266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 37]</th>\n",
       "      <td>3.860800</td>\n",
       "      <td>3.910408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 38]</th>\n",
       "      <td>2.693505</td>\n",
       "      <td>2.710431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 39]</th>\n",
       "      <td>2.364832</td>\n",
       "      <td>2.344790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 40]</th>\n",
       "      <td>-1.812784</td>\n",
       "      <td>-1.802345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 41]</th>\n",
       "      <td>-0.583196</td>\n",
       "      <td>-0.571043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 42]</th>\n",
       "      <td>-0.446164</td>\n",
       "      <td>-0.447995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 43]</th>\n",
       "      <td>-0.331809</td>\n",
       "      <td>-0.337741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 44]</th>\n",
       "      <td>0.581561</td>\n",
       "      <td>0.579856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 45]</th>\n",
       "      <td>-0.573670</td>\n",
       "      <td>-0.572225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 46]</th>\n",
       "      <td>2.059275</td>\n",
       "      <td>2.062478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0, 47]</th>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.007779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tfp    pystan\n",
       "sigma[0]      1.626218  1.620453\n",
       "a_bar[0]      1.351135  1.348769\n",
       "alpha[0, 0]   2.148302  2.140860\n",
       "alpha[0, 1]   3.182271  3.077484\n",
       "alpha[0, 2]   1.007853  0.997670\n",
       "alpha[0, 3]   3.174410  3.075111\n",
       "alpha[0, 4]   2.165270  2.146803\n",
       "alpha[0, 5]   2.177249  2.135257\n",
       "alpha[0, 6]   3.050784  3.081528\n",
       "alpha[0, 7]   2.126648  2.135861\n",
       "alpha[0, 8]  -0.182801 -0.178863\n",
       "alpha[0, 9]   2.149825  2.165780\n",
       "alpha[0, 10]  1.017289  0.998232\n",
       "alpha[0, 11]  0.587782  0.582025\n",
       "alpha[0, 12]  1.023714  1.002805\n",
       "alpha[0, 13]  0.212295  0.192227\n",
       "alpha[0, 14]  2.093900  2.131494\n",
       "alpha[0, 15]  2.170717  2.141905\n",
       "alpha[0, 16]  2.919388  2.911370\n",
       "alpha[0, 17]  2.387000  2.399580\n",
       "alpha[0, 18]  2.006687  2.006390\n",
       "alpha[0, 19]  3.695935  3.693251\n",
       "alpha[0, 20]  2.401524  2.400074\n",
       "alpha[0, 21]  2.427003  2.390040\n",
       "alpha[0, 22]  2.392018  2.402681\n",
       "alpha[0, 23]  1.684565  1.700469\n",
       "alpha[0, 24] -1.002580 -0.998900\n",
       "alpha[0, 25]  0.157274  0.166003\n",
       "alpha[0, 26] -1.444577 -1.424604\n",
       "alpha[0, 27] -0.474610 -0.466732\n",
       "alpha[0, 28]  0.158350  0.157696\n",
       "alpha[0, 29]  1.426129  1.444401\n",
       "alpha[0, 30] -0.628595 -0.633593\n",
       "alpha[0, 31] -0.303776 -0.315833\n",
       "alpha[0, 32]  3.200550  3.181673\n",
       "alpha[0, 33]  2.691208  2.713592\n",
       "alpha[0, 34]  2.711707  2.710875\n",
       "alpha[0, 35]  2.052027  2.067803\n",
       "alpha[0, 36]  2.068563  2.060266\n",
       "alpha[0, 37]  3.860800  3.910408\n",
       "alpha[0, 38]  2.693505  2.710431\n",
       "alpha[0, 39]  2.364832  2.344790\n",
       "alpha[0, 40] -1.812784 -1.802345\n",
       "alpha[0, 41] -0.583196 -0.571043\n",
       "alpha[0, 42] -0.446164 -0.447995\n",
       "alpha[0, 43] -0.331809 -0.337741\n",
       "alpha[0, 44]  0.581561  0.579856\n",
       "alpha[0, 45] -0.573670 -0.572225\n",
       "alpha[0, 46]  2.059275  2.062478\n",
       "alpha[0, 47]  0.004438  0.007779"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m13_2.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['sigma'].mean(), df['a_bar'].mean(),\n",
    "                   df['a.1'].mean(), df['a.2'].mean(),\n",
    "                   df['a.3'].mean(), df['a.4'].mean(),\n",
    "                   df['a.5'].mean(), df['a.6'].mean(),\n",
    "                   df['a.7'].mean(), df['a.8'].mean(),\n",
    "                   df['a.9'].mean(), df['a.10'].mean(),\n",
    "                   df['a.11'].mean(), df['a.12'].mean(),\n",
    "                   df['a.13'].mean(), df['a.14'].mean(),\n",
    "                   df['a.15'].mean(), df['a.16'].mean(),\n",
    "                   df['a.17'].mean(), df['a.18'].mean(),\n",
    "                   df['a.19'].mean(), df['a.20'].mean(),\n",
    "                   df['a.21'].mean(), df['a.22'].mean(),\n",
    "                   df['a.23'].mean(), df['a.24'].mean(),\n",
    "                   df['a.25'].mean(), df['a.26'].mean(),\n",
    "                   df['a.27'].mean(), df['a.28'].mean(),\n",
    "                   df['a.29'].mean(), df['a.30'].mean(),\n",
    "                   df['a.31'].mean(), df['a.32'].mean(),\n",
    "                   df['a.33'].mean(), df['a.34'].mean(),\n",
    "                   df['a.35'].mean(), df['a.36'].mean(),\n",
    "                   df['a.37'].mean(), df['a.38'].mean(),\n",
    "                   df['a.39'].mean(), df['a.40'].mean(),\n",
    "                   df['a.41'].mean(), df['a.42'].mean(),\n",
    "                   df['a.43'].mean(), df['a.44'].mean(),\n",
    "                   df['a.45'].mean(), df['a.46'].mean(),\n",
    "                   df['a.47'].mean(), df['a.48'].mean()\n",
    "                   ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Varying effects (Building issue)\n",
    "### 13.1. Speed comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import*\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "a = 3.5  # average morning wait time\n",
    "b = -1  # average difference afternoon wait time\n",
    "sigma_a = 1  # std dev in intercepts\n",
    "sigma_b = 0.5  # std dev in slopes\n",
    "rho = -0.7  # correlation between intercepts and slopes\n",
    "Mu = tf.constant([a, b])\n",
    "cov_ab = sigma_a * sigma_b * rho\n",
    "Sigma = tf.constant([[sigma_a ** 2, cov_ab], [cov_ab, sigma_b ** 2]])\n",
    "tf.transpose(tf.reshape(tf.constant([1, 2, 3, 4]), (2, 2)))\n",
    "sigmas = tf.constant([sigma_a, sigma_b])  # standard deviations\n",
    "Rho = tf.constant([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "# now matrix multiply to get covariance matrix\n",
    "Sigma = tf.linalg.tensor_diag(sigmas) @ Rho @ tf.linalg.tensor_diag(sigmas)\n",
    "Sigma\n",
    "N_cafes = 20\n",
    "\n",
    "def build_vary_effects():\n",
    "    _seed = 5\n",
    "    tf.random.set_seed(_seed)\n",
    "\n",
    "    seed = tfp.util.SeedStream(_seed, salt=\"vary_effects\")\n",
    "\n",
    "    Mu = tf.constant([a, b])\n",
    "\n",
    "    vary_effects = tfd.MultivariateNormalTriL(\n",
    "        loc=Mu, scale_tril=tf.linalg.cholesky(Sigma)\n",
    "    ).sample((N_cafes,), seed=seed())\n",
    "\n",
    "    return vary_effects\n",
    "\n",
    "vary_effects = build_vary_effects()\n",
    "a_cafe = vary_effects[:, 0]\n",
    "b_cafe = vary_effects[:, 1]\n",
    "N_visits = 10\n",
    "afternoon = np.tile(np.arange(2), N_visits * N_cafes // 2)\n",
    "cafe_id = np.repeat(np.arange(N_cafes), N_visits)\n",
    "\n",
    "def generate_data_frame():\n",
    "    sigma = 0.5  # std dev within cafes\n",
    "\n",
    "    _seed = 22\n",
    "    tf.random.set_seed(_seed)\n",
    "\n",
    "    seed = tfp.util.SeedStream(_seed, salt=\"generate_data_frame\")\n",
    "\n",
    "    mu = tf.gather(a_cafe, cafe_id) + tf.gather(b_cafe, cafe_id) * afternoon\n",
    "\n",
    "    wait = tfd.Normal(loc=mu, scale=sigma).sample(seed=seed())\n",
    "    d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n",
    "\n",
    "    return d\n",
    "d = generate_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n",
      "BI took: 0.0356 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.main_jax import*\n",
    "formula = dict(main = 'y ~ Normal(mu, sigma)',\n",
    "likelihood1 = 'mu ~ a_cafe_b_cafe[cafe] + a_cafe_b_cafe[cafe]*afternoon',\n",
    "prior0 = 'a_cafe_b_cafe ~ MultivariateNormalTriL(concat([alpha, beta],axis=-1), LinearOperatorDiag(sigma_alpha_beta, Rho))',\n",
    "prior1 = 'sigma ~ Exponential(1)',\n",
    "prior2 = 'sigma_alpha_beta ~ Exponential(1)',\n",
    "prior3 = 'alpha ~ Normal(5,2)',\n",
    "prior4 = 'beta ~ Normal(-1,0.5)',\n",
    "prior5 = 'Rho ~ LKJ(2,2)',\n",
    ")\n",
    "start = tm.time()\n",
    "m14_1 = model(formula, d)\n",
    "#m14_1.fit(observed_data = dict(y = d.wait.astype('float32').values),num_chains=4)\n",
    "end = tm.time()    \n",
    "print(f\"BI took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'likelihood1': {'input': 'mu ~ a_cafe_b_cafe[cafe] + a_cafe_b_cafe[cafe]*afternoon',\n",
       "  'output': 'mu',\n",
       "  'formula': \" jnp.squeeze(jnp.take(a_cafe_b_cafe,jnp.array(df.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))+ jnp.squeeze(jnp.take(a_cafe_b_cafe,jnp.array(df.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))*df.afternoon.astype('float32').values\",\n",
       "  'args': array(['a_cafe_b_cafe', 'afternoon', 'cafe'], dtype='<U13'),\n",
       "  'likelihood(s)': {},\n",
       "  'params': {'args': ['a_cafe_b_cafe', 'afternoon', 'cafe'], 'kwargs': {}},\n",
       "  'distribution(s)': [],\n",
       "  'cat': False,\n",
       "  'catN': 0,\n",
       "  'prior(s)': {}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = model()\n",
    "self.f = formula\n",
    "self.df = d\n",
    "self.get_model_info()\n",
    "self.lks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': \"lambda sigma, a_cafe_b_cafe : tfd.Independent(tfd.Normal( jnp.squeeze(jnp.take(jnp.take(a_cafe_b_cafe, 0, axis=-1),jnp.array(df.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))+ jnp.squeeze(jnp.take(jnp.take(a_cafe_b_cafe, 1, axis=-1),jnp.array(df.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))*df.afternoon.astype('float32').values,sigma, name ='main'), reinterpreted_batch_ndims=1)\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m14_1.main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/.local/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/jax/random_generators.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  minval = minval + np.zeros([1] * final_rank, dtype=dtype)\n",
      "/home/sosa/.local/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/jax/random_generators.py:284: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'>  is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jaxrand.randint(key=seed, shape=shape, minval=minval, maxval=maxval,\n",
      "/home/sosa/.local/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/jax/random_generators.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  minval = minval + np.zeros([1] * final_rank, dtype=dtype)\n",
      "/home/sosa/.local/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/jax/random_generators.py:284: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'>  is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jaxrand.randint(key=seed, shape=shape, minval=minval, maxval=maxval,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sigma_alpha_beta': Array([0.52429634, 0.33663043], dtype=float32),\n",
       " 'beta': Array([-1.3343351], dtype=float32),\n",
       " 'alpha': Array([7.9508276], dtype=float32),\n",
       " 'Rho': Array([[1.        , 0.06318094],\n",
       "        [0.06318094, 1.        ]], dtype=float32),\n",
       " 'a_cafe_b_cafe': Array([[ 8.095885 , -1.8026791],\n",
       "        [ 8.043759 , -1.1409795],\n",
       "        [ 7.1190205, -1.121344 ],\n",
       "        [ 8.538231 , -0.5995009],\n",
       "        [ 7.532484 , -1.1908419],\n",
       "        [ 8.239836 , -1.3412075],\n",
       "        [ 7.8195643, -1.4942721],\n",
       "        [ 7.871003 , -1.345066 ],\n",
       "        [ 7.096632 , -1.8607246],\n",
       "        [ 7.347451 , -1.5011088],\n",
       "        [ 8.312316 , -1.5030688],\n",
       "        [ 7.4287925, -1.610275 ],\n",
       "        [ 7.203971 , -1.1150628],\n",
       "        [ 8.799859 , -1.927177 ],\n",
       "        [ 7.8266544, -1.1223111],\n",
       "        [ 8.071859 , -1.4062372],\n",
       "        [ 8.35783  , -1.5710325],\n",
       "        [ 8.652503 , -1.5101093],\n",
       "        [ 8.059788 , -1.0966455],\n",
       "        [ 7.402422 , -1.9809825]], dtype=float32),\n",
       " 'sigma': Array([0.94829464], dtype=float32),\n",
       " 'y': Array([ 7.387118 ,  6.4044704,  8.470651 ,  6.049599 ,  9.034027 ,\n",
       "         4.3920226,  8.3246   ,  5.0388126,  9.305933 ,  5.9634714,\n",
       "         7.403676 ,  7.8387446,  7.4459453,  6.909276 ,  6.6418853,\n",
       "         8.275425 ,  9.361935 ,  7.6718225,  7.7039175,  5.9826417,\n",
       "         8.058443 ,  6.444084 ,  7.640615 ,  7.246278 ,  7.085793 ,\n",
       "         6.7136607,  7.83029  ,  4.6626005,  7.05677  ,  5.3058944,\n",
       "         9.16476  ,  8.999256 ,  8.576828 ,  7.8190756,  8.786551 ,\n",
       "         7.184396 ,  8.164572 ,  8.2162485,  8.533355 ,  8.131029 ,\n",
       "         7.117388 ,  5.9882226,  7.8648124,  7.382557 ,  6.024157 ,\n",
       "         7.185398 ,  6.405546 ,  6.747477 ,  7.939898 ,  8.517441 ,\n",
       "         5.9337516,  7.8089757,  9.509448 ,  8.951472 ,  8.268957 ,\n",
       "         6.4144816,  7.3561826,  6.709283 ,  7.347853 ,  7.971061 ,\n",
       "         7.885531 ,  6.9043837,  6.6217866,  6.2366843, 10.538338 ,\n",
       "         7.874321 ,  8.392672 ,  6.984454 ,  6.6937914,  5.3873105,\n",
       "         7.9579544,  6.4461365,  9.400722 ,  6.9000483,  8.579777 ,\n",
       "         7.344557 ,  8.094308 ,  5.9152846,  8.546635 ,  7.775434 ,\n",
       "         7.222471 ,  5.621357 ,  7.3558965,  7.24144  ,  7.285395 ,\n",
       "         3.660149 ,  9.504895 ,  5.270458 ,  8.439472 ,  4.6712856,\n",
       "         6.203741 ,  6.0756116,  6.275865 ,  4.9730625,  8.553631 ,\n",
       "         6.3489127,  8.830185 ,  6.0022793,  5.85334  ,  4.3643174,\n",
       "         8.047473 ,  5.8621984,  8.414518 ,  6.55154  , 10.270468 ,\n",
       "         7.867281 ,  8.998433 ,  6.0495234,  6.812228 ,  4.738604 ,\n",
       "         7.961197 ,  5.208285 ,  8.765988 ,  5.382322 ,  7.358949 ,\n",
       "         6.8205714,  7.543161 ,  5.628369 ,  7.0646324,  5.5930357,\n",
       "         8.67243  ,  4.889286 ,  7.9321494,  4.4350843,  6.866518 ,\n",
       "         6.646453 ,  7.5810513,  6.602368 ,  6.5105643,  6.3361053,\n",
       "         7.612161 ,  7.6196537,  7.788706 ,  7.690832 ,  7.7582364,\n",
       "         7.1403484,  9.951156 ,  7.113648 ,  8.124839 ,  8.445543 ,\n",
       "         6.96165  ,  6.0791225,  9.228699 ,  7.7169743,  7.7113338,\n",
       "         6.15059  ,  9.796164 ,  9.022922 ,  7.402767 ,  7.456504 ,\n",
       "         8.977258 ,  5.8993406,  7.326002 ,  7.328138 ,  8.232031 ,\n",
       "         5.3188734,  8.248167 ,  4.1952   ,  8.813782 ,  7.0376787,\n",
       "         9.146606 ,  6.8189964,  7.1282115,  7.2352476,  9.607755 ,\n",
       "         5.536555 ,  9.43594  ,  7.7292886,  9.235161 ,  6.996475 ,\n",
       "         9.753934 ,  6.9181147,  8.435638 ,  8.236721 ,  6.1927776,\n",
       "         6.126155 ,  7.7942104,  5.750973 ,  8.127598 ,  8.401534 ,\n",
       "         6.9262133,  5.6564064, 10.39086  ,  6.703147 ,  7.921782 ,\n",
       "         5.2215796,  6.4902644,  7.632938 ,  7.3546023,  7.1850247,\n",
       "         8.054149 ,  3.4185846,  8.478708 ,  5.1055684,  8.425805 ,\n",
       "         5.338855 ,  8.856872 ,  4.994933 ,  6.6810856,  5.228617 ],      dtype=float32)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {}\n",
    "m['sigma']= tfd.Sample(tfd.Exponential(1, name = 'prior1'), sample_shape = 1)\n",
    "m['sigma_alpha_beta']= tfd.Sample(tfd.Exponential(1, name = 'prior2'), sample_shape = 2)\n",
    "m['alpha']= tfd.Sample(tfd.Normal(5,2, name = 'prior3'), sample_shape = 1)\n",
    "m['beta']= tfd.Sample(tfd.Normal(-1,0.5, name = 'prior4'), sample_shape = 1)\n",
    "m['Rho']= tfd.Sample(tfd.LKJ(int(2),int(2), name = 'prior5'), sample_shape = ())\n",
    "m['a_cafe_b_cafe']= lambda sigma_alpha_beta, alpha, beta, Rho: tfd.Sample(tfd.MultivariateNormalTriL(jnp.concat([alpha,beta],axis=-1),jax_LinearOperatorDiag(sigma_alpha_beta,Rho)), sample_shape = 20)\n",
    "m['y'] = lambda sigma, a_cafe_b_cafe : tfd.Independent(\n",
    "    tfd.Normal( \n",
    "        jnp.squeeze(jnp.take(jnp.take(a_cafe_b_cafe, 0, axis=-1),\n",
    "                             jnp.array(d.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))+ \n",
    "        jnp.squeeze(jnp.take(jnp.take(a_cafe_b_cafe, 1, axis=-1),\n",
    "                             jnp.array(d.cafe.astype('float32').values, dtype=jnp.int32), axis = -1))*\n",
    "                             jnp.array(d.afternoon.astype('float32').values, dtype=jnp.int32),sigma, name ='main'), reinterpreted_batch_ndims=1)\n",
    "\n",
    "M = tfd.JointDistributionNamedAutoBatched(m)\n",
    "init_key, sample_key = random.split(random.PRNGKey(int(r.randint(0, 10000000))))\n",
    "M.sample(seed=jnp.array(init_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Sample 'Sampleprior5' batch_shape=[] event_shape=[2, 2] dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Sample(tfd.LKJ(int(2),int(2), name = 'prior5'), sample_shape = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, set_matplotlib_formats\n",
    "from matplotlib.patches import Ellipse, transforms\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "import numpy as onp\n",
    "import numpyro as numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.diagnostics import effective_sample_size, print_summary\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "numpyro.set_platform(\"cpu\")\n",
    "numpyro.set_host_device_count(4)\n",
    "a = 3.5  # average morning wait time\n",
    "b = -1  # average difference afternoon wait time\n",
    "sigma_a = 1  # std dev in intercepts\n",
    "sigma_b = 0.5  # std dev in slopes\n",
    "rho = -0.7  # correlation between intercepts and slopes\n",
    "Mu = jnp.array([a, b])\n",
    "cov_ab = sigma_a * sigma_b * rho\n",
    "Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n",
    "\n",
    "sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\n",
    "Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "# now matrix multiply to get covariance matrix\n",
    "Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\n",
    "N_cafes = 20\n",
    "seed = random.PRNGKey(5)  # used to replicate example\n",
    "vary_effects = dist.MultivariateNormal(Mu, Sigma).sample(seed, (N_cafes,))\n",
    "a_cafe = vary_effects[:, 0]\n",
    "b_cafe = vary_effects[:, 1]\n",
    "seed = random.PRNGKey(22)\n",
    "N_visits = 10\n",
    "afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\n",
    "cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\n",
    "mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n",
    "sigma = 0.5  # std dev within cafes\n",
    "wait = dist.Normal(mu, sigma).sample(seed)\n",
    "d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling.. :   0%|          | 0/2500 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Running chain 0:   0%|          | 0/2500 [00:02<?, ?it/s]\n",
      "Running chain 0:  10%|█         | 250/2500 [00:02<00:00, 2387.99it/s]\n",
      "\u001b[A\n",
      "\n",
      "Running chain 0:  25%|██▌       | 625/2500 [00:03<00:00, 3090.14it/s]\n",
      "\u001b[A\n",
      "\n",
      "Running chain 0:  45%|████▌     | 1125/2500 [00:03<00:00, 3491.04it/s]\n",
      "\u001b[A\n",
      "\n",
      "Running chain 0:  65%|██████▌   | 1625/2500 [00:03<00:00, 3675.82it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Running chain 0: 100%|██████████| 2500/2500 [00:03<00:00, 705.70it/s] \n",
      "Running chain 1: 100%|██████████| 2500/2500 [00:03<00:00, 705.86it/s] \n",
      "Running chain 2: 100%|██████████| 2500/2500 [00:03<00:00, 706.14it/s] \n",
      "Running chain 3: 100%|██████████| 2500/2500 [00:03<00:00, 706.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 3.8120 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as tm\n",
    "def model(cafe, afternoon, wait):\n",
    "    a = numpyro.sample(\"a\", dist.Normal(5, 2))\n",
    "    b = numpyro.sample(\"b\", dist.Normal(-1, 0.5))\n",
    "    sigma_cafe = numpyro.sample(\"sigma_cafe\", dist.Exponential(1).expand([2]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.Exponential(1))\n",
    "    Rho = numpyro.sample(\"Rho\", dist.LKJ(2, 2))\n",
    "    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n",
    "    a_cafe_b_cafe = numpyro.sample(\n",
    "        \"a_cafe,b_cafe\", dist.MultivariateNormal(jnp.stack([a, b]), cov).expand([20])\n",
    "    )\n",
    "    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n",
    "    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "    numpyro.sample(\"wait\", dist.Normal(mu, sigma), obs=wait)\n",
    "\n",
    "start = tm.time()    \n",
    "m14_1 = MCMC(NUTS(model), num_warmup=500, num_samples=2000, num_chains=4)\n",
    "m14_1.run(random.PRNGKey(0), jnp.array(d.cafe.values), jnp.array(d.afternoon.values), jnp.array(d.wait.values))\n",
    "end = tm.time()    \n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning in '/tmp/httpstan_ybzqyjsi/model_y5f2lzzr.stan', line 18, column 4: It\n",
      "    is suggested to reparameterize your model to replace lkj_corr with\n",
      "    lkj_corr_cholesky, the Cholesky factor variant. lkj_corr tends to run\n",
      "    slower, consume more memory, and has higher risk of numerical errors.\n",
      "Warning: The parameter b_cafe has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Warning: The parameter a_cafe has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Sampling:  25% (2500/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.000104 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 1.04 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 9.4e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.94 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 9.3e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.93 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -1.98592e+08, but Covariance matrix[2,1] = -1.98592e+08 (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 28, column 8 to column 67)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.000103 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 1.03 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in '/tmp/httpstan_q6wb7ab_/model_y5f2lzzr.stan', line 18, column 4 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pystan took: 3.0976 seconds\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    vector[200] wait;\n",
    "    array[200] int afternoon;\n",
    "    array[200] int cafe;\n",
    "}\n",
    "parameters{\n",
    "    vector[20] b_cafe;\n",
    "    vector[20] a_cafe;\n",
    "    real a;\n",
    "    real b;\n",
    "    vector<lower=0>[2] sigma_cafe;\n",
    "    real<lower=0> sigma;\n",
    "    corr_matrix[2] Rho;\n",
    "}\n",
    "model{\n",
    "    vector[200] mu;\n",
    "    Rho ~ lkj_corr( 2 );\n",
    "    sigma ~ exponential( 1 );\n",
    "    sigma_cafe ~ exponential( 1 );\n",
    "    b ~ normal( -1 , 0.5 );    \n",
    "    a ~ normal( 5 , 2 );\n",
    "    {\n",
    "        array[20] vector[2] YY;\n",
    "        vector[2] MU;\n",
    "        MU = [ a , b ]';\n",
    "        for ( j in 1:20 ) YY[j] = [ a_cafe[j] , b_cafe[j] ]';\n",
    "        YY ~ multi_normal( MU , quad_form_diag(Rho , sigma_cafe) );\n",
    "    }\n",
    "    for ( i in 1:200 ) {\n",
    "        mu[i] = a_cafe[cafe[i]] + b_cafe[cafe[i]] * afternoon[i];        \n",
    "    }\n",
    "    \n",
    "    wait ~ normal( mu , sigma );\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'wait' : d['wait'].values.astype(float),\n",
    "    'afternoon' : d['afternoon'].values.astype(int),\n",
    "    'cafe' : d['cafe'].values.astype(int)+1,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=4, num_samples=2000, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2. Output comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfp</th>\n",
       "      <th>pystan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma_alpha_beta[0]</th>\n",
       "      <td>0.069269</td>\n",
       "      <td>1.008199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_alpha_beta[1]</th>\n",
       "      <td>0.093032</td>\n",
       "      <td>0.636203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>-1.046420</td>\n",
       "      <td>-1.046213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>3.681367</td>\n",
       "      <td>3.683057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[0, 0]</th>\n",
       "      <td>19.077908</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[0, 1]</th>\n",
       "      <td>7.663390</td>\n",
       "      <td>-0.638785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 0]</th>\n",
       "      <td>-8.993556</td>\n",
       "      <td>-0.638785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 1]</th>\n",
       "      <td>17.729902</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[0, 0]</th>\n",
       "      <td>3.022518</td>\n",
       "      <td>3.029072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[0, 1]</th>\n",
       "      <td>-0.955991</td>\n",
       "      <td>-0.975012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[1, 0]</th>\n",
       "      <td>3.099597</td>\n",
       "      <td>3.108518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[1, 1]</th>\n",
       "      <td>-0.570741</td>\n",
       "      <td>-0.577019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[2, 0]</th>\n",
       "      <td>5.409749</td>\n",
       "      <td>5.390510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[2, 1]</th>\n",
       "      <td>-1.752041</td>\n",
       "      <td>-1.720160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[3, 0]</th>\n",
       "      <td>3.721581</td>\n",
       "      <td>3.718760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[3, 1]</th>\n",
       "      <td>-1.112246</td>\n",
       "      <td>-1.112538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[4, 0]</th>\n",
       "      <td>3.615035</td>\n",
       "      <td>3.614638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[4, 1]</th>\n",
       "      <td>-0.981677</td>\n",
       "      <td>-0.983274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[5, 0]</th>\n",
       "      <td>4.008890</td>\n",
       "      <td>4.003468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[5, 1]</th>\n",
       "      <td>-1.426968</td>\n",
       "      <td>-1.418964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[6, 0]</th>\n",
       "      <td>2.959016</td>\n",
       "      <td>2.963474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[6, 1]</th>\n",
       "      <td>-1.056301</td>\n",
       "      <td>-1.074891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[7, 0]</th>\n",
       "      <td>3.275664</td>\n",
       "      <td>3.269041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[7, 1]</th>\n",
       "      <td>-1.564070</td>\n",
       "      <td>-1.574821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[8, 0]</th>\n",
       "      <td>4.071586</td>\n",
       "      <td>4.069025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[8, 1]</th>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.493288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[9, 0]</th>\n",
       "      <td>5.373543</td>\n",
       "      <td>5.354284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[9, 1]</th>\n",
       "      <td>-1.563720</td>\n",
       "      <td>-1.524094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[10, 0]</th>\n",
       "      <td>5.433237</td>\n",
       "      <td>5.414587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[10, 1]</th>\n",
       "      <td>-2.224785</td>\n",
       "      <td>-2.188358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[11, 0]</th>\n",
       "      <td>2.826436</td>\n",
       "      <td>2.830057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[11, 1]</th>\n",
       "      <td>-0.737905</td>\n",
       "      <td>-0.748767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[12, 0]</th>\n",
       "      <td>3.206808</td>\n",
       "      <td>3.209968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[12, 1]</th>\n",
       "      <td>-1.334072</td>\n",
       "      <td>-1.343472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[13, 0]</th>\n",
       "      <td>4.481445</td>\n",
       "      <td>4.468323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[13, 1]</th>\n",
       "      <td>-1.647821</td>\n",
       "      <td>-1.626119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[14, 0]</th>\n",
       "      <td>3.661180</td>\n",
       "      <td>3.661574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[14, 1]</th>\n",
       "      <td>-0.757636</td>\n",
       "      <td>-0.754607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[15, 0]</th>\n",
       "      <td>3.618455</td>\n",
       "      <td>3.618095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[15, 1]</th>\n",
       "      <td>-0.811488</td>\n",
       "      <td>-0.808269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[16, 0]</th>\n",
       "      <td>2.567072</td>\n",
       "      <td>2.585427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[16, 1]</th>\n",
       "      <td>0.142840</td>\n",
       "      <td>0.123884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[17, 0]</th>\n",
       "      <td>1.587798</td>\n",
       "      <td>1.609192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[17, 1]</th>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.013442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[18, 0]</th>\n",
       "      <td>4.267727</td>\n",
       "      <td>4.270378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[18, 1]</th>\n",
       "      <td>-1.080690</td>\n",
       "      <td>-1.073312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[19, 0]</th>\n",
       "      <td>3.141394</td>\n",
       "      <td>3.150428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_cafe_b_cafe[19, 1]</th>\n",
       "      <td>-0.974128</td>\n",
       "      <td>-0.988997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma[0]</th>\n",
       "      <td>0.498896</td>\n",
       "      <td>0.498722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            tfp    pystan\n",
       "sigma_alpha_beta[0]    0.069269  1.008199\n",
       "sigma_alpha_beta[1]    0.093032  0.636203\n",
       "beta[0]               -1.046420 -1.046213\n",
       "alpha[0]               3.681367  3.683057\n",
       "Rho[0, 0]             19.077908  1.000000\n",
       "Rho[0, 1]              7.663390 -0.638785\n",
       "Rho[1, 0]             -8.993556 -0.638785\n",
       "Rho[1, 1]             17.729902  1.000000\n",
       "a_cafe_b_cafe[0, 0]    3.022518  3.029072\n",
       "a_cafe_b_cafe[0, 1]   -0.955991 -0.975012\n",
       "a_cafe_b_cafe[1, 0]    3.099597  3.108518\n",
       "a_cafe_b_cafe[1, 1]   -0.570741 -0.577019\n",
       "a_cafe_b_cafe[2, 0]    5.409749  5.390510\n",
       "a_cafe_b_cafe[2, 1]   -1.752041 -1.720160\n",
       "a_cafe_b_cafe[3, 0]    3.721581  3.718760\n",
       "a_cafe_b_cafe[3, 1]   -1.112246 -1.112538\n",
       "a_cafe_b_cafe[4, 0]    3.615035  3.614638\n",
       "a_cafe_b_cafe[4, 1]   -0.981677 -0.983274\n",
       "a_cafe_b_cafe[5, 0]    4.008890  4.003468\n",
       "a_cafe_b_cafe[5, 1]   -1.426968 -1.418964\n",
       "a_cafe_b_cafe[6, 0]    2.959016  2.963474\n",
       "a_cafe_b_cafe[6, 1]   -1.056301 -1.074891\n",
       "a_cafe_b_cafe[7, 0]    3.275664  3.269041\n",
       "a_cafe_b_cafe[7, 1]   -1.564070 -1.574821\n",
       "a_cafe_b_cafe[8, 0]    4.071586  4.069025\n",
       "a_cafe_b_cafe[8, 1]   -0.506928 -0.493288\n",
       "a_cafe_b_cafe[9, 0]    5.373543  5.354284\n",
       "a_cafe_b_cafe[9, 1]   -1.563720 -1.524094\n",
       "a_cafe_b_cafe[10, 0]   5.433237  5.414587\n",
       "a_cafe_b_cafe[10, 1]  -2.224785 -2.188358\n",
       "a_cafe_b_cafe[11, 0]   2.826436  2.830057\n",
       "a_cafe_b_cafe[11, 1]  -0.737905 -0.748767\n",
       "a_cafe_b_cafe[12, 0]   3.206808  3.209968\n",
       "a_cafe_b_cafe[12, 1]  -1.334072 -1.343472\n",
       "a_cafe_b_cafe[13, 0]   4.481445  4.468323\n",
       "a_cafe_b_cafe[13, 1]  -1.647821 -1.626119\n",
       "a_cafe_b_cafe[14, 0]   3.661180  3.661574\n",
       "a_cafe_b_cafe[14, 1]  -0.757636 -0.754607\n",
       "a_cafe_b_cafe[15, 0]   3.618455  3.618095\n",
       "a_cafe_b_cafe[15, 1]  -0.811488 -0.808269\n",
       "a_cafe_b_cafe[16, 0]   2.567072  2.585427\n",
       "a_cafe_b_cafe[16, 1]   0.142840  0.123884\n",
       "a_cafe_b_cafe[17, 0]   1.587798  1.609192\n",
       "a_cafe_b_cafe[17, 1]   0.057151  0.013442\n",
       "a_cafe_b_cafe[18, 0]   4.267727  4.270378\n",
       "a_cafe_b_cafe[18, 1]  -1.080690 -1.073312\n",
       "a_cafe_b_cafe[19, 0]   3.141394  3.150428\n",
       "a_cafe_b_cafe[19, 1]  -0.974128 -0.988997\n",
       "sigma[0]               0.498896  0.498722"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tfp\": m14_1.summary(round_to='none')['mean'],\n",
    "        \"pystan\": [df['sigma_cafe.1'].mean(),\n",
    "                   df['sigma_cafe.2'].mean(),\n",
    "                   df['b'].mean(),df['a'].mean(),\n",
    "                   df['Rho.1.1'].mean(),df['Rho.2.1'].mean(),\n",
    "                   df['Rho.1.2'].mean(),df['Rho.2.2'].mean(),\n",
    "                   df['a_cafe.1'].mean(), df['b_cafe.1'].mean(),\n",
    "                   df['a_cafe.2'].mean(), df['b_cafe.2'].mean(),\n",
    "                   df['a_cafe.3'].mean(), df['b_cafe.3'].mean(),\n",
    "                   df['a_cafe.4'].mean(), df['b_cafe.4'].mean(),\n",
    "                   df['a_cafe.5'].mean(), df['b_cafe.5'].mean(),\n",
    "                   df['a_cafe.6'].mean(), df['b_cafe.6'].mean(),\n",
    "                   df['a_cafe.7'].mean(), df['b_cafe.7'].mean(),\n",
    "                   df['a_cafe.8'].mean(), df['b_cafe.8'].mean(),\n",
    "                   df['a_cafe.9'].mean(), df['b_cafe.9'].mean(),\n",
    "                   df['a_cafe.10'].mean(), df['b_cafe.10'].mean(),\n",
    "                   df['a_cafe.11'].mean(), df['b_cafe.11'].mean(),\n",
    "                   df['a_cafe.12'].mean(), df['b_cafe.12'].mean(),\n",
    "                   df['a_cafe.13'].mean(), df['b_cafe.13'].mean(),\n",
    "                   df['a_cafe.14'].mean(), df['b_cafe.14'].mean(),\n",
    "                   df['a_cafe.15'].mean(), df['b_cafe.15'].mean(),\n",
    "                   df['a_cafe.16'].mean(), df['b_cafe.16'].mean(),\n",
    "                   df['a_cafe.17'].mean(), df['b_cafe.17'].mean(),\n",
    "                   df['a_cafe.18'].mean(), df['b_cafe.18'].mean(),\n",
    "                   df['a_cafe.19'].mean(), df['b_cafe.19'].mean(),\n",
    "                   df['a_cafe.20'].mean(), df['b_cafe.20'].mean(),\n",
    "                   df['sigma'].mean()]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Gaussian process (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main_jax import*\n",
    "tfb = tfp.bijectors\n",
    "# from: https://www.tensorflow.org/probability/examples/Gaussian_Process_Regression_In_TFP?hl=fr\n",
    "def sinusoid(x):\n",
    "  return np.sin(3 * np.pi * x[..., 0])\n",
    "\n",
    "def generate_1d_data(num_training_points, observation_noise_variance):\n",
    "  \"\"\"Generate noisy sinusoidal observations at a random set of points.\n",
    "\n",
    "  Returns:\n",
    "     observation_index_points, observations\n",
    "  \"\"\"\n",
    "  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))\n",
    "  index_points_ = index_points_.astype(np.float64)\n",
    "  # y = f(x) + noise\n",
    "  observations_ = (sinusoid(index_points_) +\n",
    "                   np.random.normal(loc=0,\n",
    "                                    scale=np.sqrt(observation_noise_variance),\n",
    "                                    size=(num_training_points)))\n",
    "  return index_points_, observations_\n",
    "\n",
    "# Generate training data with a known noise level (we'll later try to recover\n",
    "# this value from the data).\n",
    "NUM_TRAINING_POINTS = 100\n",
    "observation_index_points_, observations_ = generate_1d_data(\n",
    "    num_training_points=NUM_TRAINING_POINTS,\n",
    "    observation_noise_variance=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count  32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m formula \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(main \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations ~ GaussianProcess(kernel=tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude, length_scale),index_points=observation_index_points_,observation_noise_variance=observation_noise_variance)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                prior1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_noise_variance ~ LogNormal(0,1)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                prior2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_scale~LogNormal(0,1)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                prior3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamplitude~LogNormal(0,1)\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m                )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m model(formula, d)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BI/src/main_jax.py:127\u001b[0m, in \u001b[0;36mmodel.sample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m     init_key, sample_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;28mint\u001b[39m(r\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10000000\u001b[39m))))\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1205\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[0;32m-> 1205\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:956\u001b[0m, in \u001b[0;36mJointDistribution._call_sample_n\u001b[0;34m(self, sample_shape, seed, value, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_sample_n\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape, seed, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 956\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mallow_partially_specified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/internal/distribution_util.py:1350\u001b[0m, in \u001b[0;36mAppendDocstring.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1350\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:693\u001b[0m, in \u001b[0;36mJointDistribution._sample_n\u001b[0;34m(self, sample_shape, seed, value)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@distribution_util\u001b[39m\u001b[38;5;241m.\u001b[39mAppendDocstring(kwargs_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Tensor`s structured like `type(model)` used to parameterize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    685\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother dependent (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) distribution-making functions. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;66;03m# they're not already cached. This ensures we don't try to pass a stateless\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;66;03m# seed to a stateful sampler, or vice versa.\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_static_distribution_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m   might_have_batch_dims \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    696\u001b[0m       distribution_util\u001b[38;5;241m.\u001b[39mshape_may_be_nontrivial(sample_shape)\n\u001b[1;32m    697\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m might_have_batch_dims:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:361\u001b[0m, in \u001b[0;36mJointDistribution._get_static_distribution_attributes\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_static_distribution_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cached_static_attributes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     flat_list_of_static_attributes \u001b[38;5;241m=\u001b[39m \u001b[43mcallable_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_and_trace_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_static_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_static_attributes \u001b[38;5;241m=\u001b[39m StaticDistributionAttributes(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_list_of_static_attributes))\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_static_attributes\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/internal/callable_util.py:55\u001b[0m, in \u001b[0;36mget_output_spec\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m JAX_MODE:\n\u001b[1;32m     54\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_tensor_spec\u001b[39m(t):\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensorSpec):\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:362\u001b[0m, in \u001b[0;36mJointDistribution._get_static_distribution_attributes.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_static_distribution_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cached_static_attributes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     flat_list_of_static_attributes \u001b[38;5;241m=\u001b[39m callable_util\u001b[38;5;241m.\u001b[39mget_output_spec(\n\u001b[0;32m--> 362\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_and_trace_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_static_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_static_attributes \u001b[38;5;241m=\u001b[39m StaticDistributionAttributes(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_list_of_static_attributes))\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_static_attributes\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:1005\u001b[0m, in \u001b[0;36mJointDistribution._execute_model\u001b[0;34m(self, sample_shape, seed, value, stop_index, sample_and_trace_fn)\u001b[0m\n\u001b[1;32m   1003\u001b[0m   value_at_index \u001b[38;5;241m=\u001b[39m value[index]\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m   next_value, traced_values \u001b[38;5;241m=\u001b[39m \u001b[43msample_and_trace_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m      \u001b[49m\u001b[43mactual_distribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRoot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstateful_sample_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstateless_sample_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstateless_sample_seed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_at_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1012\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected int for argument\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m       TENSOR_SEED_MSG_PREFIX \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1014\u001b[0m           stateful_sample_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution.py:156\u001b[0m, in \u001b[0;36mtrace_static_attributes\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample_shape\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ValueWithTrace(\n\u001b[1;32m    158\u001b[0m     value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    159\u001b[0m     traced\u001b[38;5;241m=\u001b[39mStaticDistributionAttributes(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m         name\u001b[38;5;241m=\u001b[39mget_explicit_name_for_component(dist),\n\u001b[1;32m    165\u001b[0m         reparameterization_type\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mreparameterization_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1205\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[0;32m-> 1205\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1182\u001b[0m, in \u001b[0;36mDistribution._call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m sample_shape \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mconvert_to_shape_tensor(\n\u001b[1;32m   1179\u001b[0m     ps\u001b[38;5;241m.\u001b[39mcast(sample_shape, tf\u001b[38;5;241m.\u001b[39mint32), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1180\u001b[0m sample_shape, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[1;32m   1181\u001b[0m     sample_shape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1182\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m samples \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mreshape(x, ps\u001b[38;5;241m.\u001b[39mconcat([sample_shape, ps\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m1\u001b[39m:]], \u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m   1186\u001b[0m     samples)\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_sample_static_shape(samples, sample_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/independent.py:276\u001b[0m, in \u001b[0;36m_Independent._sample_n\u001b[0;34m(self, n, seed, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_n\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, seed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1205\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[0;32m-> 1205\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1182\u001b[0m, in \u001b[0;36mDistribution._call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m sample_shape \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mconvert_to_shape_tensor(\n\u001b[1;32m   1179\u001b[0m     ps\u001b[38;5;241m.\u001b[39mcast(sample_shape, tf\u001b[38;5;241m.\u001b[39mint32), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1180\u001b[0m sample_shape, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[1;32m   1181\u001b[0m     sample_shape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1182\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m samples \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mreshape(x, ps\u001b[38;5;241m.\u001b[39mconcat([sample_shape, ps\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m1\u001b[39m:]], \u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m   1186\u001b[0m     samples)\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_sample_static_shape(samples, sample_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/gaussian_process.py:616\u001b[0m, in \u001b[0;36mGaussianProcess._sample_n\u001b[0;34m(self, n, seed, index_points)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_n\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, index_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 616\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_marginal_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_points\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample(n, seed\u001b[38;5;241m=\u001b[39mseed)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/gaussian_process.py:419\u001b[0m, in \u001b[0;36mGaussianProcess.get_marginal_distribution\u001b[0;34m(self, index_points)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the marginal of this GP over function values at `index_points`.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m  marginal: a Normal distribution with vector event shape.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_marginal_distribution\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 419\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_marginal_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_points\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/gaussian_process.py:422\u001b[0m, in \u001b[0;36mGaussianProcess._get_marginal_distribution\u001b[0;34m(self, index_points, is_missing)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_marginal_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m, index_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 422\u001b[0m   index_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_index_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m   observation_noise_variance \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    424\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_noise_variance)\n\u001b[1;32m    425\u001b[0m   loc, covariance \u001b[38;5;241m=\u001b[39m stochastic_process_util\u001b[38;5;241m.\u001b[39mget_loc_and_kernel_matrix(\n\u001b[1;32m    426\u001b[0m       kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[1;32m    427\u001b[0m       mean_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mean_fn,\n\u001b[1;32m    428\u001b[0m       observation_noise_variance\u001b[38;5;241m=\u001b[39mobservation_noise_variance,\n\u001b[1;32m    429\u001b[0m       index_points\u001b[38;5;241m=\u001b[39mindex_points,\n\u001b[1;32m    430\u001b[0m       is_missing\u001b[38;5;241m=\u001b[39mis_missing)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/gaussian_process.py:513\u001b[0m, in \u001b[0;36mGaussianProcess._get_index_points\u001b[0;34m(self, index_points)\u001b[0m\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    502\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis GaussianProcess instance was not instantiated with a value for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    503\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_points. One must therefore be provided when calling sample, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man argument and returns a Normal distribution instance, whose KL \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    509\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan be computed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    510\u001b[0m index_points \u001b[38;5;241m=\u001b[39m nest_util\u001b[38;5;241m.\u001b[39mconvert_to_nested_tensor(\n\u001b[1;32m    511\u001b[0m     index_points \u001b[38;5;28;01mif\u001b[39;00m index_points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_points,\n\u001b[1;32m    512\u001b[0m     dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mdtype, allow_packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 513\u001b[0m \u001b[43mstochastic_process_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_nested_index_points\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_points\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/internal/stochastic_process_util.py:107\u001b[0m, in \u001b[0;36mcheck_nested_index_points\u001b[0;34m(kernel, index_points)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_nested_index_points\u001b[39m(kernel, index_points):\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that the example dimensions are the same or broadcastable.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m   num_index_points \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnd\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindex_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m   flat_num_index_points \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(num_index_points)\n\u001b[1;32m    111\u001b[0m   static_non_singleton_num_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    112\u001b[0m       n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m flat_num_index_points \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m n \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/python/internal/backend/jax/nest.py:320\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_util  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m    319\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tree_util\u001b[38;5;241m.\u001b[39mtree_map(func, \u001b[38;5;241m*\u001b[39mstructure)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdm_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/distributions/internal/stochastic_process_util.py:108\u001b[0m, in \u001b[0;36mcheck_nested_index_points.<locals>.<lambda>\u001b[0;34m(x, nd)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_nested_index_points\u001b[39m(kernel, index_points):\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that the example dimensions are the same or broadcastable.\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m   num_index_points \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 108\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m x, nd: tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m),\n\u001b[1;32m    109\u001b[0m       index_points, kernel\u001b[38;5;241m.\u001b[39mfeature_ndims)\n\u001b[1;32m    110\u001b[0m   flat_num_index_points \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(num_index_points)\n\u001b[1;32m    111\u001b[0m   static_non_singleton_num_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    112\u001b[0m       n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m flat_num_index_points \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m n \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "d = pd.DataFrame({'observation_index_points_' : tf.reshape(observation_index_points_, -1)})\n",
    "tfk = tfp.math.psd_kernels\n",
    "formula = dict(main = 'observations ~ GaussianProcess(kernel=tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude, length_scale),index_points=observation_index_points_,observation_noise_variance=observation_noise_variance)',\n",
    "               prior1 = 'observation_noise_variance ~ LogNormal(0,1)',\n",
    "               prior2 = 'length_scale~LogNormal(0,1)',\n",
    "               prior3 = 'amplitude~LogNormal(0,1)', \n",
    "               )\n",
    "self = model(formula, d)\n",
    "self.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation_noise_variance': \"tfd.Sample(tfd.LogNormal(0,1, name = 'prior1'), sample_shape = 1)\",\n",
       " 'length_scale': \"tfd.Sample(tfd.LogNormal(0,1, name = 'prior2'), sample_shape = 1)\",\n",
       " 'amplitude': \"tfd.Sample(tfd.LogNormal(0,1, name = 'prior3'), sample_shape = 1)\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.prior_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': \"lambda observation_noise_variance, length_scale, amplitude : tfd.Independent(tfd.GaussianProcess(kernel=tfp.substrates.jax.math.psd_kernels.ExponentiatedQuadratic(amplitude,length_scale),index_points= df.observation_index_points_.astype('float32').values,observation_noise_variance=observation_noise_variance, name ='main'), reinterpreted_batch_ndims=1)\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import *\n",
    "tfk = tfp.math.psd_kernels\n",
    "m = {}\n",
    "# GP prios \n",
    "m['amplitude'] = tfd.Sample(tfd.LogNormal(loc=0., scale=np.float64(1.)), sample_shape = ())\n",
    "m['length_scale'] =  tfd.Sample(tfd.LogNormal(loc=0., scale=np.float64(1.)), sample_shape = ())\n",
    "m['observation_noise_variance'] =  tfd.Sample(tfd.LogNormal(loc=0., scale=np.float64(1.)), sample_shape =())\n",
    "m['observations'] = lambda amplitude, length_scale, observation_noise_variance: tfd.Independent(\n",
    "      tfd.GaussianProcess(\n",
    "            kernel=tfk.ExponentiatedQuadratic(amplitude, length_scale),\n",
    "            index_points=observation_index_points_,\n",
    "            observation_noise_variance=observation_noise_variance), reinterpreted_batch_ndims = 0)\n",
    "M = tfd.JointDistributionNamed(m)\n",
    "M.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7981781 ,  0.9324094 , -0.14552708, -0.02076466,  0.1640876 ,\n",
       "        0.23517413,  0.9706768 , -0.24371578,  0.10874276,  0.03568196,\n",
       "        0.74822253,  0.48756412, -0.608367  ,  0.6805066 ,  0.2736782 ,\n",
       "        0.05549793, -0.53671634,  0.15181339,  0.19752878,  0.18706848,\n",
       "        0.68407804, -0.81314254, -0.5369563 ,  0.7712367 ,  0.2110344 ,\n",
       "       -0.38154358, -0.45545396,  0.90888685, -0.5600948 ,  0.1258239 ,\n",
       "       -0.45959264,  0.42597035, -0.8258926 ,  0.45723367, -0.872995  ,\n",
       "       -0.6902052 , -0.05995317,  0.28038418, -0.12817024, -0.49528795,\n",
       "       -0.7858174 ,  0.9235012 ,  0.04825642,  0.94129425, -0.00617822,\n",
       "       -0.86958975, -0.76767474, -0.41114312, -0.3429205 ,  0.30462423,\n",
       "        0.07367364,  0.66949457,  0.2569632 ,  0.76482403, -0.29330766,\n",
       "       -0.00441659,  0.92419386, -0.01978096,  0.6818753 , -0.7091186 ,\n",
       "       -0.43787614, -0.63688904,  0.39657643, -0.72317827, -0.21333438,\n",
       "       -0.2617598 ,  0.58282906, -0.5399419 , -0.15715776,  0.89903384,\n",
       "       -0.9386945 , -0.28896677,  0.58279926,  0.48700324,  0.12232066,\n",
       "        0.7270768 , -0.30327687,  0.58331347, -0.8348029 , -0.3506729 ,\n",
       "        0.35906267,  0.9418307 , -0.37134087, -0.68210506, -0.5191389 ,\n",
       "        0.76061916,  0.06558829, -0.23790222, -0.61735654, -0.99897194,\n",
       "        0.7483827 , -0.7338988 ,  0.57755834,  0.60111284, -0.37697953,\n",
       "        0.00183232, -0.6731626 ,  0.7237028 , -0.43050677, -0.98805875],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.observation_index_points_.astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7981781 ,  0.9324094 , -0.14552708, -0.02076466,  0.1640876 ,\n",
       "        0.23517413,  0.9706768 , -0.24371578,  0.10874276,  0.03568196,\n",
       "        0.74822253,  0.48756412, -0.608367  ,  0.6805066 ,  0.2736782 ,\n",
       "        0.05549793, -0.53671634,  0.15181339,  0.19752878,  0.18706848,\n",
       "        0.68407804, -0.81314254, -0.5369563 ,  0.7712367 ,  0.2110344 ,\n",
       "       -0.38154358, -0.45545396,  0.90888685, -0.5600948 ,  0.1258239 ,\n",
       "       -0.45959264,  0.42597035, -0.8258926 ,  0.45723367, -0.872995  ,\n",
       "       -0.6902052 , -0.05995317,  0.28038418, -0.12817024, -0.49528795,\n",
       "       -0.7858174 ,  0.9235012 ,  0.04825642,  0.94129425, -0.00617822,\n",
       "       -0.86958975, -0.76767474, -0.41114312, -0.3429205 ,  0.30462423,\n",
       "        0.07367364,  0.66949457,  0.2569632 ,  0.76482403, -0.29330766,\n",
       "       -0.00441659,  0.92419386, -0.01978096,  0.6818753 , -0.7091186 ,\n",
       "       -0.43787614, -0.63688904,  0.39657643, -0.72317827, -0.21333438,\n",
       "       -0.2617598 ,  0.58282906, -0.5399419 , -0.15715776,  0.89903384,\n",
       "       -0.9386945 , -0.28896677,  0.58279926,  0.48700324,  0.12232066,\n",
       "        0.7270768 , -0.30327687,  0.58331347, -0.8348029 , -0.3506729 ,\n",
       "        0.35906267,  0.9418307 , -0.37134087, -0.68210506, -0.5191389 ,\n",
       "        0.76061916,  0.06558829, -0.23790222, -0.61735654, -0.99897194,\n",
       "        0.7483827 , -0.7338988 ,  0.57755834,  0.60111284, -0.37697953,\n",
       "        0.00183232, -0.6731626 ,  0.7237028 , -0.43050677, -0.98805875],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.observation_index_points_.astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.JointDistributionNamedAutoBatched 'JointDistributionNamedAutoBatched' batch_shape=[] event_shape={amplitude: [1], length_scale: [1], observation_noise_variance: [1], observations: [1, 100]} dtype={amplitude: float32, length_scale: float32, observation_noise_variance: float32, observations: float32}>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.main import*\n",
    "m = {}\n",
    "m['amplitude'] = tfd.Sample(tfd.LogNormal(0,1, name = 'prior3'), sample_shape = 1)\n",
    "m['length_scale'] = tfd.Sample(tfd.LogNormal(0,1, name = 'prior2'), sample_shape = 1)\n",
    "m['observation_noise_variance'] = tfd.Sample(tfd.LogNormal(0,1, name = 'prior1'), sample_shape = 1)\n",
    "m['observations'] = lambda  observation_noise_variance, length_scale, amplitude: tfd.Independent(\n",
    "    tfd.GaussianProcess(\n",
    "        kernel=tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude,length_scale),\n",
    "        index_points= d.observation_index_points_.astype('float32').values,\n",
    "        observation_noise_variance=observation_noise_variance, name ='main'), reinterpreted_batch_ndims=1)\n",
    "m = tfd.JointDistributionNamedAutoBatched(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': \"lambda sigma, alpha : tfd.Independent(tfd.Normal( jnp.squeeze(jnp.take(alpha,jnp.array(df.index_clade.astype('float32').values, dtype=jnp.int32), axis = -1)),sigma, name ='main'), reinterpreted_batch_ndims=1)\"}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m5_9.main_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**TODO**</font>: need to arrays 1:N to n:1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
