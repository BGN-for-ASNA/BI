{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ec6592",
   "metadata": {},
   "source": [
    "\n",
    "## Betweenness Centrality Documentation\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Betweenness Centrality is a measure of centrality in a graph (network) based on shortest paths. For a given node (or edge), it quantifies the number of times that node (or edge) acts as a bridge along the shortest path between two other nodes. It was developed as a general measure of centrality applicable to a wide range of network types. Nodes with high betweenness centrality potentially have considerable influence within a network by virtue of their control over information passing between others. They are often referred to as \"brokers\" or \"gatekeepers\".\n",
    "\n",
    "\n",
    "\n",
    "### 2. Theoretical Description\n",
    "\n",
    "Imagine information flowing through a network, always choosing the shortest possible route. A node has high betweenness centrality if it lies on a large proportion of the shortest paths connecting pairs of other nodes in the network.\n",
    "\n",
    "Consider three nodes: $s$ (source), $t$ (target), and $v$ (an intermediate node). If the shortest path (or one of the shortest paths, if multiple exist) between $s$ and $t$ necessarily passes through $v$, then node $v$ has some control or influence over the interaction between $s$ and $t$. Summing up this \"control\" over all possible pairs of source ($s$) and target ($t$) nodes gives the betweenness centrality of node $v$.\n",
    "\n",
    "Unlike degree centrality (which measures direct connections) or closeness centrality (which measures efficiency in reaching others), betweenness centrality focuses on the node's role in *connecting* other nodes. A node can have low degree and low closeness but still have high betweenness if it serves as a crucial link between different clusters or parts of the network.\n",
    "\n",
    "\n",
    "### 3. What it Evaluates\n",
    "\n",
    "Betweenness Centrality evaluates the extent to which a node (or edge) **controls the flow** between other nodes in the network, assuming that flow occurs along shortest paths. It helps identify:\n",
    "\n",
    "1.  **Bottlenecks:** Nodes whose removal would significantly disrupt communication or flow between other nodes.\n",
    "2.  **Brokers / Gatekeepers:** Nodes that connect disparate groups or individuals.\n",
    "3.  **Critical Connectors:** Nodes that are essential for maintaining the overall connectivity or cohesion of the network.\n",
    "4.  **Influence:** Nodes that might have strategic importance due to their position on communication routes.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Mathematical Formula\n",
    "\n",
    "#### Node Betweenness Centrality\n",
    "\n",
    "The betweenness centrality $C_B(v)$ of a node $v$ is defined as the sum of the fraction of all-pairs shortest paths that pass through $v$:\n",
    "\n",
    "$$\n",
    "C_B(v) = Σ_{s≠v≠t} [ σ_st(v) / σ_st ]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   $s$ and $t$ are nodes in the graph, different from $v$ and from each other.\n",
    "*   $σ_st$ is the total number of shortest paths between node $s$ and node $t$.\n",
    "*   $σ_st(v)$ is the number of those shortest paths that pass *through* node $v$.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "*   The summation is over all ordered pairs $(s, t)$ such that $s$, $t$, and $v$ are distinct.\n",
    "*   If $s = t$, $σ_st = 1$ and $σ_st(v) = 0$.\n",
    "*   If $σ_st = 0$ (i.e., $t$ is not reachable from $s$), the term $σ_st(v) / σ_st$ is considered 0.\n",
    "*   The formula applies to both directed and undirected graphs. For undirected graphs, shortest paths are counted in both directions (e.g., s -> t and t -> s), but the contribution is often divided by 2 in the end or the summation is adjusted.\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "To compare betweenness scores across networks of different sizes, the raw score is often normalized by dividing by the maximum possible betweenness score for a graph of that size.\n",
    "\n",
    "*   **For Undirected Graphs:** The maximum number of pairs of nodes (excluding $v$) is $(N-1)(N-2) / 2$, where $N$ is the number of nodes.\n",
    "    $$\n",
    "    Normalized C_B(v) = C_B(v) / [ (N-1)(N-2) / 2 ]\n",
    "    $$\n",
    "*   **For Directed Graphs:** The maximum number of ordered pairs (excluding $v$) is $(N-1)(N-2)$.\n",
    "    $$\n",
    "    Normalized C_B(v) = C_B(v) / [ (N-1)(N-2) ]\n",
    "    $$\n",
    "Normalized betweenness centrality values range from 0 to 1.\n",
    "\n",
    "#### Edge Betweenness Centrality (Brief Mention)\n",
    "\n",
    "A similar concept exists for edges, measuring the fraction of shortest paths between pairs of nodes that traverse a specific edge. It's often used in community detection algorithms (like Girvan-Newman).\n",
    "\n",
    "\n",
    "\n",
    "### 5. Algorithms Used to Compute It\n",
    "\n",
    "#### Naive Approach (All-Pairs Shortest Paths)\n",
    "\n",
    "1.  Compute the shortest paths (length and number) between all pairs of nodes $(s, t)$ in the graph. This can be done using algorithms like Floyd-Warshall or running Breadth-First Search (BFS) / Dijkstra's algorithm from each node.\n",
    "2.  For each node $v$:\n",
    "    *   Initialize $C_B(v) = 0$.\n",
    "    *   Iterate through all pairs of distinct nodes $(s, t)$ where $s ≠ v$ and $t ≠ v$.\n",
    "    *   Determine $σ_st$ (number of shortest paths from $s$ to $t$).\n",
    "    *   Determine $σ_st(v)$ (number of those shortest paths passing through $v$). This often involves checking the path reconstruction information from the shortest path algorithm.\n",
    "    *   If $σ_st > 0$, add $σ_st(v) / σ_st$ to $C_B(v)$.\n",
    "3.  Normalize if desired.\n",
    "\n",
    "**Complexity:** This approach is computationally expensive. If using BFS from each node on an unweighted graph, finding all shortest paths takes O(N * (N+M)) time (N nodes, M edges). Counting paths and iterating through all triples (s, v, t) adds complexity, often leading to O(N^3) or O(N^2 * M) depending on the specific implementation details. For weighted graphs using Dijkstra, it's even higher.\n",
    "\n",
    "#### Brandes' Algorithm (Standard & Efficient)\n",
    "\n",
    "Proposed by Ulrik Brandes in 2001, this algorithm significantly improves the computation time. It avoids explicitly enumerating all shortest paths. The core idea is to compute the *dependency* of a source node $s$ on a node $v$ for reaching other nodes.\n",
    "\n",
    "**Key Idea:** Run a shortest path algorithm (like BFS for unweighted or Dijkstra for weighted graphs) *starting from each node $s$* as the source. During this process, keep track of the number of shortest paths from $s$ to all other nodes and the predecessors of each node on these shortest paths. Then, use this information in a \"backward\" phase to accumulate the dependencies.\n",
    "\n",
    "**Steps (Conceptual):**\n",
    "\n",
    "1.  Initialize betweenness scores $C_B[v] = 0$ for all nodes $v$.\n",
    "2.  For each node $s$ in the graph:\n",
    "    *   **(Forward Phase):** Perform a BFS/Dijkstra starting from $s$ to compute:\n",
    "        *   $dist[t]$: Shortest distance from $s$ to $t$.\n",
    "        *   $σ[t]$: Number of shortest paths from $s$ to $t$.\n",
    "        *   $P[t]$: List of predecessors of $t$ on shortest paths from $s$.\n",
    "        *   Maintain nodes in order of non-decreasing distance (e.g., using a stack $S$ populated during BFS/Dijkstra).\n",
    "    *   **(Backward Phase - Accumulation):**\n",
    "        *   Initialize a dependency score $δ[t] = 0$ for all $t$.\n",
    "        *   Process nodes $w$ in decreasing order of distance from $s$ (pop from stack $S$).\n",
    "        *   For each predecessor $v$ in $P[w]$:\n",
    "            *   Calculate the contribution $(σ[v] / σ[w]) * (1 + δ[w])$. This represents how much of the dependency flowing through $w$ (which is $1 + δ[w]$: 1 for the path ending at $w$, plus dependencies of nodes further away) originates from paths passing through $v$.\n",
    "            *   Add this contribution to $δ[v]$: $δ[v] += (σ[v] / σ[w]) * (1 + δ[w])$\n",
    "        *   If $w ≠ s$, add the accumulated dependency $δ[w]$ to the final betweenness score of $w$: $C_B[w] += δ[w]$.\n",
    "3.  Finalize $C_B$ scores (e.g., divide by 2 for undirected graphs if pairs were counted twice, normalize if desired).\n",
    "\n",
    "**Complexity:**\n",
    "\n",
    "*   Unweighted graphs (using BFS): **O(NM)**\n",
    "*   Weighted graphs (using Dijkstra with binary heap): **O(NM + N^2 log N)**\n",
    "\n",
    "This is significantly faster than the naive approach for sparse graphs (where M is much smaller than N^2).\n",
    "\n",
    "\n",
    "\n",
    "### 6. Pseudocode (Brandes' Algorithm for Unweighted Graphs)\n",
    "\n",
    "```\n",
    "pseudocode\n",
    "\n",
    "function BetweennessCentrality(Graph G = (V, E)):\n",
    "  // V: set of nodes, E: set of edges\n",
    "  // N: number of nodes = |V|\n",
    "\n",
    "  C_B = array of size N, initialized to 0.0 // Stores the betweenness centrality\n",
    "\n",
    "  for s in V: // Main loop: iterate through each node as source 's'\n",
    "    // --- Initialization for single-source shortest paths from 's' ---\n",
    "    S = empty stack               // Stores nodes in order of traversal\n",
    "    P = map (node -> list of nodes) // Stores predecessors: P[w] = list of predecessors of 'w' on shortest paths from 's'\n",
    "    σ = array of size N, initialized to 0.0 // σ[t] = number of shortest paths from 's' to 't'\n",
    "    d = array of size N, initialized to -1 // d[t] = distance from 's' to 't'\n",
    "\n",
    "    σ[s] = 1.0\n",
    "    d[s] = 0\n",
    "    Q = empty queue               // Queue for BFS\n",
    "    Enqueue s into Q\n",
    "\n",
    "    // --- Forward Phase: BFS ---\n",
    "    while Q is not empty:\n",
    "      v = Dequeue from Q\n",
    "      Push v onto S // Store nodes in order of discovery for backward phase\n",
    "\n",
    "      for each neighbor w of v:\n",
    "        // Path discovery: w found for the first time?\n",
    "        if d[w] < 0:\n",
    "          Enqueue w into Q\n",
    "          d[w] = d[v] + 1\n",
    "\n",
    "        // Path counting: is path through v to w a shortest path?\n",
    "        if d[w] == d[v] + 1:\n",
    "          σ[w] = σ[w] + σ[v]\n",
    "          Append v to P[w] // Add 'v' as a predecessor of 'w'\n",
    "\n",
    "    // --- Backward Phase: Accumulation ---\n",
    "    δ = array of size N, initialized to 0.0 // δ[v] = dependency of 's' on 'v'\n",
    "\n",
    "    while S is not empty:\n",
    "      w = Pop from S\n",
    "      for v in P[w]: // Iterate through predecessors of 'w'\n",
    "        // Accumulate dependency: δ[v] gets contribution from δ[w]\n",
    "        δ[v] = δ[v] + (σ[v] / σ[w]) * (1.0 + δ[w])\n",
    "\n",
    "      // Add dependency score δ[w] to C_B[w], unless w is the source 's'\n",
    "      if w != s:\n",
    "        C_B[w] = C_B[w] + δ[w]\n",
    "\n",
    "  // --- Finalization ---\n",
    "  // For undirected graphs, divide scores by 2 because paths are counted twice (s->t and t->s)\n",
    "  // if G is undirected:\n",
    "  //   for v in V:\n",
    "  //     C_B[v] = C_B[v] / 2.0\n",
    "\n",
    "  // Optional Normalization (example for undirected)\n",
    "  // N = number of nodes\n",
    "  // norm_factor = (N - 1) * (N - 2) / 2.0\n",
    "  // if norm_factor > 0:\n",
    "  //    for v in V:\n",
    "  //        C_B[v] = C_B[v] / norm_factor\n",
    "\n",
    "  return C_B\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Code Example (Python with NetworkX)\n",
    "\n",
    "NetworkX provides a built-in, efficient implementation of Brandes' algorithm.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Create a sample graph ---\n",
    "# Example: A simple path graph with a central node connected to side branches\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]) # Path\n",
    "G.add_edges_from([(2, 5), (2, 6)]) # Node 2 is central\n",
    "\n",
    "# --- Compute Betweenness Centrality ---\n",
    "\n",
    "# Calculate normalized betweenness centrality (values between 0 and 1)\n",
    "# endpoints=False: By default, pairs (s, t) are considered, not including paths starting/ending at v.\n",
    "#                  Set endpoints=True to include paths where v is an endpoint (different definition).\n",
    "betweenness_norm = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "\n",
    "# Calculate non-normalized betweenness centrality\n",
    "betweenness_abs = nx.betweenness_centrality(G, normalized=False, endpoints=False)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"Graph Nodes:\", G.nodes())\n",
    "print(\"\\nNormalized Betweenness Centrality:\")\n",
    "for node, centrality in sorted(betweenness_norm.items()):\n",
    "    print(f\"Node {node}: {centrality:.3f}\")\n",
    "\n",
    "print(\"\\nAbsolute Betweenness Centrality:\")\n",
    "for node, centrality in sorted(betweenness_abs.items()):\n",
    "    print(f\"Node {node}: {centrality:.3f}\")\n",
    "\n",
    "# --- Visualization (Optional) ---\n",
    "pos = nx.spring_layout(G, seed=42) # Positions for all nodes\n",
    "node_sizes = [v * 1500 + 500 for v in betweenness_norm.values()] # Size nodes by centrality\n",
    "node_colors = [betweenness_norm[node] for node in G.nodes()] # Color nodes by centrality\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color=node_colors,\n",
    "        cmap=plt.cm.viridis, font_color='white', font_weight='bold')\n",
    "plt.title(\"Graph with Nodes Sized and Colored by Betweenness Centrality\")\n",
    "plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=max(node_colors))),\n",
    "             label='Normalized Betweenness Centrality')\n",
    "plt.show()\n",
    "\n",
    "# Expected output commentary:\n",
    "# Node 2 should have the highest betweenness centrality because all paths\n",
    "# between {0, 1} and {3, 4, 5, 6}, as well as between 5 and 6, must go through 2.\n",
    "# Nodes 1 and 3 will have lower, but non-zero centrality.\n",
    "# Nodes 0, 4, 5, 6 (endpoints) will have zero betweenness centrality (with endpoints=False).\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 8. Applications\n",
    "\n",
    "Betweenness centrality is used in various fields:\n",
    "\n",
    "1.  **Social Network Analysis:** Identifying influential individuals who bridge different communities or control information flow.\n",
    "2.  **Transportation Networks:** Finding critical intersections or stations whose disruption would cause major delays.\n",
    "3.  **Biological Networks:** Locating essential proteins in protein-protein interaction networks or key genes in regulatory networks.\n",
    "4.  **Communication Networks:** Identifying crucial routers or servers.\n",
    "5.  **Supply Chains:** Finding critical suppliers or distribution hubs.\n",
    "6.  **Community Detection:** Edge betweenness is a core component of the Girvan-Newman algorithm for finding community structures in networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "*   Captures a different aspect of importance than degree or closeness – the \"brokerage\" or \"gatekeeping\" role.\n",
    "*   Identifies nodes critical for network cohesion and flow.\n",
    "*   Useful for understanding vulnerability and resilience in networks.\n",
    "*   Applicable to a wide range of network types (directed/undirected, weighted/unweighted).\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "*   **Computational Cost:** While Brandes' algorithm is efficient (O(NM) or O(NM + N^2 log N)), it can still be computationally intensive for very large graphs (billions of nodes/edges).\n",
    "*   **Assumption of Shortest Paths:** Assumes information/flow *only* travels along shortest paths, which might not be realistic in all scenarios (e.g., resilience might lead to alternative routes, social influence might not follow shortest paths).\n",
    "*   **Sensitivity:** Can be sensitive to small changes in network topology (addition/removal of edges).\n",
    "*   **Interpretation:** High betweenness doesn't *always* mean active control; a node might passively lie on many shortest paths. Context is important.\n",
    "*   **Definition Variations:** Inclusion/exclusion of endpoints ($s$ or $t$ being $v$) can change results, requiring careful specification.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Conclusion\n",
    "\n",
    "Betweenness Centrality is a fundamental and powerful metric in network analysis for quantifying the importance of nodes (or edges) based on their role as intermediaries in the shortest paths connecting other nodes. It provides unique insights into network structure, flow control, and vulnerability, complementing other centrality measures like degree and closeness. While computationally more demanding than simpler metrics, Brandes' algorithm provides an efficient means of calculation, making it practical for many real-world networks. Understanding its definition, calculation, and interpretation is crucial for effectively analyzing network dynamics and identifying key players within complex systems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f74ec8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modularity Documentation\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Modularity (often denoted as $Q$) is a measure of the strength of division of a network into modules (also called groups, clusters, or communities). Networks with high modularity have dense connections between the nodes *within* modules but sparse connections *between* nodes in different modules.\n",
    "\n",
    "It was introduced by Mark Newman and Michelle Girvan in 2004. Modularity is primarily used in two ways:\n",
    "\n",
    "1.  As a **quality function** to evaluate a given partition (division) of a network into communities.\n",
    "2.  As an **objective function** to be optimized by community detection algorithms, aiming to find the partition with the highest possible modularity score.\n",
    "\n",
    "A good partition into communities should correspond to a high modularity value.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Theoretical Description\n",
    "\n",
    "The core idea behind modularity is to compare the number of edges *within* the proposed communities to the number of edges that would be expected if the network were random, but maintained the same degree sequence (i.e., each node has the same number of connections as in the original graph). This random baseline is often referred to as the **configuration model** or **null model**.\n",
    "\n",
    "If the number of within-community edges is significantly higher than what would be expected by random chance, the modularity value will be high, indicating a strong community structure.\n",
    "\n",
    "*   **Positive Modularity (Q > 0):** Indicates that the number of intra-community edges is greater than expected by chance. Values typically range up to around 0.7 or 0.8 for networks with strong community structure, though values above 0.3 are often considered meaningful. Theoretically, the maximum is 1, but this is rarely achieved in real networks.\n",
    "*   **Zero Modularity (Q ≈ 0):** Suggests the number of intra-community edges is roughly what would be expected by chance. The partition is not better than a random assignment based on node degrees.\n",
    "*   **Negative Modularity (Q < 0):** Indicates that there are fewer intra-community edges than expected by chance. This suggests the partition is actively worse than random at capturing community structure (e.g., placing nodes that are strongly connected into different communities). The theoretical minimum is -0.5.\n",
    "\n",
    "\n",
    "\n",
    "### 3. What it Evaluates\n",
    "\n",
    "Modularity specifically evaluates the **quality of a particular partition** of a network into non-overlapping communities. It quantifies how well the chosen division captures the intuitive notion of communities as densely connected subgroups.\n",
    "\n",
    "It **does not** measure a property of a single node (like centrality measures). Instead, it provides a single scalar value for the entire network partition. This value helps answer the question: \"How good is this specific way of dividing the network into communities?\"\n",
    "\n",
    "When used in community detection, algorithms *search* for the partition that yields the highest modularity score, assuming this corresponds to the \"best\" community structure according to this measure.\n",
    "\n",
    "\n",
    "### 4. Mathematical Formula\n",
    "\n",
    "#### Definition and Derivation\n",
    "\n",
    "Modularity $Q$ is defined as:\n",
    "\n",
    "$$\n",
    "Q = (Fraction of edges within communities) - (Expected fraction of edges within communities in the null model)\n",
    "$$\n",
    "\n",
    "Let:\n",
    "*   $N$ be the total number of nodes in the network.\n",
    "*   $m$ be the total number of edges (for undirected graphs, $m = |E|$; for directed graphs, often $m = Σ_i k_i^{in} = Σ_i k_i^{out}$). Let's assume an undirected graph here for simplicity, where $2m = Σ_i k_i$.\n",
    "*   $A_ij$ be the adjacency matrix element (1 if an edge exists between node $i$ and $j$, 0 otherwise). $A_ii = 0$.\n",
    "*   $k_i$ be the degree of node $i$ ($k_i = Σ_j A_ij$).\n",
    "*   $c_i$ be the community assignment of node $i$.\n",
    "*   $δ(c_i, c_j)$ be the Kronecker delta function: $δ(c_i, c_j) = 1$ if $c_i = c_j$ (nodes $i$ and $j$ are in the same community), and $0$ otherwise.\n",
    "\n",
    "The number of edges between nodes $i$ and $j$ in the configuration model (random graph with preserved degrees) is proportional to the product of their degrees. The probability of an edge existing between $i$ and $j$ is $(k_i * k_j) / (2m)$.\n",
    "\n",
    "The modularity $Q$ can be expressed as a sum over all pairs of nodes $(i, j)$:\n",
    "\n",
    "$$\n",
    "Q = (1 / 2m) * Σ_{i=1}^{N} Σ_{j=1}^{N} [ A_ij - (k_i * k_j) / (2m) ] * δ(c_i, c_j)\n",
    "$$\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "*   $A_ij$: Represents the actual presence (1) or absence (0) of an edge between $i$ and $j$.\n",
    "*   $(k_i * k_j) / (2m)$: Represents the expected number of edges between $i$ and $j$ in the configuration model null hypothesis.\n",
    "*   $[ A_ij - (k_i * k_j) / (2m) ]$: This is the crucial term. It measures the difference between the actual edge connection and the expected connection for the pair $(i, j)$. It's positive if an edge exists where it's less expected (contributing positively if $i$ and $j$ are in the same community), negative if an edge is missing where expected, and strongly positive if an edge exists where highly unexpected based on degrees alone.\n",
    "*   $δ(c_i, c_j)$: This ensures that only pairs of nodes $(i, j)$ belonging to the *same* community contribute to the sum. We are summing the difference between actual and expected connections only *within* communities.\n",
    "*   $(1 / 2m)$: Normalization factor, representing the total number of edges (counting each edge twice in the summation, once for $i,j$ and once for $j,i$).\n",
    "\n",
    "#### Alternative Formulations\n",
    "\n",
    "The formula can also be expressed by summing over communities:\n",
    "\n",
    "Let $e_c$ be the fraction of all edges in the network that connect two nodes *within* community $c$.\n",
    "Let $a_c$ be the fraction of all edges in the network that have at least *one* endpoint in community $c$. In the configuration model, the expected fraction of edges within community $c$ is $a_c^2$.\n",
    "\n",
    "$$\n",
    "Q = Σ_c (e_c - a_c^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "*   $e_c = (1 / 2m) * Σ_{i,j in community c} A_ij$\n",
    "*   $a_c = (1 / 2m) * Σ_{i in community c} k_i$\n",
    "\n",
    "This formulation highlights the comparison of within-community edge density ($e_c$) to the expected density ($a_c^2$) for each community $c$, summed over all communities.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Algorithms (Related to Modularity)\n",
    "\n",
    "#### Calculating Modularity for a Given Partition\n",
    "\n",
    "Given a graph $G=(V, E)$ and a partition $C = {C_1, C_2, ..., C_k}$ where each $C_i$ is a set of nodes and $∪ C_i = V$, $C_i ∩ C_j = ∅$ for $i ≠ j$, calculating $Q$ involves:\n",
    "\n",
    "1.  Calculate the total number of edges $m$.\n",
    "2.  Calculate the degree $k_i$ for each node $i$.\n",
    "3.  Initialize $Q = 0$.\n",
    "4.  Iterate through all pairs of nodes $(i, j)$:\n",
    "    *   Check if $i$ and $j$ are in the same community ($c_i == c_j$).\n",
    "    *   If they are, calculate $A_ij - (k_i * k_j) / (2m)$ and add it to a running sum.\n",
    "5.  Divide the final sum by $2m$.\n",
    "\n",
    "**Optimization:** Instead of iterating through all $N^2$ pairs, it's often more efficient to iterate through existing edges $(i, j)$ (contributing $1 - (k_i * k_j) / (2m)$ if $c_i == c_j$) and then subtract the expected contributions for non-edge pairs within the same community, or use the community-summation formula.\n",
    "\n",
    "#### Modularity Maximization Algorithms (Community Detection)\n",
    "\n",
    "Finding the partition that *maximizes* $Q$ is computationally hard (NP-hard). Therefore, heuristic algorithms are used:\n",
    "\n",
    "1.  **Louvain Method (Blondel et al., 2008):** A fast and widely used greedy, hierarchical agglomeration algorithm.\n",
    "    *   *Phase 1:* Assign each node to its own community. Then, for each node, iteratively consider moving it to the community of one of its neighbors if doing so results in the largest positive increase in $Q$. Repeat until no move increases $Q$.\n",
    "    *   *Phase 2:* Build a new network where nodes are the communities found in Phase 1. Edges are weighted by the connections between communities. Repeat Phase 1 on this new network.\n",
    "    *   Continue iterating phases until $Q$ cannot be improved further. Extremely efficient and effective for large networks.\n",
    "2.  **Girvan-Newman Algorithm (Girvan & Newman, 2002):** A divisive (top-down) algorithm based on edge betweenness.\n",
    "    *   Calculate the edge betweenness centrality for all edges.\n",
    "    *   Remove the edge with the highest betweenness.\n",
    "    *   Recalculate edge betweenness and repeat.\n",
    "    *   As edges are removed, the network breaks into components. Calculate the modularity $Q$ for the partition defined by the components at each step of edge removal.\n",
    "    *   The partition corresponding to the maximum $Q$ encountered during the process is chosen. Slower than Louvain, $O(m^2 n)$ or $O(m n^2)$ depending on implementation.\n",
    "3.  **Clauset-Newman-Moore Algorithm (CNM) (Clauset et al., 2004):** A greedy agglomerative (bottom-up) algorithm.\n",
    "    *   Start with each node in its own community.\n",
    "    *   Repeatedly merge the pair of communities whose merging results in the largest increase in $Q$.\n",
    "    *   Uses efficient data structures to speed up finding the best merge. Faster than Girvan-Newman.\n",
    "4.  **Spectral Optimization:** Methods that use the eigenvectors of the \"modularity matrix\" $B$, where $B_ij = A_ij - (k_i k_j) / (2m)$. Finding communities can be related to finding eigenvectors of $B$.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Pseudocode (Calculating Modularity for a Given Partition)\n",
    "\n",
    "```pseudocode\n",
    "function CalculateModularity(Graph G = (V, E), Partition P):\n",
    "  // G: Graph with nodes V and edges E\n",
    "  // P: A map where P[i] gives the community ID for node i\n",
    "  // N: number of nodes = |V|\n",
    "  // M: number of edges = |E|\n",
    "\n",
    "  if M == 0:\n",
    "    return 0.0 // Or handle as appropriate\n",
    "\n",
    "  TotalSum = 0.0\n",
    "  Degrees = array of size N\n",
    "\n",
    "  // --- Precompute degrees ---\n",
    "  for i in V:\n",
    "    Degrees[i] = degree of node i in G\n",
    "\n",
    "  // --- Calculate 2*M ---\n",
    "  TwoM = 0.0\n",
    "  for i in V:\n",
    "    TwoM = TwoM + Degrees[i]\n",
    "  // Note: For undirected graphs, TwoM should equal 2 * M\n",
    "\n",
    "  if TwoM == 0:\n",
    "      return 0.0 // Avoid division by zero if graph has nodes but no edges\n",
    "\n",
    "  // --- Iterate through all pairs of nodes ---\n",
    "  // This is illustrative; edge iteration is often more efficient\n",
    "  for i in V:\n",
    "    for j in V:\n",
    "      if P[i] == P[j]: // Check if nodes are in the same community\n",
    "        // Get Adjacency matrix value (1 if edge exists, 0 otherwise)\n",
    "        A_ij = 1.0 if (i, j) is in E else 0.0\n",
    "\n",
    "        // Calculate expected edges term\n",
    "        ExpectedEdges = (Degrees[i] * Degrees[j]) / TwoM\n",
    "\n",
    "        // Add contribution to sum\n",
    "        TotalSum = TotalSum + (A_ij - ExpectedEdges)\n",
    "\n",
    "  // --- Final Calculation ---\n",
    "  Modularity_Q = TotalSum / TwoM\n",
    "\n",
    "  return Modularity_Q\n",
    "\n",
    "// --- More Efficient Pseudocode (Iterating Edges & Communities) ---\n",
    "function CalculateModularity_Efficient(Graph G = (V, E), Partition P):\n",
    "    M = number of edges\n",
    "    if M == 0: return 0.0\n",
    "\n",
    "    Degrees = compute all node degrees\n",
    "    TwoM = sum of all degrees\n",
    "\n",
    "    if TwoM == 0: return 0.0\n",
    "\n",
    "    Sum_e_c = 0.0 // Sum of internal community edges (weighted by 1/2m)\n",
    "    Sum_a_c_squared = 0.0 // Sum of squares of community degree fractions\n",
    "\n",
    "    Communities = unique community IDs in P\n",
    "    CommunityDegrees = map (community_id -> total degree of nodes in community) initialized to 0\n",
    "\n",
    "    // --- Calculate total degree within each community ---\n",
    "    for i in V:\n",
    "        CommunityDegrees[P[i]] += Degrees[i]\n",
    "\n",
    "    // --- Calculate sum of (a_c)^2 ---\n",
    "    for cid in Communities:\n",
    "        a_c = CommunityDegrees[cid] / TwoM\n",
    "        Sum_a_c_squared += a_c * a_c\n",
    "\n",
    "    // --- Calculate sum of e_c (fraction of within-community edges) ---\n",
    "    EdgesInsideCommunities = 0.0\n",
    "    for each edge (u, v) in E:\n",
    "        if P[u] == P[v]:\n",
    "            EdgesInsideCommunities += 1.0 // Add 1 for each internal edge\n",
    "\n",
    "    Sum_e_c = EdgesInsideCommunities / M // Fraction of edges that are internal\n",
    "\n",
    "    // --- Modularity Q = Σ(e_c - a_c^2) ---\n",
    "    // This alternative form needs careful implementation summing e_c per community.\n",
    "    // The first formula's implementation via edge iteration is common:\n",
    "\n",
    "    Q_via_edges = 0.0\n",
    "    for each edge (u, v) in E:\n",
    "        if P[u] == P[v]:\n",
    "            Q_via_edges += (1.0 - (Degrees[u] * Degrees[v]) / TwoM) // Contribution from existing edge\n",
    "    // Need to subtract contributions from non-edges within same communities...\n",
    "    // The initial Σ[A_ij - k_i*k_j/2m]δ(c_i,c_j) formulation is often easier to implement directly.\n",
    "\n",
    "    // Let's stick to the direct formula implementation (simplified):\n",
    "    Q_direct = 0.0\n",
    "    for u in V:\n",
    "        for v in V:\n",
    "            if P[u] == P[v]:\n",
    "                 A_uv = 1.0 if (u, v) in E else 0.0\n",
    "                 Expected = (Degrees[u] * Degrees[v]) / TwoM\n",
    "                 Q_direct += (A_uv - Expected)\n",
    "\n",
    "    return Q_direct / TwoM // Final normalization\n",
    "```\n",
    "\n",
    "*(Note: The efficient pseudocode example highlights different approaches; implementing the direct formula carefully by iterating over edges is often preferred)*\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Code Example (Python with NetworkX & python-louvain)\n",
    "\n",
    "NetworkX can calculate modularity if you provide a partition. The $community$ library (implementing the Louvain method) can find communities based on modularity maximization.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# You might need to install the community detection library:\n",
    "# pip install python-louvain networkx matplotlib\n",
    "import community as community_louvain # Best practice to import with alias\n",
    "\n",
    "# --- Create a sample graph with clear community structure ---\n",
    "# Using the famous Zachary's Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# --- Detect Communities using Louvain Method (Maximizing Modularity) ---\n",
    "# compute the best partition using Louvain algorithm\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# The 'partition' dictionary maps each node to a community ID (integer)\n",
    "print(\"Detected Partition (Node: Community ID):\")\n",
    "# Sort by node for consistent output\n",
    "for node, comm_id in sorted(partition.items()):\n",
    "    print(f\"Node {node}: {comm_id}\")\n",
    "\n",
    "# --- Calculate Modularity for the detected partition ---\n",
    "modularity_score = community_louvain.modularity(partition, G)\n",
    "print(f\"\\nModularity of the detected partition: {modularity_score:.4f}\")\n",
    "\n",
    "# --- Calculate Modularity for a different (potentially worse) partition ---\n",
    "# Example: Dividing nodes into two arbitrary groups (e.g., even vs odd node IDs)\n",
    "manual_partition = {node: 0 if node % 2 == 0 else 1 for node in G.nodes()}\n",
    "manual_modularity_score = community_louvain.modularity(manual_partition, G)\n",
    "print(f\"Modularity of manual (even/odd) partition: {manual_modularity_score:.4f}\")\n",
    "\n",
    "# --- Visualization (Optional) ---\n",
    "pos = nx.spring_layout(G, seed=42) # Position nodes\n",
    "cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1) # Color map for communities\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=300,\n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_color='white')\n",
    "plt.title(f\"Karate Club Graph Communities (Louvain Method)\\nModularity: {modularity_score:.4f}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# NetworkX also has a built-in modularity function (requires partition as a list of sets)\n",
    "# Convert partition dict to list of sets\n",
    "communities_sets = {}\n",
    "for node, comm_id in partition.items():\n",
    "    if comm_id not in communities_sets:\n",
    "        communities_sets[comm_id] = set()\n",
    "    communities_sets[comm_id].add(node)\n",
    "list_of_community_sets = list(communities_sets.values())\n",
    "\n",
    "nx_modularity = nx.community.modularity(G, list_of_community_sets)\n",
    "print(f\"\\nModularity calculated by NetworkX: {nx_modularity:.4f}\")\n",
    "# Note: Slight differences might occur due to floating point precision or minor implementation details.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 8. Applications\n",
    "\n",
    "1.  **Community Detection:** The most common application, used as an objective function in algorithms like Louvain to find meaningful clusters in networks.\n",
    "2.  **Social Network Analysis:** Identifying social groups, circles, or communities.\n",
    "3.  **Biological Networks:** Finding functional modules in protein-protein interaction networks, gene co-expression networks, or metabolic networks.\n",
    "4.  **Information Networks:** Detecting clusters of related documents, web pages, or citation networks.\n",
    "5.  **Evaluating Clustering Results:** Assessing the quality of a partition obtained by any clustering algorithm (not just modularity-based ones).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "*   **Intuitive Definition:** Based on a clear comparison to a random null model.\n",
    "*   **Single Score:** Provides a single, quantitative measure of partition quality.\n",
    "*   **Widely Used:** A standard benchmark for community detection algorithms.\n",
    "*   **No Free Parameters:** Doesn't require specifying the number of communities beforehand (algorithms find the number that optimizes Q).\n",
    "*   **Effective on Many Networks:** Modularity maximization often finds meaningful communities in real-world networks.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "*   **Resolution Limit:** This is the most significant drawback. Modularity maximization tends to fail to resolve communities smaller than a certain scale, even if they are well-defined. It may merge small, distinct communities into larger ones, especially in large networks.\n",
    "*   **NP-Hard Optimization:** Finding the true maximum modularity partition is computationally infeasible for large networks, requiring reliance on heuristic algorithms (which may find local optima).\n",
    "*   **Degeneracy:** Many different partitions can have similarly high modularity scores, making it hard to identify a single \"best\" partition.\n",
    "*   **Null Model Assumption:** The configuration model might not always be the most appropriate null model for all types of networks or community structures.\n",
    "*   **Assumes Non-Overlapping Communities:** Standard modularity is defined for partitions where each node belongs to exactly one community, which isn't suitable for networks with overlapping communities. (Extensions exist but are more complex).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Conclusion\n",
    "\n",
    "Modularity is a cornerstone concept in network science, particularly for understanding and detecting community structure. It provides a valuable quantitative measure for assessing how well a network is partitioned into dense, well-separated modules compared to random chance. While its use as an objective function in optimization algorithms like Louvain has proven highly effective and popular, it's crucial to be aware of its limitations, especially the resolution limit. Despite its drawbacks, modularity remains a fundamental tool for analyzing the mesoscopic organization of complex networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
