# Negative binomial regression
## General Principles
To model the relationship between a count outcome variable and one or more predictor variables with overdispersion, we can use Negative Binomial Regression. This approach is suitable when the outcome variable follows a negative binomial distribution, allowing for overdispersion compared to a Poisson distribution.

## Formula
We model the relationship between the predictor variables (X1, X2, ..., Xn) and the count outcome variable (Y) using the following equation:

$$logâ¡(ğœ†)= exp(\alpha + \beta ğ‘‹ + ğœ–) $$

Where:

- $\lambda$ is the mean rate parameter of the negative binomial distribution (expected count).
- $X$ is the predictor variables.
- $\beta$ is the regression coefficients.
- $\alpha$ is the intercept term.
- $\log(\lambda)$ is the log of the mean rate parameter, ensuring it is positive.
  
The relationship between the predictor variables and the log of the mean rate parameter is modeled linearly, similar to Poisson regression.

## Considerations
In Bayesian Negative Binomial regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for $W$ and $b$.

Typically, we use a Normal distribution for W1, W2, ..., Wn, and b. We often use informative or weakly informative priors based on prior knowledge or beliefs about the relationships between predictors and the outcome.

<span style="font-size:1em; color : red">
<i>
Additional conciderations : 
</i>
</span>

- We use the _exponential_ link function in Poisson regression because it ensures predicted values are positive, aligning with the nature of count data, and maintains a straightforward interpretation of coefficients.

## Example

```python
from BI import bi.hard
# Data simulation--------------------------------------------------------------------------
num_days = 30
y = tfd.Poisson(rate=1.5).sample(seed = init_key, sample_shape=(num_days,))
num_weeks = 4
y_new = tfd.Poisson(rate=0.5 * 7).sample(seed = init_key, sample_shape=(num_weeks,))
y_all = np.concatenate([y, y_new])
exposure = np.concatenate([np.repeat(1, 30), np.repeat(7, 4)])
monastery = np.concatenate([np.repeat(0, 30), np.repeat(1, 4)])
d = pd.DataFrame.from_dict(dict(y=y_all, days=exposure, monastery=monastery))
d["log_days"] = d.days.pipe(np.log)

# Model-------------------------------------------------------------------------------------
def model():
    alpha = yield normal(1, 0, 1)
    beta = yield normal(1, 0, 1)
    l = jnp.exp(log_days + alpha +  beta * monastery)
    y = yield Independent(Poisson(log_rate = l), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTSdual(model, obs = jnp.array(d.y.values))
```


## Mathematical Details
We can express the Bayesian Negative Binomial regression model using probability distributions as follows:

$$
ğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘)=NegativeBinomial(ğ‘Ÿ=1,ğ‘=11+expâ¡(ğ‘‹âˆ—ğ‘Š+ğ‘))\\
ğ‘(ğ‘Š)=Normal(0,ğ›¼Â²)\\
ğ‘(ğ‘)=Normal(0,ğ›½Â²)\\
$$

Where:

- $p(Y | X, W, b)$ is the likelihood function.
- $p(W)$ and $p(b)$ are the prior distributions for the regression coefficients and intercept.
- $r=1$ represents the number of failures until the experiment stops in the negative binomial distribution.
- $p=\frac{1}{1 + \exp(X * W + b)}$ is the success probability parameter of the negative binomial distribution, modeled as the logistic function of the linear combination of predictors.