# Linear Regression for continuous vairable
## General Principles
To study relationships between two continuous variables (e.g. heigth and weigth), we can use : _Linear regression approach_. Basically, we draw a line that cross the points clouds of the two tested variables. For this we need to have: 1) an intercept $\alpha$ which inform us about the starting point of the line, 2) a coefficient $\beta$ which in inform us about the slope of the line and 3) a error term $\sigma$ which inform us about spread of points between the line. We can interpret the intercept $\alpha$  as the mean for of _Y_ for the smaller value of _X_, the coefficient $\beta$ as how much _Y_ increase for each increment of  _X_, and $\sigma$ as the error arround the prediction. So the coefficient $\beta$ give the strength of the relationship between _X_ and _Y_ and $\sigma$ the amount of error in the model.

![Plot](https://miro.medium.com/v2/resize:fit:786/format:webp/1*WCcaObzvvVzcrg8CBi6iCQ.jpeg)

## Conciderations
- Unlike traditional linear regression, Bayesian linear regression considers uncertainty in the model parameters and provides a full posterior distribution over them. We thus need to decalre prior distribution for  $\alpha$ , $\beta$ and $\sigma^2$ . 

- Ussually, we use _Normal_ distribution for  $\alpha$ , $\beta$ and an exponential distributiuon for $\sigma^2$.

- As we concider that data is standardized (see introduction) we use a distribution of mean 0 and of standard deviation of 1.

- $\sigma$ is assumed to be normally distributed and is squared to force positive error and account for values bellow and above the line.

- Gaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a link function. This simplifies interpretation, as coefficients represent direct changes in the outcome variable.

## Example

Below is an example code snippet demonstrating Bayesian linear regression using Bayesian Inference (BI) package:

```python
from BI import bi.hard
# Import data
d = pd.read_csv('/home/sosa/BI/data/Howell1.csv', sep=';')

# Manipulate and scale data
d = d[d.age > 18]
#self.df["weight.per.g"].pipe(lambda x: (x - x.mean()) / x.std())
d.weight = d.weight - d.weight.mean()
d.age = d.age - d.age.mean()
weight = jnp.array(d.weight.values)

# Define your model
def model():
    sigma = yield uniform(1, 0, 50)
    alpha = yield normal(1, 178, 20)
    beta = yield normal(1, 0, 1)    
    y = yield Independent(Normal(alpha + beta * weight, ssigma))
    
posterior, sample_stats = NUTStrans(model, 
                                    obs = jnp.array(d.height.values), # define the dependent variable 
                                    n_chains = 4) # define number of chains
```

## Mathematical Details
### *Formula*
The following equation allow us to draw a line and is ths one that is most used in statistic clases:
$$
Y = \alpha + \beta * X + \sigma
$$

Where:
- _Y_ is the target variable.
- _X_ is the input variable.
- $\beta$ is the regression coefficient.
- $\alpha$ is the intercept term.
- $\sigma$ is the error term.

### *Bayesian model*
The equivalent version of the Bayesian linear regression model using probability distributions is as follows:
$$
p(Y | X, \alpha , \beta ) = Normal(\alpha + \beta  * X, \sigma^2)
$$

$$
p(\alpha) = Normal(0, 1)
$$

$$
p(\beta) = Normal(0, 1)
$$

$$
p(\sigma^2) = Exponential(1)
$$


Where:
- $p(Y |X, \alpha , \beta)$ is the likelihood function (equivalent of the line equation).
- $p(\beta)$ and $p(\alpha)$ are the prior distributions for the regression coefficients and intercept, respectivelly.
- $σ²$, controll the variance of the likelihood.


## Reference(s)