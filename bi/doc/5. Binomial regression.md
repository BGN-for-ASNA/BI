# Binomial Regression
## General Principles
To model the relationship between a binary outcome variable and one or more predictor variables, we can use Binomial Regression. This approach is suitable when the outcome variable follows a binomial distribution, such as success/failure, yes/no, or 1/0.

## Formula
We model the relationship between the predictor variable ($X$) and the binary outcome variable ($Y$) using the following equation:
$$
logit(ğ‘)=\alpha + \beta ğ‘‹ +ğœ–
$$

Where:

- $p$ is the probability of success (or the probability of the binary outcome being 1).
- $X$, is a predictor variables.
- $\beta$ is the regression coefficients.
- $\alpha$ is the intercept term.
- $\text{logit}(p)$ is the log-odds of success, calculated as the log of the odds ratio of success.
  
The relationship between the predictor variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of success.

##  Considerations
Like for regression on continuous variable(s), we use a Normal distribution for $\beta$ and $b$. Assuming the data is standardized, we use a distribution with a mean of 0 and a standard deviation of 1. We often use an exponential distribution for $\epsilon$ to ensure positive values, which aligns with the requirement that ğœ (the standard deviation) must be positive.

<span style="font-size:1em; color : red">
<i>
Additional conciderations : 
</i>
</span>

- We have the firs link function _logit_. The  _logit_ link function in Bayesian binomial regression converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution. 


## Example
Below is an example code snippet demonstrating Bayesian binomial regression

```python
from BI import bi.hard
d = pd.read_csv('/home/sosa/BI/data/chimpanzees.csv', sep = ';')
d["treatment"] = 1 + d.prosoc_left + 2 * d.condition
d["side"] = d.prosoc_left  # right 0, left 1
d["cond"] = d.condition  # no partner 0, partner 1
d_aggregated = (
    d.groupby(["treatment", "actor", "side", "cond"])["pulled_left"].sum().reset_index()
)
d_aggregated.rename(columns={"pulled_left": "left_pulls"}, inplace=True)
d_aggregated["actor_id"] = d_aggregated["actor"].values - 1

def model():
    a = yield normal(1,  0 , 10)
    y = yield Independent(Binomial(1, logits = a), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTSdual(model, obs = jnp.array(d.pulled_left.values), seed= 151)
```

## Mathematical Details
We can express the Bayesian binomial regression model using probability distributions as follows:

$$ ğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘) = Binomial(ğ‘›=1, ğ‘=sigmoid(ğ‘‹âˆ—ğ‘Š+ğ‘))
$$
$$
ğ‘(ğ‘Š_ğ‘–)=Normal(0,ğ›¼Â²)
$$
$$
ğ‘(ğ‘)=Normal(0,ğ›½Â²)
$$

Where:

- $p(Y | X, W, b)$ is the likelihood function.
- $p(Wi)$ and $p(b)$ are the prior distributions for the regression coefficients and intercept.
- $n=1$ represents the number of trials in the binomial distribution (binary outcome).
- $\text{sigmoid}(X * W + b)$ is the sigmoid function applied to the linear combination of predictors, mapping the log-odds to probabilities.

## Notes

- We can apply multiple variables similarly as [chapter 2](bi/doc/2.%20%Multiple%20Regression%20%for%20%Continuous%20Variables.md).
- We can apply interaction terms  similarly as [chapter 3](bi\doc\3.%20%Interaction%20%between%20%continuous%20%variables.md).
- We can apply  caterogical variables similarly as [chapter 4](bi\doc\4.%20%Categorical%20%variable.md). Below is an example code snippet demonstrating Bayesian binomial regression for caterogical variables :

```python
d = pd.read_csv('/home/sosa/BI/data/chimpanzees.csv', sep = ';')
d.actor = d.actor - 1
d["treatment"] = d.prosoc_left + 2 * d.condition
treatment = jnp.array(d["treatment"], dtype=jnp.int32)
actor = jnp.array(d["actor"] )
n_actor = len(jnp.unique(actor))
n_treatment= len(jnp.unique(treatment))

def model():
    a = yield normal(7, 0, 1.5)
    b = yield normal(4, 0, 0.5)
    p = a[actor] + b[treatment]
    y = yield Independent(Binomial(1, logits = p), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTSdual(model, obs = jnp.array(d.pulled_left.values), seed = 151)
```
