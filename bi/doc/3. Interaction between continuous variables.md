# Interaction Term between Two Continuous Variables
## General Principles
To study relationships between two continuous variables and their interaction effect on a target variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term between the two continuous variables:

## Formula
We model the relationship between the input features (X1 and X2) and the target variable (Y) using the following equation:
$$
ğ‘Œ = \alpha + \beta_1*ğ‘‹_1âˆ—+\beta_2*ğ‘‹_2+\beta_2*ğ‘‹_1âˆ—ğ‘‹_2 + ğœ–
$$

Where:

- $Y$ is the target variable.
- $X_1$ and $X_2$ are the input continuous variables.
- $\beta_1$ and $\beta_2$ are the regression coefficients for $X_1$ and $X_2$, respectively.
- $\beta_{interaction}$ is the regression coefficient for the interaction term $(X_1 * X_2)$.
- $\alpha$ is the intercept term.
- $\epsilon$ is the error term assumed to be normally distributed.

In this context, the interaction term $X_1 * X_2$ captures the joint effect of $X_1$ and $X_2$ on the target variable $Y$.

## Considerations
Like for regression on continuous variable(s), we use a Normal distribution for $W$ and $b$. Assuming the data is standardized, we use a distribution with a mean of 0 and a standard deviation of 1. We often use an exponential distribution for $\epsilon$ to ensure positive values, which aligns with the requirement that ğœ (the standard deviation) must be positive.

<span style="font-size:1em; color : red">
<i>
Additional conciderations : 
</i>
</span>

- Model relationship between Y and R to vary as a function of A. you explicitly model the hypothesis that the slope between Y and R dependsâ€”is conditionalâ€”upon A.

- For continuous interactions, the intercept becomes the grand mean of the outcome variable. This ease of interpretation alone is a good reason to center predictor variables.

- Estimate interpretation is more difficult as estimate of non-interaction terms become expected change in Y when R increases by one unit and A is at its average value and estimate of interaction terms are expected change in the influence of A on Y when increasing R by one unit and expected change in the influence of R on Y when increasing A by one unit.

- [Triptych](bi\doc\3.%20%Interaction%20%between%20%continuous%20%variables.md "Three panels arranged side by side to compare multiple datasets or conditions simultaneously. Each panel often represents a different aspect of the data or a different dataset, allowing for easy comparison and analysis. They are particularly useful for displaying complex relationships or patterns across multiple variables or experimental conditions.") plots are very handy for understanding the impact of interactions.


## Example
Below is an example code snippet demonstrating Bayesian regression with an interaction term between two continuous variables :

```python
from BI import bi.hard
d = pd.read_csv('/home/sosa/BI/data/tulips.csv', sep = ';')
d["blooms_std"] = d.blooms / d.blooms.max()
d["water_cent"] = d.water - d.water.mean()
d["shade_cent"] = d.shade - d.shade.mean()

water_cent = jnp.array(d.water_cent.values)
blooms_std = jnp.array(d.blooms_std.values)
shade_cent = jnp.array(d.shade_cent.values)

def model():
    sigma = yield exponential(1, 1)
    bws = yield normal(1,  0 , 0.25 )
    bs = yield normal(1,  0 , 0.25 )
    bw = yield normal(1,  0 , 0.25 )
    a = yield normal(1,  0.5 , 0.25 )
    mu = a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent
    y = yield tfd.Independent(tfd.Normal(mu, sigma), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTSdual(model, obs = jnp.array(d.blooms_std.values))
```

## Mathematical Details
We can express the Bayesian regression model with an interaction term between two continuous variables using probability distributions as follows:
$$
p(Yâˆ£X_1â€‹ ,X_2â€‹ ,W,b) = \\Normal(X 1â€‹ âˆ—W_1â€‹ +X_2â€‹ âˆ— W_2â€‹ +X_1â€‹ âˆ—X_2â€‹ âˆ—W_{interactionâ€‹} +b,ÏƒÂ² )\\
ğ‘(ğ‘Š_ğ‘–)=Normal(0,ğ›¼2)\\
ğ‘(ğ‘)=Normal(0,ğ›½2)\\
ğ‘(ğœ)=Exponential(ğœ†)\\
$$

Where:

- $p(Y | X_1, X_2, W, b)$ is the likelihood function.
- $p(W_i)$ and $p(b)$ are the prior distributions for the regression coefficients and intercept.
- $p(\sigma)$ is the prior distribution for the standard deviation, ensuring it is positive.
- $\sigma^2$, $\alpha^2$, and $\beta^2$ are hyperparameters controlling the variance of the likelihood and priors.
- $\lambda$ is the rate parameter for the exponential distribution of \sigma.
