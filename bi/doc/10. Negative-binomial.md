# Negative Binomial Regression
General Principles
To model the relationship between a count outcome variable and one or more predictor variables with overdispersion, we can use Negative Binomial Regression. This approach is suitable when the outcome variable follows a negative binomial distribution, allowing for overdispersion compared to a Poisson distribution.

## Formula
We model the relationship between the predictor variables (X1, X2, ..., Xn) and the count outcome variable (Y) using the following equation:
$$
log(ğœ†)=ğ‘‹_1âˆ—ğ‘Š_1+ğ‘‹_2âˆ—ğ‘Š_2+...+ğ‘‹_ğ‘›âˆ—ğ‘Š_ğ‘›+ğ‘
$$

Where:

- $\lambda$ is the mean rate parameter of the negative binomial distribution (expected count).
- $X1, X2, ..., Xn$ are the predictor variables.
- $W1, W2, ..., Wn$ are the regression coefficients.
- $b$ is the intercept term.
- $\log(\lambda)$ is the log of the mean rate parameter, ensuring it is positive.
  
The relationship between the predictor variables and the log of the mean rate parameter is modeled linearly, similar to Poisson regression.

## Considerations
In Bayesian Negative Binomial regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for W1, W2, ..., Wn, and b.

<span style="font-size:1em; color : red">
<i>
Additional conciderations : 
</i>
</span>

- The relationship between the predictor variables and the log of the mean rate parameter is modeled linearly, similar to Poisson regression.


## Example
Below is an example code snippet demonstrating Bayesian Negative Binomial regression:

```python
from BI import bi.hard
d = pd.read_csv('/home/sosa/BI/data/UCBadmit.csv', sep = ';')
d["gid"] = (d["applicant.gender"] != "male").astype(int)
gid = jnp.array(d["gid"].astype('int32').values)
applications = jnp.array(d["applications"].astype('float32').values)
admit = jnp.array(d["admit"].astype('float32').values)

def model():
    phi = yield exponential(1, 1)
    alpha = yield normal(2,0.,1.5)
    theta = phi + 2
    pbar = nn.sigmoid(alpha[gid])
    concentration1 = pbar*theta
    concentration0 = (1 - pbar) * theta
    y = yield Independent(BetaBinomial(applications, concentration1 = concentration1, concentration0 = concentration0), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTS(model, obs = admit)
```

## Mathematical Details
We can express the Bayesian Negative Binomial regression model using probability distributions as follows:
$$
ğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘)=NegativeBinomial(ğ‘Ÿ,p = \frac{1}{1 + \exp(X * W + b)})
$$

$$
p(W_i)=Normal(0,Î±^2 )
$$

$$
p(b)=Normal(0,Î²^2)
$$

Where:

- $p(Y | X, W, b)$ is the likelihood function.
- $p(Wi)$ and $p(b)$ are the prior distributions for the regression coefficients and intercept.
- $r$ represents the shape parameter of the negative binomial distribution.
- $p = \frac{1}{1 + \exp(X * W + b)}$ is the success probability parameter of the negative binomial distribution, modeled as the logistic function of the linear combination of predictors.

