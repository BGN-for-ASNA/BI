# Multiple Regression for Continuous Variables
## General Principles
To study relationships between multiple continuous variables (e.g., height, weight, and age), we can use a Multiple Regression approach. Essentially, we extend the simple linear regression model to include multiple input features:

## Formula
We model the relationship between the input features (X1, X2, ..., Xn) and the target variable (Y) using the following equation:

$$
ğ‘Œ = \alpha + ğ‘‹_1âˆ—ğ‘Š_1 + ğ‘‹_2âˆ—ğ‘Š_2 + ... + ğ‘‹_ğ‘›âˆ—ğ‘Š_ğ‘› + Îµ
$$

Where:

- $Y$ is the target variable.
- $X_1$, $X_2$, ..., $X_n$ are the input variables.
- $W_1$, $W_2$, ..., $W_n$ are the regression coefficients.
- \alpha is the intercept term.
- _Îµ_ is the error term assumed to be normally distributed.

In the context of the model, _b_ is the starting point of the line,  $W_1$, $W_2$, ..., $W_n$ are the slopes of the line for each input variable, and _Îµ_ is the spread of points around the line.

We can interpret b as the mean of $Y$ when all $X$ values are zero. Each $W_i$ represents the increase in $Y$ for a one-unit increase in $X_i$, holding all other variables constant. $Îµ$ represents the model's error, indicating how much the actual values deviate from the predicted values.

## Considerations
Unlike traditional multiple regression, Bayesian multiple regression considers uncertainty in the model parameters and provides a full posterior distribution over them. We need to declare prior distributions for $W_1$, $W_2$, ..., $W_n$, $b$, and $Îµ$.

Typically, we use a Normal distribution for $W_1$, $W_2$, ..., $W_n$, $b$. Assuming the data is standardized, we use a distribution with a mean of 0 and a standard deviation of 1. We often use an exponential distribution for Îµ due to its properties of modeling variability.

## Example
Below is an example code snippet demonstrating Bayesian multiple regression using TensorFlow Probability:


```python
from BI import bi.hard
# Import data
d = pd.read_csv('/home/sosa/BI/data/Howell1.csv', sep=';')

# Manipulate and scale data
d = d[d.age > 18]
#self.df["weight.per.g"].pipe(lambda x: (x - x.mean()) / x.std())
d.weight = d.weight - d.weight.mean()
d.age = d.age - d.age.mean()
weight = jnp.array(d.weight.values)

# Define your model
def model():
    s = yield uniform(1, 0, 50)
    a = yield normal(1, 178, 20)
    b = yield normal(1, 0, 1)    
    b2 = yield normal(1, 0, 1) 
    y = yield Independent(Normal(a+b*weight + b2*age, s))
    
posterior, sample_stats = NUTStrans(model, 
                                    obs = jnp.array(d.height.values), 
                                    n_chains = 4)
```

## Mathematical Details
We can express the Bayesian multiple regression model using probability distributions as follows:


$$
ğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘) = Normal(\sum_i^n  X_i*W_i + b, ÏƒÂ²)
$$

$$
p(W_i) = Normal(0,Î±Â²)
$$

$$
p(b) = Normal(0,Î²Â²)
$$

$$
p(Ïƒ) = Exponential(Î»)
$$

Where:

- $p(Y | X, W, b)$ is the likelihood function.
- $p(Wi)$ and $p(b)$ are the prior distributions for the regression coefficients - and intercept.
- $p(\sigma)$ is the prior distribution for the standard deviation, ensuring - it is positive.
- $\sigma^2$, $\alpha^2$, and $\beta^2$ are hyperparameters controlling the - variance of the likelihood and priors.
- $\lambda$ is the rate parameter for the exponential distribution of $\sigma$.
