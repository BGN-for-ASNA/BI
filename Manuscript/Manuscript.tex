% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{46,52,64}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.46,0.44,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.98,0.15,0.45}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.51,1.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.40,0.85,0.94}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.90,0.86,0.45}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.46,0.44,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.46,0.44,0.37}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.68,0.51,1.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.98,0.15,0.45}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.40,0.85,0.94}{\textit{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.51,1.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.46,0.44,0.37}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.65,0.89,0.18}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.51,1.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.65,0.89,0.18}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.98,0.15,0.45}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.95,0.98,0.55}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.98,0.15,0.45}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.97,0.97,0.95}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.97,0.97,0.95}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.65,0.89,0.18}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.98,0.15,0.45}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.46,0.44,0.37}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.68,0.51,1.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.90,0.86,0.45}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.90,0.86,0.45}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.97,0.97,0.95}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.90,0.86,0.45}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage[noblocks]{authblk}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand\Affilfont{\small}
\usepackage{lineno}
\linenumbers
\usepackage{etoolbox}
\usepackage{mathtools}
\pretolerance=100
\tolerance=200
\emergencystretch=10pt
   %\usepackage{nccmath}
\usepackage{pdflscape}
\usepackage{float} % for the H specifier
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{subcaption}
%\usepackage{txfonts}
%\usepackage{fancybox, graphicx, rotating, booktabs}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
%\usepackage{pgfplotstable}
\newcommand{\sbx}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{array,booktabs}
%\newcolumntype{L}{@{}>{\kern\tabcolsep}l<{\kern\tabcolsep}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{multirow}
%\usepackage{arydshln}
\usepackage{framed}
\usepackage{lipsum}
\usepackage{multicol}
\setlength{\columnsep}{0.6cm}
\usepackage{listings}
%This change code block colors
%\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}
%\definecolor{airforceblue}{rgb}{0.0, 0.53, 0.74}
%\definecolor{asparagus}{rgb}{0.31, 0.47, 0.26}
\lstset{
  language=R,                     % the language of the code
  basicstyle=\ttfamily,
  backgroundcolor=\color{shadecolor},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens       (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{airforceblue},      % keyword style
  commentstyle=\color{gray},   % comment style
  stringstyle=\color{asparagus},      % string literal style
  otherkeywords = {install_github, setup_folders, standardize_photos, build_survey, enter_data, compile_data, calculate_payouts,        check_classification, downsize, pre_process, auto_enter_all, annotate_data, simulate_selfreport_network, make_strand_data,      fit_latent_network_model, summarize_strand_results, data.frame, center, fit_block_plus_social_relations_model,   fit_social_relations_model, strand_caterpillar_plot},
  alsoletter ={_},
  deletekeywords={path, start, stop, ordered, case, colors, order, add, data, mode, distance}
}
%\newcounter{framecnt}
%newenvironment{frameenv}[1]
%    {\begin{figure}[tb]
%    \refstepcounter{framecnt}
%    \begin{shaded}
%    \renewcommand{\theHfigure}{cont.\arabic{framecnt}}
%    \textbf{\centerline{Box \arabic{framecnt} --- #1}}
%         }
%    {\end{shaded}\end{figure}
%}
%\usepackage[switch, modulo]{lineno}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Bayesian Inference library},
  pdfauthor={, , and },
  pdfkeywords={Bayesian Analysis, Python, R, JAX},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Bayesian Inference library}
\author{Sebastian Sosa\textsuperscript{1,*} \and Mary B.
McElreath\textsuperscript{1} \and Cody T. Ross\textsuperscript{1}}
\date{}

\begin{document}
\maketitle


\textsuperscript{1} Department of Human Behaviour, Ecology and Culture,
Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany

\textsuperscript{*} Correspondence:
\href{mailto:s.sosa@live.fr}{Sebastian Sosa
\textless{}s.sosa@live.fr\textgreater{}}

wordcount : 2959

\subsection{Abstract}\label{abstract}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bayesian modeling is a powerful paradigm in modern statistics and
  machine learning, offering a principled framework for inference under
  uncertainty. However, practitioners face significant obstacles,
  including \textbf{interoperability} issues, a persistent
  \textbf{accessibility-flexibility trade-off}, the limitations of
  \textbf{domain-specific limitations}, and challenges in
  \textbf{scalability}.
\item
  \textbf{\emph{Interoperability:}} The landscape of Bayesian software
  is fragmented across programming languages and abstraction levels.
  Newcomers often gravitate towards high-level interfaces (e.g.,
  \emph{brms}) within familiar environments due to their accessibility
  for standard models. However, highgh levels of abstraction frameworks
  can be restrictive, lacking the flexibility needed for custom or
  complex models as research needs evolve.
\item
  \textbf{\emph{Accessibility-Flexibility Trade-off:}} To gain the
  necessary flexibility, researchers must often transition to
  lower-level probabilistic programming languages . This transition
  imposes a steeper learning curve and requires mastery of specific
  modeling languages or complex programming frameworks, hindering
  broader adoption and rapid iteration.
\item
  \textbf{\emph{Domain-Specific Limitations:}} Similar accessibility and
  flexibility trade-offs exist in domain-specific Bayesian packages.
  While providing accessible, pre-packaged models for specific fields,
  customizing or extending these models often requires deep engagement
  with lower-level programming languages or switching tools entirely,
  limiting methodological innovation within those domains.
\item
  \textbf{\emph{Scalability:}} Computational demands remain a
  significant bottleneck, limiting the application of Bayesian methods
  to the large datasets and complex, high-dimensional models prevalent
  in modern research.
\item
  To address these challenges, we introduce \textbf{\emph{Bayesian
  Inference (BI)}}, a new Bayesian modeling software available in both
  Python and R. It aims to unify the modeling experience by integrating
  an intuitive model-building syntax (enhancing \textbf{accessibility})
  with the \textbf{flexibility} of low-level abstraction coding
  available but also pre-build function for high-level of abstraction
  and including hardware-accelerated computation via JAX for improved
  \textbf{scalability}. Its availability in both major data science
  languages directly tackles the \textbf{interoperability} barrier and
  the prebuild function for specialized model in network analysis,
  survial models and phylogenetic analysis allow to improved
  \textbf{domain-specific limitations}.
\end{enumerate}

\subsection{Introduction}\label{introduction}

Bayesian modeling has emerged as a vital tool in modern statistics and
machine learning, providing a framework for robust inference under
uncertainty and the possibility to integrate prior knowledge. Despite
its potential, the practical application of Bayesian methods is often
hindered by significant hurdles within the current software ecosystem,
preventing researchers from fully leveraging its capabilities. Key
challenges stem from the fragmented nature of software across different
programming languages (\textbf{interoperability}), gaps between
theoretical understanding and practical implementation
(\textbf{accessibility}), complexities in model specification that force
trade-offs between ease-of-use and flexibility (
\textbf{accessibility-flexibility trade-off}), the constraints of overly
specialized tools (\textbf{domain-specific limitations}), and persistent
computational scalability limitations for complex models or large
datasets (\textbf{scalability}).

The first major obstacle is the fragmented landscape of Bayesian
software, scattered across different programming languages and varying
levels of abstraction, posing significant \textbf{interoperability}
challenges. Researchers frequently encounter a disparate collection of
tools---from \emph{Stan}'s domain-specific language (DSL) to distinct
low-level of abstraction libraries (like \emph{PyMC} (Salvatier, Wiecki,
and Fonnesbeck 2016), \emph{TensorFlow Probability (TFP)} (Abadi et al.
2015), \emph{NumPyro} (Phan, Pradhan, and Jankowiak 2019)) and
high-level of abstraction libraries (like \emph{BRMS}). This
fragmentation complicates workflows and presents a confusing landscape,
especially for researchers new to Bayesian analysis. For instance,
researchers new to Bayesian analysis may initially gravitate towards
tools of high-level of abstraction available within their most familiar
programming environment (e.g., \emph{BRMS} (Bürkner 2017) in \emph{R}
(Wickham 2015)), potentially overlooking more suitable options elsewhere
due to the steep initial learning curve or perceived incompatibility
(accessibility). This linguistic and platform diversity imposes
considerable cognitive overhead, potentially hindering the adoption of
the most suitable tool for a given problem due to familiarity biases or
the friction of switching ecosystems, ultimately impacting the effective
application of Bayesian methods. This initial hurdle of navigating
disparate systems naturally leads new practitioners to prioritize tools
that appear easiest to learn, raising concerns about the balance between
accessibility and the flexibility needed for complex research.

Compounding this fragmentation is the challenge of accessibility and the
translation of theoretical knowledge into practice
\textbf{accessibility-flexibility trade-off}. Indeed, while high-level
interfaces like \emph{BRMS} offer an intuitive formula-based syntax,
significantly lowering the initial barrier to entry (e.g.~generalized
linear mixed models using \emph{BRMS}), this accessibility often comes
at the cost of flexibility. As research questions become more
sophisticated, requiring custom likelihood functions (e.g., multiple
likelihoods), intricate prior structures (e.g., XXX), or non-standard
model components (e.g., centered-random factors), the limitations of
these high-level wrappers become apparent. To gain the necessary
expressive power, the researcher must typically transition to
lower-level probabilistic programming languages (PPLs) such as
\emph{Stan} (Stan Development Team) (requiring mastery of its specific
DSL), \emph{PyMC}, \emph{NumPyro}, or \emph{TFP}. This transition
imposes a much steeper learning curve, demanding a deeper understanding
of probabilistic programming concepts (like computational graphs or
tensor manipulation) and often more verbose code. This significant jump
in complexity can deter users, divert focus from statistical modeling to
software engineering challenges, and ultimately slow down the pace of
research, particularly when trying to adapt models within specific
scientific fields.

Similar accessibility and flexibility constraints manifest as
\textbf{domain-specific limitations} within specialized Bayesian
packages. Fields like phylogenetics or network analysis benefit from
tools such as \emph{BEAST} (Bouckaert 2019), \emph{RevBayes} (Höhna et
al. 2016), \emph{STRAND} (Ross, McElreath, and Redhead 2024), or
\emph{BISON} (Hart et al. 2023), which provide accessible, pre-packaged
models tailored to common domain problems. A phylogeneticist might
initially find \emph{BEAST} convenient for standard molecular clock
models. However, when they wish to incorporate a novel evolutionary
hypothesis requiring modification of the core model structure or
integrate data types not originally envisioned by the developers, they
often encounter rigid constraints. Extending these specialized tools
frequently requires deep engagement with their underlying, often
complex, codebase (sometimes necessitating proficiency in languages like
Java or C++) or abandoning the domain-specific tool entirely in favor of
a general-purpose PPL. This forces researchers to either compromise on
their methodological innovation or undertake a significant software
development effort, potentially switching programming ecosystems and
losing the initial convenience, thereby limiting the evolution of
modeling practices within specialized domains. Even when model
specification is achievable, either in general or specialized tools, the
computational feasibility remains a major concern.

Finally, computational \textbf{scalability} continues to be a
significant bottleneck, limiting the application of Bayesian methods to
the large datasets (e.g., millions of observations) and complex,
high-dimensional models (e.g., thousands of parameters) prevalent in
modern research across fields like genomics, neuroscience, and machine
learning. While established tools like \emph{Stan} feature highly
optimized inference algorithms (particularly its NUTS sampler) and offer
effective multi-core parallelization, they can still face challenges
with long C++ compilation times for complex models and may require
substantial code restructuring or external tooling to efficiently
leverage hardware accelerators like GPUs or TPUs for certain
computations. Conversely, emerging frameworks built on \emph{JAX}
(Bradbury et al. 2018) (powering \emph{NumPyro} and parts of \emph{TFP})
promise substantial speedups via automatic differentiation, JIT
compilation, and native support for parallel hardware architectures.
However, integrating these powerful backends seamlessly into
user-friendly, flexible modeling front-ends that don't require deep
expertise in the JAX ecosystem itself is an ongoing challenge.
Domain-specific tools often inherit the scalability limitations of the
frameworks they are built upon, failing to provide a universally
efficient solution across different model types and data sizes.

Therefore, there is an evident and pressing demand for a Bayesian
modeling framework that synergistically addresses these interconnected
limitations. To address these interconnected challenges, we introduce
\textbf{\emph{Bayesian Inference (BI)}}, a new Bayesian modeling
software designed to unify the modeling experience across the two
dominant data science languages, Python and R. \emph{BI} tackles the
\textbf{interoperability} barrier head-on by offering native interfaces
in both environments. It aims to resolve the
\textbf{accessibility-flexibility trade-off} by providing an intuitive
model-building syntax familiar to users of statistical modeling
languages, while enabling advanced customization and leveraging
multiple, interchangeable inference backends for flexibility. To combat
\textbf{domain-specific limitations}, \emph{BI} includes pre-built
functions and structures tailored for specialized models in areas like
network analysis, survival analysis, and phylogenetic analysis, while
still allowing extension and modification within its general framework.
Crucially, \emph{BI} enhances \textbf{scalability} by integrating with
hardware-accelerated computation via \emph{JAX} (using \emph{NumPyro} or
\emph{TFP} as backends), enabling efficient execution on CPUs, GPUs, and
TPUs. By providing a streamlined, efficient, and unified environment for
the end-to-end Bayesian workflow---from model specification and fitting
to diagnostics and prediction---\textbf{\emph{BI}} lowers the barrier to
entry for sophisticated Bayesian modeling, aiming to empower a broader
community of researchers across disciplines to confidently apply
advanced Bayesian methods to their complex research problems.

\subsection{Software Presentation}\label{software-presentation}

\emph{BI} directly confronts the \textbf{interoperability} challenge by
offering native, feature-equivalent implementations in both Python and
R. While minor syntactic differences exist to adhere to the idiomatic
conventions of each language, the core model specification syntax, the
procedural workflow for analysis, and the underlying computational
engines remain fundamentally consistent. For instance, Python utilizes
dot notation for method calls on class objects (e.g.,
\texttt{bi.dist.normal(0,1)}), while R employs dollar sign notation for
accessing elements or methods within its object system (e.g.,
\texttt{bi\$dist\$normal(0,1)}). This dual-language availability
significantly lowers the adoption barrier for researchers, allowing them
to work entirely within their preferred programming environment without
sacrificing access to a common, powerful Bayesian modeling framework.

\emph{BI} is designed to navigate the critical
\emph{accessibility-flexibility trade-off} by providing multiple layers
of abstraction and utility, catering effectively to users with varying
levels of Bayesian modeling expertise and diverse complexity
requirements throuhg : simplified backend interaction via intuitive
syntax, pre-built components for complex model features , addressing
domain-specific limitations within a general framework, integrated
End-to-End Workflow and extensive model library and documentation.

At its computational core, \emph{BI} leverages the power and efficiency
of established Probabilistic Programming Languages (PPLs) like
\emph{NumPyro} and \emph{TFP}, both of which are built upon the
\emph{JAX} framework for high-performance numerical computation and
automatic differentiation. However, \emph{BI} deliberately abstracts
away much of the inherent complexity of these lower-level tools
(\textbf{Code block 1}). This significantly enhances
\textbf{accessibility} for a broader range of users.

\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 1: }} \textit{Prior specification differences between NumPyro, TFP, and BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{-10pt}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# NumPyro prior specification}
\NormalTok{numpyro.sample(}\StringTok{"mu"}\NormalTok{, dist.Normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)).expand([}\DecValTok{10}\NormalTok{])}

\CommentTok{\# TFP prior specification (within a JointDistributionCoroutine)}
\ControlFlowTok{yield}\NormalTok{ Root(tfd.Sample(tfd.Normal(loc}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{), sample\_shape}\OperatorTok{=}\DecValTok{10}\NormalTok{))}

\CommentTok{\# BI prior specification}
\NormalTok{bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"mu"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{10}\NormalTok{,))}
\end{Highlighting}
\end{Shaded}

To enhance \textbf{flexibility} without unduly sacrificing the
accessibility provided by the high-level syntax, \emph{BI} includes a
library of pre-built, computationally optimized functions implemented
directly in \emph{JAX} (e.g., \textbf{Code block 2}). These components
encapsulate common but potentially complex modeling structures, allowing
users to incorporate them easily within the model specification. Key
examples include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Centered Random Effects} and \emph{Non-Centered Random Effects}
  for hierarchical (multi-level) model components (McElreath 2018). The
  non-centered parameterization, often crucial for efficient sampling in
  hierarchical models (particularly with sparse data), is provided
  without requiring the user to manually implement the
  reparameterization logic.
\item
  \emph{Kernels for Gaussian Processes} for modeling spatial, temporal,
  phylogenetic, or other forms of structured correlation or dependency.
\item
  \emph{Block Model Effects} for implementing stochastic block models in
  network analysis.
\item
  \emph{SRM effects} for modeling pairwise interactions in networks
  while accounting for sender effects, receiver effects, dyadic effects,
  nodal predictors, dyadic predictors, and observation biases (Sosa et
  al., n.d.).
\item
  \emph{Network-Based Diffusion Approach (NBDA)} components for modeling
  the effect of network edges on the rates of transmission of phenomena
  (e.g., behavioral, epidemiological) while accounting for nodal or
  dyadic covariates.
\item
  \emph{Network metrics} ranging from nodal, dyadic, and global network
  measures with a total of 11 that can be used to build custom models of
  social network analysis (Sosa, Sueur, and Puga-Gonzalez 2020).
\end{enumerate}

These pre-built \emph{JAX} functions provide tailored model components
for common patterns in specific fields, while keeping them fully
integrated within the general, extensible modeling framework. By
providing these optimized building blocks within its general syntax,
\emph{BI} allows researchers in these fields to rapidly implement
standard domain models using familiar concepts. Crucially, however,
users retain the full flexibility of the \emph{BI} framework to combine
these domain-specific components with other model features (e.g.,
complex non-linear effects via splines, hierarchical structures across
groups of networks or phylogenies) or to customize or extend them using
\emph{BI}'s underlying mechanisms if needed---a capability often missing
in more narrowly focused domain-specific packages. This design aims to
foster methodological innovation \emph{within} specialized domains by
lowering the barrier to implementing more complex or novel models
{[}link to latex block{]}.

\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 2: }} \textit{Random effect specification differences between NumPyro, TFP, and BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{-10pt}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Numpyro version of rendom centered effect}
\NormalTok{a }\OperatorTok{=}\NormalTok{ numpyro.sample(}\StringTok{"a"}\NormalTok{, dist.Normal(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ numpyro.sample(}\StringTok{"b"}\NormalTok{, dist.Normal(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\NormalTok{    sigma\_cafe }\OperatorTok{=}\NormalTok{ numpyro.sample(}\StringTok{"sigma\_cafe"}\NormalTok{, dist.Exponential(}\DecValTok{1}\NormalTok{).expand([}\DecValTok{2}\NormalTok{]))}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ numpyro.sample(}\StringTok{"sigma"}\NormalTok{, dist.Exponential(}\DecValTok{1}\NormalTok{))}
\NormalTok{    Rho }\OperatorTok{=}\NormalTok{ numpyro.sample(}\StringTok{"Rho"}\NormalTok{, dist.LKJ(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\NormalTok{    cov }\OperatorTok{=}\NormalTok{ jnp.outer(sigma\_cafe, sigma\_cafe) }\OperatorTok{*}\NormalTok{ Rho}
\NormalTok{    a\_cafe\_b\_cafe }\OperatorTok{=}\NormalTok{ numpyro.sample(}
        \StringTok{"a\_cafe,b\_cafe"}\NormalTok{, }
\NormalTok{        dist.MultivariateNormal(jnp.stack([a, b]), cov).expand([}\DecValTok{20}\NormalTok{])}
\NormalTok{    )}
\NormalTok{a\_cafe, b\_cafe }\OperatorTok{=}\NormalTok{ a\_cafe\_b\_cafe[:, }\DecValTok{0}\NormalTok{], a\_cafe\_b\_cafe[:, }\DecValTok{1}\NormalTok{]}

\CommentTok{\# TFP version of rendom centered effect}
\NormalTok{alpha }\OperatorTok{=} \ControlFlowTok{yield}\NormalTok{ Root(tfd.Sample(tfd.Normal(loc}\OperatorTok{=}\FloatTok{5.0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{2.0}\NormalTok{), sample\_shape}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\NormalTok{beta }\OperatorTok{=} \ControlFlowTok{yield}\NormalTok{ Root(tfd.Sample(tfd.Normal(loc}\OperatorTok{={-}}\FloatTok{1.0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{0.5}\NormalTok{), sample\_shape}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\NormalTok{sigma }\OperatorTok{=} \ControlFlowTok{yield}\NormalTok{ Root(tfd.Sample(tfd.Exponential(rate}\OperatorTok{=}\FloatTok{1.0}\NormalTok{), sample\_shape}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\NormalTok{sigma\_alpha\_beta }\OperatorTok{=} \ControlFlowTok{yield}\NormalTok{ Root(tfd.Sample(tfd.Exponential(rate}\OperatorTok{=}\FloatTok{1.0}\NormalTok{), }
\NormalTok{sample\_shape}\OperatorTok{=}\DecValTok{2}\NormalTok{))}
\NormalTok{Rho }\OperatorTok{=} \ControlFlowTok{yield}\NormalTok{ Root(tfd.LKJ(dimension}\OperatorTok{=}\DecValTok{2}\NormalTok{, concentration}\OperatorTok{=}\FloatTok{2.0}\NormalTok{))}
\NormalTok{Mu }\OperatorTok{=}\NormalTok{ tf.concat([alpha, beta], axis}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\NormalTok{scale }\OperatorTok{=}\NormalTok{ tf.linalg.LinearOperatorDiag(sigma\_alpha\_beta).matmul(tf.squeeze(Rho))}

\CommentTok{\# BI version of rendom centered effect}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, (ni,), name }\OperatorTok{=} \StringTok{\textquotesingle{}Sigma\_individual\textquotesingle{}}\NormalTok{)}
\NormalTok{L }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{1}\NormalTok{, (ni,), name }\OperatorTok{=} \StringTok{\textquotesingle{}L\_individual\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (ni,)) }
\NormalTok{Z }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}z\_individual\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (ni,K))}
\NormalTok{alpha }\OperatorTok{=}\NormalTok{ random\_centered2(Sigma, L, Z)}
\end{Highlighting}
\end{Shaded}

\emph{BI} is designed to encapsulate the entire Bayesian modeling
workflow within a cohesive object-oriented structure, promoting a
streamlined and reproducible analysis pipeline. Typically, a user
interacts with a primary \texttt{BI} object, through which they can
sequentially:

\begin{itemize}
\item
  \textbf{Handle Data:} Load, preprocess, and associate dataset(s) with
  the model object.
\item
  \textbf{Define Model:} Specify the model structure, including the
  likelihood(s), priors for all parameters, and incorporate any
  pre-built components using an intuitive formula syntax.
\item
  \textbf{Run Inference:} Execute the model fitting process using the
  No-U-Turn Sampler (NUTS), which triggers the backend PPL (e.g.,
  \emph{NumPyro}, \emph{TFP}) to perform Markov Chain Monte Carlo (MCMC)
  sampling. Progress indicators and diagnostics are typically provided.
\item
  \textbf{Analyze Posterior:} Access, summarize, and diagnose the
  posterior distributions of parameters. This includes methods for
  calculating posterior means, medians, credible intervals, convergence
  diagnostics (e.g., \(\hat{R}\), Effective Sample Size - ESS), and
  retrieving raw posterior samples for custom analysis.
\item
  \textbf{Visualize Results:} Generate standard diagnostic plots (e.g.,
  trace plots, rank plots, posterior distributions) and visualizations
  of model parameters, effects, and predictions using integrated
  plotting functions that leverage the \emph{arviz} library.
\end{itemize}

This unified structure minimizes the need for users to juggle multiple
disparate software tools or manually transfer data and results between
different stages of the analysis, thereby enhancing efficiency and
reproducibility.

Finally, \emph{BI} includes over 21 well-documented implementations of
various standard and advanced Bayesian models. Examples include
Generalized Linear Models (GLMs), Generalized Linear Mixed Models
(GLMMs), survival analysis models (e.g., Cox proportional hazards),
Principal Component Analysis (PCA), phylogenetic comparative methods,
and various network models. Each implementation is accompanied by
detailed documentation that encompasses: 1) general principles, 2)
underlying assumptions, 3) code snippets in Python and R, and 4)
mathematical details, enabling users to gain a deeper understanding of
the modeling process and its nuances. Additionally, the framework's
flexibility allows models to be combined; for example, building a
zero-inflated model with varying intercepts and slopes, or constructing
a joint model where principal components (derived from PCA) serve as
predictors in a subsequent regression, allowing uncertainty to be
propagated through all stages of the analysis.

\subsection{Example : SRM model}\label{example-srm-model}

To illustrate how these design features of \emph{BI} coalesce to provide
a streamlined, flexible, and powerful solution, effectively addressing
the limitations identified in the existing Bayesian software landscape
we will provide a basic example of how an SRM model is declared in BI,
compare it with the equivalent model in Numpyro (Appendix 1) and STAN
(Appendix 2). We will also show how this model can be build from scratch
with BI (\textbf{Code block 3}) or its custom functions (\textbf{Code
block 4}) to highligh the aceessibility-flexibility of our package by
demonstrating how advance user can build custom model (with less code
than STAN) as well as how new user can apply pre-build \emph{BI} models.
Finally we show how it is also called in R (\textbf{Code block 5}) to
cross language use with \emph{BI}. Readers interested in further details
on data structure, data import, data manipulation, and model fitting for
SRM models can refer directly to the \emph{BI} documentation
\href{https://github.com/BGN-for-ASNA/BI/blob/main/Documentation/20.\%20Network\%20model.qmd}{Modeling
Network}.

\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 3: }} \textit{SRM model from scratch with BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{-10pt}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ model(N\_id, idx,  result\_outcomes,}
\NormalTok{          focal\_individual\_predictors, }
\NormalTok{          target\_individual\_predictors):}

    \CommentTok{\# Intercept }
\NormalTok{    intercept }\OperatorTok{=}\NormalTok{ bi.dist.normal(}
\NormalTok{        logit(}\FloatTok{0.1}\OperatorTok{/}\NormalTok{jnp.sqrt(N\_id)), }
        \FloatTok{2.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{,), name }\OperatorTok{=} \StringTok{\textquotesingle{}intercept\textquotesingle{}}
\NormalTok{    )}

    \CommentTok{\# Sender receiver  {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{    N\_var }\OperatorTok{=}\NormalTok{ focal\_individual\_predictors.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    N\_id }\OperatorTok{=}\NormalTok{ focal\_individual\_predictors.shape[}\DecValTok{1}\NormalTok{]   }
\NormalTok{    focal\_effects }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}focal\_effects\textquotesingle{}}\NormalTok{)}
\NormalTok{    target\_effects }\OperatorTok{=}\NormalTok{  dist.normal( }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}target\_effects\textquotesingle{}}\NormalTok{)}
\NormalTok{    terms }\OperatorTok{=}\NormalTok{ jnp.stack([}
\NormalTok{        focal\_effects }\OperatorTok{@}\NormalTok{ focal\_individual\_predictors,}
\NormalTok{        target\_effects }\OperatorTok{@}\NormalTok{  target\_individual\_predictors}
\NormalTok{        ], axis }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{    sr\_raw }\OperatorTok{=}\NormalTok{  dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{, N\_id), name }\OperatorTok{=} \StringTok{\textquotesingle{}sr\_raw\textquotesingle{}}\NormalTok{)}
\NormalTok{    sr\_sigma }\OperatorTok{=}\NormalTok{  dist.exponential( }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{ (}\DecValTok{2}\NormalTok{,), name }\OperatorTok{=} \StringTok{\textquotesingle{}sr\_sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    sr\_L }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{"sr\_L"}\NormalTok{)}
\NormalTok{    rf }\OperatorTok{=}\NormalTok{ deterministic(}\StringTok{\textquotesingle{}sr\_rf\textquotesingle{}}\NormalTok{,(((sr\_L }\OperatorTok{@}\NormalTok{ sr\_raw).T }\OperatorTok{*}\NormalTok{ sr\_sigma)))}
\NormalTok{    ids }\OperatorTok{=}\NormalTok{ jnp.arange(}\DecValTok{0}\NormalTok{,sr\_effects.shape[}\DecValTok{0}\NormalTok{])}
\NormalTok{    edgl\_idx }\OperatorTok{=}\NormalTok{ bi.net.vec\_node\_to\_edgle(jnp.stack([ids, ids], axis }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{    sender }\OperatorTok{=}\NormalTok{ sr\_effects[edgl\_idx[:,}\DecValTok{0}\NormalTok{],}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_effects[edgl\_idx[:,}\DecValTok{1}\NormalTok{],}\DecValTok{1}\NormalTok{]}
\NormalTok{    receiver }\OperatorTok{=}\NormalTok{ sr\_effects[edgl\_idx[:,}\DecValTok{1}\NormalTok{],}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_effects[edgl\_idx[:,}\DecValTok{0}\NormalTok{],}\DecValTok{1}\NormalTok{]}
\NormalTok{    sr }\OperatorTok{=}\NormalTok{  jnp.stack([sender, receiver], axis }\OperatorTok{=} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# dyadic effects {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{    bi.net.mat\_to\_edgl(dyadic\_effect\_mat)}
\NormalTok{    dr\_raw }\OperatorTok{=}\NormalTok{  dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{,N\_dyads), name }\OperatorTok{=} \StringTok{\textquotesingle{}dr\_raw\textquotesingle{}}\NormalTok{)}
\NormalTok{    dr\_sigma }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}dr\_sigma\textquotesingle{}}\NormalTok{ )}
\NormalTok{    dr\_L }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}dr\_L\textquotesingle{}}\NormalTok{)}
\NormalTok{    dr\_rf }\OperatorTok{=}\NormalTok{ deterministic(}\StringTok{\textquotesingle{}dr\_rf\textquotesingle{}}\NormalTok{, (}
\NormalTok{        ((dr\_L }\OperatorTok{@}\NormalTok{ dr\_raw).T }\OperatorTok{*}\NormalTok{ jnp.repeat(dr\_sigma, }\DecValTok{2}\NormalTok{))}
\NormalTok{        ))}

\NormalTok{    dyad\_effects }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }
\NormalTok{        name}\OperatorTok{=} \StringTok{\textquotesingle{}dyad\_effects\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (dyadic\_predictors.ndim }\OperatorTok{{-}} \DecValTok{1}\NormalTok{,}
\NormalTok{    ))}
\NormalTok{    dr }\OperatorTok{=}\NormalTok{ dyad\_effects }\OperatorTok{*}\NormalTok{ dyadic\_predictors}

    \CommentTok{\# Likelihood                                                       }
\NormalTok{    bi.dist.poisson(jnp.exp(intercept }\OperatorTok{+}\NormalTok{ sr }\OperatorTok{+}\NormalTok{ dr), obs}\OperatorTok{=}\NormalTok{result\_outcomes)  }
\end{Highlighting}
\end{Shaded}

\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 4: }} \textit{SRM model with prebuild functions}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{-10pt}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ model(N\_id, idx,  result\_outcomes,}
\NormalTok{          focal\_individual\_predictors, }
\NormalTok{          target\_individual\_predictors):}

    \CommentTok{\# Intercept }
\NormalTok{    intercept }\OperatorTok{=}\NormalTok{ bi.dist.normal(}
\NormalTok{        logit(}\FloatTok{0.1}\OperatorTok{/}\NormalTok{jnp.sqrt(N\_id)), }
        \FloatTok{2.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{,), name }\OperatorTok{=} \StringTok{\textquotesingle{}intercept\textquotesingle{}}
\NormalTok{    )}

    \CommentTok{\# SR }
\NormalTok{    sr }\OperatorTok{=}\NormalTok{  bi.net.sender\_receiver(}
\NormalTok{        focal\_individual\_predictors,}
\NormalTok{        target\_individual\_predictors}
\NormalTok{    )}

    \CommentTok{\# Dyadic  }
\NormalTok{    dr }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_effect(shape }\OperatorTok{=}\NormalTok{ idx.shape[}\DecValTok{0}\NormalTok{])}

    \CommentTok{\# Likelihood                                                       }
\NormalTok{    bi.dist.poisson(jnp.exp(intercept }\OperatorTok{+}\NormalTok{ sr }\OperatorTok{+}\NormalTok{ dr), obs}\OperatorTok{=}\NormalTok{result\_outcomes)  }
\end{Highlighting}
\end{Shaded}

\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 5: }} \textit{SRM model with prebuild functions}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{-10pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(N\_id, idxShape, result\_outcomes,}
\NormalTok{                focal\_individual\_predictors, target\_individual\_predictors)\{}

\NormalTok{  x}\OtherTok{=}\FloatTok{0.1}\SpecialCharTok{/}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{sqrt}\NormalTok{(N\_id)}
\NormalTok{  tmp}\OtherTok{=}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{log}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x))}
  
  \CommentTok{\# Intercept }
\NormalTok{  intercept }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(tmp, }\FloatTok{2.5}\NormalTok{, }\AttributeTok{shape=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{), }\AttributeTok{name =} \StringTok{\textquotesingle{}block\textquotesingle{}}\NormalTok{)}
  
  \CommentTok{\# SR }
\NormalTok{  sr }\OtherTok{=}\NormalTok{  m}\SpecialCharTok{$}\NormalTok{net}\SpecialCharTok{$}\FunctionTok{sender\_receiver}\NormalTok{(}
\NormalTok{            focal\_individual\_predictors,}
\NormalTok{            target\_individual\_predictors}
\NormalTok{        )}
  
  \CommentTok{\# Dyadic  }
\NormalTok{  dr }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{net}\SpecialCharTok{$}\FunctionTok{dyadic\_effect}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(idxShape))}

  \CommentTok{\# Likelihood                                                       }
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{poisson}\NormalTok{(jnp}\SpecialCharTok{$}\FunctionTok{exp}\NormalTok{(intercept }\SpecialCharTok{+}\NormalTok{ sr }\SpecialCharTok{+}\NormalTok{ dr), }\AttributeTok{obs=}\NormalTok{result\_outcomes)  }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, regarding code performance we can time the computation time for
network of size 200 in STAN and BI and observed that BI comput time is
around XXX on cpu and XXX on gpu and STAN compute time around XXX.

\subsection{Discussion}\label{discussion}

\textbf{BI} framework is built on top of the popular Python programming
language, with a focus on providing a user-friendly interface for model
development and interpretation. Our framework is designed to be modular
and extensible, allowing users to easily incorporate their own custom
models and data types into the framework. One of the key features of
this software is its comprehensive library of 21 predefined Bayesian
models, covering a wide range of common applications and use cases.
These models are accompanied by detailed explanations, making it easier
for users to understand the underlying assumptions and apply the models
to their specific research questions. In addition to these built-in
models, the software includes several custom functions tailored for
advanced statistical and network modeling. This curated library serves
not only as a collection of ready-to-use tools but also as a valuable
pedagogical resource, demonstrating best practices for constructing,
fitting, and interpreting models within the \emph{BI} framework, and
providing robust templates for users aiming to develop novel model
variants. Whether users are interested in hierarchical models,
time-series analysis, or cutting-edge network modeling approaches, our
library caters to a variety of analytical needs. This accessibility
fosters an environment where users can confidently explore and implement
Bayesian methods, ultimately enhancing their research capabilities.

By providing a streamlined and efficient environment for the end-to-end
Bayesian workflow---from model specification and fitting to diagnostics
and prediction, \emph{BI} lowers the barrier to entry for sophisticated
Bayesian modeling. We aim to empower a broader community of researchers
across disciplines to confidently apply advanced Bayesian methods to
their complex research problems.

\subsection{References}\label{references}

\phantomsection\label{3ade8a4a-fb1d-4a6c-8409-ac45482d5fc9}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-TFP}
Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, et al. 2015. {``{TensorFlow}: Large-Scale
Machine Learning on Heterogeneous Systems.''}
\url{https://www.tensorflow.org/}.

\bibitem[\citeproctext]{ref-BEAST}
Bouckaert, Timothy G. AND Barido-Sottani, Remco AND Vaughan. 2019.
{``BEAST 2.5: An Advanced Software Platform for Bayesian Evolutionary
Analysis.''} \emph{PLOS Computational Biology} 15 (4): 1--28.
\url{https://doi.org/10.1371/journal.pcbi.1006650}.

\bibitem[\citeproctext]{ref-jax2018github}
Bradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson,
Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. {``{JAX}:
Composable Transformations of {P}ython+{N}um{P}y Programs.''}
\url{http://github.com/jax-ml/jax}.

\bibitem[\citeproctext]{ref-BRMS}
Bürkner, Paul-Christian. 2017. {``{brms}: An {R} Package for {Bayesian}
Multilevel Models Using {Stan}.''} \emph{Journal of Statistical
Software} 80 (1): 1--28. \url{https://doi.org/10.18637/jss.v080.i01}.

\bibitem[\citeproctext]{ref-BISON}
Hart, Jordan, Michael Nash Weiss, Daniel Franks, and Lauren Brent. 2023.
{``BISoN: A Bayesian Framework for Inference of Social Networks.''}
\emph{Methods in Ecology and Evolution} 14 (9): 2411--20.
https://doi.org/\url{https://doi.org/10.1111/2041-210X.14171}.

\bibitem[\citeproctext]{ref-RevBayes}
Höhna, Sebastian, Michael J. Landis, Tracy A. Heath, Bastien Boussau,
Nicolas Lartillot, Brian R. Moore, John P. Huelsenbeck, and Fredrik
Ronquist. 2016. {``RevBayes: Bayesian Phylogenetic Inference Using
Graphical Models and an Interactive Model-Specification Language.''}
\emph{Systematic Biology} 65 (4): 726--36.
\url{https://doi.org/10.1093/sysbio/syw021}.

\bibitem[\citeproctext]{ref-rethinking}
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\bibitem[\citeproctext]{ref-numpyro}
Phan, Du, Neeraj Pradhan, and Martin Jankowiak. 2019. {``Composable
Effects for Flexible and Accelerated Probabilistic Programming in
NumPyro.''} \emph{arXiv Preprint arXiv:1912.11554}.

\bibitem[\citeproctext]{ref-STRAND1}
Ross, Cody T, Richard McElreath, and Daniel Redhead. 2024. {``Modelling
Animal Network Data in r Using STRAND.''} \emph{Journal of Animal
Ecology} 93 (3): 254--66.

\bibitem[\citeproctext]{ref-PyMC}
Salvatier, John, Thomas V Wiecki, and Christopher Fonnesbeck. 2016.
{``Probabilistic Programming in Python Using PyMC3.''} \emph{PeerJ
Computer Science} 2: e55.

\bibitem[\citeproctext]{ref-STRAND2025}
Sosa, Sebastian, Mary B. McElreath, Daniel Redhead, and Cody T. Ross.
n.d. {``Robust Bayesian Analysis of Animal Networks Subject to Biases in
Sampling Intensity and Censoring.''} \emph{Methods in Ecology and
Evolution} n/a (n/a).
https://doi.org/\url{https://doi.org/10.1111/2041-210X.70017}.

\bibitem[\citeproctext]{ref-sosa2020network}
Sosa, Sebastian, Cédric Sueur, and Ivan Puga-Gonzalez. 2020. {``Network
Measures in Animal Social Network Analysis: Their Strengths, Limits,
Interpretations and Uses.''}

\bibitem[\citeproctext]{ref-Stan}
Stan Development Team. {``Stan Modeling Language Users Guide and
Reference Manual, Version 2.32.''} \url{https://mc-stan.org}.

\bibitem[\citeproctext]{ref-R}
Wickham, Hadley. 2015. \emph{R Packages}. 1st ed. O'Reilly Media, Inc.

\end{CSLReferences}




\end{document}
