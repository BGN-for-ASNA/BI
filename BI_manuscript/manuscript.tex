%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions developed by 
% Overleaf for the Overleaf-Wiley pilot which ran 
% during 2017 and 2018.
% 
% This template is no longer supported, but is provided
% for historical reference. Last updated January 2019.
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Document class options:
% =======================
% blind: Anonymise all author, affiliation, correspondence
%        and funding information.
%
% lineno: Adds line numbers.
%
% serif: Sets the body font to be serif. 
%
% twocolumn: Sets the body text in two-column layout. 
% 
% num-refs: Uses numerical citation and references style 
%           (Vancouver-authoryear).
%
% alpha-refs: Uses author-year citation and references style
%             (rss).
%
% Using other bibliography styles:
% =======================
%
% To specify a different bibiography style
%
% 1) Do not use either num-refs or alpha-refs in documentclass.
% 2) Load natbib package with the options set as needed.
% 3) Use the \bibliographystyle command to specify the style
% 
% Included NJD styles are: 
%   WileyNJD-ACS
%   WileyNJD-AMA
%   WileyNJD-AMS
%   WileyNJD-APA
%   WileyNJD-Harvard
%   WileyNJD-VANCOUVER
%
% or you may upload an alternative .bst file 
% (if requested by the journal).
%
% Examples:
% =======================
%% Example: Using numerical, sort-by-authors citations.
\documentclass[num-refs]{wiley-article}

%% Example: Using author-year citations and anonymising submission
% \documentclass[blind,alpha-refs]{wiley-article}

%% Example: Using unsrtnat for numerical, in-sequence citations
% \documentclass{wiley-article}
% \usepackage[numbers]{natbib}
% \bibliographystyle{unsrtnat}

%% Example: Using WileyNJD-AMA reference style and superscript
%%          citations, two-column and serif fonts for AIChE
% \documentclass[serif,twocolumn,lineno]{wiley-article}
% \usepackage[super]{natbib}
% \bibliographystyle{WileyNJD-AMA}
% \makeatletter
% \renewcommand{\@biblabel}[1]{#1.}
% \makeatother

% Add additional packages here if required
\usepackage{siunitx}

% Update article type if known
\papertype{Practical Tools}
% Include section in journal if known, otherwise delete
\paperfield{Methods in Ecology and Evolution}

\title{Bayesian Inference library}

% List abbreviations here, if any. Please note that it is preferred that abbreviations be defined at the first instance they appear in the text, rather than creating an abbreviations list.
%\abbrevs{ABC, a black cat; DEF, doesn't ever fret; GHI, goes home immediately.}

% Include full author names and degrees, when required by the journal.
% Use the \authfn to add symbols for additional footnotes and present addresses, if any. Usually start with 1 for notes about author contributions; then continuing with 2 etc if any author has a different present address.
\author[1]{Sebastian Sosa}
\author[1]{Mary B. McElreath}
\author[1]{Cody T. Ross}


% Include full affiliation details for all authors
\affil[1]{Department of Human Behavior, Ecology and Culture. Max Planck Institute for Evolutionary Anthropology. Leipzig, Germany }

\corraddress{Sebastian Sosa}
\corremail{sebastian\_sosa@eva.mpg.de}


\fundinginfo{Max Planck Institute for Evolutionary Anthropology. Leipzig, Germany}

% Include the name of the author that should appear in the running header
\runningauthor{Sosa and Ross (2025)}


\usepackage{listings}
\usepackage{xcolor}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\definecolor{codegreen}{rgb}{0.2,0.48,0.2}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeorange}{rgb}{0.77,0.3,0.0}
\definecolor{backcolour}{rgb}{0.93,0.93,0.93}
\definecolor{codered}{rgb}{0.7,0.035,0.035}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codered},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    otherkeywords = {install_github, setup_folders, standardize_photos, build_survey, enter_data, compile_data, calculate_payouts, check_classification, downsize, pre_process, auto_enter_all, annotate_data, simulate_selfreport_network, make_strand_data, fit_latent_network_model, summarize_strand_results, data.frame, set.seed, summarize_bsrm_hh_results, simulate_multiplex_network, multiplex_plot, fit_block_plus_social_relations_hh_model, simulate_sbm_plus_srm_hh_network, fit_multiplex_model, fit_block_plus_social_relations_model, fit_social_relations_model, strand_caterpillar_plot, fit_longitudinal_model, longitudinal_plot, standardize, simulate_longitudinal_network, process_block_parameters, jnp.outer, numpyro.sample, dist.Normal, dist.Exponential, expand, dist.LKJ, dist.MultivariateNormal, jnp.stack, jax.random.PRNGKey, tfd.Sample, yield, Root, tfd.Normal, tfd.Exponential, tfd.LKJ, tf.concat, tf.linalg.LinearOperatorDiag, matmul, tf.squeeze, bi.dist.normal, tfp.distributions.Normal, random_centered2, bi.dist.lkjcholesky, bi.dist.exponential, dist.normal, dist.uniform, Normal },
  alsoletter ={_},
  deletekeywords={path, start, stop, ordered, case, colors, order, add, data, mode, R, distance}
}

\lstset{style=mystyle}



\begin{document}

\begin{frontmatter}
\maketitle

\begin{abstract}

\begin{enumerate}
    \item  Bayesian modeling is a powerful paradigm in modern statistics and machine learning. However, practitioners face significant obstacles.
   
    \item  The landscape of Bayesian software is fragmented across programming languages and abstraction levels. Newcomers often gravitate towards high-level interfaces within familiar environments due to their accessibility for standard models.

    \item To gain the necessary flexibility, researchers must often transition to lower-level probabilistic programming languages.
   
    \item  Similar accessibility and flexibility trade-offs exist in domain-specific Bayesian packages. 
    
    \item   Computational demands remain a significant bottleneck, limiting the application of Bayesian methods to the large datasets and complex, high-dimensional models prevalent in modern research.

    \item \textit{Bayesian Inference (BI)} software is available in both Python and R. It aims to unify the modeling experience by integrating an intuitive model-building syntax with the flexibility of low-level abstraction coding available but also pre-build function for high-level of abstraction and including hardware-accelerated computation for improved scalability. 

\end{enumerate}


% Please include a maximum of seven keywords
\keywords{Bayesian methods, scalability, software, Python, GPU parallelization, social networks}
\end{abstract}
\end{frontmatter}



\section{Introduction}\label{introduction}

Bayesian modeling has emerged as an essential part of the toolbox of modern statistics and machine learning, providing a framework for robust inference under uncertainty. Bayesian methods are valuable across the academic disciplines, but are especially useful in evolution, ecology, and animal behavior, where missing data, measurement bias, and non-standard (i.e., scientifically motivated) data-generating models are ubiquitous. The potential of Bayesian methods to address applied challenges in the field is large, but the current ecosystem of Bayesian software is difficult for many end-users (i.e., academic researchers) to navigate. Key challenges stem from the fragmented nature of software across different programming languages (\textbf{interoperability}), gaps between theoretical understanding and practical implementation (\textbf{accessibility}), complexities in model specification that force trade-offs between ease-of-use and flexibility ( \textbf{accessibility-flexibility trade-off}), the constraints of overly specialized tools (\textbf{domain-specific limitations}), and persistent computational scalability limitations for complex models or large datasets (\textbf{scalability}).


The first major obstacle is the fragmented landscape of Bayesian software, scattered across different programming languages  challenges (e.g., \texttt{Stan}, \texttt{TensorFlow probability (TFP)}, \texttt{NumPyro}, \texttt{JAX}) and software environments (e.g., \texttt{R}, \texttt{Python}, \texttt{C++}), and varying levels of abstraction, posing significant \textbf{interoperability}.Indeed, researchers frequently encounter a disparate collection of tools—from \textit{Stan}'s domain-specific language (DSL) to distinct low-level of abstraction libraries (like \textit{PyMC} \citep{PyMC}, \textit{TensorFlow Probability (TFP)} \citep{TFP}, \textit{NumPyro} \citep{numpyro}) and high-level of abstraction libraries (like \textit{BRMS}). This fragmentation complicates workflows and presents a confusing landscape, especially for researchers new to Bayesian analysis. For instance, researchers new to Bayesian analysis may initially gravitate towards tools of high-level of abstraction available within their most familiar programming environment (e.g., \textit{BRMS} \citep{BRMS} in \textit{R} \citep{R}), potentially overlooking more suitable options elsewhere due to the steep initial learning curve or perceived incompatibility (accessibility). This linguistic and platform diversity imposes considerable cognitive overhead, potentially hindering the adoption of the most suitable tool for a given problem due to familiarity biases or the friction of switching ecosystems, ultimately impacting the effective application of Bayesian methods. This initial hurdle of navigating disparate systems naturally leads new practitioners to prioritize tools that appear easiest to learn, raising concerns about the balance between accessibility and the flexibility needed for complex research.


Compounding this fragmentation is the challenge of accessibility and the translation of theoretical knowledge into practice \textbf{accessibility-flexibility trade-off} combine with limited formal training many researchers receive in Bayesian methods and generative model development. Indeed, while high-level interfaces like \textit{BRMS} offer an intuitive formula-based syntax, significantly lowering the initial barrier to entry (e.g. generalized linear mixed models using \textit{BRMS}), to the great benefit of the scientific community, this accessibility often comes at the cost of flexibility. As many generative scientific models are not easily defined, requiring custom likelihood functions (e.g., multiple likelihoods), intricate prior structures (e.g., XXX), or non-standard model components (e.g., centered-random factors), the limitations of these high-level wrappers become apparent. To gain the necessary expressive power, the researcher must typically transition to lower-level probabilistic programming languages (PPLs) such as \textit{Stan} \citep{Stan} (requiring mastery of its specific DSL), \textit{PyMC}, \textit{NumPyro}, or \textit{TFP}. This transition imposes a much steeper learning curve, demanding a deeper understanding of probabilistic programming concepts (like computational graphs or tensor manipulation) and often more verbose code. This significant jump in complexity can deter users, divert focus from statistical modeling to software engineering challenges, and ultimately slow down the pace of research, particularly when trying to adapt models within specific scientific fields.

Similar accessibility and flexibility constraints manifest as \textbf{domain-specific limitations} within specialized Bayesian packages. Fields like phylogenetics or network analysis benefit from tools such as \textit{BEAST} \citep{BEAST}, \textit{RevBayes} \citep{RevBayes}, \textit{STRAND} \citep{STRAND1}, or \textit{BISON} \citep{BISON}, which provide accessible, pre-packaged models tailored to common domain problems. A phylogeneticist might initially find \textit{BEAST} convenient for standard molecular clock models. However, when they wish to incorporate a novel evolutionary hypothesis requiring modification of the core model structure or integrate data types not originally envisioned by the developers, they often encounter rigid constraints. Extending these specialized tools frequently requires deep engagement with their underlying, often complex, codebase (sometimes necessitating proficiency in languages like Java or C++) or abandoning the domain-specific tool entirely in favor of a general-purpose PPL. This forces researchers to either compromise on their methodological innovation or undertake a significant software development effort, potentially switching programming ecosystems and losing the initial convenience, thereby limiting the evolution of modeling practices within specialized domains. Even when model specification is achievable, either in general or specialized tools, the computational feasibility remains a major concern.

Finally, computational \textbf{scalability} continues to be a significant bottleneck, limiting the application of Bayesian methods to the large datasets (e.g., millions of observations) and complex, high-dimensional models  prevalent in modern research across fields like genomics, neuroscience, and machine learning. While established tools like \textit{Stan} feature highly optimized inference algorithms (particularly its NUTS sampler) and offer effective multi-core parallelization, they can still face challenges with long C++ compilation times for complex models and may require substantial code restructuring or external tooling to efficiently leverage hardware accelerators like GPUs or TPUs for certain computations (e.g., social network models in \texttt{STRAND} have a parameter complexity that scales with the square of the number of nodes, and so MCMC run-times can become unacceptable with as few as a few hundred individuals in the sample). Conversely, emerging frameworks built on \textit{JAX} \citep{jax2018github} (powering \textit{NumPyro} and parts of \textit{TFP}) promise substantial speedups via automatic differentiation, JIT compilation, and native support for parallel hardware architectures. However, integrating these powerful backends seamlessly into user-friendly, flexible modeling front-ends that don't require deep expertise in the JAX ecosystem itself is an ongoing challenge. Domain-specific tools often inherit the scalability limitations of the frameworks they are built upon, failing to provide a universally efficient solution across different model types and data sizes.

We contend that the added cognitive load of learning Bayesian methods while simultaneously struggling with various programming language back-ends may be discouraging end-users from fully exploring the capabilities of Bayesian methods, simply due to the friction introduced by language barriers. Therefore, there is an evident and pressing demand for a Bayesian modeling framework that synergistically addresses these interconnected limitations.To address these interconnected challenges, we introduce \textit{\textbf{Bayesian Inference (BI)}}, a new Bayesian modeling software designed to unify the modeling experience across the two dominant data science languages, Python and R. \textit{BI} tackles the \textbf{interoperability} barrier head-on by offering native interfaces in both environments. It aims to resolve the \textbf{accessibility-flexibility trade-off} by providing an intuitive model-building syntax familiar to users of statistical modeling languages, while enabling advanced customization and leveraging multiple, interchangeable inference backends for flexibility. To combat \textbf{domain-specific limitations}, \textit{BI} includes pre-built functions and structures tailored for specialized models in areas like network analysis, survival analysis, and phylogenetic analysis, while still allowing extension and modification within its general framework. Crucially, \textit{BI} enhances \textbf{scalability} by integrating with hardware-accelerated computation via \textit{JAX} (using \textit{NumPyro} or \textit{TFP} as backends), enabling efficient execution on CPUs, GPUs, and TPUs. By providing a streamlined, efficient, and unified environment for the end-to-end Bayesian workflow—from model specification and fitting to diagnostics and prediction—\textit{\textbf{BI}} lowers the barrier to entry for sophisticated Bayesian modeling, aiming to empower a broader community of researchers across disciplines to confidently apply advanced Bayesian methods to their complex research problems.

\section{Software Presentation}\label{software-presentation}
\subsection{Interoperability}\label{Interoperability}
\textit{BI} directly confronts the \textbf{interoperability} challenge by offering native, feature-equivalent implementations in both Python and R. While minor syntactic differences exist to adhere to the idiomatic conventions of each language, the core model specification syntax, the procedural workflow for analysis, and the underlying computational engines remain fundamentally consistent. For instance, Python utilizes dot notation for method calls on class objects (e.g., \texttt{bi.dist.normal(0,1)}), while R employs dollar sign notation for accessing elements or methods within its object system (e.g., \texttt{bi\$dist\$normal(0,1)}). This dual-language availability significantly lowers the adoption barrier for researchers, allowing them to work entirely within their preferred programming environment without sacrificing access to a common, powerful Bayesian modeling framework. 
\subsection{Accessibility}\label{Accessibility}
\textit{BI} is designed to encapsulate the entire Bayesian modeling workflow within a cohesive object-oriented structure, promoting a streamlined and reproducible analysis pipeline. Typically, a user interacts with a primary \texttt{BI} object, through which they can sequentially:

\begin{enumerate}
    \item   \textbf{Handle Data:} Load, preprocess, and associate dataset(s) with the model object.
  
    \item   \textbf{Define Model:} Specify the model structure, including the likelihood(s), priors for all parameters, and incorporate any pre-built components using an intuitive formula syntax.
% ---
% -   **Specify Inference:** Select the desired inference engine (e.g., NUTS via *NumPyro* or *TFP*, potentially Variational Inference methods) and configure its parameters (e.g., number of chains, warmup iterations, target acceptance rate). Crucially, the choice of computational backend (*NumPyro*, *TFP*) can often be switched with minimal changes to the model specification code.
% --- % Converted to comment
    \item   \textbf{Run Inference:} Execute the model fitting process using the No-U-Turn Sampler (NUTS), which triggers the backend PPL (e.g., \textit{NumPyro}, \textit{TFP}) to perform Markov Chain Monte Carlo (MCMC) sampling. Progress indicators and diagnostics are typically provided.
  
    \item   \textbf{Analyze Posterior:} Access, summarize, and diagnose the posterior distributions of parameters. This includes methods for calculating posterior means, medians, credible intervals, convergence diagnostics (e.g., $\hat{R}$, Effective Sample Size - ESS), and retrieving raw posterior samples for custom analysis.
   
    \item   \textbf{Visualize Results:} Generate standard diagnostic plots (e.g., trace plots, rank plots, posterior distributions) and visualizations of model parameters, effects, and predictions using integrated plotting functions that leverage the \textit{arviz} library.
\end{enumerate}

 This unified structure minimizes the need for users to juggle multiple disparate software tools or manually transfer data and results between different stages of the analysis, thereby enhancing efficiency and reproducibility. Finally, \textit{BI} includes over 24 well-documented implementations of various standard and advanced Bayesian models \hyperlink{tab1}{Table 1}. Examples include Generalized Linear Models (GLMs), Generalized Linear Mixed Models (GLMMs), survival analysis models (e.g., Cox proportional hazards), Principal Component Analysis (PCA), phylogenetic comparative methods, and various network models. Each implementation is accompanied by detailed documentation that encompasses: 1) general principles, 2) underlying assumptions, 3) code snippets in Python and R, and 4) mathematical details, enabling users to gain a deeper understanding of the modeling process and its nuances. Additionally, the framework's flexibility allows models to be combined; for example, building a zero-inflated model with varying intercepts and slopes, or constructing a joint model where principal components (derived from PCA) serve as predictors in a subsequent regression, allowing uncertainty to be propagated through all stages of the analysis.

 \label{tab1}
\begin{table}[bt]
\caption{This is a table. Tables should be self-contained and complement, but not duplicate, information contained in the text. They should be not be provided as images. Legends should be concise but comprehensive – the table, legend and footnotes must be understandable without reference to the text. All abbreviations must be defined in footnotes.}\label{tab1}
\begin{threeparttable}
\begin{tabular}{lccc}
\headrow
\thead{Documented Models} & \thead{Documentation link} & \thead{Source} & \thead{Citation} \\
Linear Regression  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Multiple Regression  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Interactions  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Categorical models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Binomial models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Beta-binomial models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Poisson models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Gamma-Poisson models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Dirichlet models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Multinomial models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Dirichlet models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Zero-inflated models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Varying-intercept models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Varying-slopes models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Gaussian process models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Measurement-error models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Missing-data models  & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Latent variable models & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Principal Component Analysis & \url{www.sosa.com/chapter_1.html} & Statistical Rethinking, chapter X & \citep{cite} \\

Basic social network models  & \url{www.sosa.com/chapter_1.html} & STRAND & \citep{cite} \\

Sender-receiver network models  & \url{www.sosa.com/chapter_1.html} & STRAND & \citep{cite} \\

Stochastic blockmodels  & \url{www.sosa.com/chapter_1.html} & STRAND & \citep{cite} \\

Network control for data collection biases & \url{www.sosa.com/chapter_1.html} & ANTs & \citep{cite} \\

Network-based diffusion analysis & \url{www.sosa.com/chapter_1.html} & ANTs & \citep{cite} \\


\hline  % Please only put a hline at the end of the table
\end{tabular}

\begin{tablenotes}
\item  ~
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Accessibility-flexibility trade-off}\label{Accessibility-flexibility trade-off}
\textit{BI} is designed to navigate the critical \textit{accessibility-flexibility trade-off} by providing multiple layers of abstraction and utility, catering effectively to users with varying levels of Bayesian modeling expertise and diverse complexity requirements throuhg : simplified backend interaction via intuitive syntax, pre-built components for complex model features , addressing domain-specific limitations within a general framework, integrated End-to-End Workflow and extensive model library and documentation.

At its computational core, \textit{BI} leverages the power and efficiency of established Probabilistic Programming Languages (PPLs) like \textit{NumPyro} and \textit{TFP}, both of which are built upon the \textit{JAX} framework for high-performance numerical computation and automatic differentiation. However, \textit{BI} deliberately abstracts away much of the inherent complexity of these lower-level tools \hyperref[code:block1_example]{(\textbf{Code block 1})}. This significantly enhances \textbf{accessibility} for a broader range of users.

\label{code:block1_example} 
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 1: }} \textit{Prior specification differences between NumPyro, TFP, and BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{0pt}

\begin{lstlisting}[language=Python]
# NumPyro prior specification
numpyro.sample("mu", dist.Normal(0, 1)).expand([10])

# TFP prior specification (within a JointDistributionCoroutine)
yield Root(tfd.Sample(tfd.Normal(loc=1.0, scale=1.0), sample_shape=10))

# BI prior specification
bi.dist.normal(0, 1, name = "mu", shape = (10,))
\end{lstlisting}

To enhance \textbf{flexibility} without unduly sacrificing the accessibility provided by the high-level syntax, \textit{BI} includes a library of pre-built, computationally optimized functions implemented directly in \textit{JAX} (e.g., \textbf{Code block 2}). These components encapsulate common but potentially complex modeling structures, allowing users to incorporate them easily within the model specification. Key examples include:

\begin{enumerate}
    \item  \textit{Centered Random Effects} and \textit{Non-Centered Random Effects} for hierarchical (multi-level) model components \citep{rethinking}. The non-centered parameterization, often crucial for efficient sampling in hierarchical models (particularly with sparse data), is provided without requiring the user to manually implement the reparameterization logic.
   
    \item  \textit{Kernels for Gaussian Processes} for modeling spatial, temporal, phylogenetic, or other forms of structured correlation or dependency.
   
    \item  \textit{Block Model Effects} for implementing stochastic block models in network analysis.
   
    \item  \textit{SRM effects} for modeling pairwise interactions in networks while accounting for sender effects, receiver effects, dyadic effects, nodal predictors, dyadic predictors, and observation biases \citep{STRAND2025}.
   
    \item  \textit{Network-Based Diffusion Approach (NBDA)} components for modeling the effect of network edges on the rates of transmission of phenomena (e.g., behavioral, epidemiological) while accounting for nodal or dyadic covariates.

    \item \textit{Network metrics} ranging from nodal, dyadic, and global network measures with a total of 11 that can be used to build custom models of social network analysis \citep{sosa2021network}.
\end{enumerate}

These pre-built \textit{JAX} functions provide tailored model components for common patterns in specific fields, while keeping them fully integrated within the general, extensible modeling framework  \hyperref[code:block2_example]{(\textbf{Code block 2})}. By providing these optimized building blocks within its general syntax, \textit{BI} allows researchers in these fields to rapidly implement standard domain models using familiar concepts. Crucially, however, users retain the full flexibility of the \textit{BI} framework to combine these domain-specific components with other model features (e.g., complex non-linear effects via splines, hierarchical structures across groups of networks or phylogenies) or to customize or extend them using \textit{BI}'s underlying mechanisms if needed—a capability often missing in more narrowly focused domain-specific packages. This design aims to foster methodological innovation \textit{within} specialized domains by lowering the barrier to implementing more complex or novel models.

\label{code:block2_example} 
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 2: }} \textit{Random effect specification differences between NumPyro, TFP, and BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{0pt}

\begin{lstlisting}[language=Python]
# Numpyro version of rendom centered effect ----------------------------------------
a = numpyro.sample("a", dist.Normal(5, 2))
    b = numpyro.sample("b", dist.Normal(-1, 0.5))
    sigma_cafe = numpyro.sample("sigma_cafe", dist.Exponential(1).expand([2]))
    sigma = numpyro.sample("sigma", dist.Exponential(1))
    Rho = numpyro.sample("Rho", dist.LKJ(2, 2))
    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho
    a_cafe_b_cafe = numpyro.sample(
        "a_cafe,b_cafe", 
        dist.MultivariateNormal(jnp.stack([a, b]), cov).expand([20])
    )
a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]

# TFP version of rendom centered effect ----------------------------------------
alpha = yield Root(tfd.Sample(tfd.Normal(loc=5.0, scale=2.0), sample_shape=1))
beta = yield Root(tfd.Sample(tfd.Normal(loc=-1.0, scale=0.5), sample_shape=1))
sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1.0), sample_shape=1))
sigma_alpha_beta = yield Root(tfd.Sample(tfd.Exponential(rate=1.0), 
sample_shape=2))
Rho = yield Root(tfd.LKJ(dimension=2, concentration=2.0))
Mu = tf.concat([alpha, beta], axis=-1)
scale = tf.linalg.LinearOperatorDiag(sigma_alpha_beta).matmul(tf.squeeze(Rho))

# BI version of rendom centered effect----------------------------------------
Sigma = dist.exponential(1, (ni,), name = 'Sigma_individual')
L = dist.lkjcholesky(1, (ni,), name = 'L_individual', shape = (ni,)) 
Z = dist.normal(0, 1, name = 'z_individual', shape = (ni,K))
alpha = random_centered2(Sigma, L, Z)
\end{lstlisting}

\subsection{Scalability}\label{Scalability}
 Finally, a fundamental design principle of \texttt{BI}  is to directly address the \texttt{scalability} challenge by building upon modern, high-performance computational backends. By utilizing *JAX*-based libraries like \texttt{NumPyro} and \texttt{TFP}, \texttt{BI} inherits their significant computational efficiencies, stemming from features such as Just-In-Time (JIT) compilation and seamless hardware acceleration (GPU/TPU support). These are critical for tackling the large datasets and complex models prevalent in contemporary research. This strategic reliance on the \texttt{JAX} ecosystem allows \texttt{BI} users to tackle computationally intensive problems that might be infeasible or prohibitively slow using frameworks lacking comparable optimization and hardware acceleration capabilities. By abstracting the backend complexities while retaining their power, \texttt{BI}  significantly enhances the practical \texttt{scalability} of sophisticated Bayesian inference for a wider audience.


\section{Example : SRM model}
To illustrate how these design features of \textit{BI} coalesce to provide a streamlined, flexible, and powerful solution, effectively addressing the limitations identified in the existing Bayesian software landscape we will provide a basic example of how an SRM model is declared in BI, compare it with the equivalent model in Numpyro (Appendix 1) and STAN (Appendix 2). We will also show how this model can be build from scratch with BI  \hyperref[code:block3_example]{(\textbf{Code block 3})} or its custom functions  \hyperref[code:block4_example]{(\textbf{Code block 4})} to highligh the aceessibility-flexibility of our package by demonstrating how advance user can build custom model (with less code than STAN) as well as how new user can apply pre-build \textit{BI} models. Finally we show how it is also called in R \hyperref[code:block5_example]{(\textbf{Code block 5})} to cross language use with \textit{BI}. Readers interested in further details on data structure, data import, data manipulation, and model fitting for SRM models can refer directly to the \textit{BI} documentation \href{https://github.com/BGN-for-ASNA/BI/blob/main/Documentation/20.\%20Network\%20model.qmd}{Modeling Network}.

\label{code:block3_example} 
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 3: }} \textit{SRM model from scratch with BI}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[-8pt] % % Bottom rule
\vspace{-\parskip}\vspace{0pt}

\begin{lstlisting}[language=Python, label=lst:srm_scratch]
def model(N_id, idx,  result_outcomes,
          focal_individual_predictors, 
          target_individual_predictors):

    # Intercept 
    intercept = bi.dist.normal(
        logit(0.1/jnp.sqrt(N_id)), 
        2.5, shape=(1,), name = 'intercept'
    )

    # Sender receiver  ----------------------
    N_var = focal_individual_predictors.shape[0]
    N_id = focal_individual_predictors.shape[1]   
    focal_effects = dist.normal(0, 1,  name = 'focal_effects')
    target_effects =  dist.normal( 0, 1,  name = 'target_effects')
    terms = jnp.stack([
        focal_effects @ focal_individual_predictors,
        target_effects @  target_individual_predictors
        ], axis = -1)
    sr_raw =  dist.normal(0, 1, shape=(2, N_id), name = 'sr_raw')
    sr_sigma =  dist.exponential( 1, shape= (2,), name = 'sr_sigma')
    sr_L = dist.lkjcholesky(2, 2, name = "sr_L")
    rf = deterministic('sr_rf',(((sr_L @ sr_raw).T * sr_sigma)))
    ids = jnp.arange(0,sr_effects.shape[0])
    edgl_idx = bi.net.vec_node_to_edgle(jnp.stack([ids, ids], axis = -1))
    sender = sr_effects[edgl_idx[:,0],0] + sr_effects[edgl_idx[:,1],1]
    receiver = sr_effects[edgl_idx[:,1],0] + sr_effects[edgl_idx[:,0],1]
    sr =  jnp.stack([sender, receiver], axis = 1)

    # dyadic effects ------------------------------------------
    bi.net.mat_to_edgl(dyadic_effect_mat)
    dr_raw =  dist.normal(0, 1, shape=(2,N_dyads), name = 'dr_raw')
    dr_sigma = dist.exponential(1,  name = 'dr_sigma' )
    dr_L = dist.lkjcholesky(2, 2, name = 'dr_L')
    dr_rf = deterministic('dr_rf', (
        ((dr_L @ dr_raw).T * jnp.repeat(dr_sigma, 2))
        ))

    dyad_effects = dist.normal(0, 1, 
        name= 'dyad_effects', shape = (dyadic_predictors.ndim - 1,
    ))
    dr = dyad_effects * dyadic_predictors

    # Likelihood                                                       
    bi.dist.poisson(jnp.exp(intercept + sr + dr), obs=result_outcomes)  
\end{lstlisting}

\label{code:block4_example} 
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 4: }} \textit{SRM model with prebuild functions}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % % Bottom rule
\vspace{-\parskip}\vspace{0pt}

\begin{lstlisting}[language=Python, label=lst:srm_prebuild_py]
def model(N_id, idx,  result_outcomes,
          focal_individual_predictors, 
          target_individual_predictors):

    # Intercept 
    intercept = bi.dist.normal(
        logit(0.1/jnp.sqrt(N_id)), 
        2.5, shape=(1,), name = 'intercept'
    )

    # SR 
    sr =  bi.net.sender_receiver(
        focal_individual_predictors,
        target_individual_predictors
    )

    # Dyadic  
    dr = bi.net.dyadic_effect(shape = idx.shape[0])

    # Likelihood                                                       
    bi.dist.poisson(jnp.exp(intercept + sr + dr), obs=result_outcomes)  
\end{lstlisting}

\label{code:block5_example} 
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % Top rule, line break, add 2pt space
\noindent\textbf{\textit{Code block 5: }} \textit{SRM model with prebuild functions}\\[-10pt]
\noindent\rule{\linewidth}{0.4pt}\\[0pt] % % Bottom rule
\vspace{-\parskip}\vspace{0pt}

\begin{lstlisting}[language=R, label=lst:srm_prebuild_r]
model <- function(N_id, idxShape, result_outcomes,
                focal_individual_predictors, target_individual_predictors){

  x=0.1/jnp$sqrt(N_id)
  tmp=jnp$log(x / (1 - x))
  
  # Intercept 
  intercept = bi.dist.normal(tmp, 2.5, shape=c(1), name = 'block')
  
  # SR 
  sr =  m$net$sender_receiver(
            focal_individual_predictors,
            target_individual_predictors
        )
  
  # Dyadic  
  dr = m$net$dyadic_effect(shape = c(idxShape))

  # Likelihood                                                       
  m$poisson(jnp$exp(intercept + sr + dr), obs=result_outcomes)  
}
\end{lstlisting}

Finally, regarding code performance we can time the computation time for network of size 200 in STAN and BI and observed that BI comput time is around XXX on cpu and XXX on gpu and STAN compute time around XXX.

\section{Discussion}

\textbf{BI} framework is built on top of the popular Python programming language, with a focus on providing a user-friendly interface for model development and interpretation. Our framework is designed to be modular and extensible, allowing users to easily incorporate their own custom models and data types into the framework. One of the key features of this software is its comprehensive library of 21 predefined Bayesian models, covering a wide range of common applications and use cases. These models are accompanied by detailed explanations, making it easier for users to understand the underlying assumptions and apply the models to their specific research questions. In addition to these built-in models, the software includes several custom functions tailored for advanced statistical and network modeling. This curated library serves not only as a collection of ready-to-use tools but also as a valuable pedagogical resource, demonstrating best practices for constructing, fitting, and interpreting models within the \textit{BI} framework, and providing robust templates for users aiming to develop novel model variants. Whether users are interested in hierarchical models, time-series analysis, or cutting-edge network modeling approaches, our library caters to a variety of analytical needs. This accessibility fosters an environment where users can confidently explore and implement Bayesian methods, ultimately enhancing their research capabilities.

By providing a streamlined and efficient environment for the end-to-end Bayesian workflow—from model specification and fitting to diagnostics and prediction, \textit{BI} lowers the barrier to entry for sophisticated Bayesian modeling. We aim to empower a broader community of researchers across disciplines to confidently apply advanced Bayesian methods to their complex research problems.

\section*{Acknowledgements}
Acknowledgements should include contributions from anyone who does not meet the criteria for authorship (for example, to recognize contributions from people who provided technical help, collation of data, writing assistance, acquisition of funding, or a department chairperson who provided general support), as well as any funding or other support information.

\section*{Conflict of interest}
You may be asked to provide a conflict of interest statement during the submission process. Please check the journal's author guidelines for details on what to include in this section. Please ensure you liaise with all co-authors to confirm agreement with the final statement.

\section*{Supporting Information}

Supporting information is information that is not essential to the article, but provides greater depth and background. It is hosted online and appears without editing or typesetting. It may include tables, figures, videos, datasets, etc. More information can be found in the journal's author guidelines or at \url{http://www.wileyauthors.com/suppinfoFAQs}. Note: if data, scripts, or other artefacts used to generate the analyses presented in the paper are available via a publicly available data repository, authors should include a reference to the location of the material within their paper.

\printendnotes

\bibliography{sample}


\end{document}