{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Functions](#toc1_)    \n",
    "  - [Compare distributions between BI and STAN](#toc1_1_)    \n",
    "  - [Load BI for R](#toc1_2_)    \n",
    "  - [Load BI](#toc1_3_)    \n",
    "- [Rethinking](#toc2_)    \n",
    "  - [Continuous variable: Model (model 4.3)](#toc2_1_)    \n",
    "    - [BI](#toc2_1_1_)    \n",
    "    - [BIR](#toc2_1_2_)    \n",
    "    - [STAN](#toc2_1_3_)    \n",
    "    - [Output comparison](#toc2_1_4_)    \n",
    "    - [Parameter recovery](#toc2_1_5_)    \n",
    "  - [Categorical variable: Model (model 5.9)](#toc2_2_)    \n",
    "    - [BI](#toc2_2_1_)    \n",
    "    - [BIR](#toc2_2_2_)    \n",
    "    - [STAN](#toc2_2_3_)    \n",
    "    - [Output comparaison](#toc2_2_4_)    \n",
    "    - [Parameter recovery](#toc2_2_5_)    \n",
    "  - [Continuous interactions terms (model 8.3)](#toc2_3_)    \n",
    "    - [BI](#toc2_3_1_)    \n",
    "    - [BIR](#toc2_3_2_)    \n",
    "    - [STAN](#toc2_3_3_)    \n",
    "    - [Output comparison](#toc2_3_4_)    \n",
    "    - [Parameter recovery](#toc2_3_5_)    \n",
    "  - [Binomial (model 11.1)](#toc2_4_)    \n",
    "    - [BI](#toc2_4_1_)    \n",
    "    - [BIR](#toc2_4_2_)    \n",
    "    - [STAN](#toc2_4_3_)    \n",
    "    - [Output comparison](#toc2_4_4_)    \n",
    "    - [Parameter recovery](#toc2_4_5_)    \n",
    "  - [Binomial with indices (model 11.4)](#toc2_5_)    \n",
    "    - [BI](#toc2_5_1_)    \n",
    "    - [BIR](#toc2_5_2_)    \n",
    "    - [STAN](#toc2_5_3_)    \n",
    "    - [Output comparison](#toc2_5_4_)    \n",
    "    - [Parameter recovery](#toc2_5_5_)    \n",
    "  - [Poisson (model 11.10)](#toc2_6_)    \n",
    "    - [BI](#toc2_6_1_)    \n",
    "    - [BIR](#toc2_6_2_)    \n",
    "    - [STAN](#toc2_6_3_)    \n",
    "    - [Output comparison](#toc2_6_4_)    \n",
    "    - [Parameter recovery](#toc2_6_5_)    \n",
    "  - [Negative binomial (model 11.12)](#toc2_7_)    \n",
    "    - [Simulated data](#toc2_7_1_)    \n",
    "    - [BI](#toc2_7_2_)    \n",
    "    - [BIR](#toc2_7_3_)    \n",
    "    - [STAN](#toc2_7_4_)    \n",
    "    - [Output comparison](#toc2_7_5_)    \n",
    "    - [Parameter recovery](#toc2_7_6_)    \n",
    "  - [Multinomial (model 11.13)](#toc2_8_)    \n",
    "    - [Simulated data](#toc2_8_1_)    \n",
    "    - [BI](#toc2_8_2_)    \n",
    "    - [BIR](#toc2_8_3_)    \n",
    "    - [STAN](#toc2_8_4_)    \n",
    "    - [Output comparison](#toc2_8_5_)    \n",
    "    - [Parameter recovery](#toc2_8_6_)    \n",
    "  - [Beta binomial (model m12.1)](#toc2_9_)    \n",
    "    - [BI](#toc2_9_1_)    \n",
    "    - [BIR](#toc2_9_2_)    \n",
    "    - [STAN](#toc2_9_3_)    \n",
    "    - [Output comparison](#toc2_9_4_)    \n",
    "    - [Parameter recovery](#toc2_9_5_)    \n",
    "  - [Zero inflated outcomes](#toc2_10_)    \n",
    "    - [BI](#toc2_10_1_)    \n",
    "    - [BIR](#toc2_10_2_)    \n",
    "    - [STAN](#toc2_10_3_)    \n",
    "    - [Output comparison](#toc2_10_4_)    \n",
    "    - [Parameter recovery](#toc2_10_5_)    \n",
    "  - [OrderedLogistic (Todo: PB)](#toc2_11_)    \n",
    "  - [Varying interceps](#toc2_12_)    \n",
    "    - [BI](#toc2_12_1_)    \n",
    "    - [BIR](#toc2_12_2_)    \n",
    "    - [STAN](#toc2_12_3_)    \n",
    "    - [Output comparison](#toc2_12_4_)    \n",
    "    - [Parameter recovery](#toc2_12_5_)    \n",
    "  - [Varying effects](#toc2_13_)    \n",
    "    - [Data simulation](#toc2_13_1_)    \n",
    "    - [BI](#toc2_13_2_)    \n",
    "    - [BIR](#toc2_13_3_)    \n",
    "    - [STAN](#toc2_13_4_)    \n",
    "    - [Output comparison](#toc2_13_5_)    \n",
    "    - [Parameter recovery](#toc2_13_6_)    \n",
    "  - [Gaussian Processes](#toc2_14_)    \n",
    "    - [BI](#toc2_14_1_)    \n",
    "    - [BIR](#toc2_14_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Functions](#toc0_)\n",
    "## <a id='toc1_1_'></a>[Compare distributions between BI and STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_bi_data(m):\n",
    "    #data_dict = m.sampler.get_samples()\n",
    "    data_dict = m.posteriors\n",
    "    # Initialize an empty DataFrame to collect parameters\n",
    "    all_params = []\n",
    "\n",
    "    # Loop through each array in the dictionary\n",
    "    for key, array in data_dict.items():\n",
    "        # Check the shape of the array\n",
    "        if array.ndim > 1 and array.ndim < 3:\n",
    "            # Create a DataFrame from the array and add a column for each parameter\n",
    "            param_df = pd.DataFrame(array)\n",
    "            # Rename columns to include the parameter name\n",
    "            param_df.columns = [f\"{key}_{j+1}\" for j in range(array.shape[1])]\n",
    "            all_params.append(param_df)\n",
    "    \n",
    "        elif array.ndim >= 3:# we have a matrix\n",
    "            array_shape = array.shape\n",
    "            row = array_shape[1]\n",
    "            col = array_shape[2]\n",
    "            for a in range(col):\n",
    "                for b in range(row):\n",
    "                    all_params.append(pd.DataFrame({key + '_' + str(a) +  '_' + str(b): array[:,a,b]}))\n",
    "        else:\n",
    "            # If it's a 1D array, create a single column DataFrame\n",
    "            all_params.append(pd.DataFrame({key: array}))\n",
    "\n",
    "    # Concatenate all parameter DataFrames along the rows\n",
    "    df_bi = pd.concat(all_params, axis=1)\n",
    "    return df_bi\n",
    "\n",
    "def prepare_stan_data(df):\n",
    "    columns_to_remove = ['lp__',\t'accept_stat__',\t'stepsize__',\t'treedepth__',\t'n_leapfrog__',\t'divergent__',\t'energy__']\n",
    "    d = df.drop(columns=columns_to_remove)\n",
    "    return d\n",
    "\n",
    "def combine_data(df_bi, d):\n",
    "    #df_bi = pd.DataFrame(samples)\n",
    "    params = df_bi.columns.values\n",
    "    d.columns = df_bi.columns\n",
    "\n",
    "    df_bi['method'] = 'BI'\n",
    "    d['method'] = 'STAN'\n",
    "    d_comb = pd.concat([d, df_bi], ignore_index=True)\n",
    "    return d_comb\n",
    "\n",
    "def plot_comparaison(m, df):\n",
    "\n",
    "    d = prepare_stan_data(df)\n",
    "    df_bi = prepare_bi_data(m)\n",
    "    d_comb = combine_data(df_bi, d)\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    params = df_bi.columns.values[:-1]\n",
    "    num_params = len(params)\n",
    "    num_cols = 3\n",
    "    num_rows = (num_params + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 5), sharey=True)\n",
    "    if num_rows * num_cols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    for i in d_comb.columns:\n",
    "        if i in params:\n",
    "            if a == 0:\n",
    "                axes[a].set_ylabel('Density')\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=True)\n",
    "                axes[a].spines['right'].set_visible(False)\n",
    "                a += 1\n",
    "            elif a == num_params:\n",
    "                print('found')\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=True)\n",
    "                a += 1\n",
    "            elif a < num_params:\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=False)\n",
    "                a += 1\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.0, left=0.1, right=0.9)\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    # Remove extra subplots if there are any\n",
    "    for a in range(len(params), len(axes)):\n",
    "        fig.delaxes(axes[a])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.0, left=0.1, right=0.9)\n",
    "    axes[1].set_yticks([])\n",
    "    plt.show()\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Load BI for R](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WARNING: The R package \"reticulate\" only fixed recently\n",
      "    an issue that caused a segfault when used with rpy2:\n",
      "    https://github.com/rstudio/reticulate/pull/1188\n",
      "    Make sure that you use a version of that package that includes\n",
      "    the fix.\n",
      "    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa/R/x86_64-pc-linux-gnu-library/4.3/reticulate/python/rpytools/loader.py:120: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  return _find_and_load(name, import_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using virtual environment '/home/sosa/work/BI/.venv' ...\n",
      "Requirement already satisfied: ipython in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (9.4.0)\n",
      "Requirement already satisfied: decorator in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/sosa/work/BI/.venv/lib/python3.12/site-packages (from stack_data->ipython) (0.2.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "For documentation run command :  bi.doc()\n",
       "Python package 'ipython' not found; installing now...\n",
       "+ /home/sosa/work/BI/.venv/bin/python -m pip install --upgrade --no-user ipython\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "#devtools::install_github(\"https://github.com/BGN-for-ASNA/BIR\", force = T)\n",
    "library(BI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Load BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "newPath = os.path.dirname(os.path.abspath(\"\"))\n",
    "if newPath not in sys.path:\n",
    "    sys.path.append(newPath)\n",
    "from BI import bi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "m = bi(platform='cpu')\n",
    "data_path = os.path.dirname(os.path.abspath(\"\")) + \"/BI/resources/data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Rethinking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Continuous variable: Model (model 4.3)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 336.72it/s, 3 steps of size 7.80e-01. acc. prob=0.92]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>154.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>154.19</td>\n",
       "      <td>155.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>5.78</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>5.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.87</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a  154.65  0.28    154.19     155.05\n",
       "b    5.78  0.30      5.28       6.22\n",
       "s    5.17  0.20      4.87       5.48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time as tm\n",
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'Howell1.csv', sep=';') \n",
    "m.df = m.df[m.df.age > 18]\n",
    "m.scale(['weight'])\n",
    "\n",
    "\n",
    "# define model ------------------------------------------------\n",
    "def model(weight, height):    \n",
    "    a = m.dist.normal( 178, 20, name = 'a')\n",
    "    b = m.dist.log_normal( 0, 1, name = 'b')   \n",
    "    s = m.dist.uniform( 0, 50, name = 's')\n",
    "    m.dist.normal(a + b * weight , s, obs=height)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n",
    "\n",
    "# fileter data frame\n",
    "m$df = m$df[m$df$age > 18,]\n",
    "\n",
    "# Scale\n",
    "m$scale(list('weight')) \n",
    "\n",
    "# convert data to jax arrays\n",
    "m$data_to_model(list('weight', 'height'))\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(height, weight){\n",
    "  # Parameters priors distributions\n",
    "  s = bi.dist.uniform(0, 50, name = 's')\n",
    "  a = bi.dist.normal(178, 20,  name = 'a')\n",
    "  b = bi.dist.lognormal(0, 1, name = 'b')\n",
    "  \n",
    "  # Likelihood\n",
    "  m$normal(a + b * weight, s, obs = height)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "  vector[346] height;\n",
    "  vector[346] weight;\n",
    "}\n",
    "parameters{\n",
    "  real a;\n",
    "  real<lower=0> b;\n",
    "  real<lower=0,upper=50> s;\n",
    "}\n",
    "model{\n",
    "  vector[346] mu;\n",
    "  s ~ uniform( 0 , 50 );\n",
    "  b ~ lognormal( 0 , 1 );\n",
    "  a ~ normal( 178 , 20 );\n",
    "  for ( i in 1:346 ) {\n",
    "    mu[i] = a + b* weight[i] ;\n",
    "  }\n",
    "  height ~ normal( mu , s );  \n",
    "  \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'height': m.df.height.values,\n",
    "  'weight': m.df.weight.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_5_'></a>[Parameter recovery](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model(weight, height):    \n",
    "    a = m.dist.normal( 178, 20, name = 'a')   \n",
    "    b = m.dist.lognormal(  0, 1, name = 'b')   \n",
    "    s = m.dist.uniform( 0, 50, name = 's')\n",
    "    m.dist.normal(a + b * weight , s, obs=height)\n",
    "\n",
    "def simulate_height(weight, a, b, s):    \n",
    "    weight = (weight - weight.mean())/weight.std()\n",
    "    height = m.dist.normal( a + b * weight , s, sample = True)\n",
    "    return weight, height\n",
    "\n",
    "def estimate(weight, a, b, s):\n",
    "    weight, height = simulate_height(weight,a, b, s)\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"weight\": weight, \"height\": height})\n",
    "    m.scale(['weight'])\n",
    "    m.data_to_model(['weight', 'height'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(weight, a, b, s, N, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(weight[i,:], a[i,:], b[i,:], s[i,:])\n",
    "        data = {'sim': [i,i,i], 'parameter': estimations.index.values, 'simulated' : [a[i,:][0], b[i,:][0], s[i,:][0]], 'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "N = 100\n",
    "nsim = 100\n",
    "a = m.dist.normal(178, 20, shape=(nsim, 1), sample=True)\n",
    "b = m.dist.lognormal(0, 1, shape=(nsim, 1,), sample=True, seed = 2)\n",
    "s = m.dist.uniform(0, 50, shape=(nsim, 1), sample=True)\n",
    "weight = m.dist.normal( 80, 30, sample = True, shape = (nsim, N))\n",
    "res = param_recovery(weight, a, b, s, N, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Categorical variable: Model (model 5.9)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 479.33it/s, 7 steps of size 6.13e-01. acc. prob=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.5%     94.5%     n_eff     r_hat\n",
      "      a[0]     -0.46      0.24     -0.47     -0.86     -0.12    562.99      1.00\n",
      "      a[1]      0.36      0.25      0.36     -0.08      0.72    618.42      1.00\n",
      "      a[2]      0.64      0.28      0.63      0.16      1.03    668.73      1.00\n",
      "      a[3]     -0.54      0.32     -0.54     -1.03     -0.07    496.46      1.00\n",
      "         s      0.80      0.12      0.79      0.62      0.97    422.26      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'milk.csv', sep=';') \n",
    "m.index([\"clade\"])\n",
    "m.scale(['kcal_per_g'])\n",
    "\n",
    "def model(kcal_per_g, index_clade):\n",
    "    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a')\n",
    "    s = m.dist.exponential( 1, name = 's')    \n",
    "    mu = a[index_clade]\n",
    "    m.dist.normal(mu, s, obs=kcal_per_g)\n",
    "\n",
    "m.data_to_model(['kcal_per_g', \"index_clade\"])\n",
    "m.fit(model) \n",
    "m.sampler.print_summary(0.89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\n",
    "m$scale(list('kcal.per.g')) # Manipulate\n",
    "m$index(list('clade')) # Scale\n",
    "m$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(kcal_per_g, index_clade){\n",
    "  # Parameters priors distributions\n",
    "  beta =  bi.dist.normal( 0, 0.5, name = 'beta', shape=c(4))\n",
    "  sigma = bi.dist.exponential(1, name = 's')\n",
    "  # Likelihood\n",
    "  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distributionslibrary(BI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[29] K;\n",
    "    array[29] int clade_id;\n",
    "}\n",
    "parameters{\n",
    "    vector[4] a;\n",
    "    real<lower=0> s;\n",
    "}\n",
    "model{\n",
    "    vector[29] mu;\n",
    "    s ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 0.5 );\n",
    "    for ( i in 1:29 ) {\n",
    "        mu[i] = a[clade_id[i]];\n",
    "    }\n",
    "    K ~ normal( mu , s );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'clade_id': m.df.index_clade.values+1,\n",
    "  'K': m.df.kcal_per_g.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[Output comparaison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "import jax.numpy as jnp\n",
    "def model(kcal_per_g, index_clade):\n",
    "    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a')\n",
    "    s = m.dist.exponential( 1, shape = (1,), name = 's')    \n",
    "    tmp = a[index_clade]\n",
    "    m.dist.normal(tmp, s, obs=kcal_per_g)\n",
    "    \n",
    "def simulate_data(a, sigma, N):\n",
    "    index_clade = m.dist.categorical(probs=jnp.array([0.25,0.25,.25,.25]), shape = (N,), sample=True).astype(int) # Generate clade index\n",
    "    tmp = a[index_clade] # Generate mean of each clade\n",
    "    cal = m.dist.normal(tmp, sigma, sample = True) # Generate calories based on mean and std\n",
    "    return cal, index_clade\n",
    "    \n",
    "def estimate(alpha, sigma, N):\n",
    "    cal, index_clade = simulate_data(alpha, sigma, N) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"kcal_per_g\": cal, \"index_clade\": index_clade})\n",
    "    #m.scale(['kcal_per_g'])\n",
    "    m.data_to_model(['kcal_per_g', 'index_clade'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(a, sigma, N, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i], sigma[i], N)\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([jnp.array(a[i]), jnp.array([sigma[i][0]])]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "N = 30\n",
    "Ngrp = 4\n",
    "nsim = 100\n",
    "a = m.dist.normal(0, 1, shape=(nsim, Ngrp,), sample=True, seed = 0)\n",
    "sigma = m.dist.halfcauchy(1, shape=(nsim, 1,), sample=True, seed = 10)\n",
    "\n",
    "result = param_recovery(a,sigma, N = N, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Continuous interactions terms (model 8.3)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 475.14it/s, 7 steps of size 6.60e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.5%     94.5%     n_eff     r_hat\n",
      "         a      0.09      0.10      0.09     -0.07      0.26    470.31      1.00\n",
      "        bs     -0.31      0.12     -0.32     -0.49     -0.11    535.87      1.00\n",
      "        bw      0.56      0.11      0.57      0.38      0.72    430.84      1.00\n",
      "       bws     -0.32      0.11     -0.32     -0.50     -0.16    460.55      1.00\n",
      "     sigma      0.58      0.10      0.57      0.43      0.73    256.89      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'tulips.csv', sep=';') \n",
    "m.scale(['blooms', 'water', 'shade'])\n",
    "\n",
    "# define model ------------------------------------------------\n",
    "def model(blooms,shade, water):\n",
    "    sigma = m.dist.exponential(1, name = 'sigma')\n",
    "    bws = m.dist.normal(0, 0.25, name = 'bws')\n",
    "    bs = m.dist.normal(0, 0.25, name = 'bs')\n",
    "    bw = m.dist.normal(0, 0.25, name = 'bw')\n",
    "    a = m.dist.normal(0.5, 0.25, name = 'a')\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    m.dist.normal(mu, sigma, obs=blooms)\n",
    "\n",
    "# Run sampler ------------------------------------------------ \n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.sampler.print_summary(0.89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\n",
    "m$scale(list('blooms', 'water', 'shade')) # Scale\n",
    "m$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(blooms, water,shade){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n",
    "  bw = bi.dist.normal( 0,  0.25, name = 'bw')\n",
    "  bs = bi.dist.normal(  0,  0.25, name = 'bs')   \n",
    "  bws = bi.dist.normal(  0, 0.25, name = 'bws') \n",
    "  sigma = bi.dist.exponential(1, name = 's')\n",
    "  # Likelihood\n",
    "  m$normal(alpha + bw*water + bs*shade + bws*water*shade, sigma, obs=blooms)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[27] blooms_std;\n",
    "    array[27] int shade_cent;\n",
    "    array[27] int water_cent;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real bs;\n",
    "    real bw;    \n",
    "    real bws;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[27] mu;\n",
    "    sigma ~ exponential( 1 );\n",
    "    bws ~ normal( 0 , 0.25 );\n",
    "    bs ~ normal( 0 , 0.25 );\n",
    "    bw ~ normal( 0 , 0.25 );\n",
    "    a ~ normal( 0.5 , 0.25 );\n",
    "    for ( i in 1:27 ) {\n",
    "        mu[i] = a + bw * water_cent[i] + bs * shade_cent[i] + bws * water_cent[i] * shade_cent[i];\n",
    "    }\n",
    "\n",
    "    \n",
    "    blooms_std ~ normal( mu , sigma );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'blooms_std' : m.df[\"blooms\"].values,\n",
    "    \"water_cent\": m.df[\"water\"].values.astype(int),\n",
    "    \"shade_cent\": m.df[\"shade\"].values.astype(int),\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "\n",
    "def model(blooms,shade, water):\n",
    "    sigma = m.dist.exponential(1, name = 'sigma')\n",
    "    bws = m.dist.normal(0, 0.25, name = 'bws')\n",
    "    bs = m.dist.normal(0, 0.25, name = 'bs')\n",
    "    bw = m.dist.normal(0, 0.25, name = 'bw')\n",
    "    a = m.dist.normal(0.5, 0.25, name = 'a')\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    m.dist.normal(mu, sigma, obs=blooms)\n",
    "\n",
    "def simulate_bloom(water, shade, sigma, bws, bs, bw, a ):\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    return  m.dist.normal(mu, sigma, sample=True) # bloom\n",
    "    \n",
    "def estimate(water, shade, sigma,bws, bs, bw, a ):\n",
    "    blooms = simulate_bloom(water, shade, sigma,bws, bs, bw, a ) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"water\": water, \"shade\": shade, \"blooms\": blooms})\n",
    "    #m.scale(['blooms', 'shade', 'blooms'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(water, shade, sigma, bws, bs, bw, a, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(water, shade, sigma[i], bws[i], bs[i], bw[i], a[i] )\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], bs[i], bw[i], bws[i], sigma[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "# Shade and water are all possible conbinations of shade (1 to 3) and water (1 to 3)\n",
    "m.data(data_path + 'tulips.csv', sep=';') \n",
    "shade = m.df.shade.values\n",
    "water = m.df.water.values\n",
    "sigma = m.dist.exponential(1, name = 'sigma', shape = (nsim, 1,), sample = True)\n",
    "bws = m.dist.normal(0, 0.25, name = 'bws', shape = (nsim, 1,), sample = True, seed = 1)\n",
    "bs = m.dist.normal(0, 0.25, name = 'bs', shape = (nsim, 1,), sample = True, seed = 2)\n",
    "bw = m.dist.normal(0, 0.25, name = 'bw', shape = (nsim, 1,), sample = True, seed = 3)\n",
    "a = m.dist.normal(0.5, 0.25, name = 'a', shape = (nsim, 1,), sample = True, seed = 4)\n",
    "\n",
    "result = param_recovery(water, shade, sigma,bws, bs, bw, a, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Binomial (model 11.1)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:01<00:00, 634.73it/s, 3 steps of size 9.99e-01. acc. prob=0.90]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0]  0.32  0.09      0.18       0.46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.data_to_model(['pulled_left'])\n",
    "def model(pulled_left):\n",
    "    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n",
    "    m.dist.binomial(logits=a[0], obs=pulled_left)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\n",
    "m$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(pulled_left){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal( 0, 10, name = 'alpha', shape=c(1))\n",
    "  # Likelihood\n",
    "  m$binomial(logits = alpha, obs=pulled_left)\n",
    "}\n",
    "\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    a ~ normal( 0 , 10 );\n",
    "    p = a;\n",
    "    p = inv_logit(p);\n",
    "    pulled_left ~ binomial( 1 , p );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : m.df[\"pulled_left\"].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(pulled_left):\n",
    "    a = m.dist.normal( 0, 10,shape = (1,), name = 'a')\n",
    "    m.dist.binomial(logits=a, obs=pulled_left)\n",
    "\n",
    "def sim_pulled_left(a):\n",
    "    return m.dist.binomial(logits=a, sample=True, shape=(1000,))\n",
    "\n",
    "def estimate(a):\n",
    "    pulled_left = sim_pulled_left(a)[:,0] # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"pulled_left\": pulled_left})\n",
    "    #m.scale(['blooms', 'shade', 'blooms'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(a, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : a[i], \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "a = m.dist.normal( 0, 1, shape = (nsim, 1), sample=True)\n",
    "result = param_recovery(a, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Binomial with indices (model 11.4)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 438.16it/s, 7 steps of size 4.51e-01. acc. prob=0.90]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>3.93</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[2]</th>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[3]</th>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[4]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[5]</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[6]</th>\n",
       "      <td>2.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[1]</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[2]</th>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[3]</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0] -0.41  0.32     -0.91       0.07\n",
       "a[1]  3.93  0.75      2.84       5.25\n",
       "a[2] -0.71  0.32     -1.20      -0.22\n",
       "a[3] -0.71  0.32     -1.25      -0.20\n",
       "a[4] -0.41  0.32     -0.88       0.14\n",
       "a[5]  0.53  0.33      0.02       1.02\n",
       "a[6]  2.03  0.42      1.34       2.64\n",
       "b[0] -0.08  0.28     -0.50       0.39\n",
       "b[1]  0.44  0.27     -0.01       0.84\n",
       "b[2] -0.43  0.28     -0.89       0.00\n",
       "b[3]  0.33  0.26     -0.08       0.77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\n",
    "m.df['actor'] = m.df['actor'] - 1\n",
    "\n",
    "m.data_to_model(['actor', 'treatment', 'pulled_left'])\n",
    "\n",
    "def model(actor, treatment, pulled_left):\n",
    "    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n",
    "    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n",
    "    p = a[actor] + b[treatment]\n",
    "    m.dist.binomial(1, logits=p, obs=pulled_left)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\n",
    "m$df$treatment =  m$df$prosoc_left + 2 * m$df$condition\n",
    "m$df$actor = m$df$actor - 1\n",
    "keys <- c(\"actor\", \"treatment\", 'pulled_left')\n",
    "values <- list(jnp$array(as.integer(m$df$actor)),jnp$array(as.integer(m$df$treatment)), jnp$array(as.integer(m$df$pulled_left)))\n",
    "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(actor, treatment, pulled_left){\n",
    "  # Parameters priors distributions\n",
    "  a = bi.dist.normal( 0, 1.5, shape = c(7), name = 'a') # 7 actors\n",
    "  b = bi.dist.normal( 0, 0.5, shape = c(4), name = 'b') # 4 treatments\n",
    "  p = a[actor] + b[treatment]\n",
    "  # Likelihood\n",
    "  m$binomial(1,logits =  p, obs=pulled_left)\n",
    "}\n",
    "\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "    array[504] int treatment;\n",
    "    array[504] int actor;\n",
    "}\n",
    "parameters{\n",
    "    vector[7] a;\n",
    "    vector[4] b;\n",
    "}\n",
    "model{\n",
    "    vector[504] p;\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    b ~ normal( 0 , 0.5 );    \n",
    "    for ( i in 1:504 ) {\n",
    "        p[i] = a[actor[i]] + b[treatment[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    pulled_left ~ binomial( 1 , p );\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : m.df[\"pulled_left\"].values.astype(int),\n",
    "    'treatment' : m.df[\"treatment\"].values.astype(int) + 1,\n",
    "    'actor' : m.df[\"actor\"].values.astype(int) +1 \n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\n",
    "m.df['actor'] = m.df['actor'] - 1\n",
    "\n",
    "def model(actor, treatment, pulled_left):\n",
    "    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n",
    "    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n",
    "    p = a[actor] + b[treatment]\n",
    "    m.dist.binomial(1, logits=p, obs=pulled_left)\n",
    "\n",
    "def sim_pulled_left(actor, treatment, a, b):\n",
    "    p = a[actor] + b[treatment]\n",
    "    return m.dist.binomial(1, logits=p, sample=True)\n",
    "\n",
    "def estimate(actor, treatment, a, b):\n",
    "    pulled_left = sim_pulled_left(actor, treatment, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"pulled_left\": pulled_left, \"actor\": actor, \"treatment\": treatment})\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(actor, treatment, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(actor, treatment, a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "actor = jnp.array(m.df['actor'])\n",
    "treatment = jnp.array(m.df['treatment'])\n",
    "a = m.dist.normal(0, 1.5, shape = (nsim, 7), name='a', sample=True)\n",
    "b = m.dist.normal(0, 0.5, shape = (nsim, 4), name='b', sample=True)\n",
    "\n",
    "result = param_recovery(actor, treatment, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[Poisson (model 11.10)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:01<00:00, 507.07it/s, 3 steps of size 6.07e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>3.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[1]</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0]  3.21  0.10      3.06       3.37\n",
       "a[1]  3.63  0.09      3.49       3.79\n",
       "b[0]  0.36  0.05      0.28       0.44\n",
       "b[1]  0.06  0.20     -0.27       0.36"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'Kline.csv', sep=';') \n",
    "m.scale(['population'])\n",
    "m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "#m.data_to_model(['total_tools', 'population', 'cid'])\n",
    "def model(cid, population, total_tools):\n",
    "    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n",
    "    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    m.dist.poisson(l, obs=total_tools)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Kline.csv\", sep = ''), sep=';')\n",
    "m$scale(list('population'))# Scale\n",
    "m$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\n",
    "m$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(total_tools, population, cid){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n",
    "  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n",
    "  l = jnp$exp(alpha[cid] + beta[cid]*population)\n",
    "  # Likelihood\n",
    "  m$poisson(l, obs=total_tools)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[10] int T;\n",
    "    vector[10] P;\n",
    "    array[10] int cid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    vector[2] b;\n",
    "}\n",
    "model{\n",
    "    vector[10] lambda;\n",
    "    b ~ normal( 0 , 0.2 );\n",
    "    a ~ normal( 3 , 0.5 );\n",
    "    for ( i in 1:10 ) {\n",
    "       lambda[i] = a[cid[i]] + b[cid[i]] * P[i];\n",
    "       lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    T ~ poisson( lambda );\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'T' : m.df[\"total_tools\"].values.astype(int),\n",
    "    'P' : m.df[\"population\"].values.astype(float),\n",
    "    'cid' : m.df[\"cid\"].values.astype(int) +1\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(cid, population, total_tools):\n",
    "    a = m.dist.normal(3, 0.5, shape = (2,), name='a')\n",
    "    b = m.dist.normal(0, 0.2, shape = (2,), name='b')\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    m.dist.poisson(l, obs=total_tools)\n",
    "\n",
    "def sim_total_tools(cid, population, a, b):\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    return m.dist.poisson(l, sample=True)\n",
    "\n",
    "def estimate(a, b):\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'Kline.csv', sep=';') \n",
    "    m.scale(['population'])\n",
    "    m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "\n",
    "    total_tools = sim_total_tools(m.df.cid.values, m.df.population.values, a, b) # Simulate data\n",
    "    m.df = pd.DataFrame({\"cid\": m.df.cid.values, 'population' :m.df.population.values, \"total_tools\": total_tools})\n",
    "    m.fit(model, num_samples=1000, num_warmup = 1000, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(cid, population, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "m.data(data_path + 'Kline.csv', sep=';') \n",
    "m.scale(['population'])\n",
    "m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "cid = m.df.cid.values\n",
    "population = m.df.population.values\n",
    "a = m.dist.normal(3, 0.5, shape= (nsim, 2,), name='a', sample=True, seed = 1)\n",
    "b = m.dist.normal(0, 0.2, shape= (nsim, 2,), name='b', sample=True, seed = 3)\n",
    "result = param_recovery(cid, population, a, b, nsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dist.normal(0, 0.2, shape= (100, 2,), name='b', sample=True, seed = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[Negative binomial (model 11.12)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_1_'></a>[Simulated data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "import pandas as pd\n",
    "import random as random2\n",
    "import jax\n",
    "init_key, sample_key = jax.random.split(jax.random.PRNGKey(int(random2.randint(0, 10000000))))\n",
    "init_key = jnp.array(init_key)\n",
    "num_days = 3000\n",
    "y = tfd.Poisson(rate=1.5).sample(seed = init_key, sample_shape=(num_days,))\n",
    "num_weeks = 400\n",
    "y_new = tfd.Poisson(rate=0.5 * 7).sample(seed = init_key, sample_shape=(num_weeks,))\n",
    "y_all = np.concatenate([y, y_new])\n",
    "exposure = np.concatenate([np.repeat(1, num_days), np.repeat(7, num_weeks)])\n",
    "monastery = np.concatenate([np.repeat(0, num_days), np.repeat(1, num_weeks)])\n",
    "d = pd.DataFrame.from_dict(dict(y=y_all, days=exposure, monastery=monastery))\n",
    "d[\"log_days\"] = d.days.pipe(np.log)\n",
    "d.to_csv(data_path + 'Sim dat Gamma poisson.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2000/2000 [00:03<00:00, 526.93it/s, 3 steps of size 7.88e-01. acc. prob=0.89]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-1.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a  0.41  0.02      0.38       0.43\n",
       "b -1.11  0.03     -1.16      -1.06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim dat Gamma poisson.csv', sep=',') \n",
    "m.data_on_model={'y': jnp.array(m.df.y.values, dtype=int), 'log_days': jnp.array(m.df.log_days.values, dtype=float), 'monastery': jnp.array(m.df.monastery.values, dtype=int)}\n",
    "def model(log_days, monastery, y):\n",
    "    a = m.dist.normal(0, 1, name = 'a')\n",
    "    b = m.dist.normal(0, 1, name = 'b')\n",
    "    l = jnp.exp(log_days + a + b * monastery)\n",
    "    m.dist.poisson(rate = l, is_sparse=True, obs=y)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_warmup = 1000, num_samples=1000) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\n",
    "m$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(log_days, monastery, y){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n",
    "  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n",
    "  l = jnp$exp(log_days + alpha + beta * monastery)\n",
    "  # Likelihood\n",
    "  m$poisson(rate=l, obs=y)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[3400] int y;\n",
    "    array[3400] int monastery;\n",
    "    vector[3400] log_days;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real b;\n",
    "}\n",
    "model{\n",
    "    vector[3400] lambda;\n",
    "    b ~ normal( 0 , 1 );\n",
    "    a ~ normal( 0 , 1 );\n",
    "    for ( i in 1:3400 ) {\n",
    "        lambda[i] = log_days[i] + a + b * monastery[i];\n",
    "        // B1 ~ exponential( 1 );\n",
    "        // gamma(lambda[i]*B1, B1);\n",
    "        lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    \n",
    "    y ~ poisson( lambda );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'y' : np.array(m.data_on_model[\"y\"].astype(int)),\n",
    "    'monastery' : np.array(m.data_on_model[\"monastery\"].astype(int)) +1,\n",
    "    'log_days' : np.array(m.data_on_model[\"log_days\"].astype(float)),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=1000, num_warmup = 1000)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_5_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim dat Gamma poisson.csv', sep=',') \n",
    "import random as random2\n",
    "def model(log_days, monastery, y):\n",
    "    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n",
    "    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n",
    "    l = jnp.exp(log_days + a + b * monastery)\n",
    "    m.dist.poisson(rate = l, obs=y)\n",
    "\n",
    "\n",
    "def sim_rates(log_days, monastery, a, b):\n",
    "    l = jnp.exp(log_days + a +  b * monastery)\n",
    "    return m.dist.poisson(rate = l, sample=True)\n",
    "\n",
    "def estimate(log_days, monastery, a, b):\n",
    "    rates = sim_rates(log_days, monastery, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"log_days\": log_days, \"monastery\": monastery, 'y': rates})\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(log_days, monastery, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(log_days, monastery, a[i], b[i] )\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "log_days = jnp.array(m.df.log_days.values)\n",
    "monastery = jnp.array(m.df.monastery.values)\n",
    "a = m.dist.normal(0, 1, name = 'a', sample=True, shape=(nsim, 1))\n",
    "b = m.dist.normal(0, 1, name = 'b', sample=True, shape=(nsim, 1))\n",
    "\n",
    "result = param_recovery(log_days, monastery, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[Multinomial (model 11.13)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_1_'></a>[Simulated data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate career choices among 500 individuals\n",
    "N = 500  # number of individuals\n",
    "income = jnp.array([1, 2, 5])  # expected income of each career\n",
    "score = 0.5 * income  # scores for each career, based on income\n",
    "# next line converts scores to probabilities\n",
    "p = jax.nn.softmax(score)\n",
    "\n",
    "# now simulate choice\n",
    "# outcome career holds event type values, not counts\n",
    "career = jnp.repeat(jnp.nan, N)  # empty vector of choices for each individual\n",
    "# sample chosen career for each individual\n",
    "for i in range(N):\n",
    "    career = career.at[i].set(\n",
    "        m.dist.categorical(probs=p, sample=True,seed=i)\n",
    "    )\n",
    "career = career.astype(jnp.int32)\n",
    "data = {'career': career, 'income': [income[index] for index in career]}\n",
    "d = pd.DataFrame(data)\n",
    "d.to_csv(data_path + 'Sim data multinomial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 399.98it/s, 7 steps of size 2.57e-01. acc. prob=0.84]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>-2.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0] -2.13  0.26     -2.49      -1.72\n",
       "a[1] -1.59  0.16     -1.83      -1.32\n",
       "b[0]  0.06  0.05      0.00       0.13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "m = bi('cpu')\n",
    "df=pd.read_csv(data_path + 'Sim data multinomial.csv')\n",
    "m.data_on_model={}\n",
    "m.data_on_model['career']=jnp.array(df.career.values)\n",
    "m.data_on_model['income']=jnp.array(df.income.unique()).astype(jnp.int32)\n",
    "def model(career, income ):\n",
    "    a = m.dist.normal(0, 1, shape= (2,), name = 'a')\n",
    "    b = m.dist.halfnormal(0.5,  shape= (1,),name = 'b')\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    m.dist.categorical(probs =  p, obs=career)\n",
    "\n",
    "# Run sampler ------------------------------------------------ \n",
    "m.fit(model)  \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim data multinomial.csv\", sep = ''), sep=',')\n",
    "keys <- c(\"income\", \"career\")\n",
    "income = unique(m$df$income)\n",
    "income = income[order(income)]\n",
    "values <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\n",
    "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(income, career){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n",
    "  beta = bi.dist.halfnormal(0.5, name='beta')\n",
    "  \n",
    "  s_1 = alpha[0] + beta * income[0]\n",
    "  s_2 = alpha[1] + beta * income[1]\n",
    "  s_3 = 0 # reference category\n",
    "\n",
    "  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n",
    "\n",
    "  # Likelihood\n",
    "  m$categorical(probs=p, obs=career)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    int N; // number of individuals\n",
    "    int K; // number of possible careers\n",
    "    array[N] int career; // outcome\n",
    "    vector[K] career_income;\n",
    "}\n",
    "parameters{\n",
    "    vector[K-1] a; // intercepts\n",
    "    real<lower=0> b; // association of income with choice\n",
    "}\n",
    "model{\n",
    "    vector[K] p;\n",
    "    vector[K] s;\n",
    "    a ~ normal( 0 , 1 );\n",
    "    b ~ normal( 0 , 0.5 );\n",
    "    s[1] = a[1] + b*career_income[1];\n",
    "    s[2] = a[2] + b*career_income[2];\n",
    "    s[3] = 0; // pivot\n",
    "    p = softmax( s );\n",
    "    career ~ categorical( p );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'N' : 500,\n",
    "    'K' : 3,\n",
    "    'career' : df[\"career\"].values.astype(int) + 1,\n",
    "    'career_income' : df[\"income\"].unique().astype(int).tolist(),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_5_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multinomial.csv', sep=',') \n",
    "\n",
    "def model(career, income ):\n",
    "    a = m.dist.normal(0, 1, shape= (2,), name = 'a')\n",
    "    b = m.dist.halfnormal(0.5,  shape= (1,), name = 'b')\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    m.dist.categorical(probs = p, obs = career)\n",
    "    \n",
    "def sim_categories(income, career, a, b):\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    return m.dist.categorical(probs =  p, sample=True, shape=(len(career),))\n",
    "\n",
    "def estimate(income, career, a, b):\n",
    "    career = sim_categories(income, career, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = {}\n",
    "    m.data_on_model['income'] = income\n",
    "    m.data_on_model['career'] = career\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(income, career, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(income, career,a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multinomial.csv', sep=',') \n",
    "nsim = 100\n",
    "income = jnp.unique(m.df.income.values)\n",
    "career = jnp.array(m.df.career.values)\n",
    "a = m.dist.normal(0, 1, shape= (nsim,2), name = 'a', sample=True)\n",
    "b = m.dist.halfnormal(0.5, shape=(nsim,1), name = 'b', sample=True)\n",
    "result = param_recovery(income, career, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_9_'></a>[Beta binomial (model m12.1)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:03<00:00, 323.84it/s, 3 steps of size 6.16e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta</th>\n",
       "      <td>3.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.01</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean    sd  hdi_5.5%  hdi_94.5%\n",
       "alpha[0] -0.45  0.41     -1.07       0.22\n",
       "alpha[1] -0.33  0.44     -0.98       0.40\n",
       "phi       1.02  0.75      0.01       2.05\n",
       "theta     3.02  0.75      2.01       4.05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpyro\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "\n",
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1,  name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta =  numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\n",
    "m$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\n",
    "m$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ----------------------c--------------------------\n",
    "model <- function(gid, applications, admit){\n",
    "  # Parameters priors distributions\n",
    "  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n",
    "  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n",
    "  t = phi + 2\n",
    "  pbar = jax$nn$sigmoid(alpha[gid])\n",
    "  gamma = pbar * t\n",
    "  eta = (1 - pbar) * t\n",
    "  # Likelihood\n",
    "  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[12] int N;\n",
    "    array[12] int A;\n",
    "    array[12] int gid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    real<lower=0> phi;\n",
    "}\n",
    "transformed parameters{\n",
    "    real theta;\n",
    "    theta = phi + 2;\n",
    "}\n",
    "model{\n",
    "    vector[12] pbar;\n",
    "    phi ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    for ( i in 1:12 ) {\n",
    "        pbar[i] = a[gid[i]];\n",
    "        pbar[i] = inv_logit(pbar[i]);\n",
    "    }\n",
    "    A ~ beta_binomial( N , pbar*theta , (1-pbar)*theta );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'A' : m.df[\"admit\"].values.astype(int),\n",
    "    'N' : m.df[\"applications\"].values.astype(int),\n",
    "    'gid' : m.df[\"gid\"].values.astype(int) +1,\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1, shape=(1,), name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta = numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "nsim = 10\n",
    "m = bi()\n",
    "nsim = 10\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "gid = m.df.gid.values\n",
    "phi = m.dist.exponential(1,  shape = (nsim,),sample=True)\n",
    "alpha = m.dist.normal( 0., 1.5, shape=(nsim,2),sample=True)\n",
    "theta = phi + 2\n",
    "pbar = jax.vmap(lambda x: jax.nn.sigmoid(x[gid]))(alpha)\n",
    "pbar.shape\n",
    "i = 0\n",
    "applications = jnp.array(m.df['applications'].values)\n",
    "concentration1 = pbar[i]*theta[i]\n",
    "concentration0 = (1 - pbar[i]) * theta[i]\n",
    "admit = m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0,sample=True)\n",
    "m = bi()\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "m.df['admit'] = admit\n",
    "m.data_to_model(['gid', 'applications', 'admit' ])\n",
    "print(m.data_on_model)\n",
    "m.fit(model, num_samples=500, progress_bar=False) \n",
    "s = m.summary()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1,  name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta = numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "def sim_admit(theta, pbar, applications):\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "    return m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0,sample=True)\n",
    "\n",
    "\n",
    "def estimate(theta, pbar, applications):\n",
    "    admit = sim_admit(theta, pbar, applications) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "    m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "    m.df['admit'] = admit\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(phi,alpha, theta, pbar, applications, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(theta[i], pbar[i], applications)\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([alpha[i,][0][None], alpha[i,][1][None],phi[i][None],theta[i][None]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "gid=jnp.array(m.df.gid.values)\n",
    "applications=jnp.array(m.df.applications.values)\n",
    "\n",
    "\n",
    "phi = m.dist.exponential(1,  shape = (nsim,),sample=True)\n",
    "alpha = m.dist.normal( 0., 1.5, shape=(nsim,2),sample=True)\n",
    "theta = phi + 2\n",
    "pbar = jax.vmap(lambda x: jax.nn.sigmoid(x[gid]))(alpha)\n",
    "param_recovery(phi,alpha, theta, pbar, applications, nsim) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_10_'></a>[Zero inflated outcomes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 433.95it/s, 7 steps of size 5.16e-01. acc. prob=0.92]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>al</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>-1.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean    sd  hdi_5.5%  hdi_94.5%\n",
       "al  0.11  0.08     -0.03       0.24\n",
       "ap -1.37  0.37     -1.86      -0.79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.scipy.special import expit\n",
    "import random as r\n",
    "r.seed(42)\n",
    "# Define parameters\n",
    "prob_drink = 0.2  # 20% of days\n",
    "rate_work = 1     # average 1 manuscript per day\n",
    "\n",
    "# sample one year of production\n",
    "N = 365\n",
    "\n",
    "np.random.seed(365)\n",
    "drink = np.random.binomial(1, prob_drink, N)\n",
    "y = (1 - drink) * np.random.poisson(rate_work, N)\n",
    "d = pd.DataFrame(y)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m  bi()\n",
    "# import data ------------------------------------------------\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    y = jnp.array(y)\n",
    ")\n",
    "\n",
    "def model(y):\n",
    "    ap = m.dist.normal( -1.5, 1,  name = 'ap')\n",
    "    p = expit(ap)\n",
    "\n",
    "    al = m.dist.normal( 1, 0.5,  name = 'al')\n",
    "    lambda_ = jnp.exp(al)    \n",
    "    \n",
    "    m.dist.zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Simulate data ------------------------------------------------\n",
    "prob_drink = 0.2  # 20% of days\n",
    "rate_work = 1     # average 1 manuscript per day\n",
    "# sample one year of production\n",
    "N = as.integer(365)\n",
    "drink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\n",
    "y = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\n",
    "data = list()\n",
    "data$y = y \n",
    "m$data_on_model = data\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(y){\n",
    "  al =  bi.dist.normal(0, 5, name='al', shape=c(1))\n",
    "  ap = bi.dist.normal(0, 5, name='ap', shape=c(1))\n",
    "  p = jax$scipy$special$expit(ap)\n",
    "  lambda_ = jnp$exp(al)\n",
    "  m$zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[365] int y;\n",
    "}\n",
    "parameters{\n",
    "    real al;\n",
    "    real ap;\n",
    "    \n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    real lambda;\n",
    "    al ~ normal( 1 , 0.5 );\n",
    "    ap ~ normal( -1.5 , 1 );   \n",
    "    \n",
    "    lambda = al;\n",
    "    lambda = exp(lambda);\n",
    "    p = ap;\n",
    "    p = inv_logit(p);\n",
    "    for ( i in 1:365 ) {\n",
    "        if ( y[i]==0 )\n",
    "            target += log_mix( p , 0 , poisson_lpmf(0|lambda) );\n",
    "        if ( y[i] > 0 )\n",
    "            target += log1m( p ) + poisson_lpmf(y[i] | lambda );\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'y' :d.iloc[:,0].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "def model(y):\n",
    "    ap = m.dist.normal( -1.5, 1,  name = 'ap')\n",
    "    p = expit(ap)\n",
    "\n",
    "    al = m.dist.normal( 1, 0.5,  name = 'al')\n",
    "    lambda_ = jnp.exp(al)    \n",
    "    \n",
    "    m.dist.zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "\n",
    "def sim_prod(prob_drink, rate_work ):\n",
    "    drink = m.dist.binomial(1, prob_drink, shape=(365,),sample=True)\n",
    "    y = (1 - drink) *  m.dist.poisson(rate_work, shape=(365,),sample=True)\n",
    "    return drink, y\n",
    "    \n",
    "def estimate(prob_drink, rate_work):\n",
    "    drink, y = sim_prod(prob_drink, rate_work) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = dict(y = y)\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(prob_drink, rate_work,  nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(prob_drink[i], rate_work[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([rate_work[i][None],prob_drink[i][None]]), \n",
    "                'estimations': estimations.values}\n",
    "        data = pd.DataFrame(data)\n",
    "        # Converting parameters to there oringinal scale\n",
    "        data.loc[data['parameter'].isin(['al']), 'estimations'] = jnp.exp(\n",
    "            data.loc[data['parameter'].isin(['al']), 'estimations'].values\n",
    "        )\n",
    "        data.loc[data['parameter'].isin(['ap']), 'estimations'] = expit(\n",
    "            data.loc[data['parameter'].isin(['ap']), 'estimations'].values\n",
    "        )\n",
    "        df = pd.concat([df, data], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "\n",
    "prob_drink = m.link.inv_logit(m.dist.normal(-1.5, 1, shape=(nsim,), sample=True, seed = 1))\n",
    "rate_work = jnp.exp(m.dist.normal(1, 0.5, shape=(nsim,), sample=True, seed = 10))\n",
    "tmp = param_recovery(prob_drink, rate_work, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_11_'></a>[OrderedLogistic (Todo: PB)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpyro.distributions as dist\n",
    "## setup platform------------------------------------------------\n",
    "#m = bi()\n",
    "## import data ------------------------------------------------\n",
    "#m.data('resources/data/Trolley.csv', sep=';') \n",
    "#d = m.df\n",
    "## discrete proportion of each response value\n",
    "#pr_k = d.response.value_counts().sort_index().values / d.shape[0]\n",
    "## cumsum converts to cumulative proportions\n",
    "#cum_pr_k = jnp.cumsum(pr_k, -1)\n",
    "#logit = lambda x: jnp.log(x / (1 - x))  # convenience function\n",
    "#lco = logit(cum_pr_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpyro.distributions as distnp\n",
    "#from numpyro.distributions.transforms import OrderedTransform\n",
    "## setup platform------------------------------------------------\n",
    "#m = bi()\n",
    "#m.data_on_model = dict(response = jnp.array(d.response.values - 1))\n",
    "#def model(response):\n",
    "#    cutpoints = numpyro.sample(\n",
    "#        \n",
    "#        distnp.TransformedDistribution(\"cutpoints\",\n",
    "#            distnp.Normal(0, 1.5), OrderedTransform()\n",
    "#        ),\n",
    "#    )\n",
    "#    numpyro.sample(\"R\", dist.OrderedLogistic(0, cutpoints), obs=response)\n",
    "#\n",
    "## Run sampler ------------------------------------------------\n",
    "#start = tm.time()    \n",
    "#m.fit(model) \n",
    "#end = tm.time()    \n",
    "#print(f\"BI took: {end - start:.4f} seconds\")\n",
    "#\n",
    "## Diagnostic ------------------------------------------------\n",
    "#m.sampler.print_summary(0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_12_'></a>[Varying interceps](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:03<00:00, 259.12it/s, 15 steps of size 4.16e-01. acc. prob=0.89]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_bar</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>3.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.31</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[3]</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.40</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[4]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[5]</th>\n",
       "      <td>2.18</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[6]</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[7]</th>\n",
       "      <td>2.15</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[8]</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[9]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[10]</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[11]</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[12]</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[13]</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[14]</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[15]</th>\n",
       "      <td>2.14</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[16]</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.69</td>\n",
       "      <td>4.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[17]</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[18]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[19]</th>\n",
       "      <td>3.68</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[20]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[21]</th>\n",
       "      <td>2.45</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[22]</th>\n",
       "      <td>2.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[23]</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[24]</th>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[25]</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[26]</th>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[27]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[28]</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[29]</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[30]</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[31]</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[32]</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.88</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[33]</th>\n",
       "      <td>2.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[34]</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[35]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[36]</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[37]</th>\n",
       "      <td>3.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.37</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[38]</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[39]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.56</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[40]</th>\n",
       "      <td>-1.81</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[41]</th>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[42]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[43]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[44]</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[45]</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[46]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[47]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a_bar      1.35  0.24      0.97       1.72\n",
       "alpha[0]   2.05  0.85      0.65       3.31\n",
       "alpha[1]   3.13  1.13      1.31       4.81\n",
       "alpha[2]   0.99  0.62     -0.10       1.80\n",
       "alpha[3]   3.07  1.09      1.40       4.78\n",
       "alpha[4]   2.19  0.91      0.55       3.38\n",
       "alpha[5]   2.18  0.86      0.81       3.39\n",
       "alpha[6]   3.12  1.13      1.44       4.90\n",
       "alpha[7]   2.15  0.84      0.68       3.27\n",
       "alpha[8]  -0.17  0.60     -1.19       0.63\n",
       "alpha[9]   2.04  0.83      0.86       3.44\n",
       "alpha[10]  1.04  0.72     -0.13       2.09\n",
       "alpha[11]  0.56  0.61     -0.46       1.51\n",
       "alpha[12]  0.99  0.72     -0.08       2.08\n",
       "alpha[13]  0.23  0.62     -0.71       1.25\n",
       "alpha[14]  2.09  0.84      0.75       3.32\n",
       "alpha[15]  2.14  0.94      0.65       3.41\n",
       "alpha[16]  2.90  0.79      1.69       4.11\n",
       "alpha[17]  2.44  0.66      1.40       3.47\n",
       "alpha[18]  2.04  0.57      1.23       2.98\n",
       "alpha[19]  3.68  1.02      2.00       5.04\n",
       "alpha[20]  2.35  0.65      1.36       3.43\n",
       "alpha[21]  2.45  0.71      1.36       3.52\n",
       "alpha[22]  2.37  0.69      1.29       3.51\n",
       "alpha[23]  1.66  0.49      0.80       2.41\n",
       "alpha[24] -1.01  0.43     -1.54      -0.23\n",
       "alpha[25]  0.14  0.44     -0.47       0.81\n",
       "alpha[26] -1.43  0.51     -2.18      -0.64\n",
       "alpha[27] -0.47  0.42     -1.06       0.25\n",
       "alpha[28]  0.17  0.40     -0.45       0.78\n",
       "alpha[29]  1.47  0.51      0.55       2.19\n",
       "alpha[30] -0.65  0.44     -1.44      -0.02\n",
       "alpha[31] -0.31  0.42     -1.01       0.37\n",
       "alpha[32]  3.17  0.83      1.88       4.36\n",
       "alpha[33]  2.67  0.60      1.70       3.50\n",
       "alpha[34]  2.71  0.67      1.65       3.74\n",
       "alpha[35]  2.07  0.52      1.36       2.88\n",
       "alpha[36]  2.09  0.55      1.24       2.83\n",
       "alpha[37]  3.87  0.95      2.37       5.27\n",
       "alpha[38]  2.69  0.64      1.65       3.57\n",
       "alpha[39]  2.35  0.56      1.56       3.34\n",
       "alpha[40] -1.81  0.45     -2.50      -1.11\n",
       "alpha[41] -0.59  0.33     -1.17      -0.12\n",
       "alpha[42] -0.47  0.37     -1.04       0.13\n",
       "alpha[43] -0.35  0.36     -0.88       0.31\n",
       "alpha[44]  0.59  0.33      0.10       1.17\n",
       "alpha[45] -0.58  0.36     -1.10       0.02\n",
       "alpha[46]  2.07  0.47      1.40       2.86\n",
       "alpha[47]  0.00  0.34     -0.47       0.60\n",
       "sigma      1.62  0.22      1.28       1.94"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# Import Data & Data Manipulation ------------------------------------------------\n",
    "# Import\n",
    "from importlib.resources import files\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "# Manipulate\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0]) \n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "def model(tank, surv, density):\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n",
    "    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n",
    "    p = alpha[tank]\n",
    "    m.dist.binomial(total_count = density, logits = p, obs=surv)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 348.55it/s, 7 steps of size 4.93e-01. acc. prob=0.85]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_bar</th>\n",
       "      <td>1.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>3.14</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.19</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[3]</th>\n",
       "      <td>3.08</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.31</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[4]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[5]</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[6]</th>\n",
       "      <td>3.16</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[7]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[8]</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[9]</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[10]</th>\n",
       "      <td>1.07</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[11]</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[12]</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[13]</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[14]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[15]</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[16]</th>\n",
       "      <td>2.94</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[17]</th>\n",
       "      <td>2.42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[18]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[19]</th>\n",
       "      <td>3.69</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.24</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[20]</th>\n",
       "      <td>2.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[21]</th>\n",
       "      <td>2.42</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[22]</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[23]</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[24]</th>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[25]</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[26]</th>\n",
       "      <td>-1.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[27]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[28]</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[29]</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[30]</th>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[31]</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[32]</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.02</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[33]</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[34]</th>\n",
       "      <td>2.76</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.71</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[35]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[36]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[37]</th>\n",
       "      <td>3.94</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[38]</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[39]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[40]</th>\n",
       "      <td>-1.81</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[41]</th>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[42]</th>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[43]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[44]</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[45]</th>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[46]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[47]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a_bar      1.37  0.27      0.98       1.80\n",
       "alpha[0]   2.11  0.87      0.81       3.54\n",
       "alpha[1]   3.14  1.13      1.19       4.78\n",
       "alpha[2]   1.04  0.65     -0.04       2.00\n",
       "alpha[3]   3.08  1.15      1.31       4.73\n",
       "alpha[4]   2.19  0.87      0.85       3.54\n",
       "alpha[5]   2.20  0.94      0.81       3.63\n",
       "alpha[6]   3.16  1.21      1.35       5.09\n",
       "alpha[7]   2.19  0.88      0.79       3.45\n",
       "alpha[8]  -0.18  0.58     -1.02       0.80\n",
       "alpha[9]   2.13  0.88      0.82       3.45\n",
       "alpha[10]  1.07  0.75     -0.10       2.18\n",
       "alpha[11]  0.56  0.66     -0.55       1.61\n",
       "alpha[12]  1.01  0.69     -0.04       2.05\n",
       "alpha[13]  0.23  0.59     -0.71       1.10\n",
       "alpha[14]  2.19  0.89      0.97       3.70\n",
       "alpha[15]  2.11  0.90      0.70       3.49\n",
       "alpha[16]  2.94  0.78      1.63       4.08\n",
       "alpha[17]  2.42  0.67      1.33       3.43\n",
       "alpha[18]  2.04  0.60      1.06       2.86\n",
       "alpha[19]  3.69  0.97      2.24       5.18\n",
       "alpha[20]  2.40  0.67      1.31       3.48\n",
       "alpha[21]  2.42  0.71      1.33       3.52\n",
       "alpha[22]  2.44  0.71      1.23       3.46\n",
       "alpha[23]  1.70  0.53      0.90       2.48\n",
       "alpha[24] -0.97  0.47     -1.67      -0.24\n",
       "alpha[25]  0.14  0.43     -0.55       0.77\n",
       "alpha[26] -1.42  0.50     -2.15      -0.65\n",
       "alpha[27] -0.47  0.40     -1.12       0.08\n",
       "alpha[28]  0.18  0.40     -0.46       0.76\n",
       "alpha[29]  1.48  0.50      0.78       2.31\n",
       "alpha[30] -0.63  0.45     -1.30       0.06\n",
       "alpha[31] -0.34  0.43     -1.01       0.38\n",
       "alpha[32]  3.17  0.79      2.02       4.55\n",
       "alpha[33]  2.72  0.64      1.74       3.78\n",
       "alpha[34]  2.76  0.68      1.71       3.80\n",
       "alpha[35]  2.07  0.48      1.43       2.91\n",
       "alpha[36]  2.07  0.51      1.27       2.85\n",
       "alpha[37]  3.94  1.01      2.43       5.48\n",
       "alpha[38]  2.71  0.64      1.61       3.51\n",
       "alpha[39]  2.35  0.51      1.58       3.08\n",
       "alpha[40] -1.81  0.47     -2.54      -1.11\n",
       "alpha[41] -0.56  0.35     -1.10      -0.01\n",
       "alpha[42] -0.45  0.34     -1.02       0.02\n",
       "alpha[43] -0.35  0.34     -0.86       0.20\n",
       "alpha[44]  0.59  0.33      0.04       1.09\n",
       "alpha[45] -0.56  0.37     -1.09       0.08\n",
       "alpha[46]  2.07  0.50      1.21       2.71\n",
       "alpha[47]  0.00  0.35     -0.51       0.58\n",
       "sigma      1.63  0.20      1.31       1.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# Import Data & Data Manipulation ------------------------------------------------\n",
    "# Import\n",
    "from importlib.resources import files\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "# Manipulate\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0]) \n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "def model(tank, surv, density):\n",
    "    alpha = m.effects.varying_intercept(group=tank)\n",
    "    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/reedfrogs.csv\", sep = ''), sep=';')\n",
    "m$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\n",
    "m$data_to_model(list('tank', 'surv', 'density')) # Manipulate\n",
    "m$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\n",
    "m$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n",
    "\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(tank, surv, density){\n",
    "  # Parameters priors distributions\n",
    "  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n",
    "  a_bar = bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n",
    "  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n",
    "  p = alpha[tank]\n",
    "  # Likelihood\n",
    "  m$binomial(total_count = density, logits = p, obs=surv)\n",
    "} \n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[48] int N;\n",
    "    array[48] int S;\n",
    "    array[48] int tank;\n",
    "}\n",
    "parameters{\n",
    "    real a_bar;\n",
    "    vector[48] a;    \n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[48] p;\n",
    "    sigma ~ exponential( 1 );\n",
    "    a_bar ~ normal( 0 , 1.5 );\n",
    "    a ~ normal( a_bar , sigma );\n",
    "    for ( i in 1:48 ) {\n",
    "        p[i] = a[tank[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    S ~ binomial( N , p );\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'S' : m.df['surv'].values.astype(int),\n",
    "    'N' : m.df['density'].values.astype(int),\n",
    "    'tank' : m.df['tank'].values.astype(int)+1,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def model(tank, surv, density):\n",
    "    sigma = m.dist.exponential( 1, shape=(1,), name = 'sigma')\n",
    "    a_bar = m.dist.normal( 0., 1.5, shape=(1,), name = 'a_bar')\n",
    "    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n",
    "    p = alpha[tank]\n",
    "    m.dist.binomial(total_count = density, logits = p, obs=surv)\n",
    "\n",
    "def sim_surv(tank, density, sigma, a_bar, alpha):\n",
    "    p = alpha[tank]\n",
    "    return m.dist.binomial(total_count = density, logits = p, sample=True)\n",
    "\n",
    "def estimate(tank, density, sigma, a_bar, alpha):\n",
    "    surv = sim_surv(tank, density, sigma, a_bar, alpha) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "    m.df[\"tank\"] = np.arange(m.df.shape[0])\n",
    "    m.df['surv']=surv\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(tank, density, sigma, a_bar, alpha, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(tank, density, sigma[i], a_bar[i], alpha[:,i,0])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a_bar[i], alpha[:,i,0], sigma[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "nsim = 100\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0])\n",
    "tank = jnp.array(m.df[\"tank\"].values)\n",
    "density = jnp.array(m.df[\"density\"].values)\n",
    "sigma = m.dist.exponential( 1, shape = (nsim,1),  sample=True)\n",
    "a_bar = m.dist.normal( 0., 1.5,  shape = (nsim,1), sample=True)    \n",
    "alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha', sample=True)\n",
    "param_recovery(tank, density, sigma, a_bar, alpha, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_13_'></a>[Varying effects](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_1_'></a>[Data simulation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro.distributions as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "newPath = os.path.dirname(os.path.abspath(\"\"))\n",
    "if newPath not in sys.path:\n",
    "    sys.path.append(newPath)\n",
    "\n",
    "\n",
    "import time as tm\n",
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "a = 3.5  # average morning wait time\n",
    "b = -1  # average difference afternoon wait time\n",
    "sigma_a = 1  # std dev in intercepts\n",
    "sigma_b = 0.5  # std dev in slopes\n",
    "rho = -0.7  # correlation between intercepts and slopes\n",
    "Mu = jnp.array([a, b])\n",
    "cov_ab = sigma_a * sigma_b * rho\n",
    "Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n",
    "jnp.array([1, 2, 3, 4]).reshape(2, 2).T\n",
    "sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\n",
    "Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "# now matrix multiply to get covariance matrix\n",
    "Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\n",
    "\n",
    "N_cafes = 20\n",
    "seed = jax.random.PRNGKey(5)  # used to replicate example\n",
    "vary_effects = m.dist.multivariatenormal(Mu, Sigma, shape=(N_cafes,), sample = True)\n",
    "a_cafe = vary_effects[:, 0]\n",
    "b_cafe = vary_effects[:, 1]\n",
    "\n",
    "seed = jax.random.PRNGKey(22)\n",
    "N_visits = 10\n",
    "afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\n",
    "cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\n",
    "mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n",
    "sigma = 0.5  # std dev within cafes\n",
    "wait = m.dist.normal(mu, sigma, sample = True)\n",
    "d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n",
    "d.to_csv(data_path + 'Sim data multivariatenormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build in function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<~int64[]>with<DynamicJaxprTrace>, 2).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function _body_fn at /home/sosa/work/.venv/lib/python3.12/site-packages/numpyro/util.py:332 for jit. This concrete value was not available in Python because it depends on the value of the argument val[2]['N_cafes'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/jax/_src/util.py:299\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    298\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_context_in_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m              \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'DynamicJaxprTracer'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     m.dist.normal(mu, sigma, obs=wait)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Run sampler ------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_model_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Diagnostic ------------------------------------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m m.summary()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/BI/BI/Main/main.py:200\u001b[39m, in \u001b[36mbi.fit\u001b[39m\u001b[34m(self, model, obs, potential_fn, kinetic_fn, step_size, inverse_mass_matrix, adapt_step_size, adapt_mass_matrix, dense_mass, target_accept_prob, trajectory_length, max_tree_depth, init_strategy, find_heuristic_step_size, forward_mode_differentiation, regularize_mass_matrix, num_warmup, num_samples, num_chains, thinning, postprocess_fn, chain_method, progress_bar, jit_model_args, seed)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBI\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSamplers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcmc_numpyro\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mcmc_numpyro\n\u001b[32m    174\u001b[39m \u001b[38;5;28mself\u001b[39m.sampler = mcmc_numpyro(\n\u001b[32m    175\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    176\u001b[39m     potential_fn=potential_fn,\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m     jit_model_args=jit_model_args\n\u001b[32m    198\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_on_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.posteriors = \u001b[38;5;28mself\u001b[39m.sampler.get_samples()\n\u001b[32m    202\u001b[39m \u001b[38;5;28mself\u001b[39m.diag = diag(sampler = \u001b[38;5;28mself\u001b[39m.sampler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/mcmc.py:702\u001b[39m, in \u001b[36mMCMC.run\u001b[39m\u001b[34m(self, rng_key, extra_fields, init_params, *args, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m map_args = (rng_key, init_state, init_params)\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_chains == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m     states_flat, last_state = \u001b[43mpartial_map_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m     states = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[jnp.newaxis, ...], states_flat)\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/mcmc.py:489\u001b[39m, in \u001b[36mMCMC._single_chain_mcmc\u001b[39m\u001b[34m(self, init, args, kwargs, collect_fields, remove_sites)\u001b[39m\n\u001b[32m    483\u001b[39m collection_size = \u001b[38;5;28mself\u001b[39m._collection_params[\u001b[33m\"\u001b[39m\u001b[33mcollection_size\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    484\u001b[39m collection_size = (\n\u001b[32m    485\u001b[39m     collection_size\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m collection_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m collection_size // \u001b[38;5;28mself\u001b[39m.thinning\n\u001b[32m    488\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m collect_vals = \u001b[43mfori_collect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlower_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupper_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_collect_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpostprocess_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_sites\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogbar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_last_val\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthinning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthinning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogbar_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_progbar_desc_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiagnostics_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiagnostics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_chains\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchain_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchain_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m states, last_val = collect_vals\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Get first argument of type `HMCState`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/util.py:399\u001b[39m, in \u001b[36mfori_collect\u001b[39m\u001b[34m(lower, upper, body_fun, init_val, transform, progbar, return_last_val, collection_size, thinning, **progbar_opts)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm.trange(upper) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m         vals = \u001b[43m_body_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m         t.set_description(progbar_desc(i), refresh=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    402\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m diagnostics_fn:\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/util.py:335\u001b[39m, in \u001b[36mfori_collect.<locals>._body_fn\u001b[39m\u001b[34m(i, val, collection, start_idx, thinning)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;129m@partial\u001b[39m(maybe_jit, donate_argnums=\u001b[32m2\u001b[39m)\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@cached_by\u001b[39m(fori_collect, body_fun, transform)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_body_fn\u001b[39m(i, val, collection, start_idx, thinning):\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     val = \u001b[43mbody_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     idx = (i - start_idx) // thinning\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_fn\u001b[39m(collect_array, new_val):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/mcmc.py:184\u001b[39m, in \u001b[36m_sample_fn_jit_args\u001b[39m\u001b[34m(state, sampler)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sample_fn_jit_args\u001b[39m(state, sampler):\n\u001b[32m    183\u001b[39m     hmc_state, args, kwargs = state\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhmc_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, args, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc.py:816\u001b[39m, in \u001b[36mHMC.sample\u001b[39m\u001b[34m(self, state, model_args, model_kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, model_args, model_kwargs):\n\u001b[32m    807\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[33;03m    Run HMC from the given :data:`~numpyro.infer.hmc.HMCState` and return the resulting\u001b[39;00m\n\u001b[32m    809\u001b[39m \u001b[33;03m    :data:`~numpyro.infer.hmc.HMCState`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    814\u001b[39m \u001b[33;03m    :return: Next `state` after running HMC.\u001b[39;00m\n\u001b[32m    815\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc.py:491\u001b[39m, in \u001b[36mhmc.<locals>.sample_kernel\u001b[39m\u001b[34m(hmc_state, model_args, model_kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    488\u001b[39m     hmc_length_args = (\n\u001b[32m    489\u001b[39m         jnp.where(hmc_state.i < wa_steps, max_treedepth[\u001b[32m0\u001b[39m], max_treedepth[\u001b[32m1\u001b[39m]),\n\u001b[32m    490\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m vv_state, energy, num_steps, accept_prob, diverging = \u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhmc_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhmc_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvv_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrng_key_transition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mhmc_length_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# not update adapt_state after warmup phase\u001b[39;00m\n\u001b[32m    501\u001b[39m adapt_state = cond(\n\u001b[32m    502\u001b[39m     hmc_state.i < wa_steps,\n\u001b[32m    503\u001b[39m     (hmc_state.i, accept_prob, vv_state, hmc_state.adapt_state),\n\u001b[32m   (...)\u001b[39m\u001b[32m    506\u001b[39m     identity,\n\u001b[32m    507\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc.py:431\u001b[39m, in \u001b[36mhmc.<locals>._nuts_next\u001b[39m\u001b[34m(step_size, inverse_mass_matrix, vv_state, model_args, model_kwargs, rng_key, max_treedepth_current)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    429\u001b[39m     vv_update_fn = vv_update\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m binary_tree = \u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvv_update_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkinetic_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvv_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_delta_energy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_delta_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tree_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_treedepth_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_treedepth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m accept_prob = binary_tree.sum_accept_probs / binary_tree.num_proposals\n\u001b[32m    442\u001b[39m num_steps = binary_tree.num_proposals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:1179\u001b[39m, in \u001b[36mbuild_tree\u001b[39m\u001b[34m(verlet_update, kinetic_fn, verlet_state, inverse_mass_matrix, step_size, rng_key, max_delta_energy, max_tree_depth)\u001b[39m\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tree, key\n\u001b[32m   1178\u001b[39m state = (tree, rng_key)\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m tree, _ = \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cond_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_body_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/util.py:141\u001b[39m, in \u001b[36mwhile_loop\u001b[39m\u001b[34m(cond_fun, body_fun, init_val)\u001b[39m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:1163\u001b[39m, in \u001b[36mbuild_tree.<locals>._body_fn\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m   1161\u001b[39m key, direction_key, doubling_key = random.split(key, \u001b[32m3\u001b[39m)\n\u001b[32m   1162\u001b[39m going_right = random.bernoulli(direction_key)\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m tree = \u001b[43m_double_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverlet_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkinetic_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoing_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoubling_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m    \u001b[49m\u001b[43menergy_current\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_delta_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr_ckpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr_sum_ckpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree, key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:922\u001b[39m, in \u001b[36m_double_tree\u001b[39m\u001b[34m(current_tree, vv_update, kinetic_fn, inverse_mass_matrix, step_size, going_right, rng_key, energy_current, max_delta_energy, r_ckpts, r_sum_ckpts)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_double_tree\u001b[39m(\n\u001b[32m    908\u001b[39m     current_tree,\n\u001b[32m    909\u001b[39m     vv_update,\n\u001b[32m   (...)\u001b[39m\u001b[32m    918\u001b[39m     r_sum_ckpts,\n\u001b[32m    919\u001b[39m ):\n\u001b[32m    920\u001b[39m     key, transition_key = random.split(rng_key)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     new_tree = \u001b[43m_iterative_build_subtree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvv_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkinetic_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgoing_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43menergy_current\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_delta_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mr_ckpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mr_sum_ckpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _combine_tree(\n\u001b[32m    937\u001b[39m         current_tree, new_tree, inverse_mass_matrix, going_right, transition_key, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    938\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:1063\u001b[39m, in \u001b[36m_iterative_build_subtree\u001b[39m\u001b[34m(prototype_tree, vv_update, kinetic_fn, inverse_mass_matrix, step_size, going_right, rng_key, energy_current, max_delta_energy, r_ckpts, r_sum_ckpts)\u001b[39m\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tree, turning, r_ckpts, r_sum_ckpts, rng_key\n\u001b[32m   1061\u001b[39m basetree = prototype_tree._replace(num_proposals=\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m tree, turning, _, _, _ = \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_cond_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_body_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasetree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ckpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_sum_ckpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# update depth and turning condition\u001b[39;00m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TreeInfo(\n\u001b[32m   1068\u001b[39m     tree.z_left,\n\u001b[32m   1069\u001b[39m     tree.r_left,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     tree.num_proposals,\n\u001b[32m   1085\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/util.py:141\u001b[39m, in \u001b[36mwhile_loop\u001b[39m\u001b[34m(cond_fun, body_fun, init_val)\u001b[39m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:1008\u001b[39m, in \u001b[36m_iterative_build_subtree.<locals>._body_fn\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;66;03m# If we are going to the right, start from the right leaf of the current tree.\u001b[39;00m\n\u001b[32m   1007\u001b[39m z, r, z_grad = _get_leaf(current_tree, going_right)\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m new_leaf = \u001b[43m_build_basetree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvv_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkinetic_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoing_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43menergy_current\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_delta_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1020\u001b[39m new_tree = cond(\n\u001b[32m   1021\u001b[39m     current_tree.num_proposals == \u001b[32m0\u001b[39m,\n\u001b[32m   1022\u001b[39m     new_leaf,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: _combine_tree(*x, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1032\u001b[39m )\n\u001b[32m   1034\u001b[39m leaf_idx = current_tree.num_proposals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:864\u001b[39m, in \u001b[36m_build_basetree\u001b[39m\u001b[34m(vv_update, kinetic_fn, z, r, z_grad, inverse_mass_matrix, step_size, going_right, energy_current, max_delta_energy)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_build_basetree\u001b[39m(\n\u001b[32m    852\u001b[39m     vv_update,\n\u001b[32m    853\u001b[39m     kinetic_fn,\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m     max_delta_energy,\n\u001b[32m    862\u001b[39m ):\n\u001b[32m    863\u001b[39m     step_size = jnp.where(going_right, step_size, -step_size)\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     z_new, r_new, potential_energy_new, z_new_grad = \u001b[43mvv_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m     energy_new = potential_energy_new + kinetic_fn(inverse_mass_matrix, r_new)\n\u001b[32m    869\u001b[39m     delta_energy = energy_new - energy_current\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:303\u001b[39m, in \u001b[36mvelocity_verlet.<locals>.update_fn\u001b[39m\u001b[34m(step_size, inverse_mass_matrix, state)\u001b[39m\n\u001b[32m    301\u001b[39m r_grad = _kinetic_grad(kinetic_fn, inverse_mass_matrix, r)\n\u001b[32m    302\u001b[39m z = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m z, r_grad: z + step_size * r_grad, z, r_grad)  \u001b[38;5;66;03m# z(n+1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m potential_energy, z_grad = \u001b[43m_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpotential_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_mode_differentiation\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m r = jax.tree.map(\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m r, z_grad: r - \u001b[32m0.5\u001b[39m * step_size * z_grad, r, z_grad\n\u001b[32m    308\u001b[39m )  \u001b[38;5;66;03m# r(n+1)\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m IntegratorState(z, r, potential_energy, z_grad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/hmc_util.py:252\u001b[39m, in \u001b[36m_value_and_grad\u001b[39m\u001b[34m(f, x, forward_mode_differentiation)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out, grads\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/util.py:324\u001b[39m, in \u001b[36mpotential_energy\u001b[39m\u001b[34m(model, model_args, model_kwargs, params, enum)\u001b[39m\n\u001b[32m    320\u001b[39m substituted_model = substitute(\n\u001b[32m    321\u001b[39m     model, substitute_fn=partial(_unconstrain_reparam, params)\n\u001b[32m    322\u001b[39m )\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# no param is needed for log_density computation because we already substitute\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m log_joint, model_trace = \u001b[43mlog_density_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubstituted_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -log_joint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/util.py:120\u001b[39m, in \u001b[36mlog_density\u001b[39m\u001b[34m(model, model_args, model_kwargs, params)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_density\u001b[39m(model, model_args: \u001b[38;5;28mtuple\u001b[39m, model_kwargs: \u001b[38;5;28mdict\u001b[39m, params: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given latent\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    values ``params``.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03m    :return: Log of joint density and a corresponding model trace.\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     log_joint, model_trace = \u001b[43mcompute_log_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# We need to start with 0.0 instead of 0 because log_joint may be empty or only\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# contain integers, but log_density must be a floating point value to be\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# differentiable by jax.\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(log_joint.values(), start=\u001b[32m0.0\u001b[39m), model_trace\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpyro/infer/util.py:93\u001b[39m, in \u001b[36mcompute_log_probs\u001b[39m\u001b[34m(model, model_args, model_kwargs, params, sum_log_prob)\u001b[39m\n\u001b[32m     89\u001b[39m model_shape = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m     90\u001b[39m     site[\u001b[33m\"\u001b[39m\u001b[33mfn\u001b[39m\u001b[33m\"\u001b[39m].shape()\n\u001b[32m     91\u001b[39m )  \u001b[38;5;66;03m# TensorShape from tfp needs casting to tuple\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel and guide shapes disagree at site: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m vs \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m     97\u001b[39m             site[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], model_shape, guide_shape\n\u001b[32m     98\u001b[39m         )\n\u001b[32m     99\u001b[39m     )\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/jax/_src/core.py:1864\u001b[39m, in \u001b[36mcanonicalize_shape\u001b[39m\u001b[34m(shape, context)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1863\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[31mTypeError\u001b[39m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<~int64[]>with<DynamicJaxprTrace>, 2).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function _body_fn at /home/sosa/work/.venv/lib/python3.12/site-packages/numpyro/util.py:332 for jit. This concrete value was not available in Python because it depends on the value of the argument val[2]['N_cafes']."
     ]
    }
   ],
   "source": [
    "\n",
    "# import data ------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multivariatenormal.csv', sep = ',')\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n",
    "    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n",
    "    N_cafes = len(m.df.cafe.unique()),\n",
    "    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n",
    ")\n",
    "\n",
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2,  name = 'a')\n",
    "    b = m.dist.normal(-1, 0.5, name = 'b')\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "\n",
    "    varying_intercept, varying_slope = m.effects.varying_effects(\n",
    "        N_group=N_cafes,\n",
    "        global_intercept=a,\n",
    "        global_slope=b)\n",
    "\n",
    "    mu = varying_intercept[cafe] + varying_slope[cafe]* afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, jit_model_args=True) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:08<00:00, 118.85it/s, 15 steps of size 3.04e-01. acc. prob=0.83]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rho[0, 0]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[0, 1]</th>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 0]</th>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 1]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[0, 0]</th>\n",
       "      <td>3.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[0, 1]</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[1, 0]</th>\n",
       "      <td>2.02</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[1, 1]</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[2, 0]</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[2, 1]</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[3, 0]</th>\n",
       "      <td>4.45</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.09</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[3, 1]</th>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[4, 0]</th>\n",
       "      <td>3.36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[4, 1]</th>\n",
       "      <td>-1.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[5, 0]</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.51</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[5, 1]</th>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[6, 0]</th>\n",
       "      <td>4.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[6, 1]</th>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[7, 0]</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[7, 1]</th>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>-0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[8, 0]</th>\n",
       "      <td>3.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[8, 1]</th>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[9, 0]</th>\n",
       "      <td>3.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[9, 1]</th>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[10, 0]</th>\n",
       "      <td>4.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[10, 1]</th>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[11, 0]</th>\n",
       "      <td>4.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[11, 1]</th>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[12, 0]</th>\n",
       "      <td>3.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[12, 1]</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[13, 0]</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[13, 1]</th>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[14, 0]</th>\n",
       "      <td>4.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[14, 1]</th>\n",
       "      <td>-1.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[15, 0]</th>\n",
       "      <td>5.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.68</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[15, 1]</th>\n",
       "      <td>-1.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>-1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[16, 0]</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[16, 1]</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[17, 0]</th>\n",
       "      <td>4.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.03</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[17, 1]</th>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[18, 0]</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.51</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[18, 1]</th>\n",
       "      <td>-1.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[19, 0]</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[19, 1]</th>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_cafe[0]</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_cafe[1]</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean    sd  hdi_5.5%  hdi_94.5%\n",
       "Rho[0, 0]        1.00  0.00      1.00       1.00\n",
       "Rho[0, 1]       -0.69  0.17     -0.94      -0.47\n",
       "Rho[1, 0]       -0.69  0.17     -0.94      -0.47\n",
       "Rho[1, 1]        1.00  0.00      1.00       1.00\n",
       "a                3.53  0.22      3.13       3.83\n",
       "a_b_cafe[0, 0]   3.01  0.21      2.71       3.37\n",
       "a_b_cafe[0, 1]  -0.65  0.24     -1.08      -0.31\n",
       "a_b_cafe[1, 0]   2.02  0.23      1.67       2.38\n",
       "a_b_cafe[1, 1]  -0.18  0.28     -0.63       0.26\n",
       "a_b_cafe[2, 0]   2.85  0.21      2.53       3.18\n",
       "a_b_cafe[2, 1]  -0.65  0.25     -1.08      -0.27\n",
       "a_b_cafe[3, 0]   4.45  0.22      4.09       4.77\n",
       "a_b_cafe[3, 1]  -1.35  0.27     -1.78      -0.94\n",
       "a_b_cafe[4, 0]   3.36  0.21      3.03       3.70\n",
       "a_b_cafe[4, 1]  -1.11  0.25     -1.47      -0.67\n",
       "a_b_cafe[5, 0]   3.82  0.21      3.51       4.13\n",
       "a_b_cafe[5, 1]  -0.89  0.26     -1.33      -0.50\n",
       "a_b_cafe[6, 0]   4.48  0.22      4.14       4.86\n",
       "a_b_cafe[6, 1]  -1.52  0.28     -1.95      -1.07\n",
       "a_b_cafe[7, 0]   3.53  0.20      3.18       3.81\n",
       "a_b_cafe[7, 1]  -1.06  0.23     -1.43      -0.70\n",
       "a_b_cafe[8, 0]   3.67  0.19      3.36       3.94\n",
       "a_b_cafe[8, 1]  -1.15  0.24     -1.50      -0.76\n",
       "a_b_cafe[9, 0]   3.04  0.20      2.73       3.37\n",
       "a_b_cafe[9, 1]  -1.04  0.25     -1.39      -0.59\n",
       "a_b_cafe[10, 0]  4.64  0.19      4.34       4.94\n",
       "a_b_cafe[10, 1] -1.10  0.23     -1.44      -0.73\n",
       "a_b_cafe[11, 0]  4.05  0.21      3.74       4.40\n",
       "a_b_cafe[11, 1] -0.91  0.24     -1.30      -0.53\n",
       "a_b_cafe[12, 0]  3.74  0.23      3.40       4.08\n",
       "a_b_cafe[12, 1] -0.54  0.28     -1.06      -0.17\n",
       "a_b_cafe[13, 0]  3.53  0.20      3.20       3.82\n",
       "a_b_cafe[13, 1] -1.09  0.24     -1.45      -0.71\n",
       "a_b_cafe[14, 0]  4.08  0.21      3.73       4.43\n",
       "a_b_cafe[14, 1] -1.38  0.27     -1.76      -0.94\n",
       "a_b_cafe[15, 0]  5.02  0.21      4.68       5.35\n",
       "a_b_cafe[15, 1] -1.50  0.26     -1.97      -1.13\n",
       "a_b_cafe[16, 0]  1.71  0.21      1.40       2.05\n",
       "a_b_cafe[16, 1] -0.33  0.26     -0.77       0.06\n",
       "a_b_cafe[17, 0]  4.36  0.20      4.03       4.65\n",
       "a_b_cafe[17, 1] -1.05  0.26     -1.45      -0.63\n",
       "a_b_cafe[18, 0]  3.82  0.20      3.51       4.13\n",
       "a_b_cafe[18, 1] -1.17  0.24     -1.54      -0.83\n",
       "a_b_cafe[19, 0]  1.28  0.22      0.96       1.62\n",
       "a_b_cafe[19, 1] -0.40  0.27     -0.77       0.08\n",
       "b               -0.95  0.12     -1.13      -0.75\n",
       "sigma            0.51  0.03      0.46       0.55\n",
       "sigma_cafe[0]    1.01  0.15      0.77       1.23\n",
       "sigma_cafe[1]    0.46  0.10      0.30       0.61"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data ------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multivariatenormal.csv', sep = ',')\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n",
    "    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n",
    "    N_cafes = len(m.df.cafe.unique()),\n",
    "    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n",
    ")\n",
    "\n",
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2,  name = 'a')\n",
    "    b = m.dist.normal(-1, 0.5, name = 'b')\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "\n",
    "    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n",
    "    Rho = m.dist.lkj(2, 2, name = 'Rho')\n",
    "    \n",
    "    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n",
    "\n",
    "    a_cafe_b_cafe = m.dist.multivariate_normal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n",
    "\n",
    "    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n",
    "    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\n",
    "m$data_to_model(list('cafe', 'wait', 'afternoon'))\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n",
    "  a = bi.dist.normal(5, 2, name = 'a')\n",
    "  b = bi.dist.normal(-1, 0.5, name = 'b')\n",
    "  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n",
    "  sigma = bi.dist.exponential( 1, name = 'sigma')\n",
    "  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n",
    "  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n",
    "  \n",
    "  a_cafe_b_cafe = bi.dist.multivariatenormal(\n",
    "    jnp$squeeze(jnp$stack(list(a, b))), \n",
    "    cov, shape = c(N_cafes), name = 'a_cafe')  \n",
    "  \n",
    "  a_cafe = a_cafe_b_cafe[, 0]\n",
    "  b_cafe = a_cafe_b_cafe[, 1]\n",
    "  \n",
    "  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "  \n",
    "  m$normal(mu, sigma, obs=wait)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "import numpy as np\n",
    "#try:\n",
    "#  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "#except:\n",
    "#  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    int len;\n",
    "    int N_cafes;\n",
    "    vector[len] wait;\n",
    "    array[len] int afternoon;\n",
    "    array[len] int cafe;\n",
    "}\n",
    "\n",
    "parameters{\n",
    "    corr_matrix[2] Rho;\n",
    "    real a;\n",
    "    vector[N_cafes] a_cafe;\n",
    "    real b;\n",
    "    vector[N_cafes] b_cafe;      \n",
    "    real<lower=0> sigma;\n",
    "    vector<lower=0>[2] sigma_cafe;   \n",
    "    \n",
    "}\n",
    "model{\n",
    "    vector[len] mu;\n",
    "    Rho ~ lkj_corr( 2 );\n",
    "    sigma ~ exponential( 1 );\n",
    "    sigma_cafe ~ exponential( 1 );\n",
    "    b ~ normal( -1 , 0.5 );    \n",
    "    a ~ normal( 5 , 2 );\n",
    "    {\n",
    "        array[N_cafes] vector[2] YY;\n",
    "        vector[2] MU;\n",
    "        MU = [ a , b ]';\n",
    "        for ( j in 1:N_cafes ) YY[j] = [ a_cafe[j] , b_cafe[j] ]';\n",
    "        YY ~ multi_normal( MU , quad_form_diag(Rho , sigma_cafe) );\n",
    "    }\n",
    "    for ( i in 1:len ) {\n",
    "        mu[i] = a_cafe[cafe[i]] + b_cafe[cafe[i]] * afternoon[i];        \n",
    "    }\n",
    "    \n",
    "    wait ~ normal( mu , sigma );\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'wait' : d['wait'].values.astype(float),\n",
    "    'afternoon' : d['afternoon'].values.astype(int),\n",
    "    'cafe' : d['cafe'].values.astype(int)+1,\n",
    "    'N_cafes' : N_cafes,\n",
    "    'len' : len(d['wait'].values)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_5_'></a>[Output comparison](#toc0_)\n",
    "\n",
    "This can't use function to compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_array_to_dict(array, name=\"slice\"):\n",
    "\n",
    "    _, dim2, dim3 = array.shape\n",
    "    \n",
    "    # Create the dictionary\n",
    "    slices_dict = {\n",
    "        f\"{name}_{i}_{j}\": array[:, i, j]\n",
    "        for i in range(dim2)\n",
    "        for j in range(dim3)\n",
    "    }\n",
    "    \n",
    "    return slices_dict\n",
    "def slice_array_to_dict_2(array, name=\"slice\"):\n",
    "\n",
    "    _, dim2  = array.shape\n",
    "    \n",
    "    # Create the dictionary\n",
    "    slices_dict = {\n",
    "        f\"{name}_{i}\": array[:, i]\n",
    "        for i in range(dim2)\n",
    "    }\n",
    "    \n",
    "    return slices_dict\n",
    "\n",
    "# Change posteriors dictionary so that parameters  are in the same order as the data of stan\n",
    "r = slice_array_to_dict(m.posteriors['Rho'], name=\"Rho\")\n",
    "a = dict(a=m.posteriors['a'])\n",
    "b = dict(b=m.posteriors['b'])\n",
    "sigma = dict(sigma=m.posteriors['sigma'])\n",
    "a_cafe = slice_array_to_dict_2(m.posteriors['a_b_cafe'][:,:,0], name=\"a_cafe\")\n",
    "b_cafe = slice_array_to_dict_2(m.posteriors['a_b_cafe'][:,:,1], name=\"b_cafe\")\n",
    "sigma_cafe_1 = dict(sigma_cafe_1=m.posteriors['sigma_cafe'][:, 0])\n",
    "sigma_cafe_2 = dict(sigma_cafe_2=m.posteriors['sigma_cafe'][:, 1])\n",
    "r.update(a)\n",
    "r.update(a_cafe)\n",
    "r.update(b)\n",
    "r.update(b_cafe)\n",
    "r.update(sigma)\n",
    "r.update(sigma_cafe_1)\n",
    "r.update(sigma_cafe_2)\n",
    "m.posteriors=r\n",
    "d = prepare_stan_data(df)\n",
    "df_bi = prepare_bi_data(m)\n",
    "\n",
    "# Plot the density of the parameters\n",
    "df_bi.columns =d.columns\n",
    "for col in df_bi.columns:\n",
    "    if col in d.columns:  # Ensure the column exists in both DataFrames\n",
    "        plt.figure(figsize=(10, 6))  # Create a new figure for each plot\n",
    "        sns.kdeplot(df_bi[col], label=f'bi_{col}', fill=True)\n",
    "        sns.kdeplot(d[col], label=f'stan_{col}', fill=True)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Density Plot of {col}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2, name = 'a')\n",
    "    b =  m.dist.normal(-1, 0.5,  name = 'b')\n",
    "    sigma_cafe =  m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n",
    "    sigma =  m.dist.exponential( 1, name = 'sigma')\n",
    "    Rho =  m.dist.lkj(2, 2, name = 'Rho')\n",
    "    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n",
    "    a_cafe_b_cafe =  m.dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n",
    "\n",
    "    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n",
    "    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "def sim(N_cafes = 20, N_visits = 10, a = 3.5,b = -1, sigma_a = 1, sigma_b = 0.5, rho = -0.7 ):\n",
    "    Mu = jnp.array([a,b])\n",
    "    \n",
    "    cov_ab = sigma_a * sigma_b *  rho\n",
    "    \n",
    "    Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n",
    "    \n",
    "    jnp.array([1, 2, 3, 4]).reshape(2, 2).T\n",
    "    \n",
    "    sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\n",
    "    \n",
    "    Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "    # now matrix multiply to get covariance matrix\n",
    "    Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\n",
    "\n",
    "    seed = jax.random.PRNGKey(5)  # used to replicate example\n",
    "    vary_effects = m.dist.multivariatenormal(Mu, Sigma, shape=(N_cafes,), sample = True)\n",
    "    a_cafe = vary_effects[:, 0]\n",
    "    b_cafe = vary_effects[:, 1]\n",
    "\n",
    "    seed = jax.random.PRNGKey(22)\n",
    "    afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\n",
    "    cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\n",
    "    mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n",
    "    sigma = 0.5  # std dev within cafes\n",
    "    wait =  m.dist.normal(mu, sigma, sample = True)\n",
    "    d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n",
    "    return d\n",
    "\n",
    "def estimate(N_cafes = 20, N_visits = 10, a = 3.5,b = -1,sigma_a = 1, sigma_b = 0.5, rho = -0.7):\n",
    "    d = sim(N_cafes,N_visits , a, b, sigma_a, sigma_b, rho)\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = dict(\n",
    "        cafe = d.cafe.values, \n",
    "        wait = d.wait.values, \n",
    "        N_cafes = len(d.cafe.unique()),\n",
    "        afternoon =d.afternoon.values\n",
    "    )\n",
    "\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(nsim, N_cafes = 20, N_visits = 10, a = 3.5,b = -1,sigma_a = 1, sigma_b = 0.5, rho = -0.7 ):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(N_cafes = N_cafes, N_visits = N_visits,a = a[i], b = b[i], sigma_a = sigma_a[i], sigma_b = sigma_b[i], rho = rho[i])\n",
    "        estimations_filtered = estimations.loc[estimations.index.isin(['Rho[0, 1]', 'a', 'b', 'sigma_cafe[0]', 'sigma_cafe[1]'])] # Selecting paramters from model\n",
    "        data = {'sim': np.repeat(i, len(estimations_filtered.values)), \n",
    "                'parameter': estimations_filtered.index, \n",
    "                'simulated' : jnp.array([Rho[i],  a[i], b[i], sigma_a[i], sigma_b[i]]), \n",
    "                'estimations': estimations_filtered.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "a =  m.dist.normal(0, 1, shape= (nsim,), name = 'a', sample=True).tolist()\n",
    "b =  m.dist.normal(0, 0.5, shape=(nsim,), name = 'b', sample=True).tolist()\n",
    "sigma_cafe =  m.dist.exponential(1, shape=(nsim,2), name = 'sigma_cafe', sample=True)\n",
    "Rho =  m.dist.beta(2, 5, shape=(nsim,), name = 'b', sample=True).tolist()\n",
    "sigma_a =  sigma_cafe[:,0].tolist()\n",
    "sigma_b = sigma_cafe[:,1].tolist()\n",
    "i=0\n",
    "#sim(N_cafes = 20, N_visits = 10, a = a[i],b = b[i],sigma_a = sigma_a[i], sigma_b = sigma_b[i], rho = Rho[i] )\n",
    "#estimations = estimate(a = a[1], b = b[1], sigma_a = sigma_a[1], sigma_b = sigma_b[1], rho = Rho[1])\n",
    "param_recovery(nsim = nsim, N_cafes = 20, N_visits = 10, a =a, b = b, sigma_a =  sigma_cafe[:,0], sigma_b = sigma_cafe[:,1], rho = Rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_14_'></a>[Gaussian Processes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_14_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "@jax.jit\n",
    "def cov_GPL2(x, sq_eta, sq_rho, sq_sigma):\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix for a Gaussian Process using the \n",
    "    squared exponential kernel, version L2 (squared Euclidean distance).\n",
    "\n",
    "    Args:\n",
    "        x: Distance matrix between points.\n",
    "        sq_eta: Squared variance parameter (eta^2).\n",
    "        sq_rho: Squared length scale parameter (rho^2).\n",
    "        sq_sigma: Squared noise variance parameter (sigma^2).\n",
    "\n",
    "    Returns:\n",
    "        K: Covariance matrix incorporating the squared exponential kernel, version L2.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    K = sq_eta * jnp.exp(-sq_rho * jnp.square(x))\n",
    "    K = K.at[jnp.diag_indices(N)].add(sq_sigma)\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'Kline2.csv', sep=\";\")\n",
    "islandsDistMatrix = pd.read_csv(data_path + 'islandsDistMatrix.csv', index_col=0)\n",
    "m.data_to_model(['total_tools', 'population'])\n",
    "m.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\n",
    "m.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n",
    "\n",
    "\n",
    "def model(Dmat, population, society, total_tools):\n",
    "    a = m.dist.exponential(1, name = 'a')\n",
    "    b = m.dist.exponential(1, name = 'b')\n",
    "    g = m.dist.exponential(1, name = 'g')\n",
    "\n",
    "    # non-centered Gaussian Process prior\n",
    "    etasq = m.dist.exponential(2, name = 'etasq')\n",
    "    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n",
    "    SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01)\n",
    "    k = m.dist.multivariatenormal(0, SIGMA, name = 'k')\n",
    "    #k = m.gaussian.gaussian_process(Dmat, etasq, rhosq, 0.01, shape = (10,))\n",
    "    k = m.gaussian.kernel_L2(Dmat, etasq, rhosq, 0.01)\n",
    "    lambda_ = a * population**b / g * jnp.exp(k[society])\n",
    "\n",
    "    m.dist.poisson(lambda_, obs=total_tools)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters k are correlated between them, so order of comparison can't be found as they are interchangeable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_14_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "pd=import('pandas')\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Kline2.csv\", sep = ''), sep=';')\n",
    "islandsDistMatrix = pd$read_csv(paste(system.file(package = \"BI\"),\"/data/islandsDistMatrix.csv\", sep = ''), index_col=as.integer(0))\n",
    "m$data_to_model(list('total_tools', 'population'))\n",
    "m$data_on_model$society = jnp$arange(0,10, dtype='int64')\n",
    "m$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n",
    "\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(Dmat, population, society, total_tools){\n",
    "  a = bi.dist.exponential(1, name = 'a')\n",
    "  b = bi.dist.exponential(1, name = 'b')\n",
    "  g = bi.dist.exponential(1, name = 'g')\n",
    "  \n",
    "  # non-centered Gaussian Process prior\n",
    "  etasq = bi.dist.exponential(2, name = 'etasq')\n",
    "  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n",
    "  z = bi.dist.normal(0,1, name = 'z', shape = c(10))\n",
    "  r = m$kernel_sq_exp(Dmat, z, etasq, rhosq, 0.01)\n",
    "  SIGMA = r[[1]]\n",
    "  L_SIGMA = r[[2]]\n",
    "  k = r[[3]]\n",
    "  lambda_ = a * population**b / g * jnp$exp(k[society])\n",
    "  m$poisson(lambda_, obs=total_tools)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm\n",
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\" \n",
    "functions{\n",
    "  matrix cov_GPL2(matrix x, real sq_alpha, real sq_rho, real delta) {\n",
    "    int N = dims(x)[1];\n",
    "    matrix[N, N] K;\n",
    "    for (i in 1:(N-1)) {\n",
    "      K[i, i] = sq_alpha + delta;\n",
    "      for (j in (i + 1):N) {\n",
    "        K[i, j] = sq_alpha * exp(-sq_rho * square(x[i,j]) );\n",
    "        K[j, i] = K[i, j];\n",
    "      }\n",
    "    }\n",
    "    K[N, N] = sq_alpha + delta;\n",
    "    return K;\n",
    "  }\n",
    "}\n",
    "\n",
    "data{\n",
    "  array[10] int T;\n",
    "  array[10] int society;\n",
    "  array[10] int P;\n",
    "  matrix[10,10] Dmat;\n",
    "}\n",
    "\n",
    "parameters{\n",
    " real<lower=0> a;\n",
    " real<lower=0> b;\n",
    " real<lower=0> etasq;\n",
    " real<lower=0> g; \n",
    " real<lower=0> rhosq;\n",
    " vector[10] k;\n",
    "}\n",
    "\n",
    "model{\n",
    "  vector[10] lambda;\n",
    "  matrix[10,10] SIGMA;\n",
    "  rhosq ~ exponential( 0.5 );\n",
    "  etasq ~ exponential( 2 );\n",
    "  a ~ exponential( 1 );\n",
    "  b ~ exponential( 1 );\n",
    "  g ~ exponential( 1 );\n",
    "\n",
    "  SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01);\n",
    "  k ~ multi_normal( rep_vector(0,10) , SIGMA );\n",
    "  for ( i in 1:10 ) {\n",
    "    lambda[i] = (a * P[i]^b/g) * exp(k[society[i]]);\n",
    "  }\n",
    "  T ~ poisson( lambda );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'T' : m.df[\"total_tools\"].values.astype(int),\n",
    "    'P' : m.df[\"population\"].values.astype(int),\n",
    "    'society' : np.array(m.data_on_model['society']+1).astype(int),\n",
    "    'Dmat' : np.array(islandsDistMatrix)\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter recovery\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
