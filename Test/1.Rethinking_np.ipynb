{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Functions](#toc1_)    \n",
    "  - [Compare distributions between BI and STAN](#toc1_1_)    \n",
    "  - [Load BI for R](#toc1_2_)    \n",
    "  - [Load BI](#toc1_3_)    \n",
    "- [Rethinking](#toc2_)    \n",
    "  - [Continuous variable: Model (model 4.3)](#toc2_1_)    \n",
    "    - [BI](#toc2_1_1_)    \n",
    "    - [BIR](#toc2_1_2_)    \n",
    "    - [STAN](#toc2_1_3_)    \n",
    "    - [Output comparison](#toc2_1_4_)    \n",
    "    - [Parameter recovery](#toc2_1_5_)    \n",
    "  - [Categorical variable: Model (model 5.9)](#toc2_2_)    \n",
    "    - [BI](#toc2_2_1_)    \n",
    "    - [BIR](#toc2_2_2_)    \n",
    "    - [STAN](#toc2_2_3_)    \n",
    "    - [Output comparaison](#toc2_2_4_)    \n",
    "    - [Parameter recovery](#toc2_2_5_)    \n",
    "  - [Continuous interactions terms (model 8.3)](#toc2_3_)    \n",
    "    - [BI](#toc2_3_1_)    \n",
    "    - [BIR](#toc2_3_2_)    \n",
    "    - [STAN](#toc2_3_3_)    \n",
    "    - [Output comparison](#toc2_3_4_)    \n",
    "    - [Parameter recovery](#toc2_3_5_)    \n",
    "  - [Binomial (model 11.1)](#toc2_4_)    \n",
    "    - [BI](#toc2_4_1_)    \n",
    "    - [BIR](#toc2_4_2_)    \n",
    "    - [STAN](#toc2_4_3_)    \n",
    "    - [Output comparison](#toc2_4_4_)    \n",
    "    - [Parameter recovery](#toc2_4_5_)    \n",
    "  - [Binomial with indices (model 11.4)](#toc2_5_)    \n",
    "    - [BI](#toc2_5_1_)    \n",
    "    - [BIR](#toc2_5_2_)    \n",
    "    - [STAN](#toc2_5_3_)    \n",
    "    - [Output comparison](#toc2_5_4_)    \n",
    "    - [Parameter recovery](#toc2_5_5_)    \n",
    "  - [Poisson (model 11.10)](#toc2_6_)    \n",
    "    - [BI](#toc2_6_1_)    \n",
    "    - [BIR](#toc2_6_2_)    \n",
    "    - [STAN](#toc2_6_3_)    \n",
    "    - [Output comparison](#toc2_6_4_)    \n",
    "    - [Parameter recovery](#toc2_6_5_)    \n",
    "  - [Negative binomial (model 11.12)](#toc2_7_)    \n",
    "    - [Simulated data](#toc2_7_1_)    \n",
    "    - [BI](#toc2_7_2_)    \n",
    "    - [BIR](#toc2_7_3_)    \n",
    "    - [STAN](#toc2_7_4_)    \n",
    "    - [Output comparison](#toc2_7_5_)    \n",
    "    - [Parameter recovery](#toc2_7_6_)    \n",
    "  - [Multinomial (model 11.13)](#toc2_8_)    \n",
    "    - [Simulated data](#toc2_8_1_)    \n",
    "    - [BI](#toc2_8_2_)    \n",
    "    - [BIR](#toc2_8_3_)    \n",
    "    - [STAN](#toc2_8_4_)    \n",
    "    - [Output comparison](#toc2_8_5_)    \n",
    "    - [Parameter recovery](#toc2_8_6_)    \n",
    "  - [Beta binomial (model m12.1)](#toc2_9_)    \n",
    "    - [BI](#toc2_9_1_)    \n",
    "    - [BIR](#toc2_9_2_)    \n",
    "    - [STAN](#toc2_9_3_)    \n",
    "    - [Output comparison](#toc2_9_4_)    \n",
    "    - [Parameter recovery](#toc2_9_5_)    \n",
    "  - [Zero inflated outcomes](#toc2_10_)    \n",
    "    - [BI](#toc2_10_1_)    \n",
    "    - [BIR](#toc2_10_2_)    \n",
    "    - [STAN](#toc2_10_3_)    \n",
    "    - [Output comparison](#toc2_10_4_)    \n",
    "    - [Parameter recovery](#toc2_10_5_)    \n",
    "  - [OrderedLogistic (Todo: PB)](#toc2_11_)    \n",
    "  - [Varying interceps](#toc2_12_)    \n",
    "    - [BI](#toc2_12_1_)    \n",
    "    - [BIR](#toc2_12_2_)    \n",
    "    - [STAN](#toc2_12_3_)    \n",
    "    - [Output comparison](#toc2_12_4_)    \n",
    "    - [Parameter recovery](#toc2_12_5_)    \n",
    "  - [Varying effects](#toc2_13_)    \n",
    "    - [Data simulation](#toc2_13_1_)    \n",
    "    - [BI](#toc2_13_2_)    \n",
    "    - [BIR](#toc2_13_3_)    \n",
    "    - [STAN](#toc2_13_4_)    \n",
    "    - [Output comparison](#toc2_13_5_)    \n",
    "    - [Parameter recovery](#toc2_13_6_)    \n",
    "  - [Gaussian Processes](#toc2_14_)    \n",
    "    - [BI](#toc2_14_1_)    \n",
    "    - [BIR](#toc2_14_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Functions](#toc0_)\n",
    "## <a id='toc1_1_'></a>[Compare distributions between BI and STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "newPath = os.path.dirname(os.path.abspath(\"\"))\n",
    "if newPath not in sys.path:\n",
    "    sys.path.append(newPath)\n",
    "from BI import bi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "m = bi(platform='cpu')\n",
    "data_path = os.path.dirname(os.path.abspath(\"\")) + \"/BI/resources/data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_bi_data(m):\n",
    "    #data_dict = m.sampler.get_samples()\n",
    "    data_dict = m.posteriors\n",
    "    # Initialize an empty DataFrame to collect parameters\n",
    "    all_params = []\n",
    "\n",
    "    # Loop through each array in the dictionary\n",
    "    for key, array in data_dict.items():\n",
    "        # Check the shape of the array\n",
    "        if array.ndim > 1 and array.ndim < 3:\n",
    "            # Create a DataFrame from the array and add a column for each parameter\n",
    "            param_df = pd.DataFrame(array)\n",
    "            # Rename columns to include the parameter name\n",
    "            param_df.columns = [f\"{key}_{j+1}\" for j in range(array.shape[1])]\n",
    "            all_params.append(param_df)\n",
    "    \n",
    "        elif array.ndim >= 3:# we have a matrix\n",
    "            array_shape = array.shape\n",
    "            row = array_shape[1]\n",
    "            col = array_shape[2]\n",
    "            for a in range(col):\n",
    "                for b in range(row):\n",
    "                    all_params.append(pd.DataFrame({key + '_' + str(a) +  '_' + str(b): array[:,a,b]}))\n",
    "        else:\n",
    "            # If it's a 1D array, create a single column DataFrame\n",
    "            all_params.append(pd.DataFrame({key: array}))\n",
    "\n",
    "    # Concatenate all parameter DataFrames along the rows\n",
    "    df_bi = pd.concat(all_params, axis=1)\n",
    "    return df_bi\n",
    "\n",
    "def prepare_stan_data(df):\n",
    "    columns_to_remove = ['lp__',\t'accept_stat__',\t'stepsize__',\t'treedepth__',\t'n_leapfrog__',\t'divergent__',\t'energy__']\n",
    "    d = df.drop(columns=columns_to_remove)\n",
    "    return d\n",
    "\n",
    "def combine_data(df_bi, d):\n",
    "    #df_bi = pd.DataFrame(samples)\n",
    "    params = df_bi.columns.values\n",
    "    d.columns = df_bi.columns\n",
    "\n",
    "    df_bi['method'] = 'BI'\n",
    "    d['method'] = 'STAN'\n",
    "    d_comb = pd.concat([d, df_bi], ignore_index=True)\n",
    "    return d_comb\n",
    "\n",
    "def plot_comparaison(m, df):\n",
    "\n",
    "    d = prepare_stan_data(df)\n",
    "    df_bi = prepare_bi_data(m)\n",
    "    d_comb = combine_data(df_bi, d)\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    params = df_bi.columns.values[:-1]\n",
    "    num_params = len(params)\n",
    "    num_cols = 3\n",
    "    num_rows = (num_params + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 5), sharey=True)\n",
    "    if num_rows * num_cols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    for i in d_comb.columns:\n",
    "        if i in params:\n",
    "            if a == 0:\n",
    "                axes[a].set_ylabel('Density')\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=True)\n",
    "                axes[a].spines['right'].set_visible(False)\n",
    "                a += 1\n",
    "            elif a == num_params:\n",
    "                print('found')\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=True)\n",
    "                a += 1\n",
    "            elif a < num_params:\n",
    "                sns.kdeplot(data=d_comb, x=i, hue='method', ax=axes[a], fill=True, color='blue', alpha=0.5, legend=False)\n",
    "                a += 1\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.0, left=0.1, right=0.9)\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    # Remove extra subplots if there are any\n",
    "    for a in range(len(params), len(axes)):\n",
    "        fig.delaxes(axes[a])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.0, left=0.1, right=0.9)\n",
    "    axes[1].set_yticks([])\n",
    "    plt.show()\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Load BI for R](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WARNING: The R package \"reticulate\" only fixed recently\n",
      "    an issue that caused a segfault when used with rpy2:\n",
      "    https://github.com/rstudio/reticulate/pull/1188\n",
      "    Make sure that you use a version of that package that includes\n",
      "    the fix.\n",
      "    jax.local_device_count 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Virtual environment ('BayesInference') is available.\n",
       "\n",
       "----------------------------------------------------\n",
       "Loading BI\n",
       "----------------------------------------------------\n",
       "Virtual environment ('BayesInference') is available.\n",
       "Using 'BayesInference' virtual environment.\n",
       "jax and jax.numpy have been imported.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "#devtools::install_github(\"https://github.com/BGN-for-ASNA/BIR.git\", force = T)\n",
    "library(BayesianInference)\n",
    "m=importBI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Load BI](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Rethinking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Continuous variable: Model (model 4.3)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 455.14it/s, 3 steps of size 7.80e-01. acc. prob=0.92]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>154.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>154.19</td>\n",
       "      <td>155.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>5.78</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>5.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.87</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a  154.65  0.28    154.19     155.05\n",
       "b    5.78  0.30      5.28       6.22\n",
       "s    5.17  0.20      4.87       5.48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time as tm\n",
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'Howell1.csv', sep=';') \n",
    "m.df = m.df[m.df.age > 18]\n",
    "m.scale(['weight'])\n",
    "\n",
    "\n",
    "# define model ------------------------------------------------\n",
    "def model(weight, height):    \n",
    "    a = m.dist.normal( 178, 20, name = 'a')\n",
    "    b = m.dist.log_normal( 0, 1, name = 'b')   \n",
    "    s = m.dist.uniform( 0, 50, name = 's')\n",
    "    m.dist.normal(a + b * weight , s, obs=height)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "----------------------------------------------------\n",
       "Loading BI\n",
       "----------------------------------------------------\n",
       "Virtual environment ('BayesInference') is available.\n",
       "Using 'BayesInference' virtual environment.\n",
       "jax and jax.numpy have been imported.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "library(BayesianInference)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BayesianInference\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n",
    "\n",
    "# fileter data frame\n",
    "m$df = m$df[m$df$age > 18,]\n",
    "\n",
    "# Scale\n",
    "m$scale(list('weight')) \n",
    "\n",
    "# convert data to jax arrays\n",
    "m$data_to_model(list('weight', 'height'))\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(height, weight){\n",
    "  # Parameters priors distributions\n",
    "  s = bi.dist.uniform(0, 50, name = 's')\n",
    "  a = bi.dist.normal(178, 20,  name = 'a')\n",
    "  b = bi.dist.lognormal(0, 1, name = 'b')\n",
    "  \n",
    "  # Likelihood\n",
    "  m$normal(a + b * weight, s, obs = height)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "  vector[346] height;\n",
    "  vector[346] weight;\n",
    "}\n",
    "parameters{\n",
    "  real a;\n",
    "  real<lower=0> b;\n",
    "  real<lower=0,upper=50> s;\n",
    "}\n",
    "model{\n",
    "  vector[346] mu;\n",
    "  s ~ uniform( 0 , 50 );\n",
    "  b ~ lognormal( 0 , 1 );\n",
    "  a ~ normal( 178 , 20 );\n",
    "  for ( i in 1:346 ) {\n",
    "    mu[i] = a + b* weight[i] ;\n",
    "  }\n",
    "  height ~ normal( mu , s );  \n",
    "  \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'height': m.df.height.values,\n",
    "  'weight': m.df.weight.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_5_'></a>[Parameter recovery](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model(weight, height):    \n",
    "    a = m.dist.normal( 178, 20, name = 'a')   \n",
    "    b = m.dist.lognormal(  0, 1, name = 'b')   \n",
    "    s = m.dist.uniform( 0, 50, name = 's')\n",
    "    m.dist.normal(a + b * weight , s, obs=height)\n",
    "\n",
    "def simulate_height(weight, a, b, s):    \n",
    "    weight = (weight - weight.mean())/weight.std()\n",
    "    height = m.dist.normal( a + b * weight , s, sample = True)\n",
    "    return weight, height\n",
    "\n",
    "def estimate(weight, a, b, s):\n",
    "    weight, height = simulate_height(weight,a, b, s)\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"weight\": weight, \"height\": height})\n",
    "    m.scale(['weight'])\n",
    "    m.data_to_model(['weight', 'height'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(weight, a, b, s, N, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(weight[i,:], a[i,:], b[i,:], s[i,:])\n",
    "        data = {'sim': [i,i,i], 'parameter': estimations.index.values, 'simulated' : [a[i,:][0], b[i,:][0], s[i,:][0]], 'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "N = 100\n",
    "nsim = 100\n",
    "a = m.dist.normal(178, 20, shape=(nsim, 1), sample=True)\n",
    "b = m.dist.lognormal(0, 1, shape=(nsim, 1,), sample=True, seed = 2)\n",
    "s = m.dist.uniform(0, 50, shape=(nsim, 1), sample=True)\n",
    "weight = m.dist.normal( 80, 30, sample = True, shape = (nsim, N))\n",
    "res = param_recovery(weight, a, b, s, N, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Categorical variable: Model (model 5.9)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 479.33it/s, 7 steps of size 6.13e-01. acc. prob=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.5%     94.5%     n_eff     r_hat\n",
      "      a[0]     -0.46      0.24     -0.47     -0.86     -0.12    562.99      1.00\n",
      "      a[1]      0.36      0.25      0.36     -0.08      0.72    618.42      1.00\n",
      "      a[2]      0.64      0.28      0.63      0.16      1.03    668.73      1.00\n",
      "      a[3]     -0.54      0.32     -0.54     -1.03     -0.07    496.46      1.00\n",
      "         s      0.80      0.12      0.79      0.62      0.97    422.26      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'milk.csv', sep=';') \n",
    "m.index([\"clade\"])\n",
    "m.scale(['kcal_per_g'])\n",
    "\n",
    "def model(kcal_per_g, index_clade):\n",
    "    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a')\n",
    "    s = m.dist.exponential( 1, name = 's')    \n",
    "    mu = a[index_clade]\n",
    "    m.dist.normal(mu, s, obs=kcal_per_g)\n",
    "\n",
    "m.data_to_model(['kcal_per_g', \"index_clade\"])\n",
    "m.fit(model) \n",
    "m.sampler.print_summary(0.89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\n",
    "m$scale(list('kcal.per.g')) # Manipulate\n",
    "m$index(list('clade')) # Scale\n",
    "m$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(kcal_per_g, index_clade){\n",
    "  # Parameters priors distributions\n",
    "  beta =  bi.dist.normal( 0, 0.5, name = 'beta', shape=c(4))\n",
    "  sigma = bi.dist.exponential(1, name = 's')\n",
    "  # Likelihood\n",
    "  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distributionslibrary(BI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[29] K;\n",
    "    array[29] int clade_id;\n",
    "}\n",
    "parameters{\n",
    "    vector[4] a;\n",
    "    real<lower=0> s;\n",
    "}\n",
    "model{\n",
    "    vector[29] mu;\n",
    "    s ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 0.5 );\n",
    "    for ( i in 1:29 ) {\n",
    "        mu[i] = a[clade_id[i]];\n",
    "    }\n",
    "    K ~ normal( mu , s );\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "  'clade_id': m.df.index_clade.values+1,\n",
    "  'K': m.df.kcal_per_g.values,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[Output comparaison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "import jax.numpy as jnp\n",
    "def model(kcal_per_g, index_clade):\n",
    "    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a')\n",
    "    s = m.dist.exponential( 1, shape = (1,), name = 's')    \n",
    "    tmp = a[index_clade]\n",
    "    m.dist.normal(tmp, s, obs=kcal_per_g)\n",
    "    \n",
    "def simulate_data(a, sigma, N):\n",
    "    index_clade = m.dist.categorical(probs=jnp.array([0.25,0.25,.25,.25]), shape = (N,), sample=True).astype(int) # Generate clade index\n",
    "    tmp = a[index_clade] # Generate mean of each clade\n",
    "    cal = m.dist.normal(tmp, sigma, sample = True) # Generate calories based on mean and std\n",
    "    return cal, index_clade\n",
    "    \n",
    "def estimate(alpha, sigma, N):\n",
    "    cal, index_clade = simulate_data(alpha, sigma, N) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"kcal_per_g\": cal, \"index_clade\": index_clade})\n",
    "    #m.scale(['kcal_per_g'])\n",
    "    m.data_to_model(['kcal_per_g', 'index_clade'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(a, sigma, N, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i], sigma[i], N)\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([jnp.array(a[i]), jnp.array([sigma[i][0]])]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "N = 30\n",
    "Ngrp = 4\n",
    "nsim = 100\n",
    "a = m.dist.normal(0, 1, shape=(nsim, Ngrp,), sample=True, seed = 0)\n",
    "sigma = m.dist.halfcauchy(1, shape=(nsim, 1,), sample=True, seed = 10)\n",
    "\n",
    "result = param_recovery(a,sigma, N = N, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Continuous interactions terms (model 8.3)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 475.14it/s, 7 steps of size 6.60e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.5%     94.5%     n_eff     r_hat\n",
      "         a      0.09      0.10      0.09     -0.07      0.26    470.31      1.00\n",
      "        bs     -0.31      0.12     -0.32     -0.49     -0.11    535.87      1.00\n",
      "        bw      0.56      0.11      0.57      0.38      0.72    430.84      1.00\n",
      "       bws     -0.32      0.11     -0.32     -0.50     -0.16    460.55      1.00\n",
      "     sigma      0.58      0.10      0.57      0.43      0.73    256.89      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'tulips.csv', sep=';') \n",
    "m.scale(['blooms', 'water', 'shade'])\n",
    "\n",
    "# define model ------------------------------------------------\n",
    "def model(blooms,shade, water):\n",
    "    sigma = m.dist.exponential(1, name = 'sigma')\n",
    "    bws = m.dist.normal(0, 0.25, name = 'bws')\n",
    "    bs = m.dist.normal(0, 0.25, name = 'bs')\n",
    "    bw = m.dist.normal(0, 0.25, name = 'bw')\n",
    "    a = m.dist.normal(0.5, 0.25, name = 'a')\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    m.dist.normal(mu, sigma, obs=blooms)\n",
    "\n",
    "# Run sampler ------------------------------------------------ \n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.sampler.print_summary(0.89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Load csv file\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\n",
    "m$scale(list('blooms', 'water', 'shade')) # Scale\n",
    "m$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(blooms, water,shade){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n",
    "  bw = bi.dist.normal( 0,  0.25, name = 'bw')\n",
    "  bs = bi.dist.normal(  0,  0.25, name = 'bs')   \n",
    "  bws = bi.dist.normal(  0, 0.25, name = 'bws') \n",
    "  sigma = bi.dist.exponential(1, name = 's')\n",
    "  # Likelihood\n",
    "  m$normal(alpha + bw*water + bs*shade + bws*water*shade, sigma, obs=blooms)\n",
    "}\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    vector[27] blooms_std;\n",
    "    array[27] int shade_cent;\n",
    "    array[27] int water_cent;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real bs;\n",
    "    real bw;    \n",
    "    real bws;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[27] mu;\n",
    "    sigma ~ exponential( 1 );\n",
    "    bws ~ normal( 0 , 0.25 );\n",
    "    bs ~ normal( 0 , 0.25 );\n",
    "    bw ~ normal( 0 , 0.25 );\n",
    "    a ~ normal( 0.5 , 0.25 );\n",
    "    for ( i in 1:27 ) {\n",
    "        mu[i] = a + bw * water_cent[i] + bs * shade_cent[i] + bws * water_cent[i] * shade_cent[i];\n",
    "    }\n",
    "\n",
    "    \n",
    "    blooms_std ~ normal( mu , sigma );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'blooms_std' : m.df[\"blooms\"].values,\n",
    "    \"water_cent\": m.df[\"water\"].values.astype(int),\n",
    "    \"shade_cent\": m.df[\"shade\"].values.astype(int),\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "\n",
    "def model(blooms,shade, water):\n",
    "    sigma = m.dist.exponential(1, name = 'sigma')\n",
    "    bws = m.dist.normal(0, 0.25, name = 'bws')\n",
    "    bs = m.dist.normal(0, 0.25, name = 'bs')\n",
    "    bw = m.dist.normal(0, 0.25, name = 'bw')\n",
    "    a = m.dist.normal(0.5, 0.25, name = 'a')\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    m.dist.normal(mu, sigma, obs=blooms)\n",
    "\n",
    "def simulate_bloom(water, shade, sigma, bws, bs, bw, a ):\n",
    "    mu = a + bw*water + bs*shade + bws*water*shade\n",
    "    return  m.dist.normal(mu, sigma, sample=True) # bloom\n",
    "    \n",
    "def estimate(water, shade, sigma,bws, bs, bw, a ):\n",
    "    blooms = simulate_bloom(water, shade, sigma,bws, bs, bw, a ) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"water\": water, \"shade\": shade, \"blooms\": blooms})\n",
    "    #m.scale(['blooms', 'shade', 'blooms'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(water, shade, sigma, bws, bs, bw, a, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(water, shade, sigma[i], bws[i], bs[i], bw[i], a[i] )\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], bs[i], bw[i], bws[i], sigma[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "# Shade and water are all possible conbinations of shade (1 to 3) and water (1 to 3)\n",
    "m.data(data_path + 'tulips.csv', sep=';') \n",
    "shade = m.df.shade.values\n",
    "water = m.df.water.values\n",
    "sigma = m.dist.exponential(1, name = 'sigma', shape = (nsim, 1,), sample = True)\n",
    "bws = m.dist.normal(0, 0.25, name = 'bws', shape = (nsim, 1,), sample = True, seed = 1)\n",
    "bs = m.dist.normal(0, 0.25, name = 'bs', shape = (nsim, 1,), sample = True, seed = 2)\n",
    "bw = m.dist.normal(0, 0.25, name = 'bw', shape = (nsim, 1,), sample = True, seed = 3)\n",
    "a = m.dist.normal(0.5, 0.25, name = 'a', shape = (nsim, 1,), sample = True, seed = 4)\n",
    "\n",
    "result = param_recovery(water, shade, sigma,bws, bs, bw, a, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Binomial (model 11.1)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:01<00:00, 634.73it/s, 3 steps of size 9.99e-01. acc. prob=0.90]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0]  0.32  0.09      0.18       0.46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.data_to_model(['pulled_left'])\n",
    "def model(pulled_left):\n",
    "    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n",
    "    m.dist.binomial(logits=a[0], obs=pulled_left)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\n",
    "m$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(pulled_left){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal( 0, 10, name = 'alpha', shape=c(1))\n",
    "  # Likelihood\n",
    "  m$binomial(logits = alpha, obs=pulled_left)\n",
    "}\n",
    "\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    a ~ normal( 0 , 10 );\n",
    "    p = a;\n",
    "    p = inv_logit(p);\n",
    "    pulled_left ~ binomial( 1 , p );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : m.df[\"pulled_left\"].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(pulled_left):\n",
    "    a = m.dist.normal( 0, 10,shape = (1,), name = 'a')\n",
    "    m.dist.binomial(logits=a, obs=pulled_left)\n",
    "\n",
    "def sim_pulled_left(a):\n",
    "    return m.dist.binomial(logits=a, sample=True, shape=(1000,))\n",
    "\n",
    "def estimate(a):\n",
    "    pulled_left = sim_pulled_left(a)[:,0] # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"pulled_left\": pulled_left})\n",
    "    #m.scale(['blooms', 'shade', 'blooms'])\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(a, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : a[i], \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "a = m.dist.normal( 0, 1, shape = (nsim, 1), sample=True)\n",
    "result = param_recovery(a, nsim = nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Binomial with indices (model 11.4)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 438.16it/s, 7 steps of size 4.51e-01. acc. prob=0.90]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>3.93</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[2]</th>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[3]</th>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[4]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[5]</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[6]</th>\n",
       "      <td>2.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[1]</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[2]</th>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[3]</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0] -0.41  0.32     -0.91       0.07\n",
       "a[1]  3.93  0.75      2.84       5.25\n",
       "a[2] -0.71  0.32     -1.20      -0.22\n",
       "a[3] -0.71  0.32     -1.25      -0.20\n",
       "a[4] -0.41  0.32     -0.88       0.14\n",
       "a[5]  0.53  0.33      0.02       1.02\n",
       "a[6]  2.03  0.42      1.34       2.64\n",
       "b[0] -0.08  0.28     -0.50       0.39\n",
       "b[1]  0.44  0.27     -0.01       0.84\n",
       "b[2] -0.43  0.28     -0.89       0.00\n",
       "b[3]  0.33  0.26     -0.08       0.77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\n",
    "m.df['actor'] = m.df['actor'] - 1\n",
    "\n",
    "m.data_to_model(['actor', 'treatment', 'pulled_left'])\n",
    "\n",
    "def model(actor, treatment, pulled_left):\n",
    "    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n",
    "    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n",
    "    p = a[actor] + b[treatment]\n",
    "    m.dist.binomial(1, logits=p, obs=pulled_left)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\n",
    "m$df$treatment =  m$df$prosoc_left + 2 * m$df$condition\n",
    "m$df$actor = m$df$actor - 1\n",
    "keys <- c(\"actor\", \"treatment\", 'pulled_left')\n",
    "values <- list(jnp$array(as.integer(m$df$actor)),jnp$array(as.integer(m$df$treatment)), jnp$array(as.integer(m$df$pulled_left)))\n",
    "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(actor, treatment, pulled_left){\n",
    "  # Parameters priors distributions\n",
    "  a = bi.dist.normal( 0, 1.5, shape = c(7), name = 'a') # 7 actors\n",
    "  b = bi.dist.normal( 0, 0.5, shape = c(4), name = 'b') # 4 treatments\n",
    "  p = a[actor] + b[treatment]\n",
    "  # Likelihood\n",
    "  m$binomial(1,logits =  p, obs=pulled_left)\n",
    "}\n",
    "\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data{\n",
    "    array[504] int pulled_left;\n",
    "    array[504] int treatment;\n",
    "    array[504] int actor;\n",
    "}\n",
    "parameters{\n",
    "    vector[7] a;\n",
    "    vector[4] b;\n",
    "}\n",
    "model{\n",
    "    vector[504] p;\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    b ~ normal( 0 , 0.5 );    \n",
    "    for ( i in 1:504 ) {\n",
    "        p[i] = a[actor[i]] + b[treatment[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    pulled_left ~ binomial( 1 , p );\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'pulled_left' : m.df[\"pulled_left\"].values.astype(int),\n",
    "    'treatment' : m.df[\"treatment\"].values.astype(int) + 1,\n",
    "    'actor' : m.df[\"actor\"].values.astype(int) +1 \n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'chimpanzees.csv', sep=';') \n",
    "m.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\n",
    "m.df['actor'] = m.df['actor'] - 1\n",
    "\n",
    "def model(actor, treatment, pulled_left):\n",
    "    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n",
    "    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n",
    "    p = a[actor] + b[treatment]\n",
    "    m.dist.binomial(1, logits=p, obs=pulled_left)\n",
    "\n",
    "def sim_pulled_left(actor, treatment, a, b):\n",
    "    p = a[actor] + b[treatment]\n",
    "    return m.dist.binomial(1, logits=p, sample=True)\n",
    "\n",
    "def estimate(actor, treatment, a, b):\n",
    "    pulled_left = sim_pulled_left(actor, treatment, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"pulled_left\": pulled_left, \"actor\": actor, \"treatment\": treatment})\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(actor, treatment, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(actor, treatment, a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "actor = jnp.array(m.df['actor'])\n",
    "treatment = jnp.array(m.df['treatment'])\n",
    "a = m.dist.normal(0, 1.5, shape = (nsim, 7), name='a', sample=True)\n",
    "b = m.dist.normal(0, 0.5, shape = (nsim, 4), name='b', sample=True)\n",
    "\n",
    "result = param_recovery(actor, treatment, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[Poisson (model 11.10)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:01<00:00, 507.07it/s, 3 steps of size 6.07e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>3.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[1]</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0]  3.21  0.10      3.06       3.37\n",
       "a[1]  3.63  0.09      3.49       3.79\n",
       "b[0]  0.36  0.05      0.28       0.44\n",
       "b[1]  0.06  0.20     -0.27       0.36"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'Kline.csv', sep=';') \n",
    "m.scale(['population'])\n",
    "m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "#m.data_to_model(['total_tools', 'population', 'cid'])\n",
    "def model(cid, population, total_tools):\n",
    "    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n",
    "    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    m.dist.poisson(l, obs=total_tools)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Kline.csv\", sep = ''), sep=';')\n",
    "m$scale(list('population'))# Scale\n",
    "m$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\n",
    "m$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(total_tools, population, cid){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n",
    "  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n",
    "  l = jnp$exp(alpha[cid] + beta[cid]*population)\n",
    "  # Likelihood\n",
    "  m$poisson(l, obs=total_tools)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[10] int T;\n",
    "    vector[10] P;\n",
    "    array[10] int cid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    vector[2] b;\n",
    "}\n",
    "model{\n",
    "    vector[10] lambda;\n",
    "    b ~ normal( 0 , 0.2 );\n",
    "    a ~ normal( 3 , 0.5 );\n",
    "    for ( i in 1:10 ) {\n",
    "       lambda[i] = a[cid[i]] + b[cid[i]] * P[i];\n",
    "       lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    T ~ poisson( lambda );\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'T' : m.df[\"total_tools\"].values.astype(int),\n",
    "    'P' : m.df[\"population\"].values.astype(float),\n",
    "    'cid' : m.df[\"cid\"].values.astype(int) +1\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(cid, population, total_tools):\n",
    "    a = m.dist.normal(3, 0.5, shape = (2,), name='a')\n",
    "    b = m.dist.normal(0, 0.2, shape = (2,), name='b')\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    m.dist.poisson(l, obs=total_tools)\n",
    "\n",
    "def sim_total_tools(cid, population, a, b):\n",
    "    l = jnp.exp(a[cid] + b[cid]*population)\n",
    "    return m.dist.poisson(l, sample=True)\n",
    "\n",
    "def estimate(a, b):\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'Kline.csv', sep=';') \n",
    "    m.scale(['population'])\n",
    "    m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "\n",
    "    total_tools = sim_total_tools(m.df.cid.values, m.df.population.values, a, b) # Simulate data\n",
    "    m.df = pd.DataFrame({\"cid\": m.df.cid.values, 'population' :m.df.population.values, \"total_tools\": total_tools})\n",
    "    m.fit(model, num_samples=1000, num_warmup = 1000, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(cid, population, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "m.data(data_path + 'Kline.csv', sep=';') \n",
    "m.scale(['population'])\n",
    "m.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n",
    "cid = m.df.cid.values\n",
    "population = m.df.population.values\n",
    "a = m.dist.normal(3, 0.5, shape= (nsim, 2,), name='a', sample=True, seed = 1)\n",
    "b = m.dist.normal(0, 0.2, shape= (nsim, 2,), name='b', sample=True, seed = 3)\n",
    "result = param_recovery(cid, population, a, b, nsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dist.normal(0, 0.2, shape= (100, 2,), name='b', sample=True, seed = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[Negative binomial (model 11.12)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_1_'></a>[Simulated data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "import pandas as pd\n",
    "import random as random2\n",
    "import jax\n",
    "init_key, sample_key = jax.random.split(jax.random.PRNGKey(int(random2.randint(0, 10000000))))\n",
    "init_key = jnp.array(init_key)\n",
    "num_days = 3000\n",
    "y = tfd.Poisson(rate=1.5).sample(seed = init_key, sample_shape=(num_days,))\n",
    "num_weeks = 400\n",
    "y_new = tfd.Poisson(rate=0.5 * 7).sample(seed = init_key, sample_shape=(num_weeks,))\n",
    "y_all = np.concatenate([y, y_new])\n",
    "exposure = np.concatenate([np.repeat(1, num_days), np.repeat(7, num_weeks)])\n",
    "monastery = np.concatenate([np.repeat(0, num_days), np.repeat(1, num_weeks)])\n",
    "d = pd.DataFrame.from_dict(dict(y=y_all, days=exposure, monastery=monastery))\n",
    "d[\"log_days\"] = d.days.pipe(np.log)\n",
    "d.to_csv(data_path + 'Sim dat Gamma poisson.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2000/2000 [00:03<00:00, 526.93it/s, 3 steps of size 7.88e-01. acc. prob=0.89]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-1.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a  0.41  0.02      0.38       0.43\n",
       "b -1.11  0.03     -1.16      -1.06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim dat Gamma poisson.csv', sep=',') \n",
    "m.data_on_model={'y': jnp.array(m.df.y.values, dtype=int), 'log_days': jnp.array(m.df.log_days.values, dtype=float), 'monastery': jnp.array(m.df.monastery.values, dtype=int)}\n",
    "def model(log_days, monastery, y):\n",
    "    a = m.dist.normal(0, 1, name = 'a')\n",
    "    b = m.dist.normal(0, 1, name = 'b')\n",
    "    l = jnp.exp(log_days + a + b * monastery)\n",
    "    m.dist.poisson(rate = l, is_sparse=True, obs=y)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_warmup = 1000, num_samples=1000) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\n",
    "m$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(log_days, monastery, y){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n",
    "  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n",
    "  l = jnp$exp(log_days + alpha + beta * monastery)\n",
    "  # Likelihood\n",
    "  m$poisson(rate=l, obs=y)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[3400] int y;\n",
    "    array[3400] int monastery;\n",
    "    vector[3400] log_days;\n",
    "}\n",
    "parameters{\n",
    "    real a;\n",
    "    real b;\n",
    "}\n",
    "model{\n",
    "    vector[3400] lambda;\n",
    "    b ~ normal( 0 , 1 );\n",
    "    a ~ normal( 0 , 1 );\n",
    "    for ( i in 1:3400 ) {\n",
    "        lambda[i] = log_days[i] + a + b * monastery[i];\n",
    "        // B1 ~ exponential( 1 );\n",
    "        // gamma(lambda[i]*B1, B1);\n",
    "        lambda[i] = exp(lambda[i]);\n",
    "    }\n",
    "    \n",
    "    y ~ poisson( lambda );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'y' : np.array(m.data_on_model[\"y\"].astype(int)),\n",
    "    'monastery' : np.array(m.data_on_model[\"monastery\"].astype(int)) +1,\n",
    "    'log_days' : np.array(m.data_on_model[\"log_days\"].astype(float)),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=1000, num_warmup = 1000)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_5_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim dat Gamma poisson.csv', sep=',') \n",
    "import random as random2\n",
    "def model(log_days, monastery, y):\n",
    "    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n",
    "    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n",
    "    l = jnp.exp(log_days + a + b * monastery)\n",
    "    m.dist.poisson(rate = l, obs=y)\n",
    "\n",
    "\n",
    "def sim_rates(log_days, monastery, a, b):\n",
    "    l = jnp.exp(log_days + a +  b * monastery)\n",
    "    return m.dist.poisson(rate = l, sample=True)\n",
    "\n",
    "def estimate(log_days, monastery, a, b):\n",
    "    rates = sim_rates(log_days, monastery, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.df = pd.DataFrame({\"log_days\": log_days, \"monastery\": monastery, 'y': rates})\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(log_days, monastery, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(log_days, monastery, a[i], b[i] )\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "nsim = 100\n",
    "log_days = jnp.array(m.df.log_days.values)\n",
    "monastery = jnp.array(m.df.monastery.values)\n",
    "a = m.dist.normal(0, 1, name = 'a', sample=True, shape=(nsim, 1))\n",
    "b = m.dist.normal(0, 1, name = 'b', sample=True, shape=(nsim, 1))\n",
    "\n",
    "result = param_recovery(log_days, monastery, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[Multinomial (model 11.13)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_1_'></a>[Simulated data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate career choices among 500 individuals\n",
    "N = 500  # number of individuals\n",
    "income = jnp.array([1, 2, 5])  # expected income of each career\n",
    "score = 0.5 * income  # scores for each career, based on income\n",
    "# next line converts scores to probabilities\n",
    "p = jax.nn.softmax(score)\n",
    "\n",
    "# now simulate choice\n",
    "# outcome career holds event type values, not counts\n",
    "career = jnp.repeat(jnp.nan, N)  # empty vector of choices for each individual\n",
    "# sample chosen career for each individual\n",
    "for i in range(N):\n",
    "    career = career.at[i].set(\n",
    "        m.dist.categorical(probs=p, sample=True,seed=i)\n",
    "    )\n",
    "career = career.astype(jnp.int32)\n",
    "data = {'career': career, 'income': [income[index] for index in career]}\n",
    "d = pd.DataFrame(data)\n",
    "d.to_csv(data_path + 'Sim data multinomial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 399.98it/s, 7 steps of size 2.57e-01. acc. prob=0.84]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a[0]</th>\n",
       "      <td>-2.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a[1]</th>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b[0]</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a[0] -2.13  0.26     -2.49      -1.72\n",
       "a[1] -1.59  0.16     -1.83      -1.32\n",
       "b[0]  0.06  0.05      0.00       0.13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "m = bi('cpu')\n",
    "df=pd.read_csv(data_path + 'Sim data multinomial.csv')\n",
    "m.data_on_model={}\n",
    "m.data_on_model['career']=jnp.array(df.career.values)\n",
    "m.data_on_model['income']=jnp.array(df.income.unique()).astype(jnp.int32)\n",
    "def model(career, income ):\n",
    "    a = m.dist.normal(0, 1, shape= (2,), name = 'a')\n",
    "    b = m.dist.halfnormal(0.5,  shape= (1,),name = 'b')\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    m.dist.categorical(probs =  p, obs=career)\n",
    "\n",
    "# Run sampler ------------------------------------------------ \n",
    "m.fit(model)  \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim data multinomial.csv\", sep = ''), sep=',')\n",
    "keys <- c(\"income\", \"career\")\n",
    "income = unique(m$df$income)\n",
    "income = income[order(income)]\n",
    "values <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\n",
    "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(income, career){\n",
    "  # Parameters priors distributions\n",
    "  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n",
    "  beta = bi.dist.halfnormal(0.5, name='beta')\n",
    "  \n",
    "  s_1 = alpha[0] + beta * income[0]\n",
    "  s_2 = alpha[1] + beta * income[1]\n",
    "  s_3 = 0 # reference category\n",
    "\n",
    "  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n",
    "\n",
    "  # Likelihood\n",
    "  m$categorical(probs=p, obs=career)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    int N; // number of individuals\n",
    "    int K; // number of possible careers\n",
    "    array[N] int career; // outcome\n",
    "    vector[K] career_income;\n",
    "}\n",
    "parameters{\n",
    "    vector[K-1] a; // intercepts\n",
    "    real<lower=0> b; // association of income with choice\n",
    "}\n",
    "model{\n",
    "    vector[K] p;\n",
    "    vector[K] s;\n",
    "    a ~ normal( 0 , 1 );\n",
    "    b ~ normal( 0 , 0.5 );\n",
    "    s[1] = a[1] + b*career_income[1];\n",
    "    s[2] = a[2] + b*career_income[2];\n",
    "    s[3] = 0; // pivot\n",
    "    p = softmax( s );\n",
    "    career ~ categorical( p );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'N' : 500,\n",
    "    'K' : 3,\n",
    "    'career' : df[\"career\"].values.astype(int) + 1,\n",
    "    'career_income' : df[\"income\"].unique().astype(int).tolist(),\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_5_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multinomial.csv', sep=',') \n",
    "\n",
    "def model(career, income ):\n",
    "    a = m.dist.normal(0, 1, shape= (2,), name = 'a')\n",
    "    b = m.dist.halfnormal(0.5,  shape= (1,), name = 'b')\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    m.dist.categorical(probs = p, obs = career)\n",
    "    \n",
    "def sim_categories(income, career, a, b):\n",
    "    s_1 = a[0] + b * income[0]\n",
    "    s_2 = a[1] + b * income[1]\n",
    "    s_3 = [0] #pivot\n",
    "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
    "    return m.dist.categorical(probs =  p, sample=True, shape=(len(career),))\n",
    "\n",
    "def estimate(income, career, a, b):\n",
    "    career = sim_categories(income, career, a, b) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = {}\n",
    "    m.data_on_model['income'] = income\n",
    "    m.data_on_model['career'] = career\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(income, career, a, b, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(income, career,a[i], b[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a[i], b[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multinomial.csv', sep=',') \n",
    "nsim = 100\n",
    "income = jnp.unique(m.df.income.values)\n",
    "career = jnp.array(m.df.career.values)\n",
    "a = m.dist.normal(0, 1, shape= (nsim,2), name = 'a', sample=True)\n",
    "b = m.dist.halfnormal(0.5, shape=(nsim,1), name = 'b', sample=True)\n",
    "result = param_recovery(income, career, a, b, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_9_'></a>[Beta binomial (model m12.1)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:03<00:00, 323.84it/s, 3 steps of size 6.16e-01. acc. prob=0.91]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta</th>\n",
       "      <td>3.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.01</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean    sd  hdi_5.5%  hdi_94.5%\n",
       "alpha[0] -0.45  0.41     -1.07       0.22\n",
       "alpha[1] -0.33  0.44     -0.98       0.40\n",
       "phi       1.02  0.75      0.01       2.05\n",
       "theta     3.02  0.75      2.01       4.05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpyro\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m = bi()\n",
    "# import data ------------------------------------------------\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "\n",
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1,  name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta =  numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\n",
    "m$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\n",
    "m$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n",
    "\n",
    "# Define model ----------------------c--------------------------\n",
    "model <- function(gid, applications, admit){\n",
    "  # Parameters priors distributions\n",
    "  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n",
    "  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n",
    "  t = phi + 2\n",
    "  pbar = jax$nn$sigmoid(alpha[gid])\n",
    "  gamma = pbar * t\n",
    "  eta = (1 - pbar) * t\n",
    "  # Likelihood\n",
    "  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code =\"\"\" \n",
    "data{\n",
    "    array[12] int N;\n",
    "    array[12] int A;\n",
    "    array[12] int gid;\n",
    "}\n",
    "parameters{\n",
    "    vector[2] a;\n",
    "    real<lower=0> phi;\n",
    "}\n",
    "transformed parameters{\n",
    "    real theta;\n",
    "    theta = phi + 2;\n",
    "}\n",
    "model{\n",
    "    vector[12] pbar;\n",
    "    phi ~ exponential( 1 );\n",
    "    a ~ normal( 0 , 1.5 );\n",
    "    for ( i in 1:12 ) {\n",
    "        pbar[i] = a[gid[i]];\n",
    "        pbar[i] = inv_logit(pbar[i]);\n",
    "    }\n",
    "    A ~ beta_binomial( N , pbar*theta , (1-pbar)*theta );    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'A' : m.df[\"admit\"].values.astype(int),\n",
    "    'N' : m.df[\"applications\"].values.astype(int),\n",
    "    'gid' : m.df[\"gid\"].values.astype(int) +1,\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_9_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1, shape=(1,), name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta = numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "nsim = 10\n",
    "m = bi()\n",
    "nsim = 10\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "gid = m.df.gid.values\n",
    "phi = m.dist.exponential(1,  shape = (nsim,),sample=True)\n",
    "alpha = m.dist.normal( 0., 1.5, shape=(nsim,2),sample=True)\n",
    "theta = phi + 2\n",
    "pbar = jax.vmap(lambda x: jax.nn.sigmoid(x[gid]))(alpha)\n",
    "pbar.shape\n",
    "i = 0\n",
    "applications = jnp.array(m.df['applications'].values)\n",
    "concentration1 = pbar[i]*theta[i]\n",
    "concentration0 = (1 - pbar[i]) * theta[i]\n",
    "admit = m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0,sample=True)\n",
    "m = bi()\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "m.df['admit'] = admit\n",
    "m.data_to_model(['gid', 'applications', 'admit' ])\n",
    "print(m.data_on_model)\n",
    "m.fit(model, num_samples=500, progress_bar=False) \n",
    "s = m.summary()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(gid, applications, admit):\n",
    "    phi = m.dist.exponential(1,  name = 'phi')\n",
    "    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n",
    "    theta = numpyro.deterministic('theta', phi + 2)\n",
    "    pbar = jax.nn.sigmoid(alpha[gid])\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "\n",
    "    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n",
    "\n",
    "def sim_admit(theta, pbar, applications):\n",
    "    concentration1 = pbar*theta\n",
    "    concentration0 = (1 - pbar) * theta\n",
    "    return m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0,sample=True)\n",
    "\n",
    "\n",
    "def estimate(theta, pbar, applications):\n",
    "    admit = sim_admit(theta, pbar, applications) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "    m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "    m.df['admit'] = admit\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(phi,alpha, theta, pbar, applications, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(theta[i], pbar[i], applications)\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([alpha[i,][0][None], alpha[i,][1][None],phi[i][None],theta[i][None]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "m.data(data_path + 'UCBadmit.csv', sep=';') \n",
    "m.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n",
    "gid=jnp.array(m.df.gid.values)\n",
    "applications=jnp.array(m.df.applications.values)\n",
    "\n",
    "\n",
    "phi = m.dist.exponential(1,  shape = (nsim,),sample=True)\n",
    "alpha = m.dist.normal( 0., 1.5, shape=(nsim,2),sample=True)\n",
    "theta = phi + 2\n",
    "pbar = jax.vmap(lambda x: jax.nn.sigmoid(x[gid]))(alpha)\n",
    "param_recovery(phi,alpha, theta, pbar, applications, nsim) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_10_'></a>[Zero inflated outcomes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 433.95it/s, 7 steps of size 5.16e-01. acc. prob=0.92]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>al</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>-1.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean    sd  hdi_5.5%  hdi_94.5%\n",
       "al  0.11  0.08     -0.03       0.24\n",
       "ap -1.37  0.37     -1.86      -0.79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.scipy.special import expit\n",
    "import random as r\n",
    "r.seed(42)\n",
    "# Define parameters\n",
    "prob_drink = 0.2  # 20% of days\n",
    "rate_work = 1     # average 1 manuscript per day\n",
    "\n",
    "# sample one year of production\n",
    "N = 365\n",
    "\n",
    "np.random.seed(365)\n",
    "drink = np.random.binomial(1, prob_drink, N)\n",
    "y = (1 - drink) * np.random.poisson(rate_work, N)\n",
    "d = pd.DataFrame(y)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m  bi()\n",
    "# import data ------------------------------------------------\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    y = jnp.array(y)\n",
    ")\n",
    "\n",
    "def model(y):\n",
    "    ap = m.dist.normal( -1.5, 1,  name = 'ap')\n",
    "    p = expit(ap)\n",
    "\n",
    "    al = m.dist.normal( 1, 0.5,  name = 'al')\n",
    "    lambda_ = jnp.exp(al)    \n",
    "    \n",
    "    m.dist.zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Simulate data ------------------------------------------------\n",
    "prob_drink = 0.2  # 20% of days\n",
    "rate_work = 1     # average 1 manuscript per day\n",
    "# sample one year of production\n",
    "N = as.integer(365)\n",
    "drink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\n",
    "y = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\n",
    "data = list()\n",
    "data$y = y \n",
    "m$data_on_model = data\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(y){\n",
    "  al =  bi.dist.normal(0, 5, name='al', shape=c(1))\n",
    "  ap = bi.dist.normal(0, 5, name='ap', shape=c(1))\n",
    "  p = jax$scipy$special$expit(ap)\n",
    "  lambda_ = jnp$exp(al)\n",
    "  m$zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[365] int y;\n",
    "}\n",
    "parameters{\n",
    "    real al;\n",
    "    real ap;\n",
    "    \n",
    "}\n",
    "model{\n",
    "    real p;\n",
    "    real lambda;\n",
    "    al ~ normal( 1 , 0.5 );\n",
    "    ap ~ normal( -1.5 , 1 );   \n",
    "    \n",
    "    lambda = al;\n",
    "    lambda = exp(lambda);\n",
    "    p = ap;\n",
    "    p = inv_logit(p);\n",
    "    for ( i in 1:365 ) {\n",
    "        if ( y[i]==0 )\n",
    "            target += log_mix( p , 0 , poisson_lpmf(0|lambda) );\n",
    "        if ( y[i] > 0 )\n",
    "            target += log1m( p ) + poisson_lpmf(y[i] | lambda );\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'y' :d.iloc[:,0].values.astype(int)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_10_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "def model(y):\n",
    "    ap = m.dist.normal( -1.5, 1,  name = 'ap')\n",
    "    p = expit(ap)\n",
    "\n",
    "    al = m.dist.normal( 1, 0.5,  name = 'al')\n",
    "    lambda_ = jnp.exp(al)    \n",
    "    \n",
    "    m.dist.zeroinflatedpoisson(p, lambda_, obs=y)\n",
    "\n",
    "def sim_prod(prob_drink, rate_work ):\n",
    "    drink = m.dist.binomial(1, prob_drink, shape=(365,),sample=True)\n",
    "    y = (1 - drink) *  m.dist.poisson(rate_work, shape=(365,),sample=True)\n",
    "    return drink, y\n",
    "    \n",
    "def estimate(prob_drink, rate_work):\n",
    "    drink, y = sim_prod(prob_drink, rate_work) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = dict(y = y)\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(prob_drink, rate_work,  nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(prob_drink[i], rate_work[i])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([rate_work[i][None],prob_drink[i][None]]), \n",
    "                'estimations': estimations.values}\n",
    "        data = pd.DataFrame(data)\n",
    "        # Converting parameters to there oringinal scale\n",
    "        data.loc[data['parameter'].isin(['al']), 'estimations'] = jnp.exp(\n",
    "            data.loc[data['parameter'].isin(['al']), 'estimations'].values\n",
    "        )\n",
    "        data.loc[data['parameter'].isin(['ap']), 'estimations'] = expit(\n",
    "            data.loc[data['parameter'].isin(['ap']), 'estimations'].values\n",
    "        )\n",
    "        df = pd.concat([df, data], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "\n",
    "prob_drink = m.link.inv_logit(m.dist.normal(-1.5, 1, shape=(nsim,), sample=True, seed = 1))\n",
    "rate_work = jnp.exp(m.dist.normal(1, 0.5, shape=(nsim,), sample=True, seed = 10))\n",
    "tmp = param_recovery(prob_drink, rate_work, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_11_'></a>[OrderedLogistic (Todo: PB)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpyro.distributions as dist\n",
    "## setup platform------------------------------------------------\n",
    "#m = bi()\n",
    "## import data ------------------------------------------------\n",
    "#m.data('resources/data/Trolley.csv', sep=';') \n",
    "#d = m.df\n",
    "## discrete proportion of each response value\n",
    "#pr_k = d.response.value_counts().sort_index().values / d.shape[0]\n",
    "## cumsum converts to cumulative proportions\n",
    "#cum_pr_k = jnp.cumsum(pr_k, -1)\n",
    "#logit = lambda x: jnp.log(x / (1 - x))  # convenience function\n",
    "#lco = logit(cum_pr_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpyro.distributions as distnp\n",
    "#from numpyro.distributions.transforms import OrderedTransform\n",
    "## setup platform------------------------------------------------\n",
    "#m = bi()\n",
    "#m.data_on_model = dict(response = jnp.array(d.response.values - 1))\n",
    "#def model(response):\n",
    "#    cutpoints = numpyro.sample(\n",
    "#        \n",
    "#        distnp.TransformedDistribution(\"cutpoints\",\n",
    "#            distnp.Normal(0, 1.5), OrderedTransform()\n",
    "#        ),\n",
    "#    )\n",
    "#    numpyro.sample(\"R\", dist.OrderedLogistic(0, cutpoints), obs=response)\n",
    "#\n",
    "## Run sampler ------------------------------------------------\n",
    "#start = tm.time()    \n",
    "#m.fit(model) \n",
    "#end = tm.time()    \n",
    "#print(f\"BI took: {end - start:.4f} seconds\")\n",
    "#\n",
    "## Diagnostic ------------------------------------------------\n",
    "#m.sampler.print_summary(0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_12_'></a>[Varying interceps](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 410.21it/s, 15 steps of size 4.16e-01. acc. prob=0.89]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_bar</th>\n",
       "      <td>1.35</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>3.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.31</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[3]</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.40</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[4]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[5]</th>\n",
       "      <td>2.18</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[6]</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[7]</th>\n",
       "      <td>2.15</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[8]</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[9]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[10]</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[11]</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[12]</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[13]</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[14]</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[15]</th>\n",
       "      <td>2.14</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[16]</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.69</td>\n",
       "      <td>4.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[17]</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[18]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[19]</th>\n",
       "      <td>3.68</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[20]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[21]</th>\n",
       "      <td>2.45</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[22]</th>\n",
       "      <td>2.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[23]</th>\n",
       "      <td>1.66</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[24]</th>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[25]</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[26]</th>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[27]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[28]</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[29]</th>\n",
       "      <td>1.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[30]</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[31]</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[32]</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.88</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[33]</th>\n",
       "      <td>2.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[34]</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[35]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[36]</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[37]</th>\n",
       "      <td>3.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.37</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[38]</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[39]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.56</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[40]</th>\n",
       "      <td>-1.81</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[41]</th>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[42]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[43]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[44]</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[45]</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[46]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[47]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean    sd  hdi_5.5%  hdi_94.5%\n",
       "a_bar      1.35  0.24      0.97       1.72\n",
       "alpha[0]   2.05  0.85      0.65       3.31\n",
       "alpha[1]   3.13  1.13      1.31       4.81\n",
       "alpha[2]   0.99  0.62     -0.10       1.80\n",
       "alpha[3]   3.07  1.09      1.40       4.78\n",
       "alpha[4]   2.19  0.91      0.55       3.38\n",
       "alpha[5]   2.18  0.86      0.81       3.39\n",
       "alpha[6]   3.12  1.13      1.44       4.90\n",
       "alpha[7]   2.15  0.84      0.68       3.27\n",
       "alpha[8]  -0.17  0.60     -1.19       0.63\n",
       "alpha[9]   2.04  0.83      0.86       3.44\n",
       "alpha[10]  1.04  0.72     -0.13       2.09\n",
       "alpha[11]  0.56  0.61     -0.46       1.51\n",
       "alpha[12]  0.99  0.72     -0.08       2.08\n",
       "alpha[13]  0.23  0.62     -0.71       1.25\n",
       "alpha[14]  2.09  0.84      0.75       3.32\n",
       "alpha[15]  2.14  0.94      0.65       3.41\n",
       "alpha[16]  2.90  0.79      1.69       4.11\n",
       "alpha[17]  2.44  0.66      1.40       3.47\n",
       "alpha[18]  2.04  0.57      1.23       2.98\n",
       "alpha[19]  3.68  1.02      2.00       5.04\n",
       "alpha[20]  2.35  0.65      1.36       3.43\n",
       "alpha[21]  2.45  0.71      1.36       3.52\n",
       "alpha[22]  2.37  0.69      1.29       3.51\n",
       "alpha[23]  1.66  0.49      0.80       2.41\n",
       "alpha[24] -1.01  0.43     -1.54      -0.23\n",
       "alpha[25]  0.14  0.44     -0.47       0.81\n",
       "alpha[26] -1.43  0.51     -2.18      -0.64\n",
       "alpha[27] -0.47  0.42     -1.06       0.25\n",
       "alpha[28]  0.17  0.40     -0.45       0.78\n",
       "alpha[29]  1.47  0.51      0.55       2.19\n",
       "alpha[30] -0.65  0.44     -1.44      -0.02\n",
       "alpha[31] -0.31  0.42     -1.01       0.37\n",
       "alpha[32]  3.17  0.83      1.88       4.36\n",
       "alpha[33]  2.67  0.60      1.70       3.50\n",
       "alpha[34]  2.71  0.67      1.65       3.74\n",
       "alpha[35]  2.07  0.52      1.36       2.88\n",
       "alpha[36]  2.09  0.55      1.24       2.83\n",
       "alpha[37]  3.87  0.95      2.37       5.27\n",
       "alpha[38]  2.69  0.64      1.65       3.57\n",
       "alpha[39]  2.35  0.56      1.56       3.34\n",
       "alpha[40] -1.81  0.45     -2.50      -1.11\n",
       "alpha[41] -0.59  0.33     -1.17      -0.12\n",
       "alpha[42] -0.47  0.37     -1.04       0.13\n",
       "alpha[43] -0.35  0.36     -0.88       0.31\n",
       "alpha[44]  0.59  0.33      0.10       1.17\n",
       "alpha[45] -0.58  0.36     -1.10       0.02\n",
       "alpha[46]  2.07  0.47      1.40       2.86\n",
       "alpha[47]  0.00  0.34     -0.47       0.60\n",
       "sigma      1.62  0.22      1.28       1.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# Import Data & Data Manipulation ------------------------------------------------\n",
    "# Import\n",
    "from importlib.resources import files\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "# Manipulate\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0]) \n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "def model(tank, surv, density):\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n",
    "    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n",
    "    p = alpha[tank]\n",
    "    m.dist.binomial(total_count = density, logits = p, obs=surv)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 436.53it/s, 7 steps of size 4.93e-01. acc. prob=0.85]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>global_intercept_tank</th>\n",
       "      <td>1.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[0]</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[1]</th>\n",
       "      <td>3.14</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.19</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[2]</th>\n",
       "      <td>1.04</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[3]</th>\n",
       "      <td>3.08</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.31</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[4]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[5]</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[6]</th>\n",
       "      <td>3.16</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[7]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[8]</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[9]</th>\n",
       "      <td>2.13</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[10]</th>\n",
       "      <td>1.07</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[11]</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[12]</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[13]</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[14]</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[15]</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[16]</th>\n",
       "      <td>2.94</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[17]</th>\n",
       "      <td>2.42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[18]</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[19]</th>\n",
       "      <td>3.69</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.24</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[20]</th>\n",
       "      <td>2.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[21]</th>\n",
       "      <td>2.42</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[22]</th>\n",
       "      <td>2.44</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[23]</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[24]</th>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[25]</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[26]</th>\n",
       "      <td>-1.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[27]</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[28]</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[29]</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[30]</th>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[31]</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[32]</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.02</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[33]</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[34]</th>\n",
       "      <td>2.76</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.71</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[35]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[36]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[37]</th>\n",
       "      <td>3.94</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[38]</th>\n",
       "      <td>2.71</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[39]</th>\n",
       "      <td>2.35</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[40]</th>\n",
       "      <td>-1.81</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[41]</th>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[42]</th>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[43]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[44]</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[45]</th>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[46]</th>\n",
       "      <td>2.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_tank[47]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd_tank</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean    sd  hdi_5.5%  hdi_94.5%\n",
       "global_intercept_tank  1.37  0.27      0.98       1.80\n",
       "intercept_tank[0]      2.11  0.87      0.81       3.54\n",
       "intercept_tank[1]      3.14  1.13      1.19       4.78\n",
       "intercept_tank[2]      1.04  0.65     -0.04       2.00\n",
       "intercept_tank[3]      3.08  1.15      1.31       4.73\n",
       "intercept_tank[4]      2.19  0.87      0.85       3.54\n",
       "intercept_tank[5]      2.20  0.94      0.81       3.63\n",
       "intercept_tank[6]      3.16  1.21      1.35       5.09\n",
       "intercept_tank[7]      2.19  0.88      0.79       3.45\n",
       "intercept_tank[8]     -0.18  0.58     -1.02       0.80\n",
       "intercept_tank[9]      2.13  0.88      0.82       3.45\n",
       "intercept_tank[10]     1.07  0.75     -0.10       2.18\n",
       "intercept_tank[11]     0.56  0.66     -0.55       1.61\n",
       "intercept_tank[12]     1.01  0.69     -0.04       2.05\n",
       "intercept_tank[13]     0.23  0.59     -0.71       1.10\n",
       "intercept_tank[14]     2.19  0.89      0.97       3.70\n",
       "intercept_tank[15]     2.11  0.90      0.70       3.49\n",
       "intercept_tank[16]     2.94  0.78      1.63       4.08\n",
       "intercept_tank[17]     2.42  0.67      1.33       3.43\n",
       "intercept_tank[18]     2.04  0.60      1.06       2.86\n",
       "intercept_tank[19]     3.69  0.97      2.24       5.18\n",
       "intercept_tank[20]     2.40  0.67      1.31       3.48\n",
       "intercept_tank[21]     2.42  0.71      1.33       3.52\n",
       "intercept_tank[22]     2.44  0.71      1.23       3.46\n",
       "intercept_tank[23]     1.70  0.53      0.90       2.48\n",
       "intercept_tank[24]    -0.97  0.47     -1.67      -0.24\n",
       "intercept_tank[25]     0.14  0.43     -0.55       0.77\n",
       "intercept_tank[26]    -1.42  0.50     -2.15      -0.65\n",
       "intercept_tank[27]    -0.47  0.40     -1.12       0.08\n",
       "intercept_tank[28]     0.18  0.40     -0.46       0.76\n",
       "intercept_tank[29]     1.48  0.50      0.78       2.31\n",
       "intercept_tank[30]    -0.63  0.45     -1.30       0.06\n",
       "intercept_tank[31]    -0.34  0.43     -1.01       0.38\n",
       "intercept_tank[32]     3.17  0.79      2.02       4.55\n",
       "intercept_tank[33]     2.72  0.64      1.74       3.78\n",
       "intercept_tank[34]     2.76  0.68      1.71       3.80\n",
       "intercept_tank[35]     2.07  0.48      1.43       2.91\n",
       "intercept_tank[36]     2.07  0.51      1.27       2.85\n",
       "intercept_tank[37]     3.94  1.01      2.43       5.48\n",
       "intercept_tank[38]     2.71  0.64      1.61       3.51\n",
       "intercept_tank[39]     2.35  0.51      1.58       3.08\n",
       "intercept_tank[40]    -1.81  0.47     -2.54      -1.11\n",
       "intercept_tank[41]    -0.56  0.35     -1.10      -0.01\n",
       "intercept_tank[42]    -0.45  0.34     -1.02       0.02\n",
       "intercept_tank[43]    -0.35  0.34     -0.86       0.20\n",
       "intercept_tank[44]     0.59  0.33      0.04       1.09\n",
       "intercept_tank[45]    -0.56  0.37     -1.09       0.08\n",
       "intercept_tank[46]     2.07  0.50      1.21       2.71\n",
       "intercept_tank[47]     0.00  0.35     -0.51       0.58\n",
       "sd_tank                1.63  0.20      1.31       1.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "# Import Data & Data Manipulation ------------------------------------------------\n",
    "# Import\n",
    "from importlib.resources import files\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "# Manipulate\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0]) \n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "def model(tank, surv, density):\n",
    "    alpha = m.effects.varying_intercept(N_groups=48,group_idx=tank,group_name = 'tank')\n",
    "    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/reedfrogs.csv\", sep = ''), sep=';')\n",
    "m$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\n",
    "m$data_to_model(list('tank', 'surv', 'density')) # Manipulate\n",
    "m$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\n",
    "m$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n",
    "\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(tank, surv, density){\n",
    "  # Parameters priors distributions\n",
    "  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n",
    "  a_bar = bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n",
    "  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n",
    "  p = alpha[tank]\n",
    "  # Likelihood\n",
    "  m$binomial(total_count = density, logits = p, obs=surv)\n",
    "} \n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_3_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    array[48] int N;\n",
    "    array[48] int S;\n",
    "    array[48] int tank;\n",
    "}\n",
    "parameters{\n",
    "    real a_bar;\n",
    "    vector[48] a;    \n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "model{\n",
    "    vector[48] p;\n",
    "    sigma ~ exponential( 1 );\n",
    "    a_bar ~ normal( 0 , 1.5 );\n",
    "    a ~ normal( a_bar , sigma );\n",
    "    for ( i in 1:48 ) {\n",
    "        p[i] = a[tank[i]];\n",
    "        p[i] = inv_logit(p[i]);\n",
    "    }\n",
    "    S ~ binomial( N , p );\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'S' : m.df['surv'].values.astype(int),\n",
    "    'N' : m.df['density'].values.astype(int),\n",
    "    'tank' : m.df['tank'].values.astype(int)+1,\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_4_'></a>[Output comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_12_5_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def model(tank, surv, density):\n",
    "    sigma = m.dist.exponential( 1, shape=(1,), name = 'sigma')\n",
    "    a_bar = m.dist.normal( 0., 1.5, shape=(1,), name = 'a_bar')\n",
    "    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n",
    "    p = alpha[tank]\n",
    "    m.dist.binomial(total_count = density, logits = p, obs=surv)\n",
    "\n",
    "def sim_surv(tank, density, sigma, a_bar, alpha):\n",
    "    p = alpha[tank]\n",
    "    return m.dist.binomial(total_count = density, logits = p, sample=True)\n",
    "\n",
    "def estimate(tank, density, sigma, a_bar, alpha):\n",
    "    surv = sim_surv(tank, density, sigma, a_bar, alpha) # Simulate data\n",
    "    # Run model\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "    m.df[\"tank\"] = np.arange(m.df.shape[0])\n",
    "    m.df['surv']=surv\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(tank, density, sigma, a_bar, alpha, nsim):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(tank, density, sigma[i], a_bar[i], alpha[:,i,0])\n",
    "        data = {'sim': np.repeat(i, len(estimations.index.values)), \n",
    "                'parameter': estimations.index.values, \n",
    "                'simulated' : jnp.concatenate([a_bar[i], alpha[:,i,0], sigma[i]]), \n",
    "                'estimations': estimations.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "m.data(data_path + 'reedfrogs.csv', sep=';') \n",
    "nsim = 100\n",
    "m.df[\"tank\"] = np.arange(m.df.shape[0])\n",
    "tank = jnp.array(m.df[\"tank\"].values)\n",
    "density = jnp.array(m.df[\"density\"].values)\n",
    "sigma = m.dist.exponential( 1, shape = (nsim,1),  sample=True)\n",
    "a_bar = m.dist.normal( 0., 1.5,  shape = (nsim,1), sample=True)    \n",
    "alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha', sample=True)\n",
    "param_recovery(tank, density, sigma, a_bar, alpha, nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_13_'></a>[Varying effects](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_1_'></a>[Data simulation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro.distributions as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "newPath = os.path.dirname(os.path.abspath(\"\"))\n",
    "if newPath not in sys.path:\n",
    "    sys.path.append(newPath)\n",
    "\n",
    "\n",
    "import time as tm\n",
    "# setup platform------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "\n",
    "a = 3.5  # average morning wait time\n",
    "b = -1  # average difference afternoon wait time\n",
    "sigma_a = 1  # std dev in intercepts\n",
    "sigma_b = 0.5  # std dev in slopes\n",
    "rho = -0.7  # correlation between intercepts and slopes\n",
    "Mu = jnp.array([a, b])\n",
    "cov_ab = sigma_a * sigma_b * rho\n",
    "Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n",
    "jnp.array([1, 2, 3, 4]).reshape(2, 2).T\n",
    "sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\n",
    "Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "# now matrix multiply to get covariance matrix\n",
    "Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\n",
    "\n",
    "N_cafes = 20\n",
    "seed = jax.random.PRNGKey(5)  # used to replicate example\n",
    "vary_effects = m.dist.multivariatenormal(Mu, Sigma, shape=(N_cafes,), sample = True)\n",
    "a_cafe = vary_effects[:, 0]\n",
    "b_cafe = vary_effects[:, 1]\n",
    "\n",
    "seed = jax.random.PRNGKey(22)\n",
    "N_visits = 10\n",
    "afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\n",
    "cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\n",
    "mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n",
    "sigma = 0.5  # std dev within cafes\n",
    "wait = m.dist.normal(mu, sigma, sample = True)\n",
    "d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n",
    "d.to_csv(data_path + 'Sim data multivariatenormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_2_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build in function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:05<00:00, 181.51it/s, 15 steps of size 3.12e-01. acc. prob=0.85]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L_corr_cafe[0, 0]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_corr_cafe[0, 1]</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_corr_cafe[1, 0]</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_corr_cafe[1, 1]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[0, 0]</th>\n",
       "      <td>2.99</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[0, 1]</th>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[1, 0]</th>\n",
       "      <td>1.99</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[1, 1]</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[2, 0]</th>\n",
       "      <td>2.86</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[2, 1]</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[3, 0]</th>\n",
       "      <td>4.44</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[3, 1]</th>\n",
       "      <td>-1.34</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[4, 0]</th>\n",
       "      <td>3.33</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[4, 1]</th>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[5, 0]</th>\n",
       "      <td>3.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.51</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[5, 1]</th>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[6, 0]</th>\n",
       "      <td>4.46</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[6, 1]</th>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[7, 0]</th>\n",
       "      <td>3.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[7, 1]</th>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[8, 0]</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[8, 1]</th>\n",
       "      <td>-1.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[9, 0]</th>\n",
       "      <td>3.04</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[9, 1]</th>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[10, 0]</th>\n",
       "      <td>4.62</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[10, 1]</th>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>-0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[11, 0]</th>\n",
       "      <td>4.05</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[11, 1]</th>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[12, 0]</th>\n",
       "      <td>3.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.37</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[12, 1]</th>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[13, 0]</th>\n",
       "      <td>3.51</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[13, 1]</th>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>-0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[14, 0]</th>\n",
       "      <td>4.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[14, 1]</th>\n",
       "      <td>-1.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>-0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[15, 0]</th>\n",
       "      <td>5.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.69</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[15, 1]</th>\n",
       "      <td>-1.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[16, 0]</th>\n",
       "      <td>1.72</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[16, 1]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[17, 0]</th>\n",
       "      <td>4.36</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[17, 1]</th>\n",
       "      <td>-1.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[18, 0]</th>\n",
       "      <td>3.82</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[18, 1]</th>\n",
       "      <td>-1.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[19, 0]</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_params[19, 1]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_std[0]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cafe_std[1]</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean    sd  hdi_5.5%  hdi_94.5%\n",
       "L_corr_cafe[0, 0]   1.00  0.00      1.00       1.00\n",
       "L_corr_cafe[0, 1]  -0.67  0.16     -0.88      -0.44\n",
       "L_corr_cafe[1, 0]  -0.67  0.16     -0.88      -0.44\n",
       "L_corr_cafe[1, 1]   1.00  0.00      1.00       1.00\n",
       "a                   3.53  0.20      3.17       3.82\n",
       "b                  -0.95  0.12     -1.15      -0.77\n",
       "cafe_params[0, 0]   2.99  0.20      2.64       3.29\n",
       "cafe_params[0, 1]  -0.63  0.25     -1.04      -0.28\n",
       "cafe_params[1, 0]   1.99  0.22      1.65       2.33\n",
       "cafe_params[1, 1]  -0.15  0.28     -0.62       0.25\n",
       "cafe_params[2, 0]   2.86  0.22      2.53       3.21\n",
       "cafe_params[2, 1]  -0.68  0.26     -1.07      -0.28\n",
       "cafe_params[3, 0]   4.44  0.20      4.08       4.72\n",
       "cafe_params[3, 1]  -1.34  0.25     -1.71      -0.93\n",
       "cafe_params[4, 0]   3.33  0.21      2.98       3.65\n",
       "cafe_params[4, 1]  -1.09  0.26     -1.49      -0.66\n",
       "cafe_params[5, 0]   3.81  0.20      3.51       4.13\n",
       "cafe_params[5, 1]  -0.88  0.26     -1.28      -0.50\n",
       "cafe_params[6, 0]   4.46  0.20      4.14       4.77\n",
       "cafe_params[6, 1]  -1.52  0.25     -1.83      -1.04\n",
       "cafe_params[7, 0]   3.52  0.19      3.22       3.80\n",
       "cafe_params[7, 1]  -1.07  0.23     -1.45      -0.72\n",
       "cafe_params[8, 0]   3.70  0.21      3.38       4.01\n",
       "cafe_params[8, 1]  -1.16  0.24     -1.50      -0.76\n",
       "cafe_params[9, 0]   3.04  0.22      2.73       3.39\n",
       "cafe_params[9, 1]  -1.04  0.26     -1.46      -0.65\n",
       "cafe_params[10, 0]  4.62  0.21      4.29       4.95\n",
       "cafe_params[10, 1] -1.07  0.26     -1.43      -0.61\n",
       "cafe_params[11, 0]  4.05  0.22      3.70       4.39\n",
       "cafe_params[11, 1] -0.89  0.25     -1.32      -0.52\n",
       "cafe_params[12, 0]  3.72  0.22      3.37       4.04\n",
       "cafe_params[12, 1] -0.53  0.27     -1.02      -0.15\n",
       "cafe_params[13, 0]  3.51  0.22      3.17       3.86\n",
       "cafe_params[13, 1] -1.06  0.26     -1.51      -0.66\n",
       "cafe_params[14, 0]  4.08  0.22      3.77       4.47\n",
       "cafe_params[14, 1] -1.36  0.27     -1.81      -0.96\n",
       "cafe_params[15, 0]  5.03  0.21      4.69       5.35\n",
       "cafe_params[15, 1] -1.50  0.26     -1.90      -1.09\n",
       "cafe_params[16, 0]  1.72  0.21      1.35       2.03\n",
       "cafe_params[16, 1] -0.35  0.27     -0.79       0.03\n",
       "cafe_params[17, 0]  4.36  0.22      4.01       4.67\n",
       "cafe_params[17, 1] -1.03  0.28     -1.46      -0.59\n",
       "cafe_params[18, 0]  3.82  0.20      3.56       4.18\n",
       "cafe_params[18, 1] -1.17  0.23     -1.49      -0.79\n",
       "cafe_params[19, 0]  1.28  0.21      0.94       1.59\n",
       "cafe_params[19, 1] -0.41  0.25     -0.84      -0.02\n",
       "cafe_std[0]         1.00  0.17      0.77       1.27\n",
       "cafe_std[1]         0.45  0.11      0.26       0.60\n",
       "sigma               0.51  0.03      0.46       0.54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import data ------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multivariatenormal.csv', sep = ',')\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n",
    "    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n",
    "    N_cafes = len(m.df.cafe.unique()),\n",
    "    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n",
    ")\n",
    "from functools import partial\n",
    "from jax import jit\n",
    "\n",
    "static_config = {\n",
    "    'N_cafes': len(m.df.cafe.unique())\n",
    "}\n",
    "\n",
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2,  name = 'a')\n",
    "    b = m.dist.normal(-1, 0.5, name = 'b')\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "\n",
    "    varying_intercept, varying_slope = m.effects.varying_intercept_slope(\n",
    "        N_group = N_cafes,\n",
    "        group = cafe,\n",
    "        global_intercept= a,\n",
    "        global_slope= b,\n",
    "        group_name = 'cafe')\n",
    "    \n",
    "\n",
    "    mu = varying_intercept + varying_slope* afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:05<00:00, 187.18it/s, 31 steps of size 2.46e-01. acc. prob=0.89]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5.5%</th>\n",
       "      <th>hdi_94.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rho[0, 0]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[0, 1]</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 0]</th>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho[1, 1]</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[0, 0]</th>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[0, 1]</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[1, 0]</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[1, 1]</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[2, 0]</th>\n",
       "      <td>2.85</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[2, 1]</th>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[3, 0]</th>\n",
       "      <td>4.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[3, 1]</th>\n",
       "      <td>-1.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[4, 0]</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[4, 1]</th>\n",
       "      <td>-1.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[5, 0]</th>\n",
       "      <td>3.83</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.46</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[5, 1]</th>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[6, 0]</th>\n",
       "      <td>4.48</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[6, 1]</th>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[7, 0]</th>\n",
       "      <td>3.52</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[7, 1]</th>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[8, 0]</th>\n",
       "      <td>3.68</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[8, 1]</th>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>-0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[9, 0]</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[9, 1]</th>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[10, 0]</th>\n",
       "      <td>4.64</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[10, 1]</th>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[11, 0]</th>\n",
       "      <td>4.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[11, 1]</th>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[12, 0]</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.39</td>\n",
       "      <td>4.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[12, 1]</th>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[13, 0]</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[13, 1]</th>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[14, 0]</th>\n",
       "      <td>4.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[14, 1]</th>\n",
       "      <td>-1.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[15, 0]</th>\n",
       "      <td>5.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[15, 1]</th>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[16, 0]</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[16, 1]</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[17, 0]</th>\n",
       "      <td>4.37</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[17, 1]</th>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[18, 0]</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.47</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[18, 1]</th>\n",
       "      <td>-1.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>-0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[19, 0]</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_b_cafe[19, 1]</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_cafe[0]</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_cafe[1]</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean    sd  hdi_5.5%  hdi_94.5%\n",
       "Rho[0, 0]        1.00  0.00      1.00       1.00\n",
       "Rho[0, 1]       -0.68  0.17     -0.91      -0.44\n",
       "Rho[1, 0]       -0.68  0.17     -0.91      -0.44\n",
       "Rho[1, 1]        1.00  0.00      1.00       1.00\n",
       "a                3.53  0.23      3.16       3.85\n",
       "a_b_cafe[0, 0]   3.02  0.21      2.71       3.36\n",
       "a_b_cafe[0, 1]  -0.67  0.25     -1.05      -0.26\n",
       "a_b_cafe[1, 0]   2.01  0.24      1.65       2.40\n",
       "a_b_cafe[1, 1]  -0.18  0.30     -0.69       0.27\n",
       "a_b_cafe[2, 0]   2.85  0.21      2.55       3.21\n",
       "a_b_cafe[2, 1]  -0.66  0.25     -1.05      -0.27\n",
       "a_b_cafe[3, 0]   4.43  0.21      4.10       4.73\n",
       "a_b_cafe[3, 1]  -1.33  0.25     -1.70      -0.94\n",
       "a_b_cafe[4, 0]   3.35  0.21      3.00       3.69\n",
       "a_b_cafe[4, 1]  -1.12  0.26     -1.53      -0.73\n",
       "a_b_cafe[5, 0]   3.83  0.22      3.46       4.14\n",
       "a_b_cafe[5, 1]  -0.90  0.24     -1.27      -0.50\n",
       "a_b_cafe[6, 0]   4.48  0.21      4.18       4.81\n",
       "a_b_cafe[6, 1]  -1.52  0.26     -1.90      -1.12\n",
       "a_b_cafe[7, 0]   3.52  0.21      3.22       3.87\n",
       "a_b_cafe[7, 1]  -1.06  0.25     -1.44      -0.67\n",
       "a_b_cafe[8, 0]   3.68  0.19      3.33       3.96\n",
       "a_b_cafe[8, 1]  -1.15  0.24     -1.55      -0.82\n",
       "a_b_cafe[9, 0]   3.05  0.20      2.72       3.35\n",
       "a_b_cafe[9, 1]  -1.05  0.24     -1.40      -0.63\n",
       "a_b_cafe[10, 0]  4.64  0.20      4.32       4.94\n",
       "a_b_cafe[10, 1] -1.10  0.23     -1.48      -0.75\n",
       "a_b_cafe[11, 0]  4.04  0.20      3.70       4.36\n",
       "a_b_cafe[11, 1] -0.89  0.25     -1.26      -0.45\n",
       "a_b_cafe[12, 0]  3.75  0.23      3.39       4.11\n",
       "a_b_cafe[12, 1] -0.55  0.29     -1.03      -0.09\n",
       "a_b_cafe[13, 0]  3.53  0.20      3.18       3.81\n",
       "a_b_cafe[13, 1] -1.08  0.25     -1.49      -0.68\n",
       "a_b_cafe[14, 0]  4.09  0.21      3.74       4.40\n",
       "a_b_cafe[14, 1] -1.37  0.25     -1.79      -1.01\n",
       "a_b_cafe[15, 0]  5.02  0.21      4.70       5.34\n",
       "a_b_cafe[15, 1] -1.49  0.26     -1.87      -1.03\n",
       "a_b_cafe[16, 0]  1.70  0.22      1.33       2.03\n",
       "a_b_cafe[16, 1] -0.35  0.26     -0.78       0.02\n",
       "a_b_cafe[17, 0]  4.37  0.20      4.04       4.69\n",
       "a_b_cafe[17, 1] -1.06  0.25     -1.43      -0.64\n",
       "a_b_cafe[18, 0]  3.80  0.20      3.47       4.09\n",
       "a_b_cafe[18, 1] -1.17  0.24     -1.56      -0.83\n",
       "a_b_cafe[19, 0]  1.28  0.20      0.97       1.57\n",
       "a_b_cafe[19, 1] -0.41  0.25     -0.80      -0.00\n",
       "b               -0.95  0.12     -1.13      -0.77\n",
       "sigma            0.51  0.03      0.47       0.56\n",
       "sigma_cafe[0]    1.01  0.16      0.74       1.22\n",
       "sigma_cafe[1]    0.45  0.10      0.30       0.64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data ------------------------------------------------\n",
    "m = bi()\n",
    "m.data(data_path + 'Sim data multivariatenormal.csv', sep = ',')\n",
    "\n",
    "m.data_on_model = dict(\n",
    "    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n",
    "    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n",
    "    N_cafes = len(m.df.cafe.unique()),\n",
    "    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n",
    ")\n",
    "\n",
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2,  name = 'a')\n",
    "    b = m.dist.normal(-1, 0.5, name = 'b')\n",
    "    sigma = m.dist.exponential( 1,  name = 'sigma')\n",
    "\n",
    "    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n",
    "    Rho = m.dist.lkj(2, 2, name = 'Rho')\n",
    "    \n",
    "    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n",
    "\n",
    "    a_cafe_b_cafe = m.dist.multivariate_normal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n",
    "\n",
    "    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n",
    "    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Diagnostic ------------------------------------------------\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.local_device_count 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:55<00:00, 17.91it/s, 1023 steps of size 2.77e-04. acc. prob=0.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                      mean       std    median      5.5%     94.5%     n_eff     r_hat\n",
      "L_Rho_actor[0,0]      1.00      0.00      1.00      1.00      1.00       nan       nan\n",
      "L_Rho_actor[0,1]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[0,2]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[0,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[1,0]      0.21      0.10      0.21      0.06      0.36     21.96      1.04\n",
      "L_Rho_actor[1,1]      0.97      0.02      0.98      0.94      1.00     21.69      1.05\n",
      "L_Rho_actor[1,2]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[1,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[2,0]      0.77      0.05      0.77      0.71      0.86      9.84      1.11\n",
      "L_Rho_actor[2,1]     -0.10      0.05     -0.09     -0.17     -0.03      5.14      1.02\n",
      "L_Rho_actor[2,2]      0.62      0.06      0.63      0.54      0.72     10.92      1.11\n",
      "L_Rho_actor[2,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_actor[3,0]      0.48      0.10      0.50      0.35      0.65      3.45      2.39\n",
      "L_Rho_actor[3,1]     -0.09      0.32     -0.16     -0.57      0.31      2.93      3.16\n",
      "L_Rho_actor[3,2]     -0.55      0.05     -0.55     -0.62     -0.47      6.38      1.42\n",
      "L_Rho_actor[3,3]      0.58      0.04      0.58      0.53      0.64     15.04      1.00\n",
      "L_Rho_block[0,0]      1.00      0.00      1.00      1.00      1.00       nan       nan\n",
      "L_Rho_block[0,1]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[0,2]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[0,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[1,0]     -0.78      0.01     -0.79     -0.80     -0.76      5.13      1.24\n",
      "L_Rho_block[1,1]      0.62      0.02      0.62      0.60      0.65      5.17      1.23\n",
      "L_Rho_block[1,2]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[1,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[2,0]     -0.30      0.16     -0.32     -0.58     -0.08     12.71      1.10\n",
      "L_Rho_block[2,1]      0.38      0.07      0.38      0.27      0.50      5.94      1.81\n",
      "L_Rho_block[2,2]      0.86      0.05      0.86      0.79      0.94     16.86      1.01\n",
      "L_Rho_block[2,3]      0.00      0.00      0.00      0.00      0.00       nan       nan\n",
      "L_Rho_block[3,0]     -0.86      0.01     -0.86     -0.88     -0.84      7.19      1.01\n",
      "L_Rho_block[3,1]      0.11      0.13      0.11     -0.07      0.31      4.00      1.26\n",
      "L_Rho_block[3,2]      0.03      0.11      0.02     -0.11      0.20      7.33      1.00\n",
      "L_Rho_block[3,3]      0.47      0.05      0.48      0.40      0.55      4.85      1.08\n",
      "            g[0]      1.03      0.13      1.01      0.86      1.22      3.00      2.20\n",
      "            g[1]      0.27      0.07      0.27      0.18      0.39      5.21      1.66\n",
      "            g[2]     -0.03      0.21     -0.10     -0.28      0.27      2.81      2.74\n",
      "            g[3]      1.30      0.19      1.33      1.02      1.55     13.40      1.20\n",
      "  sigma_actor[0]      1.07      0.07      1.07      0.96      1.18     10.62      1.00\n",
      "  sigma_actor[1]      0.89      0.14      0.88      0.67      1.11     17.96      1.02\n",
      "  sigma_actor[2]      2.06      0.48      2.01      1.31      2.74     38.82      1.00\n",
      "  sigma_actor[3]      1.02      0.07      1.02      0.92      1.13      4.66      1.00\n",
      "  sigma_block[0]      0.24      0.10      0.21      0.10      0.40      5.73      1.26\n",
      "  sigma_block[1]      0.09      0.00      0.09      0.08      0.10      5.36      1.44\n",
      "  sigma_block[2]      0.26      0.02      0.25      0.23      0.29      3.74      1.95\n",
      "  sigma_block[3]      1.45      0.22      1.39      1.14      1.77      6.02      1.11\n",
      "    z_actor[0,0]     -0.25      0.29     -0.25     -0.76      0.19      4.49      1.56\n",
      "    z_actor[0,1]     -1.51      0.12     -1.50     -1.72     -1.35      5.92      1.08\n",
      "    z_actor[0,2]      2.25      0.39      2.19      1.75      2.92      9.65      1.07\n",
      "    z_actor[0,3]     -1.97      0.08     -1.96     -2.12     -1.87      3.96      2.38\n",
      "    z_actor[0,4]     -2.07      0.16     -2.02     -2.34     -1.86      4.77      1.36\n",
      "    z_actor[0,5]     -1.44      0.14     -1.45     -1.63     -1.23      5.24      1.18\n",
      "    z_actor[0,6]      0.36      0.24      0.40      0.03      0.74      8.59      1.34\n",
      "    z_actor[1,0]      0.04      0.51     -0.14     -0.58      0.82      3.21      1.90\n",
      "    z_actor[1,1]      0.08      0.28      0.06     -0.27      0.59      3.18      2.16\n",
      "    z_actor[1,2]      1.67      0.14      1.70      1.38      1.87      9.60      1.04\n",
      "    z_actor[1,3]      0.47      0.24      0.49      0.10      0.82      3.70      1.98\n",
      "    z_actor[1,4]      0.14      0.11      0.14     -0.04      0.31      8.47      1.35\n",
      "    z_actor[1,5]      0.06      0.19      0.09     -0.26      0.34      9.67      1.39\n",
      "    z_actor[1,6]      0.59      0.29      0.62      0.07      0.94      3.65      2.10\n",
      "    z_actor[2,0]      0.80      0.26      0.83      0.30      1.13      5.87      1.04\n",
      "    z_actor[2,1]      1.26      0.31      1.27      0.79      1.75      5.71      1.00\n",
      "    z_actor[2,2]      0.50      0.12      0.46      0.35      0.71      3.64      2.28\n",
      "    z_actor[2,3]      1.60      0.07      1.59      1.49      1.70      5.15      1.39\n",
      "    z_actor[2,4]      0.28      0.16      0.32     -0.01      0.44      5.68      1.26\n",
      "    z_actor[2,5]      1.23      0.14      1.24      0.99      1.41      3.73      1.68\n",
      "    z_actor[2,6]      0.34      0.26      0.29     -0.04      0.78      4.10      1.65\n",
      "    z_actor[3,0]     -0.75      0.21     -0.82     -0.99     -0.36      4.28      1.33\n",
      "    z_actor[3,1]      1.18      0.26      1.23      0.73      1.47      3.15      2.56\n",
      "    z_actor[3,2]      0.46      0.10      0.45      0.29      0.60      2.99      2.53\n",
      "    z_actor[3,3]      0.76      0.37      0.69      0.38      1.42      5.41      1.55\n",
      "    z_actor[3,4]     -0.04      0.21     -0.04     -0.36      0.26      5.60      1.50\n",
      "    z_actor[3,5]      1.50      0.09      1.51      1.37      1.66      8.73      1.02\n",
      "    z_actor[3,6]      0.82      0.04      0.82      0.76      0.89      8.34      1.13\n",
      "    z_block[0,0]     -0.45      0.46     -0.37     -1.11      0.25      3.96      1.71\n",
      "    z_block[0,1]      1.60      0.17      1.64      1.25      1.80      6.71      1.18\n",
      "    z_block[0,2]     -0.10      0.15     -0.15     -0.29      0.13      6.97      1.13\n",
      "    z_block[1,0]     -1.98      0.10     -1.97     -2.15     -1.83      8.92      1.07\n",
      "    z_block[1,1]     -0.29      0.92     -0.22     -1.77      1.06      3.24      2.18\n",
      "    z_block[1,2]     -0.74      0.39     -0.85     -1.28     -0.07      6.99      1.03\n",
      "    z_block[2,0]     -0.33      0.07     -0.32     -0.46     -0.25      3.25      2.11\n",
      "    z_block[2,1]      0.35      0.55      0.55     -0.75      0.86      4.42      1.32\n",
      "    z_block[2,2]      0.42      0.22      0.36      0.08      0.69      9.30      1.28\n",
      "    z_block[3,0]     -1.22      0.06     -1.22     -1.31     -1.13      3.56      1.66\n",
      "    z_block[3,1]      0.13      0.07      0.14      0.02      0.21      3.24      2.19\n",
      "    z_block[3,2]     -0.07      0.25     -0.09     -0.48      0.31      7.19      1.34\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup device------------------------------------------------\n",
    "m = bi(platform='cpu')\n",
    "# Import data\n",
    "data_path = files('BI.resources.data') / 'chimpanzees.csv'\n",
    "m.data(data_path, sep=\";\")\n",
    "m.df[\"block_id\"] = m.df.block\n",
    "m.df[\"treatment\"] = 1 + m.df.prosoc_left + 2 * m.df.condition\n",
    "m.df[\"tid\"]=m.df[\"treatment\"].values - 1\n",
    "m.data_to_model(['tid',  'actor', 'block_id','pulled_left'])\n",
    "\n",
    "\n",
    "def model(tid, actor, block_id, pulled_left):\n",
    "    # --- Fixed priors ---\n",
    "    # Global effect (intercept-like coefficients) for each of the 4 task IDs (tid)\n",
    "    g = m.dist.normal(0, 1, name = 'g', shape = (4,))\n",
    "    \n",
    "    # Actor-level variability: scale parameters (one per tid dimension, 4 total)\n",
    "    sigma_actor = m.dist.exponential(1, name = 'sigma_actor', shape = (4,))\n",
    "    # Actor-level variability: correlation structure across the 4 dimensions\n",
    "    L_Rho_actor = m.dist.lkj_cholesky(4, 2, name = \"L_Rho_actor\")\n",
    "\n",
    "    # Block-level variability: scale parameters (same structure as actor)\n",
    "    sigma_block = m.dist.exponential(1, name = \"sigma_block\", shape = (4,))\n",
    "    # Block-level variability: correlation structure across the 4 dimensions\n",
    "    L_Rho_block = m.dist.lkj_cholesky(4, 2, name = \"L_Rho_block\")\n",
    "\n",
    "    # --- Adaptive priors (non-centered parameterization) ---\n",
    "    # Latent normal variables for actor-level random effects\n",
    "    # shape (4,7) → 7 actors × 4 task IDs\n",
    "    z_actor = m.dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n",
    "    # Latent normal variables for block-level random effects\n",
    "    # shape (4,3) → 3 blocks × 4 task IDs\n",
    "    z_block = m.dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n",
    "\n",
    "    # Transform non-centered parameters into actual actor effects\n",
    "    # scaling (σ) * correlation (L_Rho) * latent normal (z_actor)\n",
    "    # transpose → shape becomes (7,4): 1 row per actor, 1 column per tid\n",
    "    alpha =  ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n",
    "\n",
    "    # Same transformation for block-level random effects\n",
    "    # transpose → shape becomes (3,4): 1 row per block, 1 column per tid\n",
    "    beta = ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    # Logit of the probability of \"pulling left\" depends on:\n",
    "    # 1) global effect g for that task ID\n",
    "    # 2) actor-specific deviation alpha for that actor and tid\n",
    "    # 3) block-specific deviation beta for that block and tid\n",
    "    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n",
    "\n",
    "    # Binomial likelihood with observed binary outcome (pulled_left)\n",
    "    m.dist.binomial(logits=logit_p, obs=pulled_left)\n",
    "\n",
    "\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m.sampler.print_summary(0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tid, actor, block_id, pulled_left):\n",
    "    # fixed priors\n",
    "    g = m.dist.normal(0, 1, name = 'g', shape = (4,))\n",
    "    sigma_actor = m.dist.exponential(1, name = 'sigma_actor', shape = (4,))\n",
    "    L_Rho_actor = m.dist.lkj_cholesky(4, 2, name = \"L_Rho_actor\")\n",
    "    sigma_block = m.dist.exponential(1, name = \"sigma_block\", shape = (4,))\n",
    "    L_Rho_block = m.dist.lkj_cholesky(4, 2, name = \"L_Rho_block\")\n",
    "\n",
    "    # adaptive priors - non-centered\n",
    "    z_actor = m.dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n",
    "    z_block = m.dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n",
    "    alpha =  ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n",
    "    beta = ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n",
    "\n",
    "    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n",
    "    m.dist.binomial(logits=logit_p, obs=pulled_left)\n",
    "\n",
    "\n",
    "# Run mcmc ------------------------------------------------\n",
    "m.fit(model) \n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m.sampler.print_summary(0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_3_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\n",
    "m$data_to_model(list('cafe', 'wait', 'afternoon'))\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n",
    "  a = bi.dist.normal(5, 2, name = 'a')\n",
    "  b = bi.dist.normal(-1, 0.5, name = 'b')\n",
    "  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n",
    "  sigma = bi.dist.exponential( 1, name = 'sigma')\n",
    "  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n",
    "  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n",
    "  \n",
    "  a_cafe_b_cafe = bi.dist.multivariatenormal(\n",
    "    jnp$squeeze(jnp$stack(list(a, b))), \n",
    "    cov, shape = c(N_cafes), name = 'a_cafe')  \n",
    "  \n",
    "  a_cafe = a_cafe_b_cafe[, 0]\n",
    "  b_cafe = a_cafe_b_cafe[, 1]\n",
    "  \n",
    "  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "  \n",
    "  m$normal(mu, sigma, obs=wait)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_4_'></a>[STAN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "import numpy as np\n",
    "#try:\n",
    "#  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "#except:\n",
    "#  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "stan_code = \"\"\" \n",
    "data{\n",
    "    int len;\n",
    "    int N_cafes;\n",
    "    vector[len] wait;\n",
    "    array[len] int afternoon;\n",
    "    array[len] int cafe;\n",
    "}\n",
    "\n",
    "parameters{\n",
    "    corr_matrix[2] Rho;\n",
    "    real a;\n",
    "    vector[N_cafes] a_cafe;\n",
    "    real b;\n",
    "    vector[N_cafes] b_cafe;      \n",
    "    real<lower=0> sigma;\n",
    "    vector<lower=0>[2] sigma_cafe;   \n",
    "    \n",
    "}\n",
    "model{\n",
    "    vector[len] mu;\n",
    "    Rho ~ lkj_corr( 2 );\n",
    "    sigma ~ exponential( 1 );\n",
    "    sigma_cafe ~ exponential( 1 );\n",
    "    b ~ normal( -1 , 0.5 );    \n",
    "    a ~ normal( 5 , 2 );\n",
    "    {\n",
    "        array[N_cafes] vector[2] YY;\n",
    "        vector[2] MU;\n",
    "        MU = [ a , b ]';\n",
    "        for ( j in 1:N_cafes ) YY[j] = [ a_cafe[j] , b_cafe[j] ]';\n",
    "        YY ~ multi_normal( MU , quad_form_diag(Rho , sigma_cafe) );\n",
    "    }\n",
    "    for ( i in 1:len ) {\n",
    "        mu[i] = a_cafe[cafe[i]] + b_cafe[cafe[i]] * afternoon[i];        \n",
    "    }\n",
    "    \n",
    "    wait ~ normal( mu , sigma );\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "data = {\n",
    "    'wait' : d['wait'].values.astype(float),\n",
    "    'afternoon' : d['afternoon'].values.astype(int),\n",
    "    'cafe' : d['cafe'].values.astype(int)+1,\n",
    "    'N_cafes' : N_cafes,\n",
    "    'len' : len(d['wait'].values)\n",
    "}\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_5_'></a>[Output comparison](#toc0_)\n",
    "\n",
    "This can't use function to compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_array_to_dict(array, name=\"slice\"):\n",
    "\n",
    "    _, dim2, dim3 = array.shape\n",
    "    \n",
    "    # Create the dictionary\n",
    "    slices_dict = {\n",
    "        f\"{name}_{i}_{j}\": array[:, i, j]\n",
    "        for i in range(dim2)\n",
    "        for j in range(dim3)\n",
    "    }\n",
    "    \n",
    "    return slices_dict\n",
    "def slice_array_to_dict_2(array, name=\"slice\"):\n",
    "\n",
    "    _, dim2  = array.shape\n",
    "    \n",
    "    # Create the dictionary\n",
    "    slices_dict = {\n",
    "        f\"{name}_{i}\": array[:, i]\n",
    "        for i in range(dim2)\n",
    "    }\n",
    "    \n",
    "    return slices_dict\n",
    "\n",
    "# Change posteriors dictionary so that parameters  are in the same order as the data of stan\n",
    "r = slice_array_to_dict(m.posteriors['Rho'], name=\"Rho\")\n",
    "a = dict(a=m.posteriors['a'])\n",
    "b = dict(b=m.posteriors['b'])\n",
    "sigma = dict(sigma=m.posteriors['sigma'])\n",
    "a_cafe = slice_array_to_dict_2(m.posteriors['a_b_cafe'][:,:,0], name=\"a_cafe\")\n",
    "b_cafe = slice_array_to_dict_2(m.posteriors['a_b_cafe'][:,:,1], name=\"b_cafe\")\n",
    "sigma_cafe_1 = dict(sigma_cafe_1=m.posteriors['sigma_cafe'][:, 0])\n",
    "sigma_cafe_2 = dict(sigma_cafe_2=m.posteriors['sigma_cafe'][:, 1])\n",
    "r.update(a)\n",
    "r.update(a_cafe)\n",
    "r.update(b)\n",
    "r.update(b_cafe)\n",
    "r.update(sigma)\n",
    "r.update(sigma_cafe_1)\n",
    "r.update(sigma_cafe_2)\n",
    "m.posteriors=r\n",
    "d = prepare_stan_data(df)\n",
    "df_bi = prepare_bi_data(m)\n",
    "\n",
    "# Plot the density of the parameters\n",
    "df_bi.columns =d.columns\n",
    "for col in df_bi.columns:\n",
    "    if col in d.columns:  # Ensure the column exists in both DataFrames\n",
    "        plt.figure(figsize=(10, 6))  # Create a new figure for each plot\n",
    "        sns.kdeplot(df_bi[col], label=f'bi_{col}', fill=True)\n",
    "        sns.kdeplot(d[col], label=f'stan_{col}', fill=True)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Density Plot of {col}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_13_6_'></a>[Parameter recovery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(cafe, wait, N_cafes, afternoon):\n",
    "    a = m.dist.normal(5, 2, name = 'a')\n",
    "    b =  m.dist.normal(-1, 0.5,  name = 'b')\n",
    "    sigma_cafe =  m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n",
    "    sigma =  m.dist.exponential( 1, name = 'sigma')\n",
    "    Rho =  m.dist.lkj(2, 2, name = 'Rho')\n",
    "    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n",
    "    a_cafe_b_cafe =  m.dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n",
    "\n",
    "    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n",
    "    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n",
    "    m.dist.normal(mu, sigma, obs=wait)\n",
    "\n",
    "def sim(N_cafes = 20, N_visits = 10, a = 3.5,b = -1, sigma_a = 1, sigma_b = 0.5, rho = -0.7 ):\n",
    "    Mu = jnp.array([a,b])\n",
    "    \n",
    "    cov_ab = sigma_a * sigma_b *  rho\n",
    "    \n",
    "    Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n",
    "    \n",
    "    jnp.array([1, 2, 3, 4]).reshape(2, 2).T\n",
    "    \n",
    "    sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\n",
    "    \n",
    "    Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n",
    "\n",
    "    # now matrix multiply to get covariance matrix\n",
    "    Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\n",
    "\n",
    "    seed = jax.random.PRNGKey(5)  # used to replicate example\n",
    "    vary_effects = m.dist.multivariatenormal(Mu, Sigma, shape=(N_cafes,), sample = True)\n",
    "    a_cafe = vary_effects[:, 0]\n",
    "    b_cafe = vary_effects[:, 1]\n",
    "\n",
    "    seed = jax.random.PRNGKey(22)\n",
    "    afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\n",
    "    cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\n",
    "    mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n",
    "    sigma = 0.5  # std dev within cafes\n",
    "    wait =  m.dist.normal(mu, sigma, sample = True)\n",
    "    d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n",
    "    return d\n",
    "\n",
    "def estimate(N_cafes = 20, N_visits = 10, a = 3.5,b = -1,sigma_a = 1, sigma_b = 0.5, rho = -0.7):\n",
    "    d = sim(N_cafes,N_visits , a, b, sigma_a, sigma_b, rho)\n",
    "    m = bi(print_devices_found=False)\n",
    "    m.data_on_model = dict(\n",
    "        cafe = d.cafe.values, \n",
    "        wait = d.wait.values, \n",
    "        N_cafes = len(d.cafe.unique()),\n",
    "        afternoon =d.afternoon.values\n",
    "    )\n",
    "\n",
    "    m.fit(model, num_samples=500, progress_bar=False) \n",
    "    s = m.summary()\n",
    "    return s.iloc[:,0]\n",
    "\n",
    "def plot_recovery(res):\n",
    "    g = sns.FacetGrid(res, col=\"parameter\", col_wrap=3, height=4, sharey=False, sharex = False)\n",
    "    res['simulated'] = res['simulated'].astype(float)\n",
    "    res['estimations'] = res['estimations'].astype(float)\n",
    "    g.map(sns.scatterplot, \"simulated\", \"estimations\")\n",
    "\n",
    "def param_recovery(nsim, N_cafes = 20, N_visits = 10, a = 3.5,b = -1,sigma_a = 1, sigma_b = 0.5, rho = -0.7 ):\n",
    "    df = pd.DataFrame(columns=['sim', 'parameter', 'simulated', 'estimations'])\n",
    "\n",
    "    for i in range(nsim):\n",
    "        estimations = estimate(N_cafes = N_cafes, N_visits = N_visits,a = a[i], b = b[i], sigma_a = sigma_a[i], sigma_b = sigma_b[i], rho = rho[i])\n",
    "        estimations_filtered = estimations.loc[estimations.index.isin(['Rho[0, 1]', 'a', 'b', 'sigma_cafe[0]', 'sigma_cafe[1]'])] # Selecting paramters from model\n",
    "        data = {'sim': np.repeat(i, len(estimations_filtered.values)), \n",
    "                'parameter': estimations_filtered.index, \n",
    "                'simulated' : jnp.array([Rho[i],  a[i], b[i], sigma_a[i], sigma_b[i]]), \n",
    "                'estimations': estimations_filtered.values}\n",
    "        df = pd.concat([df, pd.DataFrame(data)], axis = 0, ignore_index=True)\n",
    "\n",
    "    plot_recovery(df)    \n",
    "\n",
    "    return df\n",
    "\n",
    "m = bi()\n",
    "nsim = 100\n",
    "a =  m.dist.normal(0, 1, shape= (nsim,), name = 'a', sample=True).tolist()\n",
    "b =  m.dist.normal(0, 0.5, shape=(nsim,), name = 'b', sample=True).tolist()\n",
    "sigma_cafe =  m.dist.exponential(1, shape=(nsim,2), name = 'sigma_cafe', sample=True)\n",
    "Rho =  m.dist.beta(2, 5, shape=(nsim,), name = 'b', sample=True).tolist()\n",
    "sigma_a =  sigma_cafe[:,0].tolist()\n",
    "sigma_b = sigma_cafe[:,1].tolist()\n",
    "i=0\n",
    "#sim(N_cafes = 20, N_visits = 10, a = a[i],b = b[i],sigma_a = sigma_a[i], sigma_b = sigma_b[i], rho = Rho[i] )\n",
    "#estimations = estimate(a = a[1], b = b[1], sigma_a = sigma_a[1], sigma_b = sigma_b[1], rho = Rho[1])\n",
    "param_recovery(nsim = nsim, N_cafes = 20, N_visits = 10, a =a, b = b, sigma_a =  sigma_cafe[:,0], sigma_b = sigma_cafe[:,1], rho = Rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_14_'></a>[Gaussian Processes](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_14_1_'></a>[BI](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "@jax.jit\n",
    "def cov_GPL2(x, sq_eta, sq_rho, sq_sigma):\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix for a Gaussian Process using the \n",
    "    squared exponential kernel, version L2 (squared Euclidean distance).\n",
    "\n",
    "    Args:\n",
    "        x: Distance matrix between points.\n",
    "        sq_eta: Squared variance parameter (eta^2).\n",
    "        sq_rho: Squared length scale parameter (rho^2).\n",
    "        sq_sigma: Squared noise variance parameter (sigma^2).\n",
    "\n",
    "    Returns:\n",
    "        K: Covariance matrix incorporating the squared exponential kernel, version L2.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    K = sq_eta * jnp.exp(-sq_rho * jnp.square(x))\n",
    "    K = K.at[jnp.diag_indices(N)].add(sq_sigma)\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "m = bi(platform='cpu')\n",
    "m.data(data_path + 'Kline2.csv', sep=\";\")\n",
    "islandsDistMatrix = pd.read_csv(data_path + 'islandsDistMatrix.csv', index_col=0)\n",
    "m.data_to_model(['total_tools', 'population'])\n",
    "m.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\n",
    "m.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n",
    "\n",
    "\n",
    "def model(Dmat, population, society, total_tools):\n",
    "    a = m.dist.exponential(1, name = 'a')\n",
    "    b = m.dist.exponential(1, name = 'b')\n",
    "    g = m.dist.exponential(1, name = 'g')\n",
    "\n",
    "    # non-centered Gaussian Process prior\n",
    "    etasq = m.dist.exponential(2, name = 'etasq')\n",
    "    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n",
    "    SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01)\n",
    "    k = m.dist.multivariatenormal(0, SIGMA, name = 'k')\n",
    "    #k = m.gaussian.gaussian_process(Dmat, etasq, rhosq, 0.01, shape = (10,))\n",
    "    k = m.gaussian.kernel_L2(Dmat, etasq, rhosq, 0.01)\n",
    "    lambda_ = a * population**b / g * jnp.exp(k[society])\n",
    "\n",
    "    m.dist.poisson(lambda_, obs=total_tools)\n",
    "\n",
    "# Run sampler ------------------------------------------------\n",
    "m.fit(model, num_samples=500) \n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters k are correlated between them, so order of comparison can't be found as they are interchangeable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_14_2_'></a>[BIR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(BI)\n",
    "pd=import('pandas')\n",
    "# setup platform------------------------------------------------\n",
    "m=importBI(platform='cpu')\n",
    "\n",
    "# Import data ------------------------------------------------\n",
    "m$data(paste(system.file(package = \"BI\"),\"/data/Kline2.csv\", sep = ''), sep=';')\n",
    "islandsDistMatrix = pd$read_csv(paste(system.file(package = \"BI\"),\"/data/islandsDistMatrix.csv\", sep = ''), index_col=as.integer(0))\n",
    "m$data_to_model(list('total_tools', 'population'))\n",
    "m$data_on_model$society = jnp$arange(0,10, dtype='int64')\n",
    "m$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n",
    "\n",
    "\n",
    "# Define model ------------------------------------------------\n",
    "model <- function(Dmat, population, society, total_tools){\n",
    "  a = bi.dist.exponential(1, name = 'a')\n",
    "  b = bi.dist.exponential(1, name = 'b')\n",
    "  g = bi.dist.exponential(1, name = 'g')\n",
    "  \n",
    "  # non-centered Gaussian Process prior\n",
    "  etasq = bi.dist.exponential(2, name = 'etasq')\n",
    "  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n",
    "  z = bi.dist.normal(0,1, name = 'z', shape = c(10))\n",
    "  r = m$kernel_sq_exp(Dmat, z, etasq, rhosq, 0.01)\n",
    "  SIGMA = r[[1]]\n",
    "  L_SIGMA = r[[2]]\n",
    "  k = r[[3]]\n",
    "  lambda_ = a * population**b / g * jnp$exp(k[society])\n",
    "  m$poisson(lambda_, obs=total_tools)\n",
    "}\n",
    "\n",
    "# Run MCMC ------------------------------------------------\n",
    "m$fit(model) # Optimize model parameters through MCMC sampling\n",
    "\n",
    "# Summary ------------------------------------------------\n",
    "m$summary() # Get posterior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm\n",
    "import stan\n",
    "import nest_asyncio\n",
    "import httpstan.models\n",
    "import httpstan.cache\n",
    "try:\n",
    "  httpstan.cache.delete_model_directory(httpstan.models.calculate_model_name(stan_code)) # Delete  model in cache\n",
    "except:\n",
    "  pass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "stan_code = \"\"\" \n",
    "functions{\n",
    "  matrix cov_GPL2(matrix x, real sq_alpha, real sq_rho, real delta) {\n",
    "    int N = dims(x)[1];\n",
    "    matrix[N, N] K;\n",
    "    for (i in 1:(N-1)) {\n",
    "      K[i, i] = sq_alpha + delta;\n",
    "      for (j in (i + 1):N) {\n",
    "        K[i, j] = sq_alpha * exp(-sq_rho * square(x[i,j]) );\n",
    "        K[j, i] = K[i, j];\n",
    "      }\n",
    "    }\n",
    "    K[N, N] = sq_alpha + delta;\n",
    "    return K;\n",
    "  }\n",
    "}\n",
    "\n",
    "data{\n",
    "  array[10] int T;\n",
    "  array[10] int society;\n",
    "  array[10] int P;\n",
    "  matrix[10,10] Dmat;\n",
    "}\n",
    "\n",
    "parameters{\n",
    " real<lower=0> a;\n",
    " real<lower=0> b;\n",
    " real<lower=0> etasq;\n",
    " real<lower=0> g; \n",
    " real<lower=0> rhosq;\n",
    " vector[10] k;\n",
    "}\n",
    "\n",
    "model{\n",
    "  vector[10] lambda;\n",
    "  matrix[10,10] SIGMA;\n",
    "  rhosq ~ exponential( 0.5 );\n",
    "  etasq ~ exponential( 2 );\n",
    "  a ~ exponential( 1 );\n",
    "  b ~ exponential( 1 );\n",
    "  g ~ exponential( 1 );\n",
    "\n",
    "  SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01);\n",
    "  k ~ multi_normal( rep_vector(0,10) , SIGMA );\n",
    "  for ( i in 1:10 ) {\n",
    "    lambda[i] = (a * P[i]^b/g) * exp(k[society[i]]);\n",
    "  }\n",
    "  T ~ poisson( lambda );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    'T' : m.df[\"total_tools\"].values.astype(int),\n",
    "    'P' : m.df[\"population\"].values.astype(int),\n",
    "    'society' : np.array(m.data_on_model['society']+1).astype(int),\n",
    "    'Dmat' : np.array(islandsDistMatrix)\n",
    "}\n",
    "\n",
    "start = tm.time()\n",
    "stan_model = stan.build(stan_code, data = data)\n",
    "fit = stan_model.sample(num_chains=1, num_samples=500, num_warmup = 500)\n",
    "end = tm.time()    \n",
    "df = fit.to_frame()\n",
    "print(f\"Pystan took: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparaison(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter recovery\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
