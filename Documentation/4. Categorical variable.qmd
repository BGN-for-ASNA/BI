#  Regression for Categorical Variables
## General Principles
To study the relationship between a categorical independent variable and a continuous dependent variable, we use _Categorical model_ wich apply _stratification_.

_Stratification_ concist in modeling how the different categories of the independent variable affect the target continuous variable, by performing a regression for each categories and asing a regression coefficient for each categories. To realize the  _stratification_, categorical variables are often encoded using one-hot encoding or converting categories to indeces.

![](https://i0.wp.com/67.media.tumblr.com/e8ec86a7dd863cfccb64fd40e55c0e90/tumblr_inline_o8j406qB4V1qa0hyw_540.png?resize=450%2C354)


::: callout-caution
## Considerations
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- As we generate regression coefficients for each \( k \) category in the code, we need to specify a prior with a shape equal to the number of categories (see comments in the code).

- To compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. **Never compare the confidence intervals or p-values directly.**
:::

## Example
Below is an example code snippet demonstrating Bayesian regression with an independent categorical variable:

```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/milk.csv', sep=';') 
m.index(["clade"])
m.scale(['kcal_per_g'])
m.data_to_model(['kcal_per_g', "index_clade"])

# Define model ------------------------------------------------
def model(kcal_per_g, index_clade):    
    beta = bi.dist.normal(0, 0.5, shape = (4,), name = 'beta') # we specify a vector of length 4 as we have 4 categories
    sigma = bi.dist.exponential( 1,  name = 'sigma')
    lk("y", Normal(beta[index_clade], s), obs= kcal_per_g)


# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

## Mathematical Details
### *Formula*
We model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:

$$ùëå=\alpha + \beta_k X_i + \sigma$$

Where:

- $Y$ is the target variable.
- $X$ is the encoded categorical input variable .
- $\alpha$ is the intercept term.
- $\beta_k$ are the regression coefficient for each _k_ categories.
- $X$ is the independet varible
- $\sigma$ is the error term .
  

We can interpret $\beta_i$ as the effect of each category on $Y$ relative to the baseline (usually one of the categories or the intercept). 

### *Bayesian model*
We can express the Bayesian regression model accounting for prior distribution as follows:

$$
p(Y|\alpha, \beta, X) \sim Normal(\alpha +  \beta_k X, \sigma)
$$

$$
ùëù(\alpha) \sim Normal(0,1)
$$
$$
ùëù(\beta_k) \sim Normal(0,1)
$$
$$
ùëù(ùúé) \sim Exponential(1)
$$

Where:

- $p(ùëå_i‚à£\alpha, ùëã_i, \beta_i)$ is the likelihood function.
- $p(\alpha)$ is prior distributions for the intercept
- $p(\beta_k)$ are _k_ prior distributions for _k_ regression coefficients.
- $p(\sigma)$ is the prior distribution for the standard deviation, ensuring it is  positive.


::: callout-note
## Notes

- We can apply multiple variables similarly as [chapter 2: Multiple continuous Variables](2.&#32;Multiple&#32;continuous&#32;Variables.qmd).

- We can apply interaction terms  similarly as [chapter 3: Interaction between continuous variables](3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).
:::


## Reference(s)
@mcelreath2018statistical
