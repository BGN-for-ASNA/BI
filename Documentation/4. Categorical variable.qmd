# Regression for Categorical Variables
## General Principles
To study the relationship between a categorical independent variable and a continuous dependent variable, we use a _Categorical model_ which applies _stratification_.

_Stratification_ involves modeling how the *k* different categories of the independent variable affect the target continuous variable by performing a regression for each *k* category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using [<span style="color:#0D6EFD">one-hot encoding ðŸ›ˆ</span>]{#ohe} or by converting categories to [<span style="color:#0D6EFD">indices ðŸ›ˆ</span>]{#indices}.

![](https://i0.wp.com/67.media.tumblr.com/e8ec86a7dd863cfccb64fd40e55c0e90/tumblr_inline_o8j406qB4V1qa0hyw_540.png?resize=450%2C354)

## Considerations
::: callout-caution
- We have the same considerations as for [Regression for a Continuous Variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).
 
- As we generate regression coefficients for each *k* category, we need to specify a prior with a shape equal to the number of categories *k* in the code (see comments in the code).
  
- To compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. **Never compare confidence intervals or p-values directly**.


:::

## Example
Below is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (*kcal_per_g*), representing the caloric value of milk per gram, and a categorical independent variable, representing species clade membership. The goal is to estimate the differences in milk calories between clades.

::: {.panel-tabset group="language"}
### Python
```python
from main import*

# Setup device------------------------------------------------
m = bee(platform='cpu')

# Import Data & Data Manipulation ------------------------------------------------
# Import
from importlib.resources import files
data_path = files('BEE.resources.data') / 'milk.csv'
m.data(data_path, sep=';') 
m.index(["clade"]) # Manipulate
m.scale(['kcal_per_g']) # Scale
m.data_to_model(['kcal_per_g', "index_clade"]) # Send to model (convert to jax array)

# Define model ------------------------------------------------
def model(kcal_per_g, index_clade):
    a = m.bee.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades
    s = m.bee.dist.exponential( 1, name = 's')    
    mu = a[index_clade]
    m.normal(mu, s, obs=kcal_per_g)


# Run mcmc ------------------------------------------------
m.run(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m.summary()
```

### R
```R
library(BI)
m=importbee(platform='cpu')

# Load csv file
m$data(paste(system.file(package = "BI"),"/data/milk.csv", sep = ''), sep=';')
m$scale(list('kcal.per.g')) # Manipulate
m$index(list('clade')) # Scale
m$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)

# Define model ------------------------------------------------
model <- function(kcal_per_g, index_clade){
  # Parameter prior distributions
  beta = bee.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades
  sigma =bee.dist.exponential(1, name = 's')
  # Likelihood
  m$normal(beta[index_clade], sigma, obs=kcal_per_g)
}

# Run mcmc ------------------------------------------------
m$run(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m$summary() # Get posterior distributions
```
:::

## Mathematical Details
### *Frequentist formulation*
We model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:

$$
Y_i = \alpha + \beta_k X_i + \sigma
$$

Where:

- $Y_i$ is the dependent variable for observation *i*. 
  
- $\alpha$ is the intercept term.
  
- $\beta_k$ are the regression coefficients for each _k_ category.
  
- $X_i$ is the encoded categorical input variable for observation *i*. 
  
- $\sigma$ is the error term.

We can interpret $\beta_i$ as the effect of each category on $Y$ relative to the baseline (usually one of the categories or the intercept). 

### *Bayesian formulation*
In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:

$$
Y \sim Normal(\alpha +  \beta_k X, \sigma)
$$

$$
\alpha \sim Normal(0,1)
$$

$$
\beta_k \sim Normal(0,1)
$$

$$
\sigma \sim Exponential(1)
$$

Where:

- $Y_i$ is the dependent variable for observation *i*.
  
- $\alpha$ is the prior distribution for the intercept.
  
- $\beta_k$ are _k_ prior distributions for _k_ regression coefficients.
  
- $X_i$ is the encoded categorical input variable for observation *i*. 
  
- $\sigma$ is the prior distribution for the standard deviation, ensuring it is positive.

## Notes
::: callout-note

- We can apply multiple variables similarly to [Chapter 2: Multiple Continuous Variables](2.&#32;Multiple&#32;continuous&#32;Variables.qmd).

- We can apply interaction terms similarly to [Chapter 3: Interaction between Continuous Variables](3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).
:::

## Reference(s)
@mcelreath2018statistical