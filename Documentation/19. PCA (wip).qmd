# Principal Component Analysis (PCA)

## General Principles

**Principal Component Analysis (PCA)** is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance by any projection of the data comes to lie on the first coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.

In **Bayesian PCA**, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them, as well as avoid overfitting by incorporating prior knowledge.

### Goal:
- **Reduce dimensionality** while retaining as much variance as possible.
- **Infer posterior distributions** over the principal components, instead of point estimates, by incorporating prior distributions over the parameters.

### Bayesian PCA Model:
Given an observed data matrix $X \in \mathbb{R}^{N \times D}$ (where _N_ is the number of samples and _D_ is the number of dimensions), we assume the data is generated by a lower-dimensional latent variable model:

$$
X = ZW^T + \epsilon
$$

Where:
- $X$ is the observed data matrix.
- $Z \in \mathbb{R}^{N \times K}$ is the latent variable matrix (latent features with $K \ll D$.
- $W \in \mathbb{R}^{D \times K}$ is the matrix of principal components (projection matrix).
- $\epsilon$ is Gaussian noise, assumed to be normally distributed: $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$.

## Considerations

In **Bayesian PCA**, we assume prior distributions for the latent variables _Z_ and the principal component loadings _W_. We place Gaussian priors on both _Z_ and _W_, and learn their posterior distributions using the observed data _X_.

This approach differs from traditional PCA by allowing the posterior distributions to reflect uncertainty in the model parameters.

## Example

Here is an example code snippet demonstrating Bayesian PCA using TensorFlow Probability:

```python

from main import *
import seaborn as sns

m = bi(platform='cpu')

# Data simulation -------------------------------------------

plt.style.use("ggplot")
warnings.filterwarnings('ignore')

num_datapoints = 5000
data_dim = 2
latent_dim = 1
stddv_datapoints = 0.5

# Simulate data
def sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): 
    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w', sample=True, seed=seed)
    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z', sample=True, seed=seed)
    x = bi.dist.normal(w @ z, stddv_datapoints, name='x', sample=True, seed=seed)
    return w, z, x

actual_w, actual_z, x_train =sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 20)
plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1)
plt.axis([-20, 20, -20, 20])
plt.title("Data set")
plt.show()


# Model using simulated data
def model(x_train, data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): 
    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')
    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')
    lk('Y', Normal(w @ z, stddv_datapoints), obs = x_train)  
    
m.data_on_model = dict(
    x_train = x_train, 
    data_dim = data_dim, 
    latent_dim = latent_dim, 
    num_datapoints = num_datapoints, 
    stddv_datapoints = stddv_datapoints
)

m.run(model) 
summary = m.summary()
real_data = jnp.concatenate([actual_w.flatten(), actual_z.flatten()]) # concatenate the actual values of w and z
posteriors = summary.iloc[:,0]


plt.figure(figsize=(8, 6))
plt.plot(real_data, posteriors, marker='o', linestyle='None', color='b', label='Posteriors')
```

## Mathematical Details

Bayesian PCA is formulated as a latent variable model, with the following probabilistic assumptions:

### Likelihood
$$
p(X | Z, W, \sigma^2) = \prod_{i=1}^N \mathcal{N}(X_i | Z_i W^T, \sigma^2 I)
$$

Where:
- $X$ is the observed data.
- $Z$ is the latent variable matrix, which explains the structure of the data in a lower-dimensional space.
- $W$ is the matrix of principal components (the projection matrix).
- $\sigma^2$ is the noise variance, which represents the variability in the data not explained by the principal components.

### Priors
We place Gaussian priors on both the latent variables _Z_ and the principal components _W_:

$$
p(Z) = \prod_{i=1}^N \mathcal{N}(Z_i | 0, I)
$$

$$
p(W) = \prod_{j=1}^D \mathcal{N}(W_j | 0, \alpha^2 I)
$$

Where _Î±_ is a hyperparameter controlling the variance of the prior over _W_.

### Posterior Inference
We compute the posterior distribution over the latent variables and the principal components:

$$
p(Z, W | X) \propto p(X | Z, W) p(Z) p(W)
$$

Inference in Bayesian PCA involves approximating this posterior distribution using techniques like Variational Inference or Markov Chain Monte Carlo (MCMC).

## Use Cases

- **Dimensionality Reduction**: Bayesian PCA is commonly used to reduce the dimensionality of high-dimensional datasets while incorporating uncertainty about the latent structure.
  
- **Data Visualization**: By projecting data into a lower-dimensional space, PCA helps in visualizing high-dimensional datasets in 2D or 3D plots.

- **Noise Modeling**: Bayesian PCA provides an advantage over classical PCA by explicitly modeling noise and accounting for uncertainty in the data.

- **Feature Extraction**: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.
