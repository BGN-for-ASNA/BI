<!-- Toolpit -->



# Linear Regression for continuous variable

## General Principles

To study relationships between an independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:

1) An intercept $\alpha$, which represents the origin of the lineâ€”the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.

2) A coefficient $\beta$, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).

3) A variance term $\sigma$, which informs us about the spread of points around the line, i.e., the variance around the prediction.



![](https://miro.medium.com/v2/resize:fit:786/format:webp/1*WCcaObzvvVzcrg8CBi6iCQ.jpeg)

## Considerations 

::: callout-caution
-   Bayesian models consider [<span style="color:#0D6EFD">model parameter uncertainty ðŸ›ˆ</span>]{#uncertainty}, allowing for the quantification of confidence or uncertainty through the parameters'  [<span style="color:#0D6EFD">posterior distribution ðŸ›ˆ</span>]{#posterior}. Therefore, we need to declare [<span style="color:#0D6EFD">prior distributions ðŸ›ˆ</span>]{#prior} for each model parameter, in your case for: $\alpha$, $\beta$, and $\sigma^2$.

- Prior distributions are built following these considerations:

  - As the data is [<span style="color:#0D6EFD">scaled ðŸ›ˆ</span>]{#scaled} (see introduction), we can use a Normal distribution for $\alpha$ and $\beta$, with a mean of 0 and a standard deviation of 1.

  - Since $\sigma$ is strictly positive, we can use any distribution that is positively defined, such as the Exponential or Gamma distribution.

- Gaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a [<span style="color:#0D6EFD">link function ðŸ›ˆ</span>]{#linkF}. This simplifies interpretation, as coefficients represent direct changes in the outcome variable.


:::

## Example

Below is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height.

::: {.panel-tabset group="language"}
## Python

``` python
from .bi.main import*
# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import Data & Data Manipulation ------------------------------------------------
m.data('../data/Howell1.csv', sep=';') # Import
m.df = m.df[m.df.age > 18] # Manipulate
m.scale(['weight']) # Scale
m.data_to_model(['weight', 'height']) # Send to model (convert to jax array)

# Define model ------------------------------------------------
def model(height, weight): 
    # Parameters priors distributions   
    alpha = dist.normal(178, 20, name = 'alpha')
    beta = dist.normal(0, 1, name = 'beta')   
    sigma = dist.uniform(0, 50, name = 'sigma')
    # Likelihood
    lk("height", Normal(alpha + beta * weight, sigma), obs = height)

# Run mcmc ------------------------------------------------
m.run(model)  # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m.sampler.print_summary(0.89) # Get posterior distributions
```

## R

``` r
library(reticulate)
setwd(paste0(getwd(), '/bi'))

# Setup device------------------------------------------------
bi <- import("main")
load('STRAND sim sr dyad.Rdata')
m = bi$bi(platform='cpu')

# Import Data & Data Manipulation ------------------------------------------------
m$data('../data/Howell1.csv', sep=';')  # Import
m$df = m$df[m$df$age > 18,] # Manipulate
m$scale(list('weight')) # Scale
m$data_to_model(list('weight', 'height')) # Send to model (convert to jax array)

# Define model ------------------------------------------------
model <- function(height, weight){
  # Parameters priors distributions
  s = bi$dist$uniform(0, 50, name = 's', shape = tuple(as.integer(1)))
  a = bi$dist$normal(178, 20, name = 'a', shape = tuple(as.integer(1)))
  b = bi$dist$normal(0, 1, name = 'b', shape = tuple(as.integer(1)))   
  # Likelihood
  bi$lk("y", bi$Normal(a + b * weight, s), obs = height)
}

# Run mcmc ------------------------------------------------
m$run(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m$sampler$print_summary(0.89) # Get posterior distributions
```
:::

## Mathematical Details

### *Frequentist formulation*

The following equation allows us to draw a line: 
$$
Y_i = \alpha + \beta  X_i + \sigma_i
$$

Where:

-   $Y_i$ is the dependent variable for observation *i*.

-   $\alpha$ is the intercept term.

-   $\beta$ is the regression coefficient.

-   $X_i$ is the input variable for observation *i*.

-   $\sigma_i$ is a vector of error terms for observation *i*.

### *Bayesian formulation*

In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors ðŸ›ˆ</span>]{#prior}. We can express a Bayesian version of this regression model using the following model: 

$$
Y_i \sim Normal(\alpha + \beta   X_i, \sigma)
$$

$$
\alpha \sim Normal(0, 1)
$$

$$
\beta \sim Normal(0, 1)
$$

$$
\sigma \sim Uniform(0, 50)
$$

Where:

-   $Y_i$ is dependent variable for observation *i*.

-   $\alpha$ and $\beta$ are the regression coefficients and intercept parameters, respectively.

-   $X_i$ is the input variable for observation *i*.

-   $\sigma$ is a prior for the variance term standard deviation of the normal distribution that describes the variance in the relationship between the dependent variable $Y$ and the independent variable $X$.


## Notes
::: callout-note
We can observe a difference between the *Frequentist* and the *Bayesian* formulation regarding the error term. Indeed, in the *Frequentist* formulation, the error term $\sigma_i$ represents random fluctuations around the predicted values. This assumption leads to point estimates for $\alpha$ and $\beta$, without accounting for uncertainty in these estimates. In contrast, the *Bayesian* formulation treats $\sigma$ as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.
:::

## Reference(s)

@mcelreath2018statistical
