# Varying effects 
## General Principles
To model the relationship between predictor variables and an outcome variable while allowing for both varying intercepts and varying slopes (effects) across groups or clusters, we use a _Varying Effects_ model. This approach is useful when we expect the relationship between predictors and the outcome to differ across groups (e.g., different slopes for different subjects, locations, or time periods).This allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling.

The idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance.

$$
\left(\begin{array}{cc} 
\sigma_\alpha^2 & \sigma_\alpha \sigma_{\beta \rho }\\
\sigma_\alpha \sigma_{\beta \rho } & \sigma_\beta^2
\end{array}\right)
$$ 

where :
- $\sigma_\alpha^2$ is the variance of intercepts.
- $\sigma_\beta^2$ is the covariance of intercepts & slopes.
- $\sigma_\alpha \sigma_{\beta \rho }$ is the covariance between intercepts and slopes -i.e. the product of the two standard deviations-. 


## Formula
### Main equation
We model the relationship between the predictor variable ($X$) and the outcome variable (_Y_) with varying intercepts ($\alpha$) and varying slopes ($\beta$) for each group (_i_) using the following equation:

$$
Y_{i} = \alpha_i + \beta_i X_i + \epsilon
$$

Where:
- $Y_i$ is the outcome variable for group _i_.
- $X_i$  are the predictor variables for group _i_.
- $\alpha_i$ is the varying intercept for group _i_.
- $\beta_i$ are the varying regression coefficients (slope) for group _i_.
- $\epsilon ~\sim HalfCauchy(0,1)$ is the error term, typically assumed to be strictly positive.

## Priors
The varying intercepts ($\beta_i$) and slopes ($\alpha_i$) are typically modeled using a Multivariate Normal distribution:

\[
\left[
\begin{array}{c}
\alpha_i \\
\beta_i \\
\end{array}
\right] \sim 
\text{MultivariateNormal}(
   \left[
\begin{array}{c}
\alpha \\
\beta \\
\end{array}
\right]
 , S)
\]

Where:
- $\alpha \sim Normal(0,1)$, is the prior for average intercept.
- $\beta \sim Normal(0,1)$ is the prior for average slope.
- $S = $$
\left(\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right)
R
\left(\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right)
$$ $ is the covariance matrix where:

    - $\sigma_\alpha \sim Halfcauchy(0,1)$ bewing the prior stddev among intercepts.
    - $\sigma_\beta \sim Halfcauchy(0,1)$ bewing the prior stddev among slopes.
    - $R \sim LKJcorr( η)$ bewing the prior for the correlation matrix.

## Considerations
- $Halfcauchy$ distribution allow use to specify strictly positive values for a parameter\
- Parameter $η$ for $LKJcorr$ distribution is ussually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near −1 or 1.  When we use LKJ- corr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely. 





## Example

Below is an example code snippet demonstrating Bayesian regression with varying effects:

```python
from BI import bi.hard

# Simulate data-----------------------------------------------------------------
import math
import os

import arviz as az
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import pandas as pd
from IPython.display import Image, set_matplotlib_formats
from matplotlib.patches import Ellipse, transforms

import jax.numpy as jnp
from jax import random, vmap
from jax.scipy.special import expit

import numpy as onp
import numpyro as numpyro
import numpyro.distributions as dist
from numpyro.diagnostics import effective_sample_size, print_summary
from numpyro.infer import MCMC, NUTS, Predictive

az.style.use("arviz-darkgrid")
numpyro.set_platform("cpu")
numpyro.set_host_device_count(4)
a = 3.5  # average morning wait time
b = -1  # average difference afternoon wait time
sigma_a = 1  # std dev in intercepts
sigma_b = 0.5  # std dev in slopes
rho = -0.7  # correlation between intercepts and slopes
Mu = jnp.array([a, b])
cov_ab = sigma_a * sigma_b * rho
Sigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])

sigmas = jnp.array([sigma_a, sigma_b])  # standard deviations
Rho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix

# now matrix multiply to get covariance matrix
Sigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)
N_cafes = 20
seed = random.PRNGKey(5)  # used to replicate example
vary_effects = dist.MultivariateNormal(Mu, Sigma).sample(seed, (N_cafes,))
a_cafe = vary_effects[:, 0]
b_cafe = vary_effects[:, 1]
seed = random.PRNGKey(22)
N_visits = 10
afternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)
cafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)
mu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma = 0.5  # std dev within cafes
wait = dist.Normal(mu, sigma).sample(seed)
d = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))

# Define model-----------------------------------------------------------------
cafe_id = jnp.array(d.cafe.values)
def model():    
    sigma = yield exponential(1,1)
    a = yield normal(1, 5, 2)
    b = yield normal(1, -1, 0.5)
    sigma_cafe = yield exponential(2, 1)    
    Rho = yield lkj((), 2, 2)
    a_cafe_b_cafe = yield multivariatenormaltril(N_cafes, loc = jnp.stack([a, b], axis=1)[0], scale_tril =  Rho * sigma_cafe)
    mu = a_cafe_b_cafe[:, 0][cafe_id] + a_cafe_b_cafe[:, 1][cafe_id] * afternoon
    y = yield Independent(Normal(mu, sigma), reinterpreted_batch_ndims=[1])

posterior, sample_stats = NUTStrans(model, obs = jnp.array(d.wait.astype('float32').values))
```

## Mathematical Details
We can express the Bayesian regression model with varying effects using probability distributions as follows:

$$
p(Y_{i} |\mu_i , \sigma) = \text{Normal}(\mu_i , \sigma) \\
$$

$$
\mu_i =   \alpha_i + \beta_i X_i \\
$$

\[
\left[
\begin{array}{c}
\alpha_i \\
\beta_i \\
\end{array}
\right] \sim 
\text{MultivariateNormal}(
   \left[
\begin{array}{c}
\alpha \\
\beta \\
\end{array}
\right]
 , S)
\]

$$S = 
\left(\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right)
R
\left(\begin{array}{cc} 
\sigma_\alpha & 0 \\
0 & \sigma_\beta
\end{array}\right)
$$

$$
\alpha \sim Normal(0,1) \\
\beta \sim Normal(0,1) \\
\sigma_\alpha \sim Halfcauchy(0,1) \\
\sigma_\beta \sim Halfcauchy(0,1) \\
R \sim LKJcorr(2)
$$






