---
title: "Binomial Model"
description: "Modeling binary outcomes (e.g., successes/failures) across multiple independent trials."
categories: [Classification, Regression, GLM]
image: "Figures/5.png"
order: 6
---

## General Principles

To model the relationship between a binary dependent variable—e.g., counts of successes/failures, yes/no, or 1/0—and one or more independent variables, we can use a *Binomial model*.

## Considerations

::: callout-note
-   We have the same considerations as for [Regression for continuous variable](1.%20Linear%20Regression%20for%20continuous%20variable.qmd).

-   This is the first model for which we need a link function: e.g., the *logit* function. The *logit* link function converts the linear combination of predictor variables into probabilities, making it suitable for modeling the probability of binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring that model predictions fall within the bounds of the binomial distribution's success parameter $\in(0,1)$.
:::

## Example

Below is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (*pulled_left*), which represents which lever each chimpanzee pulled in an experimental setup. The goal is to evaluate the probability of pulling the left side. This example is based on @mcelreath2018statistical.

::: {.panel-tabset group="language"}
### Python

``` {python}
from BI import bi

# setup platform------------------------------------------------
m = bi(platform='cpu')
# import data ------------------------------------------------
# Import
from importlib.resources import files
data_path = m.load.chimpanzees(only_path = True)
m.data(data_path, sep=';')
m.data_to_model(['pulled_left'])

# Define model ------------------------------------------------
def model(pulled_left):
    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')
    m.dist.binomial(total_count = 1, logits=a[0], obs=pulled_left)

# Run sampler ------------------------------------------------
m.fit(model, num_samples=500)

# Diagnostic ------------------------------------------------
m.summary()
```

### R

``` r
library(BayesianInference)

# setup platform------------------------------------------------
m=importBI(platform='cpu')

# import data ------------------------------------------------
m$data(m$load$chimpanzees(only_path = T), sep=';')
m$data_to_model(list('pulled_left')) # Send to model (convert to jax array)

# Define model ------------------------------------------------
model <- function(pulled_left){
  # Parameters priors distributions
  alpha = bi.dist.normal( 0, 10, name = 'alpha')
  # Likelihood
  bi.dist.binomial(total_count = 1, logits = alpha, obs=pulled_left)
}


# Run MCMC ------------------------------------------------
m$fit(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m$summary() # Get posterior distribution
```

### Julia
```julia
using BayesianInference

# Setup device------------------------------------------------
m = importBI(platform="cpu")

# Import Data & Data Manipulation ------------------------------------------------
# Import
data_path = m.load.chimpanzees(only_path = true)
m.data(data_path, sep=';')

# Define model ------------------------------------------------
@pymodel function model(pulled_left)
    a = m.dist.normal( 0, 10, shape=(1,), name = "a")
    m.dist.binomial(total_count = 1, logits=a[0], obs=pulled_left)
end

# Run mcmc ------------------------------------------------
m.fit(model)  # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m.summary() # Get posterior distributions
```
:::

## Mathematical Details
### *Bayesian formulation*

We can express the Bayesian Binomial regression model including prior distributions as follows:

$$
Y_i \sim \text{Binomial}(N_i, p_i)
$$

$$
logit(p_i) = \alpha + \beta X_i
$$

$$
\alpha \sim \text{Normal}(0,1)
$$

$$
\beta \sim \text{Normal}(0,1)
$$

Where:

-   $Y_i$ is the count of successes for observation *i* (often a binary 0 or 1).
   
-   $N_i$ is the count of trials for observation *i* (1 in the case of binary outcomes, as in the example for `total_count` above).

-   $p_i$ is the probability of success (0 \< $p_i$ \< 1) for observation *i*, the probability of a success.

-   $logit(p_i)$ is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation *i*.

-   $\beta$ and $\alpha$ are the regression coefficient and intercept, respectively.

## Notes

::: callout-note
-   We can apply multiple variables similarly to [chapter 2](2.%20Multiple%20continuous%20variables.qmd).

-   We can apply interaction terms similarly to [chapter 3](3.%20Interaction%20between%20continuous%20variables.qmd).

-   We can apply categorical variables similarly to [chapter 4](4.%20Categorical%20variable.qmd).

-   Below is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (*pulled_left*), which represents which side individuals pulled, and three independent variables (*actor*, *side*, *cond*). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as for the different conditions.

``` python
from BI import bi
m = bi(platform='cpu')
m.data('../resources/data/chimpanzees.csv', sep=';')
m.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition
m.df['actor'] = m.df['actor'] - 1

m.data_to_model(['actor', 'treatment', 'pulled_left'])

def model(actor, treatment, pulled_left):
    a = m.dist.normal(0, 1.5, shape = (7,), name='a')
    b = m.dist.normal(0, 0.5, shape = (4,), name='b')
    p = a[actor] + b[treatment]
    m.dist.binomial(total_count = 1, logits=p, obs=pulled_left)

# Run sampler ------------------------------------------------
m.fit(model)
# Diagnostic ------------------------------------------------
m.summary()
```

``` r
library(BayesianInference)

# setup platform------------------------------------------------
m=importBI(platform='cpu')

# import data ------------------------------------------------
m$data(paste(system.file(package = "BayesianInference"),"/data/chimpanzees.csv", sep = ''), sep=';')
m$data_to_model(list('pulled_left')) # Send to model (convert to jax array)

# Define model ------------------------------------------------
model <- function(pulled_left){
  # Parameters priors distributions
  a = bi.dist.normal( 0, 1.5, name = 'a')
  b = bi.dist.normal( 0, 0.5, name = 'b')
  p = a[actor] + b[treatment]
  # Likelihood
  m$binomial(total_count = 1, logits = alpha, obs=pulled_left)
}

# Run MCMC ------------------------------------------------
m$fit(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m$summary() # Get posterior distribution
```
:::

## Reference(s)

::: {#refs}
:::