{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Model\"\n",
        "description: \"Modeling the counts of outcomes across multiple categorical trials.\"\n",
        "categories: [Regression, GLM, Classification]\n",
        "image: \"Figures/9.png\"\n",
        "order: 12\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Multinomial_ model.\n",
        "\n",
        "## Considerations\n",
        "::: callout-note\n",
        "- We have the same considerations as for the [Categorical model](9.&#32;Categorical&#32;model.qmd).\n",
        "\n",
        ":::\n",
        "\n",
        "## Example\n",
        "Below is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "## Python"
      ],
      "id": "246eb585"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi, jnp\n",
        "import jax\n",
        "# Setup device ------------------------------------------------\n",
        "m = bi('cpu')\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "from importlib.resources import files\n",
        "data_path = m.load.sim_multinomial(only_path=True)\n",
        "m.data(data_path, sep=',') \n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "def model(income, career):\n",
        "    # Parameter prior distributions\n",
        "    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n",
        "    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n",
        "    s_1 = alpha[0] + beta * income[0]\n",
        "    s_2 = alpha[1] + beta * income[1]\n",
        "    s_3 = [0]\n",
        "    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
        "    # Likelihood\n",
        "    m.dist.multinomial(probs = p[career], obs=career)\n",
        "    \n",
        "# Run sampler ------------------------------------------------ \n",
        "m.fit(model)  \n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary()"
      ],
      "id": "cedb0195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R\n",
        "\n",
        "```R\n",
        "library(BayesianInference)\n",
        "jax = reticulate::import('jax')\n",
        "# Setup platform------------------------------------------------\n",
        "m=importBI(platform='cpu')\n",
        "\n",
        "# import data ------------------------------------------------\n",
        "m$data(m$load$sim_multinomial(only_path=T), sep=',')\n",
        "keys <- c(\"income\", \"career\")\n",
        "income = unique(m$df$income)\n",
        "income = income[order(income)]\n",
        "values <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\n",
        "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "model <- function(income, career){\n",
        "  # Parameter prior distributions\n",
        "  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n",
        "  beta = bi.dist.normal(0.5, name='beta')\n",
        "  \n",
        "  s_1 = alpha[0] + beta * income[0]\n",
        "  s_2 = alpha[1] + beta * income[1]\n",
        "  s_3 = 0 # reference category\n",
        "  \n",
        "  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n",
        "  \n",
        "  # Likelihood\n",
        "  m$dist$multinomial(probs=p[career], obs=career)\n",
        "}\n",
        "\n",
        "\n",
        "# Run sampler ------------------------------------------------ \n",
        "m$fit(model)  \n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m$summary()\n",
        "```\n",
        "\n",
        "## Julia\n",
        "```julia\n",
        "using BayesianInference\n",
        "\n",
        "# Setup device------------------------------------------------\n",
        "m = importBI(platform=\"cpu\")\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "data_path = m.load.sim_multinomial(only_path = true)\n",
        "m.data(data_path, sep=',')\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "@BI function model(income, career)\n",
        "    # Parameter prior distributions\n",
        "    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n",
        "    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n",
        "    s_1 = alpha[0] + beta * income[0]\n",
        "    s_2 = alpha[1] + beta * income[1]\n",
        "    # âš ï¸  Use jnp.array to create a Python object, so [0] indexing works\n",
        "    s_3 = jnp.array([0.0]) \n",
        "    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
        "    # Likelihood\n",
        "    m.dist.multinomial(probs = p[career], obs=career)\n",
        "end\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m.fit(model)  # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary() # Get posterior distributions\n",
        "```\n",
        ":::\n",
        "\n",
        "## Mathematical Details\n",
        "We can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $K$ categories, the *Dirichlet* likelihood function is:\n",
        "\n",
        "$$\n",
        "Y_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n",
        "\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n",
        "\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n",
        "\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n",
        "... \\\\\n",
        "\\phi_{[i,k]} = 0 \\\\\n",
        "\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n",
        "\\beta_{k} \\sim \\text{Normal}(0.1)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Y_i$ is the outcome (i.e. the vector of frequencies for each  $k$ categories) for observation *i*.\n",
        "  \n",
        "- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n",
        "  \n",
        "- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n",
        "  \n",
        "- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n",
        "\n",
        "\n",
        "## Reference(s)\n"
      ],
      "id": "4ddcf33f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}