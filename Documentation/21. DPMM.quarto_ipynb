{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Dirichlet Process Mixture Models ðŸš§\"\n",
        "description: \"A non-parametric Bayesian clustering method that automatically determines the number of clusters.\"\n",
        "categories: [Clustering, Unsupervised Learning, Non-parametric]\n",
        "image: \"Figures/20.png\"\n",
        "order: 24\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## General Principles\n",
        "\n",
        "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a **Dirichlet Process Mixture Model (DPMM)**. This is a unsupervised clustering method @gershman2012tutorial. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n",
        "\n",
        "1.  **How many clusters (`K`) exist**: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\n",
        "2.  **The properties of each cluster**: For each inferred cluster, it estimates its location and its spread.\n",
        "3.  **The assignment of each data point**: It determines the probability of each data point belonging to each cluster.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "::: callout-caution\n",
        "-   A DPMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation where we cap the maximum number of clusters at $K$ and use the [Stick-Breaking Process ðŸ›ˆ]{style=\"color:#0D6EFD\"}.\n",
        "\n",
        "-   The key parameters and their priors are:\n",
        "\n",
        "    - **Concentration** $\\alpha$: This single parameter controls the tendency to create new clusters. A low `Î±` favors fewer, larger clusters, while a high `Î±` allows for many smaller clusters. We typically place a `Gamma` prior on $\\alpha$ to learn its value from the data.\n",
        "- \n",
        "    - **Cluster Weights `w`**: Generated via the Stick-Breaking process from $\\alpha$. These are the probabilities of drawing a data point from any given cluster.\n",
        "  \n",
        "    - **Cluster Parameters (**$\\mu$, $\\Sigma$): Each potential cluster has a mean $\\mu$ and a covariance matrix $\\Sigma$. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, [14](14.%20Varying%20slopes.qmd)). However, if the data is one-dimensional, we use a univariate normal distribution.\n",
        "\n",
        "-   The model is often implemented in its [marginalized form ðŸ›ˆ]{style=\"color:#0D6EFD\"}. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "Below is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "## Python\n"
      ],
      "id": "3528fe7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi, jnp\n",
        "from BI.Models.DPMM import mix_weights\n",
        "from sklearn.datasets import make_blobs\n",
        "import numpyro\n",
        "\n",
        "m = bi()\n",
        "\n",
        "# Generate synthetic data\n",
        "data, true_labels = make_blobs(\n",
        "    n_samples=500, centers=8, cluster_std=0.8,\n",
        "    center_box=(-10,10), random_state=101\n",
        ")\n",
        "\n",
        "#  The model\n",
        "def dpmm(data, T=10):\n",
        "    N, D = data.shape  # Number of features\n",
        "    data_mean = jnp.mean(data, axis=0)\n",
        "    data_std = jnp.std(data, axis=0)*2\n",
        "\n",
        "    # 1) stick-breaking weights\n",
        "    alpha = m.dist.gamma(1.0, 10.0,name='alpha')\n",
        "\n",
        "    with m.dist.plate(\"beta_plate\", T - 1):\n",
        "        beta = m.dist.beta(1, alpha)\n",
        "\n",
        "    w = numpyro.deterministic(\"w\",mix_weights(beta))\n",
        "\n",
        "    # 2) component parameters\n",
        "    with m.dist.plate(\"components\", T):\n",
        "        mu = m.dist.multivariate_normal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n",
        "        sigma = m.dist.log_normal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n",
        "        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n",
        "\n",
        "        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n",
        "\n",
        "    # 3) Latent cluster assignments for each data point\n",
        "    with m.dist.plate(\"data\", N):\n",
        "        # Sample the assignment for each data point\n",
        "        z = m.dist.categorical(w, name = 'z') # shape (N,)  \n",
        "\n",
        "        # Sample the data point from the assigned component\n",
        "        m.dist.multivariate_normal(loc=mu[z], scale_tril=scale_tril[z],\n",
        "            obs=data, name = 'Y'\n",
        "        )  \n",
        "\n",
        "m.data_on_model = dict(data=data)\n",
        "m.fit(dpmm)  # Optimize model parameters through MCMC sampling\n",
        "m.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM"
      ],
      "id": "6c63b59c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R\n",
        "\n",
        "``` r\n",
        "\n",
        "```\n",
        "![](travaux-routiers.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "## Mathematical Details\n",
        "\n",
        "The process involves two keys submodels. The first, aims to identify the location and scale of $K$ potential clusters. The second, aims to identify which cluster is most likely to have generated a given data point. \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\begin{pmatrix}\n",
        "Y_{i,1} \\\\\n",
        "\\vdots \\\\\n",
        "Y_{i,D}\n",
        "\\end{pmatrix}\n",
        "&\\sim\n",
        "\\text{MVN}\\!\\left(\n",
        "\\begin{pmatrix}\n",
        "\\mu_{z_i,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{z_i,D}\n",
        "\\end{pmatrix},\n",
        "\\,\n",
        "\\Sigma_{z_i}\n",
        "\\right) \\\\\n",
        "\\\\\n",
        "\\begin{pmatrix}\n",
        "\\mu_{k,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{k,D}\n",
        "\\end{pmatrix}\n",
        "&\\sim\n",
        "\\text{MVN}\\!\\left(\n",
        "\\begin{pmatrix}\n",
        "A_{1} \\\\\n",
        "\\vdots \\\\\n",
        "A_{D}\n",
        "\\end{pmatrix},\n",
        "\\, B\n",
        "\\right) \\\\\n",
        "\\\\\n",
        "\\Sigma_k &= \\sigma_k \\Omega_k \\sigma_k \\\\\n",
        "\\\\\n",
        "\\sigma_{k} &\\sim \\text{HalfCauchy}(1) \\\\\n",
        "\\\\\n",
        "\\Omega_k &\\sim \\text{LKJ}(2) \\\\\n",
        "\\\\\n",
        "z_{i} &\\sim \\text{Categorical}(\\pi) \\\\\n",
        "\\\\\n",
        "\\pi_{i}(\\beta_{1:K})  &=  \\beta_i \\prod_{j<K} (1-\\beta_j) \\\\\n",
        "\\\\\n",
        "\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n",
        "\\\\\n",
        "\\alpha &\\sim \\text{Gamma}(1, 10) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where : \n",
        "\n",
        "*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n",
        "\n",
        "*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n",
        "\n",
        "*   $\\begin{pmatrix} A_{1} \\\\ \\vdots \\\\ A_{D} \\end{pmatrix}$ is a prior for the  mean vector as derived from mean of the raw data. \n",
        "\n",
        "*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n",
        "\n",
        "*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n",
        "\n",
        "*   $\\sigma_k$ is a diagonal matrix of standard deviations for the $k$-th cluster.\n",
        "\n",
        "*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n",
        "\n",
        "*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n",
        "\n",
        "*   $\\pi$ is a vector of $K$ cluster weights, some of which may be close to zero if the predicted number of clusters is less than the maximum number of clusters.\n",
        "  \n",
        "*   $\\beta_k$: The set of $K$ Beta-distributed random variables used in the stick-breaking process to construct the mixture weights.\n",
        "\n",
        "*   $\\alpha$: The concentration parameter, controlling the effective number of clusters. \n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        "::: callout-note\n",
        "* The primary advantage of the DPMM is the **automatic inference of the number of clusters**. The posterior distribution of the weights `w` reveals which components are \"active\", giving a probabilistic estimate of `K`.\n",
        "\n",
        "* Prior $\\alpha$ strongly influence the predicted number of clusters. Below are examples of this relationship:\n",
        "\n",
        "| Shape | Rate | $E[\\alpha]$ | $E[K]$ (approx) | Behavior |\n",
        "|:---:|:---:|:---:|:---:|:---|\n",
        "| 1     | 15   | 0.067       | 0.35            | Forces very few clusters |\n",
        "| 5     | 1    | 5           | 26              | Encourages many small clusters |\n",
        "| 10    | 2    | 5           | 26              | Same mean, less variance |\n",
        "| 2     | 0.5  | 4           | 21              | Moderately many clusters |\n",
        "| 15    | 1    | 15          | 78              | Explosive prior cluster count |\n",
        ": Impact of Gamma Prior Hyperparameters on Cluster Counts {tbl-colwidths=\"[10,10,15,25,40]\"}\n",
        "\n",
        ":::\n",
        "\n",
        "## Reference(s)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Dirichlet_process\n",
        "https://pyro.ai/examples/dirichlet_process_mixture.html"
      ],
      "id": "a55a552f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}