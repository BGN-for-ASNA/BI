{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Dirichlet Process Mixture Models\"\n",
        "description: \"A non-parametric Bayesian clustering method that automatically determines the number of clusters.\"\n",
        "categories: [Clustering, Unsupervised Learning, Non-parametric]\n",
        "image: \"Figures/20.png\"\n",
        "order: 24\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "\n",
        "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a **Dirichlet Process Mixture Model (DPMM)** @gershman2012tutorial. This is a [<span style=\"color:#0D6EFD\"> unsupervised clustering method ðŸ›ˆ</span>]{#unsupervised}. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n",
        "\n",
        "1.  **How many clusters (`K`) exist**: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\n",
        "2.  **The properties of each cluster**: For each inferred cluster, it estimates its location and its spread.\n",
        "3.  **The assignment of each data point**: It determines the probability of each data point belonging to each cluster.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "::: callout-caution\n",
        "-   A DPMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation where we cap the maximum number of clusters at $K$ and use the [<span style=\"color:#0D6EFD\"> Stick-Breaking Process ðŸ›ˆ</span>]{#stickProcess}.\n",
        "\n",
        "-   The key parameters and their priors are:\n",
        "\n",
        "    - **Concentration** $\\alpha$: This single parameter controls the tendency to create new clusters. A low `Î±` favors fewer, larger clusters, while a high `Î±` allows for many smaller clusters. We typically place a `Gamma` prior on $\\alpha$ to learn its value from the data.\n",
        "- \n",
        "    - **Cluster Weights `w`**: Generated via the Stick-Breaking process from $\\alpha$. These are the probabilities of drawing a data point from any given cluster.\n",
        "  \n",
        "    - **Cluster Parameters (**$\\mu$, $\\Sigma$): Each potential cluster has a mean $\\mu$ and a covariance matrix $\\Sigma$. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, [14](14.%20Varying%20slopes.qmd)). However, if the data is one-dimensional, we use a univariate normal distribution.\n",
        "\n",
        "-   The model is often implemented in its [marginalized form ðŸ›ˆ]{style=\"color:#0D6EFD\"}. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "Below is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "## Python"
      ],
      "id": "3bfea5ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi, jnp \n",
        "from sklearn.datasets import make_blobs\n",
        "import numpyro\n",
        "\n",
        "m = bi(rand_seed = False)\n",
        "\n",
        "# Generate synthetic data\n",
        "data, true_labels = make_blobs(\n",
        "    n_samples=500, centers=8, cluster_std=0.8,\n",
        "    center_box=(-10,10), random_state=101\n",
        ")\n",
        "data_mean = jnp.mean(data, axis=0)\n",
        "data_std = jnp.std(data, axis=0)*2\n",
        "\n",
        "#  The model\n",
        "def dpmm(data, K, data_mean, data_std):\n",
        "    N, D = data.shape  # Number of features\n",
        "\n",
        "\n",
        "    # 1) stick-breaking weights\n",
        "    alpha = m.dist.gamma(1.0, 10.0,name='alpha')\n",
        "\n",
        "    with m.dist.plate(\"beta_plate\", K - 1):\n",
        "        beta = m.dist.beta(1, alpha, name = \"beta\")\n",
        "\n",
        "    w = numpyro.deterministic(\"w\",m.models.dpmm.mix_weights(beta))\n",
        "\n",
        "    # 2) component parameters\n",
        "    with m.dist.plate(\"components\", K):\n",
        "        mu = m.dist.multivariate_normal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n",
        "        sigma = m.dist.log_normal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n",
        "        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n",
        "\n",
        "        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n",
        "\n",
        "    # 3) Latent cluster assignments for each data point\n",
        "    m.dist.mixture_same_family(\n",
        "        mixing_distribution=m.dist.categorical(probs=w, create_obj=True),\n",
        "        component_distribution=m.dist.multivariate_normal(\n",
        "            loc=mu, \n",
        "            scale_tril=scale_tril, \n",
        "            create_obj=True\n",
        "        ),\n",
        "        obs=data\n",
        "    )\n",
        "\n",
        "m.data_on_model = dict(data=data,K = 10, data_mean=data_mean, data_std=data_std)\n",
        "m.fit(dpmm)  # Optimize model parameters through MCMC sampling\n",
        "m.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n"
      ],
      "id": "443ac7af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R\n",
        "\n",
        "``` r\n",
        "\n",
        "```\n",
        "![](travaux-routiers.png){fig-align=\"center\"}\n",
        "\n",
        "## Julia\n",
        "```julia\n",
        "using BayesianInference\n",
        "using PythonCall\n",
        "numpyro = pyimport(\"numpyro\")\n",
        "\n",
        "m = importBI(rand_seed = false)\n",
        "\n",
        "# 1. Generate Data\n",
        "sk_datasets = pyimport(\"sklearn.datasets\")\n",
        "output = sk_datasets.make_blobs(n_samples=500, centers=8, cluster_std=0.8, center_box=(-10, 10), random_state=101)\n",
        "data = output[0]\n",
        "data_mean = jnp.mean(data, axis=0)\n",
        "data_std = jnp.std(data, axis=0) * 2\n",
        "m.data_on_model = pydict(data=data, K=10, data_mean = data_mean, data_std = data_std)\n",
        "\n",
        "\n",
        "@BI function dpmm(data, K, data_mean , data_std)\n",
        "    N, D = data.shape \n",
        "\n",
        "    alpha = m.dist.gamma(1.0, 10.0, name=\"alpha\")\n",
        "\n",
        "    beta = pywith(m.dist.plate(\"beta_plate\", K - 1)) do _\n",
        "        m.dist.beta(1, alpha, name = \"beta\")\n",
        "    end\n",
        "\n",
        "    w = numpyro.deterministic(\"w\", m.models.dpmm.mix_weights(beta))\n",
        "\n",
        "    mu, scale_tril = pywith(m.dist.plate(\"components\", K)) do _\n",
        "        mu_val = m.dist.multivariate_normal(\n",
        "            loc=data_mean, \n",
        "            covariance_matrix=data_std * jnp.eye(D),\n",
        "            name=\"mu\"\n",
        "        )\n",
        "        \n",
        "        sigma = m.dist.log_normal(0.0, 1.0, shape=(D,), event=1, name=\"sigma\")\n",
        "        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name=\"Lcorr\")\n",
        "        scale_tril_inner = jnp.expand_dims(sigma, -1) * Lcorr\n",
        "        (mu_val, scale_tril_inner)\n",
        "    end\n",
        "    \n",
        "    m.dist.mixture_same_family(\n",
        "        mixing_distribution=m.dist.categorical(probs=w, create_obj=true),\n",
        "        component_distribution=m.dist.multivariate_normal(\n",
        "            loc=mu, \n",
        "            scale_tril=scale_tril, \n",
        "            create_obj=true\n",
        "        ),\n",
        "        obs=data\n",
        "    )\n",
        "end\n",
        "\n",
        "# 4. Run\n",
        "\n",
        "m.fit(dpmm) \n",
        "\n",
        "@pyplot m.models.dpmm.plot_dpmm(m.data_on_model[\"data\"], m.sampler)\n",
        "```\n",
        ":::\n",
        "\n",
        "## Mathematical Details\n",
        "\n",
        "The process involves two keys submodels. The first, aims to identify the location and scale of $K$ potential clusters. The second, aims to identify which cluster is most likely to have generated a given data point. \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\begin{pmatrix}\n",
        "Y_{i,1} \\\\\n",
        "\\vdots \\\\\n",
        "Y_{i,D}\n",
        "\\end{pmatrix}\n",
        "&\\sim\n",
        "\\text{MVN}\\!\\left(\n",
        "\\begin{pmatrix}\n",
        "\\mu_{z_i,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{z_i,D}\n",
        "\\end{pmatrix},\n",
        "\\,\n",
        "\\Sigma_{z_i}\n",
        "\\right) \\\\\n",
        "\\\\\n",
        "\\begin{pmatrix}\n",
        "\\mu_{k,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{k,D}\n",
        "\\end{pmatrix}\n",
        "&\\sim\n",
        "\\text{MVN}\\!\\left(\n",
        "\\begin{pmatrix}\n",
        "A_{1} \\\\\n",
        "\\vdots \\\\\n",
        "A_{D}\n",
        "\\end{pmatrix},\n",
        "\\, B\n",
        "\\right) \\\\\n",
        "\\\\\n",
        "\\Sigma_k &= \\text{Diag}(\\sigma_k) \\Omega_k  \\text{Diag}(\\sigma_k) \\\\\n",
        "\\\\\n",
        "\\sigma_{[k,d]} &\\sim \\text{HalfCauchy}(1) \\\\\n",
        "\\\\\n",
        "\\Omega_k &\\sim \\text{LKJ}(2) \\\\\n",
        "\\\\\n",
        "z_{i} &\\sim \\text{Categorical}(\\pi) \\\\\n",
        "\\\\\n",
        "\\pi_{i}(\\beta_{1:K})  &=  \\beta_i \\prod_{j<K} (1-\\beta_j) \\\\\n",
        "\\\\\n",
        "\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n",
        "\\\\\n",
        "\\alpha &\\sim \\text{Gamma}(1, 10) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where : \n",
        "\n",
        "*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n",
        "\n",
        "*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n",
        "\n",
        "*   $\\begin{pmatrix} A_{1} \\\\ \\vdots \\\\ A_{D} \\end{pmatrix}$ is a prior for the  mean vector as derived from mean of the raw data. \n",
        "\n",
        "*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n",
        "\n",
        "*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n",
        "\n",
        "*  $\\text{Diag}(\\sigma_k)$ is a diagonal matrix whose diagonal entries are the standard deviations:\n",
        "  $$\n",
        "  \\text{Diag}(\\sigma_k) =\n",
        "  \\begin{pmatrix}\n",
        "  \\sigma_{[k,1]} & 0 & \\cdots & 0 \\\\\n",
        "  0 & \\sigma_{[k,2]} &        & \\vdots \\\\\n",
        "  \\vdots &        & \\ddots & 0 \\\\\n",
        "  0 & \\cdots & 0 & \\sigma_{[k,D]}\n",
        "  \\end{pmatrix}.\n",
        "  $$\n",
        "\n",
        "*   $\\sigma_{k}$ is a $D$-vector of standard deviations for the $k$-th cluster where each element, $d$, has a half-cauchy prior.\n",
        "\n",
        "*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n",
        "\n",
        "*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n",
        "\n",
        "*   $\\pi$ is a vector of $K$ cluster weights, some of which may be close to zero if the predicted number of clusters is less than the maximum number of clusters.\n",
        "  \n",
        "*   $\\beta_k$: The set of $K$ Beta-distributed random variables used in the stick-breaking process to construct the mixture weights.\n",
        "\n",
        "*   $\\alpha$: The concentration parameter, controlling the effective number of clusters. \n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        "::: callout-note\n",
        "* The primary advantage of the DPMM is the **automatic inference of the number of clusters**. The posterior distribution of the weights `w` reveals which components are \"active\", giving a probabilistic estimate of `K`.\n",
        "\n",
        "* Prior $\\alpha$ strongly influence the predicted number of clusters. Below are examples of this relationship:\n",
        "\n",
        "| Shape | Rate | Behavior |\n",
        "|:---:|:---:|:---:|\n",
        "| 1     | 15    | Forces very few clusters |\n",
        "| 5     | 1     | Encourages many small clusters |\n",
        "| 10    | 2     | Same mean, less variance |\n",
        "| 2     | 0.5   | Moderately many clusters |\n",
        "| 15    | 1     | Explosive prior cluster count |\n",
        ": Impact of Gamma Prior Hyperparameters on Cluster Counts {tbl-colwidths=\"[10,10,15,25,40]\"}\n",
        ":::\n",
        "\n",
        "## Reference(s)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Dirichlet_process\n",
        "https://pyro.ai/examples/dirichlet_process_mixture.html"
      ],
      "id": "0e79cbe2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}