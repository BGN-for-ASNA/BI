# Varying intercepts
## General Principles
To model the relationship between predictor variables and an independent variable while allowing for different intercepts across groups or clusters, we can use a _Varying Intercepts_ model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.

::: callout-caution 
## Considerations
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- The main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept $ \beta$ is defined based on the $k$ declared groups.
  
- Each intercept have is own _Normal distribution_ -i.e. a [hyper-prior ðŸ›ˆ](12.&#32;Varying&#32;intercepts.qmd "A hyperprior is a prior distribution placed on the parameters (called hyperparameters) of another prior distribution. Essentially, it introduces a second layer of uncertainty by modeling the parameters of the prior as random variables, allowing for more flexibility and robustness in the Bayesian model.")-. In the code below, the _hyper-prior_ is ```_a_bar_```.
:::

## Example

Below is an example code snippet demonstrating Bayesian regression with varying intercepts:

```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/reedfrogs.csv', sep=';') 
m.df["tank"] = np.arange(m.df.shape[0])
tank = jnp.array(m.df["tank"].astype('int32').values)
density = jnp.array(m.df["density"].astype('float32').values)
surv = jnp.array(m.df["surv"].astype('int32').values)
m.data_on_model = dict(
    tank = tank,
    surv = surv
)

# Define model ------------------------------------------------
def model(tank, surv):
    sigma = dist.exponential( 1,  name = 'sigma')
    a_bar = dist.normal( 0., 1.5, name = 'a_bar')
    alpha = dist.normal( a_bar, sigma, shape= [48], name = 'alpha')
    p = jnp.squeeze(alpha[tank])[0]
    lk("y", Binomial(total_count = density, logits = p), obs=surv)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

## Mathematical Details
### *Formula*
We model the relationship between the independent variables _X_ and the outcome variable _Y_ with varying intercepts $\alpha$ for each group _k_ using the following equation:

$$
Y_{ik} = \alpha_k + \beta X_{ik} + \sigma
$$

Where:
- $Y_{ik}$ is the outcome variable for observation _i_ in group _k_.
- $\alpha_k$ is the varying intercept for group _k_.
- $X_{ik}$ is the independent variables for observation _i_ in group _k_.
- $\beta$ is the regression coefficients term.
- $\sigma$ is the error term, typically assumed to be normally distributed and positive.

### *Bayesian model*
We can express the Bayesian regression model accounting for prior distribution as follows:

$$
p(Y_{ik} | \mu_{ik}, \sigma) = Normal(\mu_{ik}, \sigma)
$$
$$
\mu_{ik} = \alpha_j + \beta X_{ik} + \sigma 
$$
$$
\alpha_k \sim \text{Normal}(\mu_{alpha_k}, \sigma_{alpha_k}) 
$$
$$
p(\beta) \sim \text{Normal}(0, 1)
$$
$$
p(\sigma) \sim \text{Exponential}(1)
$$
$$
p(\mu_{alpha_k}) \sim \text{Normal}(0, 1)
$$
$$
p(\sigma_{alpha_k}) \sim \text{Exponential}(1)
$$

Where:
- $p(Y_{ij} | \mu_{ij}, \sigma)$ is the likelihood function for the outcome variable.
- $\alpha_k$ is the varying intercepts across groups.
- $\mu_alpha_k$ is the overall mean intercept.
- $\sigma_alpha_k$ is the variance of the intercepts across groups.
- $p(\beta)$ is the prior distributions for the regression coefficients.
- $p(\sigma)$ is the prior distributions for the error term.
  

::: callout-note 
## Notes
- We can apply multiple variables similarly as [chapter 2](/2.&#32;Multiple&#32;Regression&#32;for&#32;Continuous&#32;Variables.qmd).

- We can apply interaction terms  similarly as [chapter 3](\3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).

- We can apply  caterogical variables similarly as [chapter 4](4.&#32;Categorical&#32;variable.qmd). 

- We can apply  varying intercepts with any distribution developped in previous chapters. 
:::

## Reference(s)
@mcelreath2018statistical
