# Interaction Term between Two Continuous Variables
## General Principles
To study relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.

[comment]: <> (Example to change)

Parallel lines indicate that there is no interaction effect while different slopes suggest that one might be present. Below is the plot for Food x Condiment. The crossed lines on the graph suggest that there is an interaction effect, which the significant p-value for the Food*Condiment term confirms. The graph shows that enjoyment levels are higher for chocolate sauce when the food is ice cream. Conversely, satisfaction levels are higher for mustard when the food is a hot dog. If you put mustard on ice cream or chocolate sauce on hot dogs, you wonâ€™t be happy!

![](https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2017/10/interactions_plot_categorical.png?w=576&ssl=1)

## Considerations
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- Model relationship between Y and R to vary as a function of A. you explicitly model the hypothesis that the slope between Y and R dependsâ€”is conditionalâ€”upon A.

- For continuous interactions, the intercept becomes the grand mean of the outcome variable. This ease of interpretation alone is a good reason to center predictor variables.

- Estimate interpretation is more difficult as estimate of non-interaction terms become expected change in Y when R increases by one unit and A is at its average value and estimate of interaction terms are expected change in the influence of A on Y when increasing R by one unit and expected change in the influence of R on Y when increasing A by one unit.

- [Triptych ğŸ›ˆ](3.%20%Interaction%20%between%20%continuous%20%variables.qmd "Three panels arranged side by side to compare multiple datasets or conditions simultaneously. Each panel often represents a different aspect of the data or a different dataset, allowing for easy comparison and analysis. They are particularly useful for displaying complex relationships or patterns across multiple variables or experimental conditions.") plots are very handy for understanding the impact of interactions.


## Example
Below is an example code snippet demonstrating Bayesian regression with an interaction term between two continuous variables with the Bayesian Inference (BI) package:

```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/tulips.csv', sep=';') 
m.scale(['blooms', 'water', 'shade'])
m.data_to_model(['blooms', 'water', 'shade'])

# Define model ------------------------------------------------
def model(blooms,shade, water):
    alpha = dist.normal(0.5, 0.25, name = 'alpha')
    sigma = dist.exponential(1, name = 'sigma')
    beta1 = dist.normal(0, 0.25, name = 'beta1')
    beta2 = dist.normal(0, 0.25, name = 'beta2')
    beta_interaction_ = dist.normal(0, 0.25, name = 'beta_interaction_')    
    lk("y", Normal(a + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma), obs=blooms)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)

```

## Mathematical Details
## *Formula*
We model the relationship between the input features (X1 and X2) and the target variable (Y) using the following equation:
$$
ğ‘Œ = \alpha + \beta_1ğ‘‹_1âˆ—+\beta_2ğ‘‹_2+\\beta_{interaction}ğ‘‹_1ğ‘‹_2 + $\sigma$
$$

Where:

- $Y$ is the target variable.
- $\alpha$ is the intercept term.
- $X_1$ and $X_2$ are the two independent continuous variables.
- $\beta_1$ and $\beta_2$ are the regression coefficients for $X_1$ and $X_2$, respectively.
- $\beta_{interaction}$ is the regression coefficient for the interaction term $(X_1  X_2)$.
- $\sigma$ is the error term assumed to be normally distributed.

In this context, the interaction term $X_1 * X_2$ captures the joint effect of $X_1$ and $X_2$ on the target variable $Y$.

### *Bayesian model*
We can express the Bayesian regression model with an interaction term between two continuous variables using probability distributions as follows:

$$
p(Yâˆ£X_1â€‹ ,X_2â€‹ , \beta_1, \beta_2, \beta_{interaction} ) = Normal(\alpha +  \beta_1  X_1â€‹ + \beta_2  X_2â€‹â€‹ + \beta_{interaction}  X_1â€‹ X_2â€‹ ,  $\sigma$)
$$

$$
ğ‘(\alpha)=Normal(0,1)
$$

$$
ğ‘(\beta_1)=Normal(0,1)
$$

$$
ğ‘(\beta_2)=Normal(0,1)
$$

$$
ğ‘(\beta_{interaction})=Normal(0,1)
$$

$$
ğ‘(Ïƒ)=Exponential(1)
$$

Where:

- $p(Y | X_1â€‹ ,X_2â€‹ , \beta_1, \beta_2, \beta_{interaction})$ is the likelihood function.
- $p(\alpha)$ is the prior distribution for the intercept
- $p(\beta_1)$,  $p(\beta_2)$ and $\beta_{interaction}$ are the prior distributions for the regression coefficients.
- $p(\sigma)$ is the prior distribution for the standard deviation, ensuring it is positive.

## Reference(s)
@mcelreath2018statistical