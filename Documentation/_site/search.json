[
  {
    "objectID": "22. Network model.html",
    "href": "22. Network model.html",
    "title": "Network Models",
    "section": "",
    "text": "A network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random intercepts and slopes. By adding such parameters to the model, we can account for the correlations between node link relationships.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#considerations",
    "href": "22. Network model.html#considerations",
    "title": "Network Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThe particularity here is that varying intercepts and slopes are generated for both nodal effects üõà and dyadic effects üõà. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#example",
    "href": "22. Network model.html#example",
    "title": "Network Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect:",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#python",
    "href": "22. Network model.html#python",
    "title": "Network Models",
    "section": "Python",
    "text": "Python\nfrom BI import bi\n\n# Setup device------------------------------------------------\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nm.fit(model2) \nsummary = m.summary()\nsummary",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#r",
    "href": "22. Network model.html#r",
    "title": "Network Models",
    "section": "R",
    "text": "R\nlibrary(BI)\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\n\nload(paste(system.file(package = \"BI\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nids = 0:(model_dat$N_id-1)\nidx = m$net$vec_node_to_edgle(jnp$stack(jnp$array(list(ids, ids)), axis = -as.integer(1)))\n\nkeys &lt;- c(\"idx\",\n          'idxShape',\n          \"result_outcomes\",\n          'focal_individual_predictors',\n          'target_individual_predictors')\n\nvalues &lt;- list(\n  idx,\n  idx$shape[[1]],\n  m$net$mat_to_edgl(model_dat$outcomes[,,1]),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50)),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50))\n)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(idx, idxShape, result_outcomes,focal_individual_predictors, target_individual_predictors){\n  N_id = 50\n  x=0.1/jnp$sqrt(N_id)\n  tmp=jnp$log(x / (1 - x))\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(tmp, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(idxShape))\n\n  ## SR ---------------------------------------                                                      \n  m$poisson(jnp$exp(B + sr + dr), obs=result_outcomes)  \n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\nsummary[rownames(summary) %in% c('focal_effects[0]', 'target_effects[0]', 'block[0]'),]",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#mathematical-details",
    "href": "22. Network model.html#mathematical-details",
    "title": "Network Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe simple model that can be built to model link weights between nodes i and j can be defined using a Poisson distribution:\n\\[\nG_{ij} \\sim Poisson(Y_{ij})\n\\]\n\\[\nlog(Y_{ij}) =  \\lambda_i + \\pi_j + \\delta_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\) is the weight of the link between i and j.\n\\(\\lambda_i\\) is the sender effect üõà.\n\\(\\pi_j\\) is the receiver effect üõà.\n\\(\\delta_{ij}\\) is the dyadic effect üõà.\nDefining formula sub-equations and prior distributions\n\n\\(\\lambda_i\\) and \\(\\pi_j\\) are varying intercepts and slopes identical to those described in previous chapters and are defined through the following equations:\n\\[\n\\left(\\begin{array}{cc}\n\\lambda_i \\\\\n\\pi_j\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL *\n\\left(\\begin{array}{cc}\n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\lambda \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\pi \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJ(2)\n\\]\nSimilarly, for each dyad we can define varying intercepts and slopes to account for the correlation between the propensity to emit and receive links of a dyad:\n\\[\n\\left(\\begin{array}{cc}\n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL_\\delta *\n\\left(\\begin{array}{cc}\n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\nL_\\delta \\sim LKJ(2)\n\\]",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#notes",
    "href": "22. Network model.html#notes",
    "title": "Network Models",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nNote that any additional covariates can be summed with a regression coefficient to \\(\\lambda_i\\), \\(\\pi_j\\) and \\(\\delta_{ij}\\). Of course, for \\(\\lambda_i\\) and \\(\\pi_j\\), as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for \\(\\delta_{ij}\\), as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, individual_predictors, and a matrix of dyadic characteristics, kinship, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively, as follows:\n\ndef model2(idx, result_outcomes, dyad_effects, focal_individual_predictors, target_individual_predictors):\n    N_id = ids.shape[0]\n\n    # Sender Receiver effect (SR), its shape is equal to N_id ---------------------------------------\n    ## Covariates for SR\n    sr_terms, focal_effects, target_effects = m.net.nodes_terms(focal_individual_predictors, target_individual_predictors) \n\n    ## Varying intercept and slope for SR\n    sr_rf, sr_raw, sr_sigma, sr_L = m.net.nodes_random_effects(N_id, cholesky_density = 2) \n\n    sender_receiver = sr_terms + sr_rf\n\n    # Dyadic effect (D), its shape is equal to n dyads -----------------------------------------\n    ## Covariates for D\n    dr_terms, dyad_effects = m.net.dyadic_terms(dyad_effects)\n\n    ## Varying intercept and slope for D\n    rf, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(sender_receiver.shape[0], cholesky_density = 2)\n    dr = dr_terms + rf\n\n    lk('Y', Poisson(jnp.exp( sender_receiver + dr ), is_sparse = False), obs=result_outcomes) # is_sparse = True; if the matrix has many zeros, it can help speed up computation.\n\nm.data_on_model = dict(\n    idx = idx,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    dyad_effects = m.net.prepare_dyadic_effect(kinship), # Can be a jax array of multiple dimensions\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n)\n\nm.fit(model2) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nWe can apply multiple variables as in chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms as in chapter 3: Interaction Between Continuous Variables.\nNetwork links can be modeled using Bernoulli, Binomial, Poisson, or zero-inflated Poisson distributions. So, by replacing the Poisson distribution with a binomial distribution, we can model the existence or absence of a link ‚Äî i.e., model binary networks.\nIf the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms \\(\\lambda_i\\), \\(\\pi_j\\), and \\(\\delta_{ij}\\) are no longer required. (Is it correct?)\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html",
    "href": "14. Varying slopes.html",
    "title": "Varying Slopes Models",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#general-principles",
    "href": "14. Varying slopes.html#general-principles",
    "title": "Varying Slopes Models",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#considerations",
    "href": "14. Varying slopes.html#considerations",
    "title": "Varying Slopes Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for 12. Varying intercepts.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance üõà.\nThe covariance matrix requires a correlation matrix distribution which is modeled using an \\(LKJcorr\\) distribution that holds a parameter \\(Œ∑\\). \\(Œ∑\\) is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near ‚àí1 or 1. When we use \\(LKJcorr(1)\\), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\nThe Half-Cauchy distribution is used when modeling the covariance matrix to specify strictly positive values for the diagonal of the covariance matrix, ensuring positive variances.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#example",
    "href": "14. Varying slopes.html#example",
    "title": "Varying Slopes Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects:\n\nSimulated data\n\nPythonR\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    Rho = m.dist.lkj(2, 2, name = 'Rho')\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariatenormal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  m$normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details",
    "href": "14. Varying slopes.html#mathematical-details",
    "title": "Varying Slopes Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormula\nWe model the relationship between the independent variable \\(X\\) and the outcome variable \\(Y\\) with varying intercepts (\\(\\alpha\\)) and varying slopes (\\(\\beta\\)) for each group (\\(k\\)) using the following equation:\n\\[\nY_{ik} = \\alpha_k + \\beta_k X_{ik} + \\sigma\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(X_{ik}\\) is the independent variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(\\beta_k\\) is the varying regression coefficient for group \\(k\\).\n\\(\\sigma\\) is the error term, assumed to be strictly positive.\n\n\n\nBayesian Model\nWe can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik} , \\sigma)\n\\] \\[\n\\mu_{ik} = \\alpha_k + \\beta_k X_{ik}\n\\] \\[\n\\alpha_k \\sim Normal(0,1)\n\\] \\[\n\\beta_k \\sim Normal(0,1)\n\\] \\[\n\\sigma \\sim Exponential(1)\n\\]\nThe varying intercepts (\\(\\alpha_k\\)) and slopes (\\(\\beta_k\\)) are modeled using a Multivariate Normal distribution:\n\\[\n\\begin{pmatrix}\n\\alpha_k \\\\\n\\beta_k\n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left(\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta \\\\\n\\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta & \\sigma_\\beta^2\n\\end{pmatrix}\n\\right)\n\\]\nWhere:\n\n\\(\\left(\\begin{array}{cc} 0 \\\\ 0 \\end{array}\\right)\\) is the prior for the average intercept.\n\\(\\left(\\begin{array}{cc} \\sigma_\\alpha^2 & \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta \\\\ \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta & \\sigma_\\beta^2 \\end{array}\\right)\\) is the covariance matrix which specifies the variance and covariance of \\(\\alpha_k\\) and \\(\\beta_k\\),\nwhere:\n\n\\(\\sigma_\\alpha^2\\) is the variance of \\(\\alpha_k\\).\n\\(\\sigma_\\beta^2\\) is the variance of \\(\\beta_k\\).\n\\(\\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta\\) is the covariance between \\(\\alpha_k\\) and \\(\\beta_k\\).\n\n\nFor computational reasons, it is often better to implement a non-centered parameterization üõà that is equivalent to the Multivariate Normal distribution approach:\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\beta\n\\end{array}\\right) \\circ\nL \\cdot\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\\right)\n\\]\n\nWhere:\n\n\\(\\sigma_\\alpha \\sim \\text{Exponential}(1)\\) is the prior standard deviation among intercepts.\n\\(\\sigma_\\beta \\sim \\text{Exponential}(1)\\) is the prior standard deviation among slopes.\n\\(L \\sim \\text{LKJcorr}(\\eta)\\) is the prior for the correlation matrix using the Cholesky Factor üõà\n\n\nThe full non-centered version of the model is thus:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik} , \\sigma) \\\\\n\\]\n\\[\n\\mu_{ik} =   \\alpha_k + \\beta_k X_{ik} \\\\\n\\]\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\beta\n\\end{array}\\right) \\circ\nL \\cdot\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\\right)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta \\sim Normal(0,1)\n\\] \\[\n\\sigma_\\alpha \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\beta \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "href": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "title": "Varying Slopes Models",
    "section": "Multivariate Model with One Random Slope for Each Variable",
    "text": "Multivariate Model with One Random Slope for Each Variable\nWe can apply a multivariate model similarly to Chapter 2. In this case, we apply the same principle, but with a covariance matrix with a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for \\(i\\) observations in a model with two independent variables \\(X_1\\) and \\(X_2\\), we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma)\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{1i} X_{1i}  + \\beta_{2i} X_{2i}\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{i}\\\\\n\\beta_{1i}\\\\\n\\beta_{2i}\n\\end{pmatrix}\n\\sim \\begin{pmatrix}\n\\sigma_{\\alpha}\\\\\n\\sigma_{\\beta_1}\\\\\n\\sigma_{\\beta_2}\n\\end{pmatrix} \\circ L \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{i} \\\\\n\\widehat{\\beta}_{1i} \\\\\n\\widehat{\\beta}_{2i}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_1} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2} \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "href": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "title": "Varying Slopes Models",
    "section": "Multivariate Random Slopes on a Single Variable",
    "text": "Multivariate Random Slopes on a Single Variable\nFor more than two varying effects, we apply the same principle but with a covariance matrix for each varying effect that is summed to generate the varying intercept and slope. For example, if we want to generate random slopes for \\(i\\) actors and \\(k\\) groups, we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma) \\\\\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{i} X_i\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{i} = \\beta + \\beta_{actor[i]} + \\beta_{group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{\\text{actor}}\n\\end{pmatrix}\n\\sim\n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\beta a}\n\\end{pmatrix} \\circ L_a \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\beta}_{ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{\\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\beta g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\beta}_{kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#notes",
    "href": "14. Varying slopes.html#notes",
    "title": "Varying Slopes Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying slopes with any distribution presented in previous chapters. Below is the formula and the code snippet for a Binomial multivariate model with an interaction between two independent variables \\(X_1\\) and \\(X_2\\) and multiple varying effects for each actor and each group.\n\n\\[\np(Y_{i} |n , p_i) \\sim \\text{Binomial}(n = 1, p_i) \\\\\n\\]\n\\[\n\\text{logit}(p_i)=   \\alpha_i + \\beta_{1i}X_{1i}  + \\beta_{2i} X_{1i}X_{2i}\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{1i} = \\beta_1 + \\beta_{1, actor[i]} + \\beta_{1, group[i]}\n\\] \\[\n\\beta_{2i} = \\beta_2 + \\beta_{2, actor[i]} + \\beta_{2, group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta_1 \\sim Normal(0,1)\n\\] \\[\n\\beta_2 \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{1, \\text{actor}} \\\\\n\\beta_{2, \\text{actor}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\beta_1 a} \\\\\n\\sigma_{\\beta_2 a}\n\\end{pmatrix} \\circ L_a \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\beta}_{1,ka} \\\\\n\\widehat{\\beta}_{2,ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\]\n\\[\n\\sigma_{\\beta_1 a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2 a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{1, \\text{group}} \\\\\n\\beta_{2, \\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\beta_1 g} \\\\\n\\sigma_{\\beta_2 g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\beta}_{1,kg} \\\\\n\\widehat{\\beta}_{2,kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_1 g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2 g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]\nfrom main import *\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Import data\nm.read_csv(\"../data/chimpanzees.csv\", sep=\";\")\nm.df[\"block_id\"] = m.df.block\nm.df[\"treatment\"] = 1 + m.df.prosoc_left + 2 * m.df.condition\nm.data_to_model(['pulled_left', 'treatment', 'actor', 'block_id'])\n\n\ndef model(tid, actor, block_id, L=None, link=False):\n    # fixed priors\n    g = dist.normal(0, 1, name = 'g', shape = (4,))\n    sigma_actor = dist.exponential(1, name = 'sigma_actor', shape = (4,))\n    L_Rho_actor = dist.lkjcholesky(4, 2, name = \"L_Rho_actor\")\n    sigma_block = dist.exponential(1, name = \"sigma_block\", shape = (4,))\n    L_Rho_block = dist.lkjcholesky(4, 2, name = \"L_Rho_block\")\n\n    # adaptive priors - non-centered\n    z_actor = dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n    z_block = dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n    alpha = deterministic(\n        \"alpha\", ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n    )\n    beta = deterministic(\n        \"beta\", ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n    )\n\n    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n    dist(\"L\", dist.Binomial(logits=logit_p), obs=L)\n\n    # compute ordinary correlation matrices from Cholesky factors\n    if link:\n        deterministic(\"Rho_actor\", L_Rho_actor @ L_Rho_actor.T)\n        deterministic(\"Rho_block\", L_Rho_block @ L_Rho_block.T)\n        deterministic(\"p\", expit(logit_p))\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#references",
    "href": "14. Varying slopes.html#references",
    "title": "Varying Slopes Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "19. PCA.html",
    "href": "19. PCA.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto the first coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\n\n\n\nReduce dimensionality while retaining as much variance as possible.\nInfer posterior distributions over the principal components, instead of point estimates, by incorporating prior distributions over the parameters.\n\n\n\n\n\nDimensionality Reduction: Bayesian PCA is commonly used to reduce the dimensionality of high-dimensional datasets while incorporating uncertainty about the latent structure.\nData Visualization: By projecting data into a lower-dimensional space, PCA helps in visualizing high-dimensional datasets in 2D or 3D plots.\nNoise Modeling: Bayesian PCA provides an advantage over classical PCA by explicitly modeling noise and accounting for uncertainty in the data.\nFeature Extraction: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.\nLatent Variable Modeling: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#general-principles",
    "href": "19. PCA.html#general-principles",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto the first coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\n\n\n\nReduce dimensionality while retaining as much variance as possible.\nInfer posterior distributions over the principal components, instead of point estimates, by incorporating prior distributions over the parameters.\n\n\n\n\n\nDimensionality Reduction: Bayesian PCA is commonly used to reduce the dimensionality of high-dimensional datasets while incorporating uncertainty about the latent structure.\nData Visualization: By projecting data into a lower-dimensional space, PCA helps in visualizing high-dimensional datasets in 2D or 3D plots.\nNoise Modeling: Bayesian PCA provides an advantage over classical PCA by explicitly modeling noise and accounting for uncertainty in the data.\nFeature Extraction: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.\nLatent Variable Modeling: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#considerations",
    "href": "19. PCA.html#considerations",
    "title": "Principal Component Analysis (PCA)",
    "section": "Considerations",
    "text": "Considerations\nIn Bayesian PCA, we assume prior distributions for the latent variables Z and the principal component loadings W. We place Gaussian priors on both Z and W and learn their posterior distributions using the observed data X.\nThis approach differs from traditional PCA by allowing the posterior distributions to reflect uncertainty in the model parameters.",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#example",
    "href": "19. PCA.html#example",
    "title": "Principal Component Analysis (PCA)",
    "section": "Example",
    "text": "Example\nHere is an example code snippet demonstrating Bayesian PCA using TensorFlow Probability:\n\nfrom main import *\nimport seaborn as sns\n\nm = bi(platform='cpu')\n\n# Data simulation -------------------------------------------\n\nplt.style.use(\"ggplot\")\nwarnings.filterwarnings('ignore')\n\nnum_datapoints = 5000\ndata_dim = 2\nlatent_dim = 1\nstddv_datapoints = 0.5\n\n# Simulate data\ndef sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): \n    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w', sample=True, seed=seed)\n    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z', sample=True, seed=seed)\n    x = bi.dist.normal(w @ z, stddv_datapoints, name='x', sample=True, seed=seed)\n    return w, z, x\n\nactual_w, actual_z, x_train =sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 20)\nplt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1)\nplt.axis([-20, 20, -20, 20])\nplt.title(\"Dataset\")\nplt.show()\n\n\n# Model using simulated data\ndef model(x_train, data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): \n    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')\n    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')\n    lk('Y', Normal(w @ z, stddv_datapoints), obs = x_train)  \n    \nm.data_on_model = dict(\n    x_train = x_train, \n    data_dim = data_dim, \n    latent_dim = latent_dim, \n    num_datapoints = num_datapoints, \n    stddv_datapoints = stddv_datapoints\n)\n\nm.fit(model) \nsummary = m.summary()\nreal_data = jnp.concatenate([actual_w.flatten(), actual_z.flatten()]) # concatenate the actual values of w and z\nposteriors = summary.iloc[:,0]\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(real_data, posteriors, marker='o', linestyle='None', color='b', label='Posteriors')",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#mathematical-details",
    "href": "19. PCA.html#mathematical-details",
    "title": "Principal Component Analysis (PCA)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormulation\nGiven an observed data matrix \\(X \\in \\mathbb{R}^{N \\times D}\\) (where N is the number of samples and D is the number of dimensions), we assume the data is generated by a lower-dimensional latent variable model:\n\\[\nX = ZW^T + \\epsilon\n\\]\n\\[\nZ \\sim \\mathcal{N}(0, I)\n\\]\n\\[\nW \\sim \\mathcal{N}(0, I)\n\\]\n\\[\n\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\n\\]\nWhere:\n\n\\(X\\) is the observed data matrix.\n\\(Z \\in \\mathbb{R}^{N \\times K}\\) is the latent variable matrix (latent features with \\(K \\ll D\\)). \\(Z\\) is defined by a Normal distribution with mean 0 and variance 1.\n\\(W \\in \\mathbb{R}^{D \\times K}\\) is the matrix of principal components (projection matrix). \\(W\\) is defined by a Normal distribution with mean 0 and variance 1.\n\\(\\epsilon\\) is Gaussian noise, assumed to be normally distributed: \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\).",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#note",
    "href": "19. PCA.html#note",
    "title": "Principal Component Analysis (PCA)",
    "section": "Note",
    "text": "Note\n\nTo account for sign ambiguity üõà in PCA, we can align the signs of the estimated parameters with the true parameters before comparison. To do this, calculate the dot product between the true and estimated parameters. If it is negative, multiply the estimated parameters by -1 to align them with the true parameters. Below, a code snippet highlights how to do this:\n\n\ntrue_params = jnp.array(real_data)      \nestimated_params = jnp.array(posteriors) \n\n# Compute dot product\ndot_product = jnp.dot(true_params, estimated_params)\n\n# Align signs if necessary\nif dot_product &lt; 0:\n    estimated_params = -estimated_params\n\n# Plot the aligned parameters\nplt.scatter(true_params, estimated_params, alpha=0.7)\nplt.plot([min(true_params), max(true_params)], [min(true_params), max(true_params)], 'r--')\nplt.xlabel('True Parameters')\nplt.ylabel('Estimated Parameters')\nplt.title('True vs. Estimated Parameters After Sign Alignment')\nplt.show()",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "19. PCA.html#references",
    "href": "19. PCA.html#references",
    "title": "Principal Component Analysis (PCA)",
    "section": "Reference(s)",
    "text": "Reference(s)\nhttps://www.tensorflow.org/probability/examples/Probabilistic_PCA",
    "crumbs": [
      "Models",
      "Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#considerations",
    "href": "17. Missing data (wip).html#considerations",
    "title": "Handling Missing Data (WIP)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#example",
    "href": "17. Missing data (wip).html#example",
    "title": "Handling Missing Data (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Missing data model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#mathematical-details",
    "href": "17. Missing data (wip).html#mathematical-details",
    "title": "Handling Missing Data (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\n\n\nBayesian formulation",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#notes",
    "href": "17. Missing data (wip).html#notes",
    "title": "Handling Missing Data (WIP)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#references",
    "href": "17. Missing data (wip).html#references",
    "title": "Handling Missing Data (WIP)",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "get started.html",
    "href": "get started.html",
    "title": "Installation",
    "section": "",
    "text": "You can run BI on python or R. For R users you need to have installed python and the R reticulate package.\n\nPythonR\n\n\npip install BayesInference\n\n\npackage.install(BayesInference)"
  },
  {
    "objectID": "get started.html#import-tabular-data-from-a-csv-file",
    "href": "get started.html#import-tabular-data-from-a-csv-file",
    "title": "Installation",
    "section": "Import tabular data from a csv file",
    "text": "Import tabular data from a csv file\n\nPythonR\n\n\nm.data(data_path, sep=';') \n\n\nm$data(data_path,  sep=';')"
  },
  {
    "objectID": "get started.html#import-non-tabular-data",
    "href": "get started.html#import-non-tabular-data",
    "title": "Installation",
    "section": "Import non tabular data",
    "text": "Import non tabular data\nFirst you need to create our own dictionary with the data. ::: {.panel-tabset group=‚Äúlanguage‚Äù} ## Python\nm.data_on_model = dict(\n    ID1 = Value1,\n    ID2 = Value2, \n)"
  },
  {
    "objectID": "get started.html#r-3",
    "href": "get started.html#r-3",
    "title": "Installation",
    "section": "R",
    "text": "R\nkeys &lt;- c(\"ID1\",\"ID2\")\nvalues &lt;- list(Value1,Value2)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data\n::: # Define model A function that declare a mathematical model need to be defined first and sent it to the BI class. ::: {.panel-tabset group=‚Äúlanguage‚Äù} ## Python\ndef model(ID1, ID2):\n    # Define model here\n    pass"
  },
  {
    "objectID": "get started.html#r-4",
    "href": "get started.html#r-4",
    "title": "Installation",
    "section": "R",
    "text": "R\nmodel &lt;- function(ID1, ID2){\n  # Define model here\n  pass\n}"
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "Regression with Categorical Variables",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#general-principles",
    "href": "4. Categorical variable.html#general-principles",
    "title": "Regression with Categorical Variables",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "Regression with Categorical Variables",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a Continuous Variable.\nAs we generate regression coefficients for each k category, we need to specify a prior with a shape equal to the number of categories k in the code (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare confidence intervals or p-values directly.",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "Regression with Categorical Variables",
    "section": "Example",
    "text": "Example\nBelow is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (kcal_per_g), representing the caloric value of milk per gram, and a categorical independent variable, representing species clade membership. The goal is to estimate the differences in milk calories between clades.\n\nPythonR\n\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'milk.csv'\nm.data(data_path, sep=';') \nm.index([\"clade\"]) # Manipulate\nm.scale(['kcal_per_g']) # Scale\nm.data_to_model(['kcal_per_g', \"index_clade\"]) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade):\n    a = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    s = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]\n    m.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(kcal_per_g, index_clade){\n  # Parameter prior distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma =bi.dist.exponential(1, name = 's')\n  # Likelihood\n  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "Regression with Categorical Variables",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\\[\nY_i = \\alpha + \\beta_k X_i + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_k\\) are the regression coefficients for each k category.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the error term.\n\nWe can interpret \\(\\beta_i\\) as the effect of each category on \\(Y\\) relative to the baseline (usually one of the categories or the intercept).\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Normal(\\alpha +  \\beta_k X, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are k prior distributions for k regression coefficients.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "Regression with Categorical Variables",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms similarly to Chapter 3: Interaction between Continuous Variables.",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "Regression with Categorical Variables",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Regression with Categorical Variables"
    ]
  },
  {
    "objectID": "12. Survival analysis.html",
    "href": "12. Survival analysis.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving beyond a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#general-principles",
    "href": "12. Survival analysis.html#general-principles",
    "title": "Survival Analysis",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving beyond a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#considerations",
    "href": "12. Survival analysis.html#considerations",
    "title": "Survival Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nBayesian models provide a framework to account for uncertainty üõà in parameter estimates through posterior distributions. You will need to define prior distributions üõà for all model parameters, such as baseline hazard, covariate effects, and variance terms.\nIn survival analysis:\n\nThe baseline hazard can follow distributions like Exponential, Weibull, or Gompertz, depending on the data.\nCensoring (when the event is not observed for some subjects) must be accounted for in the likelihood function. Proper handling is essential for unbiased results.\n\nBayesian survival models allow flexible handling of time-dependent covariates, random effects, and incorporate uncertainty more naturally than Frequentist methods.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#example",
    "href": "12. Survival analysis.html#example",
    "title": "Survival Analysis",
    "section": "Example",
    "text": "Example\nHere‚Äôs an example of a Bayesian survival analysis using the Bayesian Inference (BI) package. The data come from a clinical trial of mastectomy for breast cancer. The goal is to estimate the effect of the metastasized covariate, coded as 0 (no metastasis) and 1 (metastasis), on the survival outcome event for each patient. Time is continuous and censoring is indicated by the event variable.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'mastectomy.csv'\nm.data(data_path, sep=',') \n\nm.df.metastasized = (m.df.metastasized == \"yes\").astype(np.int64)\nm.df.event = jnp.array(m.df.event.values, dtype=jnp.int32)\n\n## Create survival object\nm.surv_object(time='time', event='event', cov='metastasized', interval_length=3)\n\n# Plot censoring ------------------------------------------------\nm.plot_censoring(cov='metastasized')\n\n# Model ------------------------------------------------\ndef model(intervals, death, metastasized, exposure):\n    # Parameter prior distributions-------------------------\n    ## Base hazard distribution\n    lambda0 = m.dist.gamma(0.01, 0.01, shape= intervals.shape, name = 'lambda0')\n    ## Covariate effect distribution\n    beta = m.dist.normal(0, 1000, shape = (1,),  name='beta')\n    ### Likelihood\n    #### Compute hazard rate based on covariate effect\n    lambda_ = m.hazard_rate(cov = metastasized, beta = beta, lambda0 = lambda0)\n    #### Compute exposure rates\n    mu = exposure * lambda_\n\n    # Likelihood calculation\n    y = m.poisson(mu + jnp.finfo(mu.dtype).tiny, obs = death)\n\n# Run mcmc ------------------------------------------------\nm.fit(model, num_samples=500) \n\n# Summary ------------------------------------------------\nprint(m.summary())\n\n# Plot hazards and survival function ------------------------------------------------\nm.plot_surv()",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#mathematical-details",
    "href": "12. Survival analysis.html#mathematical-details",
    "title": "Survival Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nThe Cox proportional hazards model can be expressed as: \\[\nh(t | X) = h_0(t) \\exp(\\beta^T X)\n\\]\n\nWhere:\n\n\\(h(t | X)\\) is the hazard at time \\(t\\) for covariates \\(X\\).\n\\(h_0(t)\\) is the baseline hazard function (e.g., exponential, Weibull).\n\\(X\\) represents the covariates (such as age, treatment).\n\\(\\beta\\) are the regression coefficients to be estimated.\n\nCensoring is accounted for by multiplying the hazard function by a factor that depends on the censoring distribution, usually modeled as independent censoring with a rate Œ¥(t):\n\n\\(Y_i(t) = Poisson(h(t | X) * Œ¥(t))\\)\n\n\n\n\nBayesian formulation\nIn Bayesian survival analysis, we define priors for each parameter:\n\nHazard Function: The hazard rate at time \\(t\\) for an individual is given by: \\[Y_i(t) = Poisson(\\lambda(t) * censoring(t))\\] \\[\\lambda(t) = \\lambda_0(t)\\exp(x\\beta)\\]\n\\[ \\beta \\sim Normal(\\mu_\\beta, \\sigma^2_\\beta) \\]\n\\[\\mu_\\beta \\sim Normal(0, 10^2)\\]\n\\[\\sigma^2_\\beta \\sim Uniform(0, 10)\\]\nWhere:\n\n\\(Y_i(t)\\) is the status of the \\(i\\)-th subject at time \\(t\\) coded as a binary variable: \\[\nY_i(t) =\n\\begin{cases}\n1 & \\text{if subject } i \\text{ died at time $t$  }, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\n\\(\\lambda(t)\\): Hazard function at time \\(t\\).\n\\(\\lambda_0(t)\\): Baseline hazard function (e.g., exponential or Weibull).\n\\(x\\): Covariates (e.g., age, treatment).\n\\(\\beta\\): Regression coefficients capturing the effect of \\(x\\) on the hazard are assigned a normal prior.\n\\(\\mu_\\beta\\): Mean of the normal distribution.\n\\(\\sigma^2_\\beta\\): Variance of the normal distribution.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#references",
    "href": "12. Survival analysis.html#references",
    "title": "Survival Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "7. Poisson model.html",
    "href": "7. Poisson model.html",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable‚Äîe.g., counts of events occurring in a fixed interval of time or space‚Äîand one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials \\(n\\) is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#general-principles",
    "href": "7. Poisson model.html#general-principles",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable‚Äîe.g., counts of events occurring in a fixed interval of time or space‚Äîand one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials \\(n\\) is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#considerations",
    "href": "7. Poisson model.html#considerations",
    "title": "Poisson Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nWe have the second link function üõà: log. The log link ensures that Œª is always positive.\nTo invert the log link function and linearly model the relationship between the predictor variables and the log of the mean rate parameter, we can apply the exponential function (see comment in code).",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#example",
    "href": "7. Poisson model.html#example",
    "title": "Poisson Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Poisson model using the Bayesian Inference (BI) package. Data consist of:\n\nA continuous dependent variable total_tools, which represents the number of tools produced by a civilization.\nA continuous independent variable population representing population size.\nA categorical independent variable cid representing different civilizations.\n\nThe goal is to estimate the production of tools based on population size, accounting for each civilization.\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline.csv'\nm.data(data_path, sep=';')\nm.scale(['population'])\nm.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n#m.data_to_model(['total_tools', 'population', 'cid'])\ndef model(cid, population, total_tools):\n    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n    l = jnp.exp(a[cid] + b[cid]*population)\n    m.poisson(l, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline.csv\", sep = ''), sep=';')\nm$scale(list('population'))# Scale\nm$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\nm$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(total_tools, population, cid){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n  l = jnp$exp(alpha[cid] + beta[cid]*population)\n  # Likelihood\n  m$poisson(l, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#mathematical-details",
    "href": "7. Poisson model.html#mathematical-details",
    "title": "Poisson Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the predictor variable (\\(X_i\\)) and the count outcome variable (\\(Y_i\\)) using the following equation:\n\\[\n\\log(\\lambda_i) = \\alpha + \\beta  X_i\n\\]\nWhere:\n\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution (expected count) for observation i, modeled as the exponential function of the linear combination of predictors.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i, ensuring it is positive.\n\\(\\beta\\) is the regression coefficient.\n\\(\\alpha\\) is the intercept term.\n\\(X_i\\) is the value of the independent variable for observation i.\n\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Poisson(\\lambda_i)\n\\]\n\\[\n\\log(\\lambda_i) = \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim Normal(0, 1)\n\\]\n\\[\n\\beta \\sim Normal(0, 1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution for observation i, modeled as the exponential function of the linear combination of predictors.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i.\n\\(\\alpha\\) and \\(\\beta\\) are the prior distributions for the intercept and the regression coefficients, respectively.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution, modeled as the exponential function of the linear combination of predictors.\n\\(X_i\\) is the value of the independent variable for observation i.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#notes",
    "href": "7. Poisson model.html#notes",
    "title": "Poisson Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\n\n\nReference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#references",
    "href": "7. Poisson model.html#references",
    "title": "Poisson Model",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html",
    "href": "8. Gamma-Poisson.html",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion üõà, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#general-principles",
    "href": "8. Gamma-Poisson.html#general-principles",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion üõà, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#considerations",
    "href": "8. Gamma-Poisson.html#considerations",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Poisson model.\nOverdispersion is handled because the Negative Binomial model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is log_days).",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#example",
    "href": "8. Gamma-Poisson.html#example",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package:\n\nPythonR\n\n\nfrom BI import bi\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim dat Gamma poisson.csv'\nm.data(data_path, sep=',') \nm.data_to_model(['log_days', 'monastery', 'y']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    l = m.jnp.exp(log_days + a + b * monastery)\n    m.poisson(rate = l, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  l = jnp$exp(log_days + alpha + beta * monastery)\n  # Likelihood\n  m$poisson(rate=l, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#mathematical-details",
    "href": "8. Gamma-Poisson.html#mathematical-details",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variable \\(X\\) and the count outcome variable \\(Y\\) using the following equation:\n\\[\n\\log(\\lambda_i) = \\exp(\\text{rates}_i + \\alpha + \\beta X_i)\n\\]\nWhere:\n\n\\(\\lambda_i\\) is the mean rate parameter of the negative binomial distribution (expected count) for observation i.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter, ensuring it is positive for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the predictor variable for observation i.\n\n\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\n\\[\n\\log(\\lambda_i) = \\text{rates}_i + \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim \\text{Normal}(0,1)\n\\]\n\\[\n\\beta \\sim \\text{Normal}(0,1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution for observation i, assuming that each Poisson count observation has its own \\(rate_i\\).\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i, ensuring it is positive.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the predictor variable for observation i.",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#notes",
    "href": "8. Gamma-Poisson.html#notes",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in chapter 2.\nWe can apply interaction terms similarly as in chapter 3.\nWe can apply categorical variables similarly as in chapter 4.",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#references",
    "href": "8. Gamma-Poisson.html#references",
    "title": "Gamma-Poisson (Negative Binomial) Model",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Gamma-Poisson (Negative Binomial) Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html",
    "href": "10. Dirichlet model (wip).html",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "",
    "text": "To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables with overdispersion üõà, we can use a Dirichlet model.",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#general-principles",
    "href": "10. Dirichlet model (wip).html#general-principles",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "",
    "text": "To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables with overdispersion üõà, we can use a Dirichlet model.",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#considerations",
    "href": "10. Dirichlet model (wip).html#considerations",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Multinomial model.\nOne major difference from the multinomial model is that the Dirichlet model doesn‚Äôt require a simplex but rather strictly positive values. We can thus exponentiate the outputs from the categorical regressions instead of using the softmax function.",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#example",
    "href": "10. Dirichlet model (wip).html#example",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "Example",
    "text": "Example\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.halfnormal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = alpha[0] + beta * income[0]\n    p = jax.nn.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dirichletmultinomial(p[career], lambda_, obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#mathematical-details",
    "href": "10. Dirichlet model (wip).html#mathematical-details",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormula\n\n\nBayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#references",
    "href": "10. Dirichlet model (wip).html#references",
    "title": "Dirichlet-Multinomial Model (WIP)",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Dirichlet-Multinomial Model (WIP)"
    ]
  },
  {
    "objectID": "27. BNN.html",
    "href": "27. BNN.html",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of ‚Äúneurons.‚Äù Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our ‚Äúknobs.‚Äù\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\(\\beta\\)). In a neural network, which can have thousands or millions of weights, we don‚Äôt define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network‚Äôs predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\(\\sigma\\) that quantifies the data‚Äôs noise around the model‚Äôs predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#general-principles",
    "href": "27. BNN.html#general-principles",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of ‚Äúneurons.‚Äù Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our ‚Äúknobs.‚Äù\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\(\\beta\\)). In a neural network, which can have thousands or millions of weights, we don‚Äôt define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network‚Äôs predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\(\\sigma\\) that quantifies the data‚Äôs noise around the model‚Äôs predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#considerations",
    "href": "27. BNN.html#considerations",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nLike all Bayesian models, BNNs consider model parameter uncertainty üõà. The parameters here are the network‚Äôs weights (W) and biases (b). We quantify our uncertainty about them through their posterior distribution üõà. Therefore, we must declare prior distributions üõà for all weights and biases, as well as for the output variance \\(\\sigma\\).\nUnlike in a linear regression where the coefficient Œ≤ has a direct interpretation (e.g., the effect of weight on height), the individual weights and biases in a BNN are not directly interpretable. A single weight‚Äôs influence is entangled with thousands of other parameters through non-linear functions. Consequently, BNNs are best viewed as powerful predictive tools rather than explanatory ones. They excel at learning complex patterns and quantifying predictive uncertainty, but if the goal is to isolate and interpret the effect of a specific variable, a simpler model is often more appropriate.\nPrior distributions are built following these considerations:\n\nAs the data is typically scaled üõà (see introduction), we can use a standard Normal distribution (mean 0, standard deviation 1) as a weakly-informative prior for all weights and biases. This acts as a form of regularization.\nSince the output variance \\(\\sigma\\) must be positive, we can use a positively-defined distribution, such as the Exponential or Half-Normal.\n\nBNNs can be used for both regression and classification. The final layer‚Äôs activation and the chosen likelihood distribution depend on the task. For binary classification, a sigmoid activation is paired with a Bernoulli likelihood, which requires a link function üõà (logit) to connect the linear output of the network to the probability space [0, 1]. For regression, the identity activation is often used with a Gaussian likelihood.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#example",
    "href": "27. BNN.html#example",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Neural Network for regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to predict height from weight using a non-linear model.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):\n    # Define the BNN architecture and get its output (mu)\n    # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n    # Priors for weights/biases are Normal(0,1) by default\n    mu = m.bnn(x=weight, n_neurons=[10, 1], activations=['tanh', 'identity'], name='bnn')\n\n    # Prior for the output standard deviation\n    s = m.dist.exponential(1, name='s')\n    \n    # Likelihood\n    m.normal(mu, s, obs=height)\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Approximate posterior distributions for weights, biases, and sigma\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# Filter data frame\nm$df = m$df[m$df$age &gt; 18,]\n\n# Scale\nm$scale(list('weight')) \n\n# Convert data to JAX arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Define the BNN architecture\n  # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n  # Priors for weights/biases are Normal(0,1) by default\n  mu &lt;- bi$bnn(x = weight, n_neurons = list(10, 1), activations = list('tanh', 'identity'), name = 'bnn')\n\n  # Prior for the output standard deviation\n  s = bi$dist$exponential(1, name = 's')\n  \n  # Likelihood\n  m$normal(mu, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Approximate posterior distributions\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#mathematical-details",
    "href": "27. BNN.html#mathematical-details",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist Formulation\nA standard (non-Bayesian) neural network with one hidden layer is defined by forward propagation:\n\\[\nh_i = f(X_i W_1 + b_1)\n\\]\n\\[\n\\hat{Y}_i = g(h_i W_2 + b_2)\n\\]\nWhere: - \\(Y_i\\) is the predicted output for observation i. - \\(X_i\\) is the input vector for observation i. - \\(W_1, b_1\\) are the weight matrix and bias vector for the hidden layer. - \\(W_2, b_2\\) are the weight matrix and bias vector for the output layer. - \\(h_i\\) is the activation of the hidden layer. - \\(f\\) and \\(g\\) are activation functions (e.g., ReLU, tanh, sigmoid, identity). - Parameters \\(W_1, b_1, W_2, b_2\\) are learned as single optimal values via optimization.\n\n\nBayesian Formulation\nIn the Bayesian formulation, we place priors üõà on all weights and biases and define a likelihood for the output. For a regression task with a one-layer BNN:\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = g(h_i W_2 + b_2)\n\\]\n\\[\nh_i = f(X_i W_1 + b_1)\n\\]\nThe parameters are now distributions: \\[\nW_1 \\sim Normal(0, 1)\n\\] \\[\nb_1 \\sim Normal(0, 1)\n\\] \\[\nW_2 \\sim Normal(0, 1)\n\\] \\[\nb_2 \\sim Normal(0, 1)\n\\] \\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere: - \\(Y_i\\) is the observed dependent variable for observation i. - \\(\\mu_i\\) is the mean predicted by the network, which is itself a distribution because it is a function of the distributions of weights and biases. - \\(W_1, b_1, W_2, b_2\\) are the weights and biases, treated as random variables. - \\(\\sigma\\) is the standard deviation of the normal distribution, quantifying observation noise.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#notes",
    "href": "27. BNN.html#notes",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe primary difference between a Frequentist and Bayesian neural network lies in how parameters are treated. In the frequentist approach, weights and biases are point estimates found by minimizing a loss function (e.g., via gradient descent). Techniques like Dropout or L2 regularization are often used to prevent overfitting, which can be interpreted as approximations to a Bayesian treatment. In contrast, the Bayesian formulation does not seek a single best set of weights. Instead, it uses methods like MCMC or Variational Inference to approximate the entire posterior distribution for every weight and bias. This provides a principled and direct way to quantify model uncertainty.\nWhile present an example of non-linear regression, the Bayesian Neural Network can be used for linear regressions as well (keeping in mind that interpretation of the weights are impossible).\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):\n    # Define the BNN architecture and get its output (mu)\n    # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n    # Priors for weights/biases are Normal(0,1) by default\n    mu = m.bnn(x=weight, n_neurons=[10, 1], activations=['tanh', 'identity'], name='bnn')\n\n    # Prior for the output standard deviation\n    s = m.dist.exponential(1, name='s')\n    \n    # Likelihood\n    m.normal(mu, s, obs=height)\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Approximate posterior distributions for weights, biases, and sigma\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "27. BNN.html#references",
    "href": "27. BNN.html#references",
    "title": "Bayesian Neural Networks (BNN)",
    "section": "Reference(s)",
    "text": "Reference(s)\n(neal1995bayesian?)",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks (BNN)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "Welcome to the BI Documentation model gallery. Here you will find a collection of statistical models and techniques, each with a detailed explanation and examples. Click on any card to explore the topic.\nThis gallery provides a quick visual overview to help you find the model you‚Äôre looking for. You can use the sidebar for a more structured, chapter-by-chapter view.\n\nGallery of Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression for a Continuous Variable\n\n\nAn introduction to linear regression models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\n\n\nExtending linear regression to model a continuous outcome using multiple predictor variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteraction Terms in Regression\n\n\nModeling how the effect of one predictor on the outcome changes based on the value of another.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression with Categorical Variables\n\n\nIncorporating categorical predictors (factors) into a regression model using dummy variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Model (Logistic Regression)\n\n\nModeling a binary outcome (e.g., success/failure) based on one or more predictor variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta-Binomial Model\n\n\nModeling binomial-type data with overdispersion, where the probability of success follows a beta distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Model\n\n\nModeling count data, such as the number of events occurring in a fixed interval of time or space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma-Poisson (Negative Binomial) Model\n\n\nAn extension of the Poisson model for count data that exhibits overdispersion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Model\n\n\nModeling a categorical dependent variable with more than two nominal outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirichlet-Multinomial Model (WIP)\n\n\nModeling compositional data or multinomial outcomes with overdispersion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero-Inflated Models\n\n\nHandling count data with an excess of zero counts by combining a count model with a logistic model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Analysis\n\n\nAnalyzing time-to-event data, focusing on the duration until an event of interest occurs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarying Intercepts Models\n\n\nModeling grouped or hierarchical data by allowing the intercept to vary for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarying Slopes Models\n\n\nExtending mixed-effects models by allowing the relationship (slope) to vary across different groups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Processes\n\n\nA non-parametric Bayesian approach to regression and classification that defines a distribution over functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement Error Models\n\n\nStatistical models that explicitly account for errors in the measurement of predictor variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data (WIP)\n\n\nAn overview of strategies for dealing with missing values in a dataset, such as imputation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Variable Models (WIP)\n\n\nModels that relate a set of observable variables to a set of unobserved (latent) variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nA dimensionality reduction technique to transform a large set of variables into a smaller one.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Mixture Models (GMM)\n\n\nA probabilistic model for representing normally distributed subpopulations, often used for clustering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirichlet Process Mixture Models (DPMM)\n\n\nA non-parametric Bayesian clustering method that automatically determines the number of clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Models\n\n\nModeling relationships and interactions between entities as a graph of nodes and edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Block Models (SBM)\n\n\nA generative model for random graphs used to find communities of nodes with similar connection patterns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControlling for Network Biases\n\n\nTechniques to account for biases in data collection that can affect the structure and analysis of networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Metrics\n\n\nAn overview of key metrics used to describe and analyze the structure and properties of networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork-Based Diffusion Analysis\n\n\nModeling how information, behaviors, or diseases spread through a network over time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Neural Networks (BNN)\n\n\nNeural networks that incorporate Bayesian inference to quantify uncertainty in their predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\(\\beta_x\\) for each continuous variable (e.g., \\(\\beta_{weight}\\) and \\(\\beta_{age}\\)).",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#general-principles",
    "href": "2. Multiple continuous Variables.html#general-principles",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\(\\beta_x\\) for each continuous variable (e.g., \\(\\beta_{weight}\\) and \\(\\beta_{age}\\)).",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "Multiple Linear Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Regression for continuous variable.\nThe model interpretation of the regression coefficients \\(\\beta_x\\) is considered for fixed values of the other independent variable(s)‚Äô regression coefficients‚Äîi.e., for a given age, \\(\\beta_{weight}\\) represents the expected change in the dependent variable (height) for each one-unit increase in weight, holding all other variable(s) constant (age).",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (height, weight, age), and the goal is to estimate the effect of weight and age on height.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nfrom importlib.resources import files\n# Import\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight', 'age']) # Scale\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0,50, name = 'sigma')\n    # Likelihood\n    m.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')# Import\nm$df = m$df[m$df$age &gt; 18,] # Manipulate\nm$scale(list('weight', 'age')) # Scale\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight, age){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0, 0.5, name = 'a')\n  beta1 = bi.dist.normal( 0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(  0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "Multiple Linear Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variables \\((X_{1i}, X_{2i}, ..., X_{ni})\\) and the dependent variable Y using the following equation:\n\\[\nùëå_i = \\alpha +\\beta_1  ùëã_{1i} + \\beta_2  ùëã_{2i} + ... + \\beta_n  ùëã_{ni} + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\beta_1\\), \\(\\beta_2\\), ‚Ä¶, \\(\\beta_n\\) are the regression coefficients.\n\\(\\sigma_i\\) is the error term for observation i.\n\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian model as follows:\n\\[\nùëå \\sim Normal(\\alpha + \\sum_k^n  \\beta_k  X, œÉ¬≤)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\nœÉ \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are the prior distributions for the k distinct regression coefficients.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring that it is positive.",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "Multiple Linear Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "16. Measuring error.html",
    "href": "16. Measuring error.html",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#general-principles",
    "href": "16. Measuring error.html#general-principles",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#example",
    "href": "16. Measuring error.html#example",
    "title": "Measurement Error Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian measurement error model using the Bayesian Inference (BI) package. The data consist of three continuous variables (marriage rate, divorce rate, age), and the goal is to estimate the effect of age and marriage rate on the divorce rate while considering that the divorce rate has a measurement error.\n\nPython\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'WaffleDivorce.csv'\nm.data(data_path, sep=';') \nm.scale(['Divorce', 'Divorce SE', 'MedianAgeMarriage']) # Scale\ndat = dict(\n    D_obs = jnp.array(m.df['Divorce'].values),   \n    D_sd = jnp.array(m.df['Divorce'].values), \n    A = jnp.array(m.df['MedianAgeMarriage'].values), \n    N = m.df.shape[0]   \n)\nm.data_on_model = dat # Send to model (convert to jax array)\n\n\n# Define model ------------------------------------------------\ndef model(D_obs, D_sd, A, N):  \n    a = m.dist.normal(0, 0.2, name = 'a') \n    bA = m.dist.normal(0, 0.5, name = 'bA') \n    s = m.dist.exponential(1, name = 's') \n    mu = a + bA * A + bM * M\n    D_true = m.dist.normal(mu, s, name = 'D_true') \n    m.normal(D_true , D_sd, obs = D_obs) \n\n# Run MCMC ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#mathematical-details",
    "href": "16. Measuring error.html#mathematical-details",
    "title": "Measurement Error Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian formulation\n\\[\nD_i^* \\sim Normal(D_i, \\sigma_i)\n\\]\n\\[\nD_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\alpha + \\beta_A A_i + \\beta_M M_i\n\\]\n\\[\n\\sigma \\sim Normal(1)\n\\]\nwhere:\n\n\\(D_i^*\\) is the observed divorce rate.\n\\(D_i\\) is the true divorce rate.\n\\(\\mu_i\\) is the mean of the true divorce rate.\n\\(\\sigma\\) is the standard deviation of the true divorce rate.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_A\\) is the regression coefficient for age.\n\\(\\beta_M\\) is the regression coefficient for marriage rate.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#notes",
    "href": "16. Measuring error.html#notes",
    "title": "Measurement Error Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThis is an approach that can be extended to any kind of model previously described. For example, one could generate a Bernoulli measurement error model by generating a process for the probabilities of success and failure. We can even go further by potentially having an error rate that is present only in one of the two outcomes.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#references",
    "href": "16. Measuring error.html#references",
    "title": "Measurement Error Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "To study the relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#general-principles",
    "href": "3. Interaction between continuous variables.html#general-principles",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "To study the relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "Interaction Terms in Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same assumptions as for Regression for continuous variable.\nWe wish to model the relationship between dependent variable Y and independent variable \\(X_1\\) to vary as a function of independent variable \\(X_2\\). To do this, we explicitly model the hypothesis that the slope between Y and \\(X_1\\) depends‚Äîis conditional‚Äîupon \\(X_2\\).\nFor continuous interactions with scaled data, the intercept becomes the grand mean üõà of the outcome variable.\nThe interpretation of estimates is more complex. The estimate for a non-interaction term reflects the expected change in Y when \\(X_1\\) increases by one unit, holding \\(X_2\\) constant at its average value. The estimate for the interaction term represents how the effect of \\(X_1\\) on Y changes depending on the value of \\(X_2\\), and vice versa, showing how the relationship between the two variables influences the outcome Y.\nTriptych üõà plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "Interaction Terms in Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. The data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'tulips.csv'\nm.data(data_path, sep=';')\nm.scale(['blooms', 'water', 'shade']) # Scale\n\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.normal(mu, sigma, obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Scale\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(blooms, water,shade){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')\n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint')\n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "Interaction Terms in Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#frequentist-formulation",
    "href": "3. Interaction between continuous variables.html#frequentist-formulation",
    "title": "Interaction Terms in Regression",
    "section": "Frequentist formulation",
    "text": "Frequentist formulation\nWe model the relationship between the input features (\\(X_1\\) and \\(X_2\\)) and the target variable (\\(Y\\)) using the following equation: \\[\nùëå_i = \\alpha + \\beta_1 ùëã_{1i} + \\beta_2 ùëã_{2i} + \\beta_{interaction} ùëã_{1i} ùëã_{2i} + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\beta_1\\) and \\(\\beta_2\\) are the regression coefficients for \\(X_{1}\\) and \\(X_{2}\\), respectively.\n\\(\\beta_{interaction}\\) is the regression coefficient for the interaction term \\((X_{1} X_{2})\\).\n\\(\\sigma\\) is the error term, assumed to be normally distributed.\n\nIn this context, the interaction term \\(X_{1i} * X_{2i}\\) captures the joint effect of \\(X_{1i}\\) and \\(X_{2i}\\) on the target variable \\(Y_i\\).\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Normal(\\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_{interaction} X_{1i} X_{2i}, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_1 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_2 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_{interaction} \\sim Normal(0,1)\n\\]\n\\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_{interaction}\\) are the prior distributions for the regression coefficients.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "Interaction Terms in Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "5. Binomial model.html",
    "href": "5. Binomial model.html",
    "title": "Binomial Model (Logistic Regression)",
    "section": "",
    "text": "To model the relationship between a binary dependent variable‚Äîe.g., success/failure, yes/no, or 1/0‚Äîand one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#general-principles",
    "href": "5. Binomial model.html#general-principles",
    "title": "Binomial Model (Logistic Regression)",
    "section": "",
    "text": "To model the relationship between a binary dependent variable‚Äîe.g., success/failure, yes/no, or 1/0‚Äîand one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#considerations",
    "href": "5. Binomial model.html#considerations",
    "title": "Binomial Model (Logistic Regression)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nThe first link function is the logit. The logit link function in the Bayesian binomial model converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution \\(\\in[0,1]\\).",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#example",
    "href": "5. Binomial model.html#example",
    "title": "Binomial Model (Logistic Regression)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled. The goal is to evaluate the probability of pulling the left side.\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'chimpanzees.csv'\nm.data(data_path, sep=';')\nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.binomial(logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  m$binomial(logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#mathematical-details",
    "href": "5. Binomial model.html#mathematical-details",
    "title": "Binomial Model (Logistic Regression)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variable (\\(X_i\\)) and the binary dependent variable (\\(Y_i\\)) using the following equation: \\[\nlogit(Y_i) = \\alpha + \\beta X_i\n\\]\nWhere:\n\n\\(Y_i\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the independent variable for observation i.\n\\(logit(Y_i)\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.\n\n\n\nBayesian formulation\npriors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim Binomial(n = 1, p)\n\\]\n\\[\nlogit(p) \\sim \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\nWhere:\n\n\\(Y_i\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(n = 1\\) represents the number of trials in the binomial distribution (binary outcome).\n\\(\\beta\\) and \\(\\alpha\\) are the prior distributions for the regression coefficient and intercept, respectively.\n\\(logit\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#notes",
    "href": "5. Binomial model.html#notes",
    "title": "Binomial Model (Logistic Regression)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\nBelow is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled, and three independent variables (actor, side, cond). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as the different conditions.\n\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';')\nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.lk(\"y\", m.dist.binomial(1, logits=p), obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n# Diagnostic ------------------------------------------------\nm.summary()",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "5. Binomial model.html#references",
    "href": "5. Binomial model.html#references",
    "title": "Binomial Model (Logistic Regression)",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Binomial Model (Logistic Regression)"
    ]
  },
  {
    "objectID": "21. DPMM.html",
    "href": "21. DPMM.html",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a non-parametric üõà clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its center (mean \\(\\mu\\)) and its shape/spread (covariance \\(\\sigma\\)).\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#general-principles",
    "href": "21. DPMM.html#general-principles",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a non-parametric üõà clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its center (mean \\(\\mu\\)) and its shape/spread (covariance \\(\\sigma\\)).\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#considerations",
    "href": "21. DPMM.html#considerations",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA DPMM is a Bayesian model üõà that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation called the Stick-Breaking Process üõà.\nThe key parameters and their priors are:\n\nConcentration \\(\\alpha\\): This single parameter controls the tendency to create new clusters. A low Œ± favors fewer, larger clusters, while a high Œ± allows for many smaller clusters. We typically place a Gamma prior on \\(\\alpha\\) to learn its value from the data.\nCluster Weights w: Generated via the Stick-Breaking process from \\(\\alpha\\). These are the probabilities of drawing a data point from any given cluster.\nCluster Parameters (\\(\\mu\\), \\(\\sigma\\)): Each potential cluster has a mean \\(\\mu\\) and a covariance matrix \\(\\sigma\\). If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, 14). Howver, if the data is one-dimensional, we use a univariate normal distribution.\n\nThe model is often implemented in its marginalized form üõà. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#example",
    "href": "21. DPMM.html#example",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "Example",
    "text": "Example\nBelow is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\nPythonR\n\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n#  The model\ndef dpmm(data, T=10):\n    N, D = data.shape  # Number of features\n    data_mean = jnp.mean(data, axis=0)\n    data_std = jnp.std(data, axis=0)*2\n\n    # 1) stick-breaking weights\n    alpha = dist.gamma(1.0, 10.0,name='alpha')\n\n    with m.plate(\"beta_plate\", T - 1):\n        beta = m.dist.Beta(1, alpha))\n\n    w = numpyro.deterministic(\"w\",mix_weights(beta))\n\n\n    # 2) component parameters\n    with m.plate(\"components\", T):\n        mu = m.dist.multivariatenormal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n        sigma = m.dist.lognormal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n        Lcorr = m.dist.lkjcholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n\n        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n\n    # 3) Latent cluster assignments for each data point\n    with m.plate(\"data\", N):\n        # Sample the assignment for each data point\n        z = m.dist.Categorical(w) # shape (N,)  \n\n        # Sample the data point from the assigned component\n        m.dist.MultivariateNormal(loc=mu[z], scale_tril=scale_tril[z],\n            obs=data\n        )  \n\nm.data_on_model = dict(data=data)\nm.fit(dpmm)  # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#mathematical-details",
    "href": "21. DPMM.html#mathematical-details",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThis level describes how any single data point, \\(x_i\\), is generated. The process involves two steps: first, assigning the data point to a cluster, and second, drawing it from that cluster‚Äôs specific distribution. We use a truncation level \\(T\\) as a finite approximation for the infinite number of possible clusters in a true Dirichlet Process.\n\\[\\begin{align*}\nx_i \\mid z_i=k &\\sim \\text{MultivariateNormal}(\\mu_k, \\Sigma_{\\text{obs}}) \\\\\nz_i &\\sim \\text{Categorical}(w) \\\\\nw &= \\text{StickBreaking}(\\beta_1, ..., \\beta_{T-1}) \\\\\n\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n\\alpha &\\sim \\text{Gamma}(1, 10)\\\\\n\\mu_k &\\sim \\text{MultivariateNormal}(\\mu_0, \\Sigma_0)  \\\\\n\\Sigma_{\\text{obs}} &= I_D \\\\\n\\end{align*}\\]\nParameter Definitions: * Observed Data: * \\(x_i\\): The \\(i\\)-th observed D-dimensional data point.\n\nLatent Variables (Inferred):\n\n\\(z_i\\): The integer cluster assignment for the \\(i\\)-th data point.\n\\(w\\): The vector of mixture weights, where \\(w_k\\) is the probability of belonging to cluster \\(k\\).\n\\(\\beta_k\\): The set of Beta-distributed random variables for the stick-breaking process.\n\\(\\alpha\\): The concentration parameter, controlling the effective number of clusters.\n\n\\(\\mu_k\\): The D-dimensional mean vector of the \\(k\\)-th cluster.\n\nHyperparameters (Fixed):\n\n\\(\\mu_0\\): The prior mean for the cluster centers (e.g., mean(data)).\n\\(\\Sigma_0\\): The prior covariance for the cluster centers (e.g., 10 * I_D).",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#notes",
    "href": "21. DPMM.html#notes",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary advantage of the DPMM over methods like K-Means or a GMM is the automatic inference of the number of clusters. Instead of running the model multiple times with different values of K and comparing them, the DPMM explores different numbers of clusters as part of its fitting process. The posterior distribution of the weights w reveals which components are ‚Äúactive‚Äù (have significant weight) and thus gives a probabilistic estimate of the number of clusters supported by the data.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "21. DPMM.html#references",
    "href": "21. DPMM.html#references",
    "title": "Dirichlet Process Mixture Models (DPMM)",
    "section": "Reference(s)",
    "text": "Reference(s)\nGershman and Blei (2012)",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models (DPMM)"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "Linear Regression for a Continuous Variable",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\(\\alpha\\), which represents the origin of the line‚Äîthe expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\(\\beta\\), which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA variance term \\(\\sigma\\), which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#general-principles",
    "href": "1. Linear Regression for continuous variable.html#general-principles",
    "title": "Linear Regression for a Continuous Variable",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\(\\alpha\\), which represents the origin of the line‚Äîthe expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\(\\beta\\), which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA variance term \\(\\sigma\\), which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#considerations",
    "href": "1. Linear Regression for continuous variable.html#considerations",
    "title": "Linear Regression for a Continuous Variable",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nBayesian models consider model parameter uncertainty üõà, allowing for the quantification of confidence or uncertainty through the parameters‚Äô posterior distribution üõà. Therefore, we need to declare prior distributions üõà for each model parameter, in this case for: \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\).\nPrior distributions are built following these considerations:\n\nAs the data is scaled üõà (see introduction), we can use a Normal distribution for \\(\\alpha\\) and \\(\\beta\\), with a mean of 0 and a standard deviation of 1.\nSince \\(\\sigma\\) is strictly positive, we can use any distribution that is positively defined, such as the Exponential or Uniform distribution.\n\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a link function üõà (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "Linear Regression for a Continuous Variable",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.lognormal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# Filter data frame\nm$df = m$df[m$df$age &gt; 18,]\n\n# Scale\nm$scale(list('weight')) \n\n# Convert data to JAX arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Parameter prior distributions\n  s = bi.dist.uniform(0, 50, name = 's')\n  a = bi.dist.normal(178, 20,  name = 'a')\n  b = bi.dist.normal(0, 1, name = 'b')\n  \n  # Likelihood\n  m$normal(a + b * weight, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "Linear Regression for a Continuous Variable",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist Formulation\nThe following equation allows us to draw a line: \\[\nY_i = \\alpha + \\beta  X_i + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma_i\\) is the error term for observation i.\n\n\n\nBayesian Formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this regression model using the following model:\n\\[\nY_i \\sim Normal(\\alpha + \\beta   X_i, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0, 1)\n\\]\n\\[\n\\beta \\sim Normal(0, 1)\n\\]\n\\[\n\\sigma \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) and \\(\\beta\\) are the intercept and regression coefficient, respectively.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma\\) is the standard deviation of the normal distribution, which describes the variance in the relationship between the dependent variable \\(Y\\) and the independent variable \\(X\\).",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#notes",
    "href": "1. Linear Regression for continuous variable.html#notes",
    "title": "Linear Regression for a Continuous Variable",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nWe can observe a difference between the Frequentist and the Bayesian formulation regarding the error term. Indeed, in the Frequentist formulation, the error term \\(\\sigma_i\\) represents random fluctuations around the predicted values. This assumption leads to point estimates for \\(\\alpha\\) and \\(\\beta\\), without accounting for uncertainty in these estimates. In contrast, the Bayesian formulation treats \\(\\sigma\\) as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "Linear Regression for a Continuous Variable",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Linear Regression for a Continuous Variable"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html",
    "href": "15. Gaussian processes.html",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. Basically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a kernel function üõà.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#general-principles",
    "href": "15. Gaussian processes.html#general-principles",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. Basically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a kernel function üõà.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#considerations",
    "href": "15. Gaussian processes.html#considerations",
    "title": "Gaussian Processes",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nTo capture complex, non-linear relationships in data where the underlying function is smooth but has an unknown functional form, GPs use a kernel üõà.\nGPs assume normally distributed errors and may not be appropriate for all types of noise.\nThe choice of kernel hyperparameters can significantly impact results; thus, GPs require choosing an appropriate kernel function that captures the expected behavior of your data.\nThrough kernel definition, we can incorporate domain knowledge.\nThey scale poorly with dataset size (O(n¬≥) complexity) due to matrix operations; thus, memory requirements can be substantial for large datasets, which has led to neural networks being used instead to resolve large non-linear problems.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#example",
    "href": "15. Gaussian processes.html#example",
    "title": "Gaussian Processes",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Gaussian Process regression using the Bayesian Inference (BI) package. Data consist of a continuous dependent variable (total_tools), representing the number of tools invented in the islands, and a continuous independent variable (population), representing the population of the islands. The goal is to estimate the effect of population on the total tools. We use the distance matrix of the islands for the kernel function in order to capture the spatial dependence of the relationship.\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport numpyro\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline2.csv'\nm.data(data_path, sep=';') \n\ndata_path2 = files('BI.resources.data') / 'Kline2.csv'\nislandsDistMatrix = pd.read_csv(data_path2, index_col=0)\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    # non-centered Gaussian Process prior\n    etasq = m.dist.exponential(2, name = 'etasq')\n    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n    SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01)\n    k = m.dist.multivariatenormal(0, SIGMA, name = 'k')\n    #k = m.gaussian.gaussian_process(Dmat, etasq, rhosq, 0.01, shape = (10,))\n    k = m.gaussian.kernel_L2(Dmat, etasq, rhosq, 0.01)\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n\n\nlibrary(BI)\npd=import('pandas')\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline2.csv\", sep = ''), sep=';')\nislandsDistMatrix = pd$read_csv(paste(system.file(package = \"BI\"),\"/data/islandsDistMatrix.csv\", sep = ''), index_col=as.integer(0))\nm$data_to_model(list('total_tools', 'population'))\nm$data_on_model$society = jnp$arange(0,10, dtype='int64')\nm$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(Dmat, population, society, total_tools){\n  a = bi.dist.exponential(1, name = 'a')\n  b = bi.dist.exponential(1, name = 'b')\n  g = bi.dist.exponential(1, name = 'g')\n  \n  # non-centered Gaussian Process prior\n  etasq = bi.dist.exponential(2, name = 'etasq')\n  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n  z = bi.dist.normal(0,1, name = 'z', shape = c(10))\n  r = m$kernel_sq_exp(Dmat, z, etasq, rhosq, 0.01)\n  SIGMA = r[[1]]\n  L_SIGMA = r[[2]]\n  k = r[[3]]\n  lambda_ = a * population**b / g * jnp$exp(k[society])\n  m$poisson(lambda_, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#mathematical-details",
    "href": "15. Gaussian processes.html#mathematical-details",
    "title": "Gaussian Processes",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormula\nThe following equation allows us to evaluate the relationship between the dependent variable \\(Y\\) and the independent variable \\(X\\) while incorporating a GP for variable \\(Z\\):\n\\[\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\\]\nwhere: - \\(Y_i\\) is the i-th value for the dependent variable \\(Y\\).\n\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient term.\n\\(X_i\\) is the i-th value for the independent variable \\(X\\).\n\\(\\gamma_{Z_i}\\) is the Gaussian process i-th value for the independent variable \\(Z\\).\n\nThe GP \\(\\gamma_{Z_i}\\) follows a multivariate normal distribution:\n\\[\n\\begin{pmatrix}\n    Z_1 \\\\\n    \\vdots \\\\\n    Z_{n}\n\\end{pmatrix}\n\\sim MVNormal \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\\]\nwhere:\n\n\\((Z_1, ..., Z_n)\\) represents a collection of all values of the random variable \\(Z\\).\n\\((0, ..., 0)\\) represents the mean vector of the multivariate normal distribution of the same size as the number of random variables and set to zero üõà.\n\\(K\\) is the covariance matrix of the random variable \\(Z\\). Each element \\(K_{ij}\\) of the matrix is given by the kernel function evaluated at the corresponding points: \\(K_{ij} = k(Z_i, Z_j)\\)\n\n\\[\nK = \\begin{pmatrix}\n    k(Z_1, Z_1) & k(Z_1, Z_2) & \\cdots & k(Z_1, Z_{n}) \\\\\n    k(Z_2, Z_1) & k(Z_2, Z_2) & \\cdots & k(Z_2, Z_{n}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    k(Z_{n}, Z_1) & k(Z_{n}, Z_2) & \\cdots & k(Z_{n}, Z_{n})\n\\end{pmatrix}\n\\]\n\nMultiple kernel functions exist and will be discussed in the Note(s) section. But the most common one is the quadratic kernel:\n\n\\[\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\\]\nWhere:\n\n\\(\\eta\\) is the signal variance, representing the overall variance of the outputs of the Gaussian process. It scales the influence of the kernel function. A larger \\(\\eta^2\\) indicates a wider range of values the function can take.\n\\(p\\) determines the rate of decline.\n\\(D_{ij}\\) is the distance between the \\(i\\)-th and \\(j\\)-th points.\n\\(\\delta_{ij}\\) is the Kronecker delta, taking a value of one when \\(i = j\\) and zero otherwise, allowing the self-covariance to be included in the calculation.\n\\(\\sigma^2\\) is the noise variance, which accounts for the observation noise in the data. It represents the uncertainty or variability in the measurements or outputs at each point. The term effectively adds this noise variance only when \\(i = j\\), ensuring that the diagonal elements of the covariance matrix represent the total variance at each input point.\n\n\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this GP using the following model:\n\\[\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\\]\n\\[\n\\gamma \\sim MVNormal \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\\]\n\\[\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\eta^2 \\sim HalfCauchy(0,1)\n\\]\n\\[\np^2 \\sim HalfCauchy(0,1)\n\\]\nwhere:\n\n\\(Y_i\\) is the i-th value for the dependent variable \\(Y\\).\n\\(\\alpha\\) is the intercept term with a prior of \\(Normal(0,1)\\).\n\\(\\beta\\) is the regression coefficient term with a prior of \\(Normal(0,1)\\).\n\\(X_i\\) is the i-th value for the independent variable \\(X\\).\n\\(\\gamma_{Z_i}\\) is the Gaussian process i-th value for the independent variable \\(Z\\).\n\\(\\gamma\\) is the latent function modeled by the GP.\n\\(K_{ij}\\) is the kernel function evaluated at the corresponding points, \\(K_{ij} = k(Z_i, Z_j)\\), with priors of HalfCauchy(0,1) for \\(\\eta^2\\) and \\(p^2\\) to ensure positive values.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#notes",
    "href": "15. Gaussian processes.html#notes",
    "title": "Gaussian Processes",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nCommon kernel functions include:\n\nRadial Basis Function (RBF) or Squared Exponential Kernel: \\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\\]\nRational Quadratic Kernel, this kernel is equivalent to adding together many RBF kernels with different length scales: \\[k(x,x') = \\sigma^2 \\left(1 + \\frac{||x-x'||^2}{2l^2}\\right)^{-\\alpha}\\]\nPeriodic kernel allows for modeling functions that repeat themselves exactly: \\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)\\]\nLocally Periodic Kernel:\n\n\\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right) \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\\]\n\nGPs can be extended to classification problems using link functions.\nMulti-output problems can be addressed using matrix-valued kernels.\nDeep learning can be combined with GPs through Deep Kernel Learning.\nComputational tricks for large datasets include:\n\nSparse approximations (e.g., FITC, VFE)\nInducing points methods\nRandom Fourier features",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#references",
    "href": "15. Gaussian processes.html#references",
    "title": "Gaussian Processes",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html",
    "href": "26. Network Based Diffusion analysis (wip).html",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links. The basic model underlying NBDA states that at time \\(t\\) an individual, \\(i\\), learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time \\(t-1\\)).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "href": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links. The basic model underlying NBDA states that at time \\(t\\) an individual, \\(i\\), learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time \\(t-1\\)).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#considerations",
    "href": "26. Network Based Diffusion analysis (wip).html#considerations",
    "title": "Network-Based Diffusion Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThere are two main NBDA variants: order-of-acquisition diffusion analysis (OADA), which takes as data the order in which individuals acquired the target behaviour, and time-of-acquisition diffusion analysis (TADA), which uses the times of acquisition of the target behaviour.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#example",
    "href": "26. Network Based Diffusion analysis (wip).html#example",
    "title": "Network-Based Diffusion Analysis",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Multiplex network model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "href": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "title": "Network-Based Diffusion Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormulation\nThere are two parameters of interest in the basic time of acquisition diffusion analysis model: the rate of social transmission be-tween individuals per unit of network connection,s, and the baseline rate of trait performance in the absence of social transmission, \\(Œª_0\\).\n\\[\n\\lambda_i(t) = \\lambda_0(t) (1- z_i(t))  \\left[ s \\sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \\right]\n\\]\nWhere:\n\n\\(\\lambda_i(t)\\) is the rate at which individuals i acquire the task solution at time t.\n\\(\\lambda_0(t)\\) is a baseline acquisition function determining the distribution of latencies to acquisition in the absence of social transmission (that is, through asocial learning). It can be specify by an exponential or Weibull distrbution.\n\\(z_i(t)\\) gives the status (1 = informed, 0 = na√Øve) of individual i at time t.\n\\(s\\) is the regression coefficients capturing the effect of \\(x\\) on the hazard have an assigned a normal prior.\n\\((1- z_i(t))\\) and \\(z_j (-1)\\) terms ensure that the task solution is only transmitted from informed to uninformed individuals:\n\n\\[\nz_j(t) =  Y_i \\sim \\begin{cases}\n0, & \\text{if j is naive} \\\\\n1, & \\text{if j is informed}\n\\end{cases}\n\\]",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#notes",
    "href": "26. Network Based Diffusion analysis (wip).html#notes",
    "title": "Network-Based Diffusion Analysis",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#references",
    "href": "26. Network Based Diffusion analysis (wip).html#references",
    "title": "Network-Based Diffusion Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\nhttps://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13307\n\n(PDF) Quantifying diffusion in social networks: a Bayesian approach. Available from: https://www.researchgate.net/publication/270048687_Quantifying_diffusion_in_social_networks_a_Bayesian_approach [accessed Oct 24 2024].",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "Bayesian analysis with BI",
    "section": "",
    "text": "This document is a guide to Bayesian analysis and the implementation of Bayesian inference (BI). It is intended for users ranging from those with little or no experience to advanced practitioners. In this introduction, we outline the main steps of Bayesian analysis. Subsequent chapters present increasingly complex models. Each following chapter will have the same structure in order to allow users to easily find the information they are looking for. The structure is as follows:\nWe recommend reading the introduction first since some key concepts here will not be revisited in later chapters.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#modeling-likelihood",
    "href": "0. Introduction.html#modeling-likelihood",
    "title": "Bayesian analysis with BI",
    "section": "Modeling Likelihood",
    "text": "Modeling Likelihood\nOnce the likelihood is defined, we can now define the mathematical equations that describe our parameters (\\(\\mu\\) and \\(\\sigma\\)) and their relationship with the dependent variable \\(y\\). We can express this relationship in the form of a linear function:\n\\[\n\\mu = \\alpha + \\beta x\n\\]\nWhere \\(\\alpha\\) is the intercept üõà and \\(\\beta\\) is the slope üõà of the line. These parameters are the unknowns that we want to estimate to evaluate the strength and direction of the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\).",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions",
    "href": "0. Introduction.html#link-functions",
    "title": "Bayesian analysis with BI",
    "section": "Link functions",
    "text": "Link functions\nDepending on the type of problem you are trying to solve (classification, regression, etc.) and the type of data you are working with (continuous, discrete, binomial, etc.), you will need to choose the appropriate distribution to describe the relationship between the data. By using a different distribution, you will need to use a different link function üõà.\nFor the moment, we just need to know that these different distributions require a link function (for each specific family we will discuss the corresponding link function); however, below is a table summarizing some of the most common link functions, the mathematical form of each, their typical applications, and how to interpret them. Link functions in BI can be accessed through the class BI.link.XXX where XXX is the name of the link function.\n\n\n\n\n\n\n\n\n\nLink Function\nMathematical Form\nTypical Use / Model\nInterpretation & Range\n\n\n\n\nIdentity\n\\(g(\\mu) = \\mu\\)\nLinear regression (Normal)\nDirectly models \\(\\mu\\); \\(\\mu\\) can be any real number.\n\n\nLogit\n\\(g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\nLogistic regression (Binomial)\nModels probabilities (0, 1); coefficients reflect log-odds.\n\n\nProbit\n\\(g(\\mu) = \\Phi^{-1}(\\mu)\\)\nProbit regression (Binomial)\nSimilar to logit; uses the inverse standard normal CDF.\n\n\nLog\n\\(g(\\mu) = \\log(\\mu)\\)\nPoisson, Gamma regression (Count data)\nEnsures \\(\\mu &gt; 0\\); coefficients represent multiplicative effects.\n\n\nInverse\n\\(g(\\mu) = \\frac{1}{\\mu}\\)\nGamma regression\nModels positive \\(\\mu\\); relates changes inversely to \\(\\mu\\).",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#the-prior-distributions",
    "href": "0. Introduction.html#the-prior-distributions",
    "title": "Bayesian analysis with BI",
    "section": "The Prior Distributions",
    "text": "The Prior Distributions\nFor each parameter of our equation that describes \\(\\mu\\), we need to define a prior distribution üõà that encodes our initial beliefs about the parameter. In the case of the linear regression model, we need to specify prior distributions for the intercept \\(\\alpha\\), the slope \\(\\beta\\), and standard deviation \\(\\sigma\\).\n\\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nAnd with this, we can write our entire model as:\n\\[\ny \\sim \\text{Normal}(\\mu, \\sigma)\n\\] \\[\n\\mu = \\alpha + \\beta x\n\\] \\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nIn BI, you will need to define this model within a function in which you will be able to use any probability distributions, link functions, and mathematical operations required for your model. BI has been designed to allow you to declare your model as close as possible to the mathematical notation. For example, the model above can be written in BI as:\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\ndat = dict(\n    x = m.dist.normal(178, 20, sample = True),   \n    y = m.dist.lognormal( 0, 1, sample = True)   \n)\nm.data_on_model = dat\n\ndef model(x, y):    \n    alpha = m.dist.normal( 0, 1, name = 'alpha', shape= (1,))\n    beta = m.dist.normal( 0, 1, name = 'beta', shape= (1,))   \n    sigma = m.dist.uniform( 0, 50, name = 'sigma', shape = (1,))\n    m.normal(alpha + beta * x, sigma, obs=y)\nThe code snippet provides several key features of the BI package:\n\nFirst, you need to initialize a bi object.\nThen, you can store data as a JAX array dictionary using the m.data_on_model function. If all the data can be stored in a data frame (e.g., Linear Regression for continuous variable), you do not need to use m.data_on_model, as the BI object automatically detects the data provided in the model arguments. However, sometimes you may need different data structures such as vectors and 2D arrays (e.g., Network model).\nRegarding distribution parameters, note the difference depending on whether you are generating data outside a function (e.g., for simulation purposes) or specifying priors inside a model function. In the former case, the argument sample should be set to True. However, if you are specifying priors within a model function, this argument must be set to False.\nFinally, note that each parameter declared in the model must have a unique name as well as a shape. The shape refers to the number of parameters you want to estimate. For example, if you want to estimate a different \\(\\beta\\) for each independent variable, you would declare \\(\\beta\\) with a shape equal to the number of independent variables. By default, the shape is one, so technically you don‚Äôt need to specify it. In this example, we highlight this feature explicitly.\n\n\nWhich prior distribution range to use?\nThe choice of prior ranges can significantly affect Bayesian analysis results. There are several approaches to selecting them:\n\nExpert Knowledge: The prior distributions can be based on expert knowledge or historical data. This approach is useful when there is a lot of information available about the parameters.\nNoninformative Priors: When there is little or no information about the parameters, noninformative priors can be used. These priors are designed to have minimal influence on the posterior distribution, allowing the data to dominate the inference process.\nScaled data: If the data is scaled üõà, the prior distributions can be chosen to reflect this. For example, if the data are scaled, the prior distributions for the intercept and slope can be centered around 0 and 1, respectively. By scaling the independent variable, we obtain a unit of change based on variance; that is, the effect represents a one‚Äìstandard‚Äìdeviation change in \\(x\\) on \\(y\\). Scaling the data improves both numerical stability and interpretability. When all data are scaled to the same range, it leads to more stable numerical behavior during estimation. Additionally, it facilitates setting priors that are both meaningful and relatively uninformative. By aligning the scale of the data with the scale assumed in the priors, we ensure that the posterior distributions exhibit reasonable spread and that our uncertainty quantification is consistent with the data‚Äôs scale. For the remainder of this document, we will assume that the data are scaled.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fit-and-posterior-distribution",
    "href": "0. Introduction.html#model-fit-and-posterior-distribution",
    "title": "Bayesian analysis with BI",
    "section": "Model fit and posterior distribution",
    "text": "Model fit and posterior distribution\nOnce data are observed, Bayes‚Äô Theorem üõà is used to evaluate how well a given set of parameter values fits the data:\n\\[\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\\]\nWhere:\n\n\\(\\theta\\) represents the unknown parameters we are interested in.\n\\(P(\\theta)\\) is the prior distribution, representing our beliefs about \\(\\theta\\) before seeing the data.\n\\(P(\\text{data} \\mid \\theta)\\) is the likelihood, representing the model of how the data are generated given \\(\\theta\\).\n\\(P(\\theta \\mid \\text{data})\\) is the posterior distribution, representing our updated beliefs after observing the data. It tells us not only the most likely value of \\(\\theta\\) (e.g., \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) in our case) but also quantifies the uncertainty in these estimates.\n\nWe can use Bayesian updating üõà using the Bayesian theorem to ‚Äòreshape‚Äô the prior distributions by considering every possible combination of values for our parameters, and scoring each combination by its relative plausibility in light of the data. These relative plausibilities are the posterior probabilities of each combination of our parameters: the posterior distributions. Various techniques can be used to approximate the mathematical definition of Bayes‚Äô theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Descriptions of these algorithms are out of the scope of this document. For more information, please refer to the Bayesian Inference. In BI, we use MCMC and it can be called as m.fit(model) where model is the function that describes the model.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "Bayesian analysis with BI",
    "section": "Model ‚Äòdiagnostic‚Äô",
    "text": "Model ‚Äòdiagnostic‚Äô\nOnce a Bayesian model has been fit, it is crucial to evaluate how well it captures the observed data and to assess whether the Markov chain Monte Carlo (MCMC) sampling has converged. Bayesian model diagnostics help us answer questions like: ‚ÄúAre our uncertainty estimates reliable?‚Äù, ‚ÄúDoes the model generate data similar to what we observed?‚Äù, and ‚ÄúHave the chains mixed well?‚Äù Multiple diagnostics approaches can be used to assess the model‚Äôs performance. Below are some key diagnostic tools and techniques available in BI within the class BI.diag.XXX where XXX is the name of the diagnostic tool.\n\n\n\n\n\n\n\n\n\nDiagnostic Tool\nPurpose\nKey Indicator\nInterpretation\n\n\n\n\nposterior predictive checks (PPCs) üõà\nAssess if the model can reproduce observed data\nGraphs, p-values, summary stats\nGood fit if simulated data resemble observed data\n\n\nCredible Interval (CI)\nQuantify uncertainty in parameter estimates\n95% CI or other percentage\n95% probability the parameter lies within the interval\n\n\nhighest posterior density intervals (HPDI) üõà\nIdentify the narrowest interval containing a given probability mass\n95% HPDI\nSmallest interval capturing 95% of the posterior density\n\n\neffective sample size (ESS) üõà\nMeasure independent information in the chain\nESS value (ideally high)\nLow ESS indicates high autocorrelation (poor mixing)\n\n\npotential scale reduction factor (Rhat) üõà\nCheck convergence across multiple chains\nRhat ‚âà 1 (typically &lt;1.1)\nValues near 1 indicate convergence; &gt;&gt;1 suggests non-convergence\n\n\nTrace plots üõà\nVisualize the sampling path to check convergence and mixing\nPlot showing parameter values over iterations\nStationary, ‚Äòhairy caterpillar‚Äô pattern suggests convergence\n\n\nautocorrelation plots üõà\nAssess dependency between samples over lags\nAutocorrelation values across lags\nRapid decay to zero suggests good mixing; slow decay indicates poor mixing\n\n\ndensity plots üõà\nVisualize the posterior distribution of a parameter\nSmoothness and shape of the curve\nUnimodal and smooth suggests convergence; multimodal or irregular may suggest poor mixing",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-comparison",
    "href": "0. Introduction.html#model-comparison",
    "title": "Bayesian analysis with BI",
    "section": "Model comparison",
    "text": "Model comparison\nModel comparison is performed by evaluating how well different models explain the observed data while accounting for model complexity. Multiple criteria can be used to compare models, and are summarized in the table below. In BI, we can compare models using Watanabe-Akaike Information Criterion (WAIC) with the function m.diag.waic(model1, model2).\n\n\n\n\n\n\n\n\n\n\nCriterion\nPurpose\nInterpretation\nStrengths\nWeaknesses\n\n\n\n\nDIC (Deviance Information Criterion)\nMeasures model fit while penalizing complexity\nLower values indicate better model fit\nSimple to compute, useful for hierarchical models\nSensitive to the number of parameters, not always reliable in complex models\n\n\nWAIC (Watanabe-Akaike Information Criterion)\nEstimates out-of-sample predictive accuracy while penalizing complexity\nLower values indicate better models\nMore robust than DIC, accounts for overfitting\nComputationally intensive for large models\n\n\nBayes Factor (BF)\nQuantifies relative support for two models based on marginal likelihoods\nBF &gt; 1 favors the numerator model, BF &lt; 1 favors the denominator\nProvides direct evidence comparison, works with different model types\nSensitive to prior choices, requires good model specification",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html",
    "href": "6. Beta binomial model.html",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#general-principles",
    "href": "6. Beta binomial model.html#general-principles",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#considerations",
    "href": "6. Beta binomial model.html#considerations",
    "title": "Beta-Binomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Binomial regression.\nA Beta-Binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.\nA Beta distribution has two parameters: the rates for each probability and a shape parameter Œ∏. Œ∏ influences how probabilities are distributed between 0 and 1. Specifically, it consists of two parameters, \\(\\alpha\\) and \\(\\beta\\), which determine the concentration of probability around 0 and 1.\n\nIf both are equal to or greater than 1, the distribution is bell-shaped and centered around 0.5.\nIf \\(\\alpha &gt; \\beta\\), the distribution is skewed toward 1, and if \\(\\beta &gt; \\alpha\\), it is skewed toward 0. Thus, the shape parameters \\(\\gamma\\) and \\(\\eta\\) provide flexibility in modeling various types of prior beliefs about probabilities.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#example",
    "href": "6. Beta binomial model.html#example",
    "title": "Beta-Binomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression using the Bayesian Inference (BI) package. The data consist of:\n\nOne binary dependent variable (admit), which represents candidates‚Äô admission status.\nOne independent categorical variable representing individuals‚Äô gender (gid).\nAdditionally, we have the number of applications (applications) per gender, which will be used to account for independent rates.\n\nThe goal is to evaluate whether the probability of admission is different between genders, while accounting for differences in the number of applications between genders.\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'UCBadmit.csv'\nm.data(data_path, sep=';') \nm.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n\n# Define model ------------------------------------------------\ndef model(gid, applications, admit):\n    phi = m.dist.exponential(1,  name = 'phi')\n    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n    theta =  phi + 2\n    pbar = jax.nn.sigmoid(alpha[gid])\n    concentration1 = pbar*theta\n    concentration0 = (1 - pbar) * theta\n\n    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\nm$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\nm$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(gid, applications, admit){\n  # Parameter prior distributions\n  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n  t = phi + 2\n  pbar = jax$nn$sigmoid(alpha[gid])\n  gamma = pbar * t\n  eta = (1 - pbar) * t\n  # Likelihood\n  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#mathematical-details",
    "href": "6. Beta binomial model.html#mathematical-details",
    "title": "Beta-Binomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim BetaBinomial(n_i, \\gamma_i, \\eta_i)\n\\]\n\\[\n\\gamma_i = \\overline{\\rho}   \\tau\n\\]\n\\[\n\\eta_i = (1 - \\overline{\\rho} ) \\tau\n\\]\n\\[\n\\overline{\\rho} = logit(\\alpha_i)\n\\]\n\\[\n\\tau = \\phi + 2\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\phi \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the count of successes for the i-th observation, which follows a beta-binomial distribution with \\(n_i\\) trials.\n\\(\\gamma_i\\) represents the concentration parameter for the number of successes, derived from the probability of success and scaled by \\(\\tau\\).\n\\(\\eta_i\\) represents the concentration parameter for failures, derived from the probability of failure \\((1 - \\overline{\\rho} )\\) and also scaled by \\(\\tau\\).\n\\(\\overline{\\rho}\\) is the probability of success for the i-th observation. The logit function transforms the linear predictor Œ± (which can take any real value) into a probability value between 0 and 1.\n\\(\\tau\\) is derived from ùúô and is used as a scaling factor for the shape parameters ùõæ and ùúÇ.\nŒ± is a vector of parameters, each representing the effect of group i on the success probability.\nœï is a random variable following an exponential distribution with a rate of 1.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#references",
    "href": "6. Beta binomial model.html#references",
    "title": "Beta-Binomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "20. GMM.html",
    "href": "20. GMM.html",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric üõà clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model‚Äôs goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\(\\mu\\)) and its shape/spread (covariance \\(\\Sigma\\)).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#general-principles",
    "href": "20. GMM.html#general-principles",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric üõà clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model‚Äôs goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\(\\mu\\)) and its shape/spread (covariance \\(\\Sigma\\)).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#considerations",
    "href": "20. GMM.html#considerations",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA GMM is a Bayesian model üõà that considers uncertainty in all its parameters, except for the number of clusters, K, which must be fixed in advance.\nThe key parameters and their priors are:\n\nNumber of Clusters K: This is a fixed hyperparameter that you must choose before running the model. Choosing the right K often involves running the model multiple times and using model comparison criteria (like cross-validation, AIC, or BIC).\nCluster Weights w: These are the probabilities of drawing a data point from any given cluster. Since there are a fixed number K of them and they must sum to 1, they are typically given a Dirichlet prior. A symmetric Dirichlet prior (e.g., Dirichlet(1, 1, ..., 1)) represents an initial belief that all clusters are equally likely.\nCluster Parameters (\\(\\boldsymbol{\\mu}\\), \\(\\Sigma\\)): Each of the K clusters has a mean \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\). We place priors on these to define our beliefs about their plausible values.\n\nLike the DPMM, the model is often implemented in its marginalized form üõà. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\nTo increase accuracy we run a k-means algorithm to initialize the cluster mean priors.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#example",
    "href": "20. GMM.html#example",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "Example",
    "text": "Example\nBelow is an example of a GMM implemented in BI. The goal is to cluster a synthetic dataset into a pre-specified K=4 groups.\n\nPythonR\n\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n\n#  The model\ndef gmm(data, K, initial_means): # Here K is the *exact* number of clusters\n    D = data.shape[1]  # Number of features\n    alpha_prior = 0.5 * jnp.ones(K)\n    w = dist.dirichlet(concentration=alpha_prior, name='weights') \n\n    with dist.plate(\"components\", K): # Use fixed K\n        mu = dist.multivariatenormal(loc=initial_means, covariance_matrix=0.1*jnp.eye(D), name='mu')        \n        sigma = dist.halfcauchy(1, shape=(D,), event=1, name='sigma')\n        Lcorr = dist.lkjcholesky(dimension=D, concentration=1.0, name='Lcorr')\n\n        scale_tril = sigma[..., None] * Lcorr\n\n    dist.mixturesamefamily(\n        mixing_distribution=dist.categorical(probs=w, create_obj=True),\n        component_distribution=dist.multivariatenormal(loc=mu, scale_tril=scale_tril, create_obj=True),\n        name=\"obs\",\n        obs=data\n    )\n\nm.data_on_model = {\"data\": data,\"K\": 4 }\nm.fit(gmm) # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#mathematical-details",
    "href": "20. GMM.html#mathematical-details",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThis section describes the generative process for a GMM. For each data point \\(x_i\\), the model first selects one of the K clusters according to the weights \\(w\\), and then draws the point from that cluster‚Äôs Gaussian distribution.\n\\[\\begin{align*}\nz_i &\\sim \\text{Categorical}(w) && \\text{for } i=1, \\dots, N \\\\\nx_i \\mid z_i=k &\\sim \\text{MultivariateNormal}(\\mu_k, \\Sigma_k) && \\text{for } i=1, \\dots, N\n\\end{align*}\\] \\[\nw \\sim \\text{Dirichlet}(\\alpha_0) \\quad \\text{(Mixture weights vector for K clusters)}\n\\]\n\\[\\begin{align*}\n\\mu_k &\\sim \\text{MultivariateNormal}(\\mu_0, \\Sigma_0) && \\text{for } k=1, \\dots, K \\\\\n\\sigma_k &\\sim \\text{HalfCauchy}(1) && \\text{for } k=1, \\dots, K \\\\\nL_{\\text{corr}, k} &\\sim \\text{LKJCholesky}(D, 1.0) && \\text{for } k=1, \\dots, K \\\\\n\\Sigma_k &= \\text{diag}(\\sigma_k) \\cdot L_{\\text{corr}, k} \\cdot L_{\\text{corr}, k}^T \\cdot \\text{diag}(\\sigma_k)\n\\end{align*}\\]\nParameter Definitions: * Observed Data: * \\(x_i\\): The \\(i\\)-th observed D-dimensional data point.\n\nLatent Variables (Inferred):\n\n\\(z_i\\): The integer cluster assignment for the \\(i\\)-th data point.\n\\(w\\): The K-dimensional vector of mixture weights.\n\\(\\mu_k\\): The D-dimensional mean vector of the \\(k\\)-th cluster.\n\\(\\Sigma_k\\): The DxD covariance matrix of the \\(k\\)-th cluster (composed of \\(\\sigma_k\\) and \\(L_{\\text{corr},k}\\)).\n\nHyperparameters (Fixed):\n\n\\(K\\): The total number of clusters.\n\\(\\alpha_0\\): The concentration parameter vector for the Dirichlet prior on weights (e.g., [1, 1, ..., 1]).\n\\(\\mu_0\\): The prior mean for the cluster centers.\n\\(\\Sigma_0\\): The prior covariance for the cluster centers.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#notes",
    "href": "20. GMM.html#notes",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary challenge of the GMM compared to the DPMM is the need to manually specify the number of clusters K. If the chosen K is too small, the model may merge distinct clusters. If K is too large, it may split natural clusters into meaningless sub-groups. Therefore, applying a GMM often involves an outer loop of model selection where one fits the model for a range of K values and uses a scoring metric to select the best one.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "20. GMM.html#references",
    "href": "20. GMM.html#references",
    "title": "Gaussian Mixture Models (GMM)",
    "section": "Reference(s)",
    "text": "Reference(s)\nC. M. Bishop (2006). Pattern Recognition and Machine Learning. Springer. (Chapter 9)",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models (GMM)"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html",
    "href": "24. Network control for data collection biases (wip).html",
    "title": "Controlling for Network Biases",
    "section": "",
    "text": "Data collection biases are a persistent issue in studies of social networks. Two main types of biases can be considered: exposure biases üõà and censoring biases üõà.\nTo account for exposure biases, we can switch the network link probability model from a Poisson distribution to a Binomial distribution, as the binomial distribution allows us to account for the number of trials for each data estimation.\nTo address censoring biases, we need to add an additional equation to account for the probability of missing an interaction during observation when modeling the interaction between individuals i and j.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#considerations",
    "href": "24. Network control for data collection biases (wip).html#considerations",
    "title": "Controlling for Network Biases",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-1",
    "href": "24. Network control for data collection biases (wip).html#example-1",
    "title": "Controlling for Network Biases",
    "section": "Example 1",
    "text": "Example 1\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases:\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure_mat,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.binomial(total_count = m.net.mat_to_edgl(exposure_mat), logits = jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-2",
    "href": "24. Network control for data collection biases (wip).html#example-2",
    "title": "Controlling for Network Biases",
    "section": "Example 2",
    "text": "Example 2\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases and censoring biases:",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#mathematical-details",
    "href": "24. Network control for data collection biases (wip).html#mathematical-details",
    "title": "Controlling for Network Biases",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\n\\[\nY_{[i,j]} \\sim \\text{Binomial}\\Big(E_{[i,j]}, Q_{[i,j]}  \\Big)\n\\]\n\\[\nQ_{[i,j]} = \\phi_{[i,j]}\\eta_{[i]}\\eta_{[j]}\n\\]\nWhere:\n\n\\(E_{[i,j]}\\) is the number of trials for each observation (i.e., the sampling effort).\n\\(Q_{[i,j]}\\) is the indicator of a true tie between \\(i\\) and \\(j\\), defined as: \\[\nQ_{[i,j]} \\sim \\begin{cases}\n0 & \\text{if no interaction occurs or if } i \\text{ or } j \\text{ is not detectable} \\\\\n1 & \\text{if } i \\text{ and } j \\text{ are both detectable}\n\\end{cases}\n\\]\n\\(\\phi_{[i,j]}\\) is the probability of a true tie between \\(i\\) and \\(j\\).\n\\(\\eta_{[i]}\\) is the probability of individual \\(i\\) being detectable.\n\\(\\eta_{[j]}\\) is the probability of individual \\(j\\) being detectable.\n\n\n\nDefining formula sub-equations and prior distributions\nWe can let \\(\\eta_{[i]}\\) depend on individual-specific covariates. To model the probability of censoring, we can model \\(1-\\eta_{[i]}\\): \\[\n\\text{logit}(1-\\eta_{[i]}) = \\mu_\\psi + \\hat\\psi_{[i]}  \\sigma_\\psi + \\dots\n\\]\nWhere:\n\n\\(\\mu_\\psi\\) is the intercept.\n\\(\\sigma_\\psi\\) is a scalar for the variance of random effects.\n\\(\\hat\\psi_{[i]}\\sim \\text{Normal}(0,1)\\), and the ellipsis signifies any linear model of coefficients and individual-level covariates. For example, if \\(C\\) is an animal-specific measure, like a binary variable for cryptic coloration, then the ellipsis may be replaced with \\(\\kappa_{[5]}C_{[i]}\\) to give the effects of coloration on censoring probability.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#notes",
    "href": "24. Network control for data collection biases (wip).html#notes",
    "title": "Controlling for Network Biases",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nOne major limitation of this model is the necessity of having an estimation of the censoring bias for each individual.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "9. Multinomial model.html",
    "href": "9. Multinomial model.html",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables, we can use a Multinomial model.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#general-principles",
    "href": "9. Multinomial model.html#general-principles",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables, we can use a Multinomial model.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#considerations",
    "href": "9. Multinomial model.html#considerations",
    "title": "Multinomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nOne way to interpret a multinomial model is to consider that we need to build \\(K - 1\\) linear models, where \\(K\\) is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a simplex üõà. To do this, we convert the regression outputs using the softmax function üõà (see the ‚Äújax.nn.softmax‚Äù line in the code).\nThe intercept \\(\\alpha\\) captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\nOn the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients \\(\\beta\\) are shared across categories.\nThe relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#example",
    "href": "9. Multinomial model.html#example",
    "title": "Multinomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package:\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(career, income):\n    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n    b = m.dist.halfnormal(0.5, shape=(1,), name = 'b')\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0] #pivot\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    m.dist.categorical(probs=p, obs=career)\n\n# Run sampler ------------------------------------------------ \nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multinomial.csv\", sep = ''), sep=',')\nkeys &lt;- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues &lt;- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.halfnormal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n\n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n\n  # Likelihood\n  m$categorical(probs=p[career], obs=career)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#mathematical-details",
    "href": "9. Multinomial model.html#mathematical-details",
    "title": "Multinomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the predictor variables (X1, X2, ‚Ä¶, Xn) and the categorical outcome variable (\\(Y_i\\)) using the following equation:\n\\[\nlogit(p_{ik}) = Œ±_k + Œ≤ X_i\n\\]\nWhere:\n\n\\(p_{ik}\\) is the probability of the ùëñ-th observation being in category ùëò.\n\\(Œ±_k\\) is the intercept for category ùëò.\n\\(Œ≤\\) is the regression coefficients common to all categories.\n\\(X_i\\) is the vector of independent variables for the ùëñ-th observation.\nA reference category is often chosen to simplify the model.\n\n\n\nBayesian model\nIn Bayesian multinomial modeling, the likelihood function of the data is specified using a multinomial distribution. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable ùë¶ with ùêæ categories, the multinomial likelihood function is: \\[\nMultinomial(y|Œ∏)=\\frac{N!}{\\prod^K_{k=1}y_k!} \\prod_{k=1}^{K} Œ∏_{k}^{y_k}\n\\]\nWhere:\n\n\\(y=(y_1, y_2,‚Ä¶,y_K)\\) represents the counts of observations in each of the ùêæ categories.\n\\(N\\) is the total number of observations or trials.\n\\(Œ∏=(Œ∏_1,Œ∏_2,‚Ä¶,Œ∏_K)\\) is a simplex of category probabilities, with \\(Œ∏_k\\) representing the probability of category ùëò.\n\\(\\frac{N!}{\\prod^K_{k=1}y_k!}\\) is the multinomial coefficient that accounts for the number of ways to arrange the observations into the categories. This coefficient ensures that the likelihood function properly accounts for the permutations of the counts across different categories.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#references",
    "href": "9. Multinomial model.html#references",
    "title": "Multinomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "11. Zero inflated.html",
    "href": "11. Zero inflated.html",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#general-principles",
    "href": "11. Zero inflated.html#general-principles",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#considerations",
    "href": "11. Zero inflated.html#considerations",
    "title": "Zero-Inflated Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nIn Bayesian Zero-Inflated regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for \\(W_{1\\pi}, W_{2\\pi}, ..., W_{n\\pi}\\), \\(W_{1\\lambda}, W_{2\\lambda}, ..., W_{n\\lambda}\\), \\(b_\\pi\\), and \\(b_\\lambda\\).",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#example",
    "href": "11. Zero inflated.html#example",
    "title": "Zero-Inflated Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Zero-Inflated Poisson regression using the Bayesian Inference (BI) package. The data represent the production of books in a monastery (y), which is affected by the number of days that individuals work, as well as the number of days individuals drink.\n\nPythonR\n\n\nfrom BI import bi\nfrom jax.scipy.special import expit\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Simulated data------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n\n# Sample one year of production\nN = 365\ndrink = m.dist.binomial(1, prob_drink, shape = (N,), sample = True)\ny = (1 - drink) * m.dist.poisson(rate_work, shape = (N,), sample = True)\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.data_on_model = dict(\n    y=jnp.array(y)\n)\n\n# Define model ------------------------------------------------\ndef model(y):\n    al = dist.normal(1, 0.5, name='al')\n    ap = dist.normal(-1.5, 1, name='ap')\n    p = expit(ap)\n    lambda_ = jnp.exp(al)\n    m.zeroinflatedpoisson(p, lambda_, obs=y)\n\n# Run MCMC ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Simulate data ------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n# sample one year of production\nN = as.integer(365)\ndrink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\ny = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\ndata = list()\ndata$y = y\nm$data_on_model = data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(y){\n  al = bi.dist.normal(1, 0.5, name='al', shape=c(1))\n  ap = bi.dist.normal(-1, 1, name='ap', shape=c(1))\n  p = jax$scipy$special$expit(ap)\n  lambda_ = jnp$exp(al)\n  m$zeroinflatedpoisson(p, lambda_, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#references",
    "href": "11. Zero inflated.html#references",
    "title": "Zero-Inflated Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html",
    "href": "18. Latent variable (wip).html",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables‚Äîvariables that are not directly observed but are inferred from the data‚Äîcan help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\\[\nY = f(X, Z) + \\epsilon\n\\]\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#general-principles",
    "href": "18. Latent variable (wip).html#general-principles",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables‚Äîvariables that are not directly observed but are inferred from the data‚Äîcan help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\\[\nY = f(X, Z) + \\epsilon\n\\]\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#considerations",
    "href": "18. Latent variable (wip).html#considerations",
    "title": "Latent Variable Models (WIP)",
    "section": "Considerations",
    "text": "Considerations\nIn Bayesian regression with latent variables, we consider the uncertainty in both the observed and latent variables. We declare prior distributions for the latent variables, in addition to the usual priors for regression coefficients and intercepts. These latent variables are often modeled using Gaussian distributions (Normal priors) or more flexible distributions such as Multivariate Normal for correlations among the latent variables.\nThe goal is to infer the posterior distribution over both the parameters and the latent variables, given the observed data.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#example",
    "href": "18. Latent variable (wip).html#example",
    "title": "Latent Variable Models (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with latent variables using TensorFlow Probability:\nfrom BI import bi\nimport numpy as np\nimport jax.numpy as jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Data Simulation ------------------------------------------------\nNY = 4  # Number of dependent variables or outcomes (e.g., dimensions for latent variables)\nNV = 8  # Number of observations or individual-level data points (e.g., subjects)\n\n# Initialize the matrix Y2 with shape (NV, NY) filled with NaN values, to be filled later\nY2 = np.full((NV, NY), np.nan)\n\n# Generate the means and offsets for the data\n# means: Generate random normal means for each of the NY outcomes\n# offsets: Generate random normal offsets for each of the NV observations\nmeans = m.dist.normal(0, 1, shape=(NY,), sample=True, seed=10)\noffsets = m.dist.normal(0, 1, shape=(NV, 1), sample=True, seed=20)\n\n# Fill the matrix Y2 with simulated data based on the generated means and offsets\n# Each observation (i) is the sum of an individual-specific offset and an outcome-specific mean\nfor i in range(NV):\n    for k in range(NY):\n        Y2[i, k] = means[k] + offsets[i]\n\n# Simulate individual-level random effects (e.g., random slopes or intercepts)\n# b_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates\nb_individual = BI.distribution.normal(0, 1, shape=(N, K), sample=True, seed=0)\n\n# mu: Add an additional effect 'a' to the individual-level random effects 'b_individual'\n# 'a' could represent a population-level effect or a baseline\nmu = b_individual + a\n\n# Convert Y2 to a JAX array for further computation in a JAX-based framework\nY2 = jnp.array(Y2)\n\n\n# Set data ------------------------------------------------\ndat = dict(\n    NY = NY,\n    NV = NV,\n    Y2 = Y2\n)\nm.data_on_model = dat\n\n# Define model ------------------------------------------------\ndef model(NY, NV, Y2):\n    means = m.dist.normal(0, 1, shape=(NY,), name='means')\n    offset = m.dist.normal(0, 1, shape=(NV, 1), name='offset')\n    sigma = m.dist.exponential(1, shape=(NY,), name='sigma')\n    tmp = jnp.tile(means, (NV, 1)).reshape(NV, NY)\n    mu_l = tmp + offset\n    m.normal(mu_l, jnp.tile(sigma, [NV, 1]), obs=Y2)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#mathematical-details",
    "href": "18. Latent variable (wip).html#mathematical-details",
    "title": "Latent Variable Models (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can express the Bayesian latent variable model using probability distributions as follows:\n\\[\n\\begin{aligned}\n& p(Y | X, Z, W, \\sigma) = \\text{Normal}(X \\cdot W + Z, \\sigma^2) \\\\\n& p(Z) = \\text{Normal}(0, \\tau^2) \\\\\n& p(W) = \\text{Normal}(0, \\alpha^2) \\\\\n\\end{aligned}\n\\]\nWhere: - p(Y | X, Z, W, ) is the likelihood function for the observed outcome variable, which depends on both the observed predictor X and the latent variable Z. - p(Z) is the prior distribution for the latent variable Z, often modeled as Normal with a mean of 0 and variance ^2. - p(W) is the prior distribution for the regression coefficient(s) W, typically assumed to follow a Normal distribution with mean 0 and variance ^2.\nThe latent variable Z introduces additional flexibility to the model, capturing unobserved influences on the outcome Y.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "href": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "title": "Latent Variable Models (WIP)",
    "section": "Interpretation of Latent Variables",
    "text": "Interpretation of Latent Variables\n\nLatent Variable (Z): Represents hidden factors not captured by the observed variables, allowing the model to explain more of the variance in the outcome. For instance, in a psychological model, Z might represent a latent trait such as intelligence or anxiety that influences the outcome.\nPosterior Inference: The posterior distribution of the latent variable Z can give insights into how much the unobserved factors contribute to the outcome.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#use-cases",
    "href": "18. Latent variable (wip).html#use-cases",
    "title": "Latent Variable Models (WIP)",
    "section": "Use Cases",
    "text": "Use Cases\n\nLatent Factors in Psychometrics: In psychometric models, latent variables represent traits or abilities that are not directly observed, such as cognitive ability or personality traits.\nTime-Varying Effects: Latent variables can represent unobserved time trends or individual-specific effects in time-series or longitudinal models.\nMixed Models: In hierarchical or mixed models, latent variables can represent group-specific intercepts or slopes.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html",
    "href": "13. Varying intercepts.html",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#general-principles",
    "href": "13. Varying intercepts.html#general-principles",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#considerations",
    "href": "13. Varying intercepts.html#considerations",
    "title": "Varying Intercepts Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nThe main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept \\(\\alpha_k\\) is defined based on the \\(k\\) declared groups.\nEach intercept has its own prior - i.e., a hyper-prior üõà.\nIn the code below, the hyper-prior is a_bar.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#example",
    "href": "13. Varying intercepts.html#example",
    "title": "Varying Intercepts Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying intercepts using the Bayesian Inference (BI) package. The data consists of a dependent variable representing individuals‚Äô survival (surv) and an independent categorical variable (tank), which indicates the tank where the individual was born, with a total of 48 tanks.\n\nPythonR\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n    p = alpha[tank]\n    m.dist.binomial(total_count = density, logits = p, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/reedfrogs.csv\", sep = ''), sep=';')\nm$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\nm$data_to_model(list('tank', 'surv', 'density')) # Manipulate\nm$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\nm$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(tank, surv, density){\n  # Parameter prior distributions\n  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n  a_bar =  bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n  p = alpha[tank]\n  # Likelihood\n  m$binomial(total_count = density, logits = p, obs=surv)\n} \n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#mathematical-details",
    "href": "13. Varying intercepts.html#mathematical-details",
    "title": "Varying Intercepts Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variable \\(X\\) and the outcome variable \\(Y\\) with varying intercepts \\(\\alpha\\) for each group \\(k\\) using the following equation:\n\\[\nY_{ik} = \\alpha_k + \\beta X_{ik} + \\sigma\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(X_{ik}\\) is the independent variable for observation \\(i\\) in group \\(k\\).\n\\(\\beta\\) is the regression coefficient.\n\\(\\sigma\\) is the error term, typically assumed to be normally distributed and positive.\n\n\n\nBayesian Model\nWe can express the Bayesian regression model accounting for priors üõà distributions as follows:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik}, \\sigma)\n\\] \\[\n\\mu_{ik} = \\alpha_k + \\beta X_{ik}\n\\] \\[\n\\alpha_k \\sim \\text{Normal}(\\mu_{\\alpha_k}, \\sigma_{\\alpha_k})\n\\] \\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma \\sim \\text{Exponential}(1)\n\\] \\[\n\\mu_{\\alpha_k} \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma_{\\alpha_k} \\sim \\text{Exponential}(1)\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(\\mu_{\\alpha_k}\\) is the overall mean intercept.\n\\(\\sigma_{\\alpha_k}\\) is the variance of the intercepts across groups.\n\\(\\beta\\) is the regression coefficient.\n\\(\\sigma\\) is the standard deviation of the error term.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#notes",
    "href": "13. Varying intercepts.html#notes",
    "title": "Varying Intercepts Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2.\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying intercepts with any distribution developed in previous chapters.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#references",
    "href": "13. Varying intercepts.html#references",
    "title": "Varying Intercepts Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html",
    "href": "23. Network with block model.html",
    "title": "Stochastic Block Models (SBM)",
    "section": "",
    "text": "Within networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.",
    "crumbs": [
      "Models",
      "Stochastic Block Models (SBM)"
    ]
  },
  {
    "objectID": "23. Network with block model.html#considerations",
    "href": "23. Network with block model.html#considerations",
    "title": "Stochastic Block Models (SBM)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\nIn addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (Any in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.",
    "crumbs": [
      "Models",
      "Stochastic Block Models (SBM)"
    ]
  },
  {
    "objectID": "23. Network with block model.html#example",
    "href": "23. Network with block model.html#example",
    "title": "Stochastic Block Models (SBM)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the Network model example, with the addition of covariates Any, Merica, and Quantum, representing the block membership of each node.\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\n    m.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs=result_outcomes)\n\nm.data_on_model = dict(\n    idx=idx,\n    Any=Any-1, \n    Merica=Merica-1, \n    Quantum=Quantum-1,\n    result_outcomes=m.net.mat_to_edgl(data['outcomes']), \n    kinship=m.net.mat_to_edgl(kinship),\n    focal_individual_predictors=data['individual_predictors'],\n    target_individual_predictors=data['individual_predictors']\n)\n\nm.fit(model3) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "Models",
      "Stochastic Block Models (SBM)"
    ]
  },
  {
    "objectID": "23. Network with block model.html#mathematical-details",
    "href": "23. Network with block model.html#mathematical-details",
    "title": "Stochastic Block Models (SBM)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe model‚Äôs block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the Network model chapter:\n\\[\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\\]\n\\[\n\\log(Y_{ij}) = B_{ij} + B_{ji}\n\\]\nwhere:\n\n\\(B_{ij}\\) is the link probability between category \\(i\\) and \\(j\\).\n\\(B_{ji}\\) is the link probability between category \\(j\\) to \\(i\\).\n\n\n\nDefining formula sub-equations and prior distributions\nTo account for all link probabilities between categories, we can define a square matrix \\(B\\) as follows: the off-diagonal elements represent the link probabilities between categories \\(i\\) and \\(j\\), while the diagonal elements represent the link probabilities within category \\(i\\).\n\\[\nB_{i,j} =\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j}\n\\end{bmatrix}\n\\]\nWhere:\n\n\\(B[i,j]\\) is the link probability between category \\(i\\) and \\(j\\) when \\(i \\neq j\\).\n\\(B[i,j]\\) is the link probability within category \\(i\\) when \\(i = j\\).\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks. Basic priors could be:\n\\[\n\\beta_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n\\]\n\\[\n\\beta_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n\\]\nwhere:\n\n\\(k \\rightarrow k\\) indicates a diagonal element.\n\\(k \\rightarrow \\tilde{k}\\) indicates an off-diagonal element.",
    "crumbs": [
      "Models",
      "Stochastic Block Models (SBM)"
    ]
  },
  {
    "objectID": "23. Network with block model.html#notes",
    "href": "23. Network with block model.html#notes",
    "title": "Stochastic Block Models (SBM)",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nBy defining this block model within our network model, we are estimating assortativity üõà and disassortativity üõà for categorical variables.\nSimilarly, for continuous variables, we can generate a block model that includes all continuous variables.",
    "crumbs": [
      "Models",
      "Stochastic Block Models (SBM)"
    ]
  },
  {
    "objectID": "25. Network Metrics.html",
    "href": "25. Network Metrics.html",
    "title": "Network Metrics",
    "section": "",
    "text": "Network metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#general-principles",
    "href": "25. Network Metrics.html#general-principles",
    "title": "Network Metrics",
    "section": "",
    "text": "Network metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#nodal-metrics",
    "href": "25. Network Metrics.html#nodal-metrics",
    "title": "Network Metrics",
    "section": "Nodal metrics",
    "text": "Nodal metrics\nNodal metrics* enable the assessment of nodes‚Äô social heterogeneity and the understanding of underlying mechanisms such as individual characteristics (e.g., the ageing process), ecological factors (e.g., demographic variation), and evolutionary processes (e.g., differences in social styles). Node measures are calculated at a nodal level and assess, in different ways and with different meanings, how an individual is connected. Connections can be ego‚Äôs* direct links only (e.g., degree, strength), its alters‚Äô* links as well (e.g., eigenvector, clustering coefficient), or even all the links in the network (e.g., betweenness). Node measures can also be used to describe the overall network structure through distributions, means, and coefficients of variation.\n\nDegree and strength\nThe degree m.net.degree measures the number of links of a node. When computed on an undirected network, the degree represents the number of alters of an ego. When the network is directed, it represents the number of either incoming or outgoing* links of an ego, and it is then called in-degree m.net.indegree or out-degree m.net.outdegree, respectively. Note that degree can also be computed in directed networks; in this case, it represents the sum of incoming and outgoing links and not the number of alters.\n\\[\nD_i = \\sum_{j=1}^N a_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\nStrength (or weighted degree) m.net.strength is the sum of the links‚Äô weights in a weighted network*. When the network comprises directed links, then it is also possible to differentiate between in-strength m.net.instrength (the sum of weights of incoming links) and out-strength m.net.outstrength (the sum of weights of outgoing links). While degree and strength can be considered correlated, it may not always be the case, as individuals can interact frequently with a few social partners or vice versa (Liao, Sosa, Wu, & Zhang, 2018). Therefore, it is necessary to test their correlation prior to the analysis.\n\\[\nS_i = \\sum_{j=1}^N a_{ij} w_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\n\n\nEigenvector centrality\nEigenvector centrality m.net.eigenvector is the first non-negative eigenvector value obtained by transforming an adjacency matrix linearly. It can be computed on weighted, binary, directed, or undirected networks. It measures centrality by examining the connectedness of an ego as well as that of its alters. Thus, a node‚Äôs eigenvector value can be linked either to its own degree or strength or to the degrees or strengths of the nodes to which it is connected. Eigenvector may be interpreted as the social support or social capital of an individual (Brent, Semple, Dubuc, Heistermann, & MacLarnon, 2011), that is, the real or perceived availability of social resources.\n\\[\n\\lambda c = W c\n\\]\nWhere \\(\\lambda\\) is the largest eigenvalue of the adjacency matrix \\(W\\). Isolated node(s) can be considered as zero(s).\n\n\nLocal clustering coefficient\nThe local clustering coefficient m.net.cc measures the number of closed triplets* over the total theoretical number of triplets (i.e., open and closed), where a triplet is a set of three nodes that are connected by either two (open triplet) or three (closed triplet) edges. This measure aims to examine the links that may exist between the alters of an ego and measures the cohesion of the network. The main topological effect of closed triplets is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity (see corresponding section). The local clustering coefficient can be computed in a binary network by measuring the proportion of links between the nodes of an ego-network* divided by the number of potential links between them. In weighted networks, several versions exist, such as those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or Opsahl and Panzarasa (2009).\n\nBinary Local Clustering Coefficient\n\\[\nC_i^b = \\frac{2L}{N_i (N_i - 1)}\n\\] Where \\(L\\) is the number of links in the ego-network of node \\(i\\).\n\n\nBarrat‚Äôs Local Clustering Coefficient\n\\[\nC_i^W = \\frac{1}{S_i (D_i - 1)} \\sum_{j \\neq h \\in N} \\frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}\n\\]\nWhere \\(S_i\\) and \\(D_i\\) are the strength and the degree of node \\(i\\), respectively. \\(w_{ij}\\) and \\(w_{ih}\\) are the weights of the links, and \\(a_{ij}\\), \\(a_{ih}\\), \\(a_{jh}\\) are the links between the nodes.\n\n\nOpsahl‚Äôs Local Clustering Coefficient\n\\[\nC^W(G) = \\frac{\\sum_{\\tau_\\Delta} w}{\\sum_\\tau w}\n\\] Where \\(\\tau_\\Delta\\) represents closed triplets, and \\(w\\) is the chosen weighting scheme (maximum, minimum, arithmetic, or geometric mean).\n\n\n\nBetweenness\nBetweenness (WIP) is the number of times a node is included in the shortest paths (geodesic distances) generated by every combination of two nodes. The value of the betweenness indicates the theoretical role of a node in social transmission (information, disease, etc., see Figure 1), as it indicates to what extent a node connects subgroups, as a bridge, and thus is likely to spread an entity across the whole network (Newman, 2005).\n\\[\nb = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\\]\nWhere \\(\\sigma_{st}\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\), and \\(\\sigma_{st}(v)\\) is the number of those paths that pass through \\(v\\). As no paths go through isolated nodes, their betweenness value can be considered zero.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#polyadic-metrics",
    "href": "25. Network Metrics.html#polyadic-metrics",
    "title": "Network Metrics",
    "section": "Polyadic metrics",
    "text": "Polyadic metrics\nPatterns of interactions (how and with whom individuals interact) can be examined using specific network measures* that analyse local-scale interactions within a network and make it possible to test hypotheses about the mechanisms underlying network connectivity. These types of measures are generally used to test mechanistic biological questions, such as what factors (e.g., ecological as well as sociodemographic) affect individuals‚Äô interactions/associations.\n\nAssortativity\nAssortativity (Newman, 2003) (WIP) is probably the most used measure to study homophily (preferential associations or interactions among individuals sharing the same characteristics; Lazarsfeld & Merton, 1954). Assortativity values range from ‚àí1 (total disassortativity, i.e., all the nodes associate or interact with those with the opposite characteristic, such as males interacting exclusively with females) to 1 (total assortativity, i.e., all the nodes associate or interact with those with the same characteristic, such as males interacting only with males). The assortativity coefficient measures the proportion of links between and within clusters of nodes with the same characteristics. Individuals‚Äô characteristics can be continuous (e.g., age, individual network measure, personality) or categorical features (e.g., sex, matriline belonging; Figure 2). Assortativity does not consider directionality* and can be measured in weighted (Leung & Chau, 2007) or binary (Newman, 2003) networks using categorical or continuous characteristics (Figure 2). The use of one or the other assortativity variant depends on the type of characteristics being examined and, whenever possible, the weighted version should be preferred since it is more reliable than the binary version (Farine, 2014).\n\nBinary Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1 - \\sum_i a_i b_i}\n\\]\nWhere \\(e_{ii}\\) is the proportion of specific links, \\(a_i\\) is the proportion of outgoing links, and \\(b_i\\) is the proportion of incoming links.\n\n\nWeighted Continuous Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii}^w - \\sum_i a_i^w b_i^w}{1 - \\sum_i a_i^w b_i^w}\n\\] Where \\(e_{ii}^w\\) is the proportion of weighted links, and \\(a_i^w\\), \\(b_i^w\\) are the proportions of weighted outgoing and incoming links.\n\n\n\nTransitive triplets\nTransitive triplets (WIP) are closed triplets where the links among the nodes follow a specific temporal pattern of creation, that is, when the establishment of links between nodes A and B and between nodes A and C is followed by the establishment of a link between nodes B and C. This network measure can be computed in directed, binary, or weighted networks. These types of connections can be studied over time based on the creation of links. From a static perspective, directionality can be considered by calculating the number of transitive triplets divided by the number of potential transitive triplets, and weights can also be considered by using Opsahl‚Äôs variants, which are discussed in the section on local clustering coefficient (Opsahl & Panzarasa, 2009). While transitivity is importantly related to the clustering coefficient (the clustering coefficient includes transitive triplets), not all closed triplets are transitive. Transitive triplets are one of the 16 possible configurations of a triplet considering open and closed triplets as well as link directionality (i.e., triad census).",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#global-metrics",
    "href": "25. Network Metrics.html#global-metrics",
    "title": "Network Metrics",
    "section": "Global metrics",
    "text": "Global metrics\nThe structure of this section is based on the distinction between network connectivity and social diffusion (information or disease spread). However, the social diffusion section contains measures specifically designed to study theoretical (i.e., considering the diffusion is perfectly related to network links and link weights) social diffusion features based on geodesic distances (see corresponding section). Aspects of the structure and properties of a group (e.g., cohesion, sub-grouping) can be quantified using global network measures. For instance, one may quantify properties such as network resilience (see Diameter), network clustering* (see Modularity) through network connectivity analysis, or network transmission efficiency* (see Global efficiency) through network theoretical social diffusion analysis.\n\nDensity\nThe density m.net.density is the ratio of existing links to all potential links in a network. This measure is easy to interpret; it assesses how fully connected a network is. Density considers neither directionality nor link weights.\n\\[\nD = \\frac{2|L|}{|N|(|N| - 1)}\n\\]\nWhere \\(L\\) is the number of links and \\(N\\) is the number of nodes. Isolated node(s) can be considered as zero(s).\n\n\nGeodesic Distance\nGeodesic distance m.net.geodesic_distance is the shortest path considering all potential dyads in a network. This measure thereby indicates the fastest path of diffusion. Geodesic distance can be calculated in binary, weighted*, directed, or undirected networks. In weighted networks, it can be normalized (by dividing all links by the network‚Äôs mean weight), and the strongest or the weakest links can be considered as the fastest route between two nodes. This great number of variants of geodesic distance can greatly affect the results and interpretations. Researchers must thus have knowledge of the variants and know which one is the most appropriate according to their research question (Opsahl, Agneessens, & Skvoretz, 2010).\nThe computation uses algorithms like breadth-first search, depth-first search, or Dijkstra‚Äôs algorithm. None handle isolated nodes.\n\n\nDiameter\nThe diameter m.net.diameter of a network represents the longest of the shortest paths in the network. The diameter is used in ASNA to examine aspects such as network cohesion and the rapidness of information or disease transmission. While global efficiency measures the theoretical social diffusion spread, diameter informs on the maximum path length of diffusion required to reach all nodes.\n\n\nGlobal efficiency\nGlobal efficiency (WIP) is the ratio between the number of individuals and the number of connections multiplied by the network diameter. It provides a quantitative measure of how efficiently information is exchanged among the nodes of the network. As global efficiency gives a probability of social diffusion, it may help to better understand social transmission phenomena in the short and long term (Migliano et al., 2017). Pasquaretta et al.¬†(2014) found a positive correlation between the neocortex ratio and global efficiency in primate species with a higher neocortex ratio. By drawing a parallel between cognitive capacities and social network efficiency, this study showed that in species with a higher neocortex ratio, individuals may adjust their social relationships to gain better access to social information and thus optimize network efficiency. Alternatively, studies on epidemiology in ant colonies showed that ants adapt their interaction rate to decrease network efficiency when infected by a pathogen (Stroeymeyt et al., 2018).\n\n\nModularity\nModularity (WIP) is a measure designed to quantify the degree to which a network can be divided into different groups or clusters, and its value ranges from 0 to 1. Networks with high modularity have dense connections within the modules but sparse connections between them. Modularity can be computed in weighted, binary, directed, or undirected networks.\n\\[\nQ = \\sum_{s=1}^m \\left[ \\frac{l_s}{|E|} - \\left(\\frac{d_s}{2|E|}\\right)^2 \\right]\n\\]\nWhere \\(l_s\\) is the number of edges in the \\(s\\)-th community, and \\(d_s\\) is the sum of the degrees of the nodes in the community.\n\n\nGlobal Clustering Coefficient\nThe global clustering coefficient (WIP), like the local clustering coefficient, evaluates how well the alters of an ego are interconnected and measures the cohesion of the network. Its main topological effect is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity. However, it becomes highly correlated with density and less so with modularity as density grows. Several variants of the global clustering coefficient can be found: (a) the ratio of closed triplets to all triplets (open and closed), and (b) the binary local mean clustering coefficient derived from the node level (see Local clustering coefficient). The binary local mean clustering coefficient allows us to consider node heterogeneity and thus should be preferred over the first variant. Weighted versions also exist and are based on the same variants described in the section on the local clustering coefficient and require the same considerations.\n\\[\nC^b(G) = \\frac{\\sum \\tau_\\Delta}{\\sum \\tau}\n\\]\nWhere \\(\\tau\\) is the total number of triplets and \\(\\tau_\\Delta\\) represents closed triplets.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#references",
    "href": "25. Network Metrics.html#references",
    "title": "Network Metrics",
    "section": "Reference(s)",
    "text": "Reference(s)\nSosa, Sueur, and Puga-Gonzalez (2021)",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  }
]