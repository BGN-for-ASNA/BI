[
  {
    "objectID": "16. Measuring error.html",
    "href": "16. Measuring error.html",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#general-principles",
    "href": "16. Measuring error.html#general-principles",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#example",
    "href": "16. Measuring error.html#example",
    "title": "Measurement Error Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian measurement error model using the Bayesian Inference (BI) package. The data consist of three continuous variables (marriage rate, divorce rate, age), and the goal is to estimate the effect of age and marriage rate on the divorce rate while considering that the divorce rate has a measurement error. This example is based on McElreath (2018).\n\nPython\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'WaffleDivorce.csv'\nm.data(data_path, sep=';') \nm.scale(['Divorce', 'Divorce SE', 'MedianAgeMarriage']) # Scale\ndat = dict(\n    D_obs = jnp.array(m.df['Divorce'].values),   \n    D_sd = jnp.array(m.df['Divorce'].values), \n    A = jnp.array(m.df['MedianAgeMarriage'].values), \n    N = m.df.shape[0]   \n)\nm.data_on_model = dat # Send to model (convert to jax array)\n\n\n# Define model ------------------------------------------------\ndef model(D_obs, D_sd, A, N):  \n    a = m.dist.normal(0, 0.2, name = 'a') \n    beta = m.dist.normal(0, 0.5, name = 'beta')\n    eta = m.dist.normal(0, 0.5, name = 'eta')  \n    s = m.dist.exponential(1, name = 's') \n    mu = a + beta * A + eta * M\n    D_true = m.dist.normal(mu, s, name = 'D_true') \n    m.normal(D_true , D_sd, obs = D_obs) \n\n# Run MCMC ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#mathematical-details",
    "href": "16. Measuring error.html#mathematical-details",
    "title": "Measurement Error Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian formulation\n\nD_i^* \\sim \\text{Normal}(D_i, \\varsigma_i)\n\n\nD_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\n\n\\mu_i = \\alpha + \\beta A_i + \\eta M_i\n\n\n\\sigma \\sim \\text{Normal}(1)\n\nwhere:\n\nD_i^* is the observed divorce rate.\nD_i is the true divorce rate.\n\\mu_i is the mean of the true divorce rate.\n\\sigma is the standard deviation of the true divorce rate.\n\\alpha is the intercept term.\n\\beta is the regression coefficient for age.\n\\eta is the regression coefficient for marriage rate.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#notes",
    "href": "16. Measuring error.html#notes",
    "title": "Measurement Error Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThis is an approach that can be extended to any kind of model previously described. For example, one could generate a Bernoulli measurement error model by generating a process for the probabilities of success and failure. We can even go further by potentially having an error rate that is present only in one of the two outcomes.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#references",
    "href": "16. Measuring error.html#references",
    "title": "Measurement Error Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "api_dist.html",
    "href": "api_dist.html",
    "title": "Distributions",
    "section": "",
    "text": "Sampled from distributions\nUtils.np_dists.UnifiedDist is a class to unify various distribution methods and provide a consistent interface for sampling and inference.\n\nAsymmetric Laplace\nSamples from an Asymmetric Laplace distribution.\nThe Asymmetric Laplace distribution is a generalization of the Laplace distribution, where the two sides of the distribution are scaled differently. It is defined by a location parameter (loc), a scale parameter (scale), and an asymmetry parameter (asymmetry).\n\nf(x) = \\frac{\\text{asymmetry}}{\\text{scale}(\\text{asymmetry}^2+1)} \\exp\\left(-\\frac{\\text{asymmetry}}{\\text{scale}}( \\text{loc} - x)\\right) \\\\\n\\text{if } x &lt; \\text{loc} \\\\\n\\frac{\\text{asymmetry}}{\\text{scale}(\\text{asymmetry}^2+1)} \\exp\\left(-\\frac{1}{\\text{scale} \\cdot \\text{asymmetry}}(x - \\text{loc})\\right) \\\\             \n\\text{if } x \\ge \\text{loc}\n\n\nArgs:\nbi.dist.asymmetric_laplace(\nloc=0.0,\nscale=1.0,\nasymmetry=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray or float): Location parameter of the distribution.\nscale (jnp.ndarray or float): Scale parameter of the distribution.\nasymmetry (jnp.ndarray or float): Asymmetry parameter of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\nvalidate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\n\n\n\nReturns:\n\nWhen sample=False: A BI AsymmetricLaplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaBinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI AsymmetricLaplace distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.asymmetric_laplace(loc=0.0, scale=1.0, asymmetry=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#asymmetriclaplace\n\n\n\n\nAsymmetric Laplace Quantile\nSamples from an AsymmetricLaplaceQuantile distribution.\nThis distribution is an alternative parameterization of the AsymmetricLaplace distribution, commonly used in Bayesian quantile regression. It utilizes a quantile parameter to define the balance between the left- and right-hand sides of the distribution, representing the proportion of probability density that falls to the left-hand side.\n   \nf(x) = \\frac{1}{2 \\sigma} \\exp\\left(-\\frac{|x - \\mu|}{\\sigma} \\frac{1}{q-1}\\right) \\left(1 - \\frac{1}{2q}\\right)\n\n\nArgs:\nbi.dist.asymmetric_laplace_quantile(\nloc=0.0,\nscale=1.0,\nquantile=0.5,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (float): The location parameter of the distribution.\nsample (float): The scale parameter of the distribution.\n\nquantile (float): The quantile parameter, representing the proportion of probability density to the left of the median. Must be between 0 and 1.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI AsymmetricLaplaceQuantile distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaBinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI AsymmetricLaplaceQuantile distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.asymmetric_laplace_quantile(loc=0.0, scale=1.0, quantile=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#asymmetriclaplacequantile\n\n\n\n\nBernoulli\nThe Bernoulli distribution models a single trial with two possible outcomes: success or failure. It is parameterized by the probability of success, often denoted as ‘p’.\n   P(X=1) = p \\\\\nP(X=0) = 1 - p\n\n\nArgs:\nbi.dist.bernoulli(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray, optional): Probability of success for each Bernoulli trial. Must be between 0 and 1. logits (jnp.ndarray, optional): Log-odds of success for each Bernoulli trial. probs = sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Bernoulli distribution object (for model building) when sample=False. JAX array of samples drawn from the Bernoulli distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli(probs=0.7, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#bernoulli\n\n\n\n\nBernoulli Logits\nSamples from a Bernoulli distribution parameterized by logits.\nThe Bernoulli distribution models a single binary event (success or failure), parameterized by the log-odds ratio of success. The probability of success is given by the sigmoid function applied to the logit.\n\nP(x) = \\sigma(logits)\n\n\nArgs:\nbi.dist.bernoulli_logits(\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nlogits (jnp.ndarray, optional): Log-odds ratio of success. Must be real-valued.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as\nevent dimensions (used in model building). Defaults to 0.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BernoulliLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli_logits(logits=jnp.array([0.2, 1, 2]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#bernoulli-logits\n\n\n\n\nBernoulli Probs\nSamples from a Bernoulli distribution parameterized by probabilities.\nThe Bernoulli distribution models the probability of success in a single trial, where the outcome is binary (success or failure). It is characterized by a single parameter, probs, representing the probability of success.\n\nP(X=1) = p\n\nwhere: p is the probability of success (0 &lt;= p &lt;= 1)\n\nArgs:\nbi.dist.bernoulli_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): The probability of success for each Bernoulli trial. Must be between 0 and 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BernoulliProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BernoulliProbs object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli_probs(probs=jnp.array([0.2, 0.7, 0.5]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#bernoulliprobs\n\n\n\n\nBeta\nSamples from a Beta distribution, defined on the interval [0, 1]. The Beta distribution is a versatile distribution often used to model probabilities or proportions. It is parameterized by two positive shape parameters, often referred to as concentration parameters in the BI context.\n\nf(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\nwhere \\alpha and \\beta are the concentration parameters, and B(x, y) is the Beta function.\n\nArgs:\nbi.dist.beta(\nconcentration1,\nconcentration0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nconcentration1 (jnp.ndarray): The first concentration parameter (shape parameter). Must be positive.\nconcentration0 (jnp.ndarray): The second concentration parameter (shape parameter). Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Beta distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Beta object (for advanced use cases).\n\n\n\n#### Example Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.beta(concentration1=1.0, concentration0=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#beta\n\n\n\n\nBetaBinomial\nSamples from a BetaBinomial distribution, a compound distribution where the probability of success in a binomial experiment is drawn from a Beta distribution. This models situations where the underlying probability of success is not fixed but varies according to a prior belief represented by the Beta distribution.\n   \nP(X=k) = \\binom{n}{k} \\frac{\\Gamma(\\alpha + k)}{\\Gamma(\\alpha + \\beta + n - k)} \\frac{\\Gamma(\\beta + n - k)}{\\Gamma(\\beta)}\n\n\nArgs:\nbi.dist.beta_binomial(\nconcentration1,\nconcentration0,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nconcentration1 (jnp.ndarray): The first concentration parameter (alpha) of the Beta distribution.\nconcentration0 (jnp.ndarray): The second concentration parameter (beta) of the Beta distribution.\ntotal_count (jnp.ndarray): The number of Bernoulli trials in the Binomial part of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BetaBinomial distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaBinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI BetaBinomial distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.beta_binomial(concentration1=1.0, concentration0=1.0, total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#betabinomial\n\n\n\n\nBeta Proportion\nThe BetaProportion distribution is a reparameterization of the conventional Beta distribution in terms of a the variate mean and a precision parameter. It’s useful for modeling rates and proportions.\n\nf(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\n\nArgs:\nbi.dist.beta_proportion(\nmean,\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nmean (jnp.ndarray): The mean of the BetaProportion distribution,must be between 0 and 1.\nconcentration (jnp.ndarray): The concentration parameter of the BetaProportion distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BetaProportion distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaBinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI BetaProportion distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nsamples = m.dist.beta_proportion(mean=0.5, concentration=2.0, sample=True, shape=(1000,))\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#beta_proportion\n\n\n\n\nBinomial\nThe Binomial distribution models the number of successes in a sequence of independent Bernoulli trials. It represents the probability of obtaining exactly k successes in n trials, where each trial has a probability p of success.\n   P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\nArgs:\nbi.dist.binomial(\ntotal_count=1,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\ntotal_count (int): The number of trials n.\nprobs (jnp.ndarray, optional): The probability of success p for each trial. Must be between 0 and 1.\nlogits (jnp.ndarray, optional): The log-odds of success for each trial. probs = jax.nn.sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBinomial distribution object (for model building) when sample=False. JAX array of samples drawn from the Binomial distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial(total_count=10, probs=0.5, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#binomial\n\n\n\n\nBinomial Logits\nThe BinomialLogits distribution represents a binomial distribution parameterized by logits. It is useful when the probability of success is not directly known but is instead expressed as logits, which are the natural logarithm of the odds ratio.\n\nP(X=k) = \\binom{n}{k} \\frac{e^{logits_k}}{1 + e^{logits_k}}\n\n\nArgs:\nbi.dist.binomial_logits(\nlogits,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlogits (jnp.ndarray): Log-odds of each success. total_count (int): Number of trials.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BinomialLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BinomialLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial_logits(logits=jnp.zeros(10), total_count=5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#binomialllogits\n\n\n\n\nBinomial Probs\nSamples from a Binomial distribution with specified probabilities for each trial.\nThe Binomial distribution models the number of successes in a sequence of independent Bernoulli trials, where each trial has the same probability of success.\n   \nP(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\nArgs:\nbi.dist.binomial_probs(\nprobs,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): The probability of success for each trial. Must be between 0 and 1.\ntotal_count (int): The number of trials in each sequence.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI BinomialProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BinomialLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial_probs(probs=0.5, total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#binomialprobs\n\n\n\n\nConditional Autoregressive (CAR)\nThe CAR distribution models a vector of variables where each variable is a linear combination of its neighbors in a graph.\n   \np(x) = \\prod_{i=1}^{K} \\mathcal{N}(x_i | \\mu_i, \\Sigma_i)\n\nwhere \\mu_i is a function of the values of the neighbors of site i and \\Sigma_i is the variance of site i.\n.. note::\nThe CAR distribution is a special case of the multivariate normal distribution. It is used to model spatial data, such as temperature or precipitation.\n\nArgs:\nbi.dist.car(\nloc,\ncorrelation,\nconditional_precision,\nadj_matrix,\nis_sparse=False,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (Union[float, Array]): Mean of the distribution.\ncorrelation (Union[float, Array]): Correlation between variables.\nconditional_precision (Union[float, Array]): Precision of the distribution.\nadj_matrix (Union[Array, scipy.sparse.spmatrix]): Adjacency matrix defining the graph. is_sparse (bool): Whether the adjacency matrix is sparse. Defaults to False.\n*validate_args/ (bool): Whether to validate arguments. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI CAR distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CAR object (for advanced use cases).\n\n\n\n\n\nCategorical distribution.\nThe Categorical distribution, also known as the multinomial distribution, describes the probability of different outcomes from a finite set of possibilities. It is commonly used to model discrete choices or classifications.\n   \nP(k) = \\frac{e^{\\log(p_k)}}{\\sum_{j=1}^{K} e^{\\log(p_j)}}\n\nwhere p_k is the probability of outcome k, and the sum is over all possible outcomes.\n\nArgs:\nbi.dist.categorical(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): A 1D array of probabilities for each category. Must sum to 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Categorical distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Categorical distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical(probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#categorical\n\n\n\n\nCategorical Logits\nSamples from a Categorical distribution with logits. This distribution represents a discrete probability distribution over a finite set of outcomes, where the probabilities are determined by the logits. The probability of each outcome is given by the softmax function applied to the logits.\n\nP(k) = \\frac{e^{logits_k}}{\\sum_{j=1}^{K} e^{logits_j}}\n\n\nArgs:\nbi.dist.categorical_logits(\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nlogits (jnp.ndarray): Log-odds of each category.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI CategoricalLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CategoricalLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical_logits(logits=jnp.zeros(5), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#categoricallogits\n\n\n\n\nCategorical Probs distribution.\nSamples from a Categorical distribution.\nThe Categorical distribution is a discrete probability distribution that represents the probability of each outcome from a finite set of possibilities. It is often used to model the outcome of a random process with a fixed number of possible outcomes, such as the roll of a die or the selection of an item from a list.\n   \nP(x) = \\frac{probs_i}{\\sum_{k=1}^{K} probs_k}\n\n\nArgs:\nbi.dist.categorical_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): Probabilities for each category. Must sum to 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI CategoricalProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CategoricalProbs object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical_probs(probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#categoricalprobs\n\n\n\n\nCauchy\nSamples from a Cauchy\nThe Cauchy distribution, also known as the Lorentz distribution, is a continuous probability distribution that arises frequently in various fields, including physics and statistics. It is characterized by its heavy tails, which extend indefinitely.\n\nf(x) = \\frac{1}{\\pi \\gamma} \\left[ \\frac{\\gamma^2}{(x - \\mu)^2 + \\gamma^2} \\right]\n\n\nArgs:\nbi.dist.cauchy(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray or float, optional): Location parameter. Defaults to 0.0.\nsample (jnp.ndarray or float, optional): Scale parameter. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building). Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Cauchy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Cauchy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.cauchy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#cauchy\n\n\n\n\nChi-squared\nThe Chi-squared distribution is a continuous probability distribution that arises frequently in hypothesis testing, particularly in ANOVA and chi-squared tests. It is defined by a single positive parameter, degrees of freedom (df), which determines the shape of the distribution.\n   \np(x; df) = \\frac{1}{2^{df/2} \\Gamma(df/2)} x^{df/2 - 1} e^{-x/2}\n\n\nArgs:\nbi.dist.chi2(\ndf,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ndf (jnp.ndarray): Degrees of freedom. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Chi2 distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Chi2 object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu)\nm.dist.chi2(df=3.0, sample = True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#chi2\n\n\n\n\nCirculant Normal Multivariate normal\nCirculant Normal Multivariate normal distribution with covariance matrix \\mathbf{C} that is positive-definite and circulant [1], i.e., has periodic boundary conditions. The density of a sample \\mathbf{x}\\in\\mathbb{R}^n is the standard multivariate normal density\n\np\\left(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\mathbf{C}\\right) =\n\\frac{\\left(\\mathrm{det}\\,\\mathbf{C}\\right)^{-1/2}}{\\left(2\\pi\\right)^{n / 2}}\n\\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\intercal\n\\mathbf{C}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)\\right),\n\nwhere \\mathrm{det} denotes the determinant and ^\\intercal the transpose. Circulant matrices can be diagnolized efficiently using the discrete Fourier transform [1], allowing the log likelihood to be evaluated in n \\log n time for n observations [2].\n\nloc: Mean of the distribution \\boldsymbol{\\mu}.\ncovariance_row: First row of the circulant covariance matrix \\boldsymbol{C}. Because of periodic boundary conditions, the covariance matrix is fully determined by its first row (see :func:jax.scipy.linalg.toeplitz for further details).\ncovariance_rfft: Real part of the real fast Fourier transform of :code:covariance_row, the first row of the circulant covariance matrix \\boldsymbol{C}.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\nReferences:\n\nWikipedia. (n.d.). Circulant matrix. Retrieved March 6, 2025, from https://en.wikipedia.org/wiki/Circulant_matrix\nWood, A. T. A., & Chan, G. (1994). Simulation of Stationary Gaussian Processes in \\left[0, 1\\right]^d. Journal of Computational and Graphical Statistics, 3(4), 409–432. https://doi.org/10.1080/10618600.1994.10474655\n\n\nArgs:\nbi.dist.circulant_normal(\nloc: jax.Array,\ncovariance_row: jax.Array = None,\ncovariance_rfft: jax.Array = None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc : jnp.ndarray Mean of the distribution \\boldsymbol{\\mu}.\ncovariance_row : jnp.ndarray, optional. First row of the circulant covariance matrix \\mathbf{C}. Defaults to None.\ncovariance_rfft : jnp.ndarray, optional Real part of the real fast Fourier transform of :code:covariance_row. Defaults to None.\n\n\n\nReturns:\n\nWhen sample=False: A BI Circulant Normal Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Circulant Normal Distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#normal\n\n\n\n\nDelta\nThe Delta distribution, also known as a point mass distribution, assigns probability 1 to a single point and 0 elsewhere. It’s useful for representing deterministic variables or as a building block for more complex distributions.\n   P(x = v) = 1\n\n\nArgs:\nbi.dist.delta(\nv=0.0,\nlog_density=0.0,\nevent_dim=0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nv (jnp.ndarray): The location of the point mass. log_density (float, optional): The log probability density of the point mass. This is primarily for creating distributions that are non-normalized or for specific advanced use cases. For a standard delta distribution, this should be 0. Defaults to 0.0.\nevent_dim (int, optional): The number of rightmost dimensions of v to interpret as event dimensions. Defaults to 0. - shape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\n\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Circulant Delta Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Circulant Delta Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.delta(v=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#delta\n\n\n\n\nDirichlet\nSamples from a Dirichlet distribution.\nThe Dirichlet distribution is a multivariate generalization of the Beta distribution. It is a probability distribution over a simplex, which is a set of vectors where each element is non-negative and sums to one. It is often used as a prior distribution for categorical distributions.\n   P(x_1, ..., x_K) = \\frac{\\Gamma(\\sum_{i=1}^K \\alpha_i)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n\n\nArgs:\nbi.dist.dirichlet(\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The concentration parameter(s) of the Dirichlet distribution. Must be a positive array.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Dirichlet Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Dirichlet Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.dirichlet(concentration=jnp.array([1.0, 1.0, 1.0]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#dirichlet\n\n\n\n\nDirichlet-Multinomial\nCreates a Dirichlet-Multinomial compound distribution, which is a Multinomial distribution with a Dirichlet prior on its probabilities. It is often used in Bayesian statistics to model count data where the proportions of categories are uncertain.\nThe probability mass function is given by:\n\nP(\\mathbf{x} | \\boldsymbol{\\alpha}, n) = \\frac{n!}{\\prod_{i=1}^k x_i!} \\frac{\\Gamma(\\sum_{i=1}^k \\alpha_i)}{\\Gamma(n + \\sum_{i=1}^k \\alpha_i)} \\prod_{i=1}^k \\frac{\\Gamma(x_i + \\alpha_i)}{\\Gamma(\\alpha_i)}\nwhere \\mathbf{x} is a vector of counts, n is the total number of trials (total_count), and \\boldsymbol{\\alpha} is the concentration parameter vector for the Dirichlet prior.\n\nArgs:\nbi.dist.dirichlet_multinomial(\nconcentration,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The concentration parameter (alpha) for the Dirichlet prior. Values must be positive. The last dimension is interpreted as the number of categories. total_count (int, jnp.ndarray, optional): The total number of trials (n). This must be a non-negative integer. Defaults to 1. validate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None. - name (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’. - obs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\n\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\n\nWhen sample=False: A BI dirichlet_multinomial Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI dirichlet_multinomial Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling\n# Sample a single vector of counts for 10 trials from 3 categories\ncounts = m.dist.dirichlet_multinomial(concentration=jnp.array([1.0, 1.0, 1.0]),total_count=10,sample=True)\n\n# Usage within a model\ndef my_model(obs_data=None):\n# Define a prior on the concentration parameter\nalpha = m.dist.half_cauchy(scale=jnp.ones(5), name='alpha', shape=(5,))\n\n# Model observed counts\nwith m.plate('data', len(obs_data)):\ny = m.dist.dirichlet_multinomial(\nconcentration=alpha,\ntotal_count=100,\nname='y',\nobs=obs_data\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#dirichletmultinomial\n\n\n\n\nDiscrete Uniform\nSamples from a Discrete Uniform distribution.\nThe Discrete Uniform distribution defines a uniform distribution over a range of integers. It is characterized by a lower bound (low) and an upper bound (high), inclusive.\n   P(X = k) = \\frac{1}{high - low + 1}, \\quad k \\in \\{low, low+1, ..., high\\}\n\n\nArgs:\nbi.dist.discrete_uniform(\nlow=0,\nhigh=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlow (jnp.ndarray): The lower bound of the uniform range, inclusive. high (jnp.ndarray): The upper bound of the uniform range, inclusive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI DiscreteUniform Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI dirichlet_multinomial Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.discrete_uniform(low=0, high=5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#discreteuniform\n\n\n\n\nDoubly Truncated Power Law\nThis distribution represents a continuous power law with a finite support bounded between low and high, and with an exponent alpha. It is normalized over the interval [low, high] to ensure the area under the density function is 1.\nThe probability density function (PDF) is defined as:\n\nf(x;\\,\\alpha,a,b) = \\frac{x^{\\alpha}}{Z(\\alpha,a,b)},\n\\quad x \\in [a,b]\n\nwhere the normalization constant Z(\\alpha,a,b) is given by\n\nZ(\\alpha,a,b) =\n\\begin{cases}\n\\log(b) - \\log(a), & \\text{if } \\alpha = -1, \\\\\n\\dfrac{b^{1+\\alpha} - a^{1+\\alpha}}{1+\\alpha}, & \\text{otherwise}.\n\\end{cases}\n\nThis distribution is useful for modeling data that follows a power-law behavior but is naturally bounded due to measurement or theoretical constraints (e.g., finite-size systems).\n\nArgs:\nbi.dist.doubly_truncated_power_law(\nalpha,\nlow,\nhigh,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nalpha (float or array-like): Power-law exponent.\nlow (float or array-like): Lower bound of the distribution (must be ≥ 0).\nhigh (float or array-like): Upper bound of the distribution (must be &gt; 0).\n\nshape (tuple, optional): The shape of the output tensor. Defaults to None.\n\nvalidate_args (bool, optional): Whether to validate the arguments. Defaults to True.\n\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI doubly_truncated_power_law Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI doubly_truncated_power_law Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')        \nm.dist.doubly_truncated_power_law(low=0.1, high=10.0, alpha=2.0, sample=True)\n\n\n\n\nEuler–Maruyama\nEuler–Maruyama methode is a method for the approximate numerical solution of a stochastic differential equation (SDE). It simulates the solution to an SDE by iteratively applying the Euler method to each time step, incorporating a random perturbation to account for the diffusion term.\n\ndX_t = f(X_t, t) dt + g(X_t, t) dW_t\nwhere: - X_t is the state of the system at time t. - f(X_t, t) is the drift coefficient. - g(X_t, t) is the diffusion coefficient. - dW_t is a Wiener process (Brownian motion).\n\nArgs:\nbi.dist.euler_maruyama(\nt,\nsde_fn,\ninit_dist,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nt (jnp.ndarray): Discretized time steps.\nsde_fn (callable): A function that takes the current state and time as input and returns the drift and diffusion coefficients. init_dist (Distribution): The initial distribution of the system.\n\nshape (tuple, optional): The shape of the output tensor. Defaults to None.\n\nsample_shape (tuple, optional): The shape of the samples to draw. Defaults to None.\nvalidate_args (bool, optional): Whether to validate the arguments. Defaults to True.\n\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\njnp.ndarray: Samples drawn from the Euler–Maruyama distribution.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.euler_maruyama(t=jnp.array([0.0, 0.1, 0.2]), sde_fn=lambda x, t: (x, 1.0), init_dist=m.dist.normal(0.0, 1.0, create_obj=True), sample = True)\n\n\n\n\nExponential\nThe Exponential distribution is a continuous probability distribution that models the time until an event occurs in a Poisson process, where events occur continuously and independently at a constant average rate. It is often used to model the duration of events, such as the time until a machine fails or the length of a phone call.\n   f(x) = \\lambda e^{-\\lambda x} \\text{ for } x \\geq 0\n\n\nArgs:\nbi.dist.exponential(\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nrate (jnp.ndarray): The rate parameter, \\lambda. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Exponential distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Exponential distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.exponential(rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#exponential\n\n\n\n\nFolded\nSamples from a Folded distribution, which is the absolute value of a base univariate distribution. This distribution reflects the base distribution across the origin, effectively taking the absolute value of each sample.\n\np(x) = \\sum_{k=-\\infty}^{\\infty} p(x - 2k)\n\n\nArgs:\nbi.dist.folded_distribution(\nbase_dist,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (float, optional): Location parameter of the base distribution. Defaults to 0.0.\nsample (float, optional): Scale parameter of the base distribution. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI FoldedDistribution distribution object (for model building) when sample=False. JAX array of samples drawn from the FoldedDistribution distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.folded_distribution(m.dist.normal(loc=0.0, scale=1.0, create_obj = True), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#foldeddistribution\n\n\n\n\nGamma\nSamples from a Gamma distribution.\nThe Gamma distribution is a continuous probability distribution that arises frequently in Bayesian statistics, particularly in prior distributions for variance parameters. It is defined by two positive shape parameters, concentration (k) and rate (theta).\n\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x &gt; 0\n\n\nArgs:\nbi.dist.gamma(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The shape parameter of the Gamma distribution (k &gt; 0).\nrate (jnp.ndarray): The rate parameter of the Gamma distribution (theta &gt; 0).\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nGamma: A BI Gamma distribution object (for model building).\njnp.ndarray: A JAX array of samples drawn from the Gamma distribution (for direct sampling).\nGamma: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gamma(concentration=2.0, rate=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gamma\n\n\n\n\nGamma Poisson\nA compound distribution comprising of a gamma-poisson pair, also referred to as a gamma-poisson mixture. The rate parameter for the :class:~numpyro.distributions.Poisson distribution is unknown and randomly drawn from a :class:~numpyro.distributions.Gamma distribution.\n   P(X = x) = \\int_0^\\infty \\frac{1}{x} \\exp(-x \\lambda) \\frac{1}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x \\beta} dx\n\n\nArgs:\nbi.dist.gamma_poisson(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): Shape parameter (alpha) of the Gamma distribution. rate (jnp.ndarray): Rate parameter (beta) for the Gamma distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI GammaPoisson distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the GammaPoisson distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gamma_poisson(concentration=1.0, rate=2.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gammapoisson\n\n\n\n\nGaussian Copula\nA distribution that links the batch_shape[:-1] of a marginal distribution with a multivariate Gaussian copula, modelling the correlation between the axes. A copula is a multivariate distribution over the uniform distribution on [0, 1]. The Gaussian copula links the marginal distributions through a multivariate normal distribution.\n\nf(x_1, ..., x_d) = \\prod_{i=1}^{d} f_i(x_i) \\cdot \\phi(F_1(x_1), ..., F_d(x_d); \\mu, \\Sigma)\n\nwhere: - f_i is the probability density function of the i-th marginal distribution. - F_i is the cumulative distribution function of the i-th marginal distribution. - \\phi is the standard normal PDF. - \\mu is the mean vector of the multivariate normal distribution. - \\Sigma is the covariance matrix of the multivariate normal distribution.\n\nArgs:\nbi.dist.gaussian_copula(\nmarginal_dist,\ncorrelation_matrix=None,\ncorrelation_cholesky=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nmarginal_dist (Distribution): Distribution whose last batch axis is to be coupled.\ncorrelation_matrix (array_like, optional): Correlation matrix of the coupling multivariate normal distribution. Defaults to None.\ncorrelation_cholesky (array_like, optional): Correlation Cholesky factor of the coupling multivariate normal distribution. Defaults to None.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like Mi  xtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI GaussianCopula distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_copula(\nmarginal_dist = m.dist.beta(2.0, 5.0, create_obj = True), \ncorrelation_matrix = jnp.array([[1.0, 0.7],[0.7, 1.0]]), \nsample = True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussiancopula\n\n\n\n\nGaussian Copula Beta\nThis distribution combines a Gaussian copula with a Beta distribution. The Gaussian copula models the dependence structure between random variables, while the Beta distribution defines the marginal distributions of each variable.\n\nf(x) = \\int_{-\\infty}^{\\infty} g(x|u) h(u) du\n\nWhere: - g(x|u) is the Gaussian copula density. - h(u) is the Beta density.\n\nArgs:\nbi.dist.gaussian_copula_beta(\nconcentration1,\nconcentration0,\ncorrelation_matrix=None,\ncorrelation_cholesky=None,\nvalidate_args=False,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration1 (jnp.ndarray): The first shape parameter of the Beta distribution.\nconcentration0 (jnp.ndarray): The second shape parameter of the Beta distribution.\ncorrelation_matrix (array_like, optional): Correlation matrix of the coupling multivariate normal distribution. Defaults to None.\ncorrelation_cholesky (jnp.ndarray): The Cholesky decomposition of the correlation matrix. - shape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\n\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nGaussianCopulaBeta: A BI GaussianCopulaBeta distribution object (for model building). jnp.ndarray: A JAX array of samples drawn from the GaussianCopulaBeta distribution (for direct sampling). Distribution: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_copula_beta(\nconcentration1 = jnp.array([2.0, 3.0]), \nconcentration0 = jnp.array([5.0, 3.0]),\ncorrelation_cholesky = jnp.linalg.cholesky(jnp.array([[1.0, 0.7],[0.7, 1.0]])), \nsample = True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussiancopulabetadistribution\n\n\n\n\nGaussian Random Walk\nCreates a distribution over a Gaussian random walk of a specified number of steps. This is a discrete-time stochastic process where the value at each step is the previous value plus a Gaussian-distributed increment. The distribution is over the entire path.\n\nX_t = X_{t-1} + \\epsilon_t, \\quad \\text{where} \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\nwith the initial state X_0 = 0. The resulting sample is a vector of length num_steps, representing the path (X_1, X_2, \\dots, X_{\\text{num\\_steps}}).\n\nArgs:\nbi.dist.gaussian_random_walk(\nscale=1.0,\nnum_steps=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (float, jnp.ndarray, optional): The standard deviation (\\sigma) of the Gaussian increments. Must be positive. Defaults to 1.0. num_steps (int, optional): The number of steps in the random walk, which determines the event shape of the distribution. Must be positive. Defaults to 1. validate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\nBI.primitives.Messenger: A BI sample site object when used in a model context (sample=False). jnp.ndarray: A JAX array of samples drawn from the GaussianRandomWalk distribution (for direct sampling, sample=True). numpyro.distributions.Distribution: The raw BI distribution object (if create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling of a random walk with 100 steps\npath = m.dist.gaussian_random_walk(scale=0.5, num_steps=100, sample=True)\n\n# Usage within a model for a latent time series\ndef my_model(data=None):\n# Prior on the volatility of the random walk\nvolatility = m.dist.half_cauchy(scale=1.0, name='volatility')\n\n# The latent random walk\nlatent_process = m.dist.gaussian_random_walk(\nscale=volatility,\nnum_steps=len(data) if data is not None else 10,\nname='latent_process'\n)\n\n# Observation model\n# Assumes the observed data is the latent process plus some noise\nobs_noise = m.dist.half_cauchy(scale=1.0, name='obs_noise')\nwith m.plate('time', len(data) if data is not None else 10):\nreturn m.dist.normal(loc=latent_process, scale=obs_noise, obs=data, name='obs')\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussianrandomwalk\n\n\n\n\nGaussian State Space\nSamples from a Gaussian state space model.\n           \n\\mathbf{z}_{t} = \\mathbf{A} \\mathbf{z}_{t - 1} + \\boldsymbol{\\epsilon}_t\\\\\n=\\sum_{k=1} \\mathbf{A}^{t-k} \\boldsymbol{\\epsilon}_t,\n\nwhere \\mathbf{z}_t is the state vector at step t, \\mathbf{A} is the transition matrix, and \\boldsymbol\\epsilon is the innovation noise.\n\nArgs:\nbi.dist.gaussian_state_space(\nnum_steps,\ntransition_matrix,\ncovariance_matrix=None,\nprecision_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nnum_steps (int): Number of steps.\ntransition_matrix (jnp.ndarray): State space transition matrix \\mathbf{A}.\ncovariance_matrix (jnp.ndarray, optional): Covariance of the innovation noise \\boldsymbol{\\epsilon}. Defaults to None.\nprecision_matrix (jnp.ndarray, optional): Precision matrix of the innovation noise \\boldsymbol{\\epsilon}. Defaults to None.\nscale_tril (jnp.ndarray, optional): Scale matrix of the innovation noise \\boldsymbol{\\epsilon}. Defaults to None.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI GaussianStateSpace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the GaussianStateSpace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_state_space(num_steps=5, transition_matrix=jnp.array([[0.5]]), covariance_matrix =  jnp.array([[1.0, 0.6],[0.6, 1.0]]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussianstate\n\n\n\n\nGeometric distribution.\nThe Geometric distribution models the number of failures before the first success in a sequence of Bernoulli trials. It is characterized by a single parameter, the probability of success on each trial.\n   P(X = k) = (1 - p)^k p\n\n\nArgs:\nbi.dist.geometric(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray, optional): Probability of success on each trial. Must be between 0 and 1. logits (jnp.ndarray, optional): Log-odds of success on each trial. probs = jax.nn.sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Geometric distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Geometric distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric(probs=0.5, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#geometric\n\n\n\n\nGeometricLogits\nSamples from a GeometricLogits distribution, which models the number of failures before the first success in a sequence of independent Bernoulli trials. It is parameterized by logits, which are transformed into probabilities using the sigmoid function.\n\nP(X = k) = (1 - p)^k p\n\nwhere:\n\nX is the number of failures before the first success.\nk is the number of failures.\np is the probability of success on each trial (derived from the logits).\n\n\nArgs:\nbi.dist.geometric_logits(\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlogits (jnp.ndarray): Log-odds parameterization of the probability of success.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI GeometricLogits distribution object (for model building) when sample=False. JAX array of samples drawn from the GeometricLogits distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric_logits(logits=jnp.zeros(10), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#geometriclogits\n\n\n\n\nGeometricProbs\nSamples from a Geometric\nThe Geometric distribution models the number of trials until the first success in a sequence of independent Bernoulli trials, where each trial has the same probability of success.\n\nP(X = k) = (1 - p)^k p\n\n\nArgs:\nbi.dist.geometric_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): Probability of success on each trial. Must be between 0 and 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI GeometricProbs distribution object (for model building). JAX array of samples drawn from the GeometricProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric_probs(probs=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#geometricprobs\n\n\n\n\nGompertz\nThe Gompertz distribution is a distribution with support on the positive real line that is closely related to the Gumbel distribution. This implementation follows the notation used in the Wikipedia entry for the Gompertz distribution. See https://en.wikipedia.org/wiki/Gompertz_distribution.\nThe probability density function (PDF) is:\n\nf(x) = \\frac{con}{rate} \\exp \\left\\{ - \\frac{con}{rate} \\left [ \\exp\\{x * rate \\} - 1 \\right ] \\right\\} \\exp(-x * rate)\n\n\nArgs:\nbi.dist.gompertz(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The concentration parameter. Must be positive. rate (jnp.ndarray): The rate parameter. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Gompertz distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gompertz(concentration=1.0, rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gompertz\n\n\n\n\nGumbel\nSamples from a Gumbel (or Extreme Value) distribution.\nThe Gumbel distribution is a continuous probability distribution named after German mathematician Carl Gumbel. It is often used to model the distribution of maximum values in a sequence of independent random variables.\n   f(x) = \\frac{1}{s} e^{-(x - \\mu) / s} e^{-e^{- (x - \\mu) / s}}\n\n\nArgs:\nbi.dist.gumbel(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray or float, optional): Location parameter. Defaults to 0.0.\nsample (jnp.ndarray or float, optional): Scale parameter. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building). Defaults to 1.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Gumbel distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Gumbel distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gumbel(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gumbel\n\n\n\n\nHalfCauchy\nThe HalfCauchy distribution is a probability distribution that is half of the Cauchy distribution. It is defined on the positive real numbers and is often used in situations where only positive values are relevant.\n   \nf(x) = \\frac{1}{2} \\cdot \\frac{1}{\\pi \\cdot \\frac{1}{scale} \\cdot (x^2 + \\frac{1}{scale^2})}\n\n\nArgs:\nbi.dist.half_cauchy(\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (jnp.ndarray): The scale parameter of the Cauchy distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI HalfCauchy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the HalfCauchy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.half_cauchy(scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#halfcauchy\n\n\n\n\nHalfNormal\nSamples from a HalfNormal distribution.\nThe HalfNormal distribution is a distribution of the absolute value of a normal random variable. It is defined by a location parameter (implicitly 0) and a scale parameter.\n   f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{x^2}{2\\sigma^2}} \\text{ for } x &gt; 0\n\n\nArgs:\nbi.dist.half_normal(\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (float, array): The scale parameter of the distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI HalfNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the HalfNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.half_normal(scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#halfnormal\n\n\n\n\nImproper Uniform\nA helper distribution with zero :meth:log_prob over the support domain.\n   p(x) = 0\n\n\nArgs:\nbi.dist.improper_uniform(\nsupport,\nbatch_shape,\nevent_shape,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nsupport (numpyro.distributions.constraints.Constraint): The support of this distribution.\nbatch_shape (tuple): Batch shape of this distribution. It is usually safe to set batch_shape=().\nevent_shape (tuple): Event shape of this distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI ImproperUniform distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ImproperUniform distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import sample\nfrom numpyro.distributions import ImproperUniform, Normal, constraints\n\ndef model():\nx = sample('x', ImproperUniform(constraints.ordered_vector, (), event_shape=(10,)))\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#improperuniform\n\n\n\n\nInverse Gamma\nThe InverseGamma distribution is a two-parameter family of continuous probability distributions. It is defined by its shape and rate parameters. It is often used as a prior distribution for precision parameters (inverse variance) in Bayesian statistics.\n\np(x) = \\frac{1}{Gamma(\\alpha)} \\left( \\frac{\\beta}{\\Gamma(\\alpha)} \\right)^{\\alpha} x^{\\alpha - 1} e^{-\\beta x}\n\\text{ for } x &gt; 0\n\n\nArgs:\nbi.dist.inverse_gamma(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The shape parameter (\\alpha) of the InverseGamma distribution. Must be positive. rate (jnp.ndarray): The rate parameter (\\beta) of the InverseGamma distribution. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI InverseGamma distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the InverseGamma distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.inverse_gamma(concentration=2.0, rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#inversegamma\n\n\n\n\nKumaraswamy\nThe Kumaraswamy distribution is a continuous probability distribution defined on the interval [0, 1]. It is a flexible distribution that can take on various shapes depending on its parameters.\n\nf(x; a, b) = a b x^{a b - 1} (1 - x)^{b - 1}\n\n\nArgs:\nbi.dist.kumaraswamy(\nconcentration1,\nconcentration0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration1 (jnp.ndarray): The first shape parameter. Must be positive. concentration0 (jnp.ndarray): The second shape parameter. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Kumaraswamy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Kumaraswamy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.kumaraswamy(concentration1=2.0, concentration0=3.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#kumaraswamy\n\n\n\n\nLaplace\nSamples from a Laplace distribution, also known as the double exponential distribution. The Laplace distribution is defined by its location parameter (loc) and scale parameter (scale).\n   f(x) = \\frac{1}{2s} \\exp\\left(-\\frac{|x - \\mu|}{s}\\right)\n\n\nArgs:\nbi.dist.laplace(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray): Location parameter of the Laplace distribution.\nsample (jnp.ndarray): Scale parameter of the Laplace distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Laplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Laplace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.laplace(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#laplace\n\n\n\n\nLeft Truncated\nSamples from a left-truncated distribution.\nA left-truncated distribution is a probability distribution obtained by restricting the support of another distribution to values greater than a specified lower bound. This is useful when dealing with data that is known to be greater than a certain value.\n   \nf(x) =\n\\begin{cases}\n\\dfrac{p(x)}{P(X &gt; \\text{low})}, & x &gt; \\text{low}, \\\\[6pt]\n0, & \\text{otherwise},\n\\end{cases}\n #### Args:\nbi.dist.left_truncated_distribution(\nbase_dist,\nlow=0.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbase_dist: The base distribution to truncate. Must be univariate and have real support. low: The lower truncation bound. Values less than this are excluded from the distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\nReturns:\n\nWhen sample=False: A BI LeftTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LeftTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#lefttruncateddistribution\n\n\n\n\nLevy\nSamples from a Levy distribution.\nThe probability density function is given by,\n\nf(x\\mid \\mu, c) = \\sqrt{\\frac{c}{2\\pi(x-\\mu)^{3}}} \\exp\\left(-\\frac{c}{2(x-\\mu)}\\right), \\qquad x &gt; \\mu\n\nwhere \\mu is the location parameter and c is the scale parameter.\n\nArgs:\nbi.dist.levy(\nloc,\nscale,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray): Location parameter.\nsample (jnp.ndarray): Scale parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Levy distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.levy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#levy\n\n\n\n\nLewandowski Kurowicka Joe (LKJ)\nThe LKJ distribution is controlled by the concentration parameter \\eta to make the probability of the correlation matrix M proportional to \\det(M)^{\\eta - 1}. When \\eta = 1, the distribution is uniform over correlation matrices. When \\eta &gt; 1, the distribution favors samples with large determinants. When \\eta &lt; 1, the distribution favors samples with small determinants.\n\nP(M) \\propto |\\det(M)|^{\\eta - 1}\n\n\nArgs:\nbi.dist.lkj(\ndimension,\nconcentration=1.0,\nsample_method='onion',\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ndimension (int): The dimension of the correlation matrices.\nconcentration (ndarray): The concentration/shape parameter of the distribution (often referred to as eta). Must be positive.\nsample_method (str): Either “cvine” or “onion”. Both methods are proposed in [1] and offer the same distribution over correlation matrices. But they are different in how to generate samples. Defaults to “onion”.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI LKJ distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LKJ distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.lkj(dimension=2, concentration=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lkj\n\n\n\n\nLKJ Cholesky\nThe LKJ (Leonard-Kjærgaard-Jørgensen) Cholesky distribution is a family of distributions on symmetric matrices, often used as a prior for the Cholesky decomposition of a symmetric matrix. It is particularly useful in Bayesian inference for models with covariance structure. $$\n\nArgs:\nbi.dist.lkj_cholesky(\ndimension,\nconcentration=1.0,\nsample_method='onion',\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ndimension (int): The dimension of the correlation matrices.\nconcentration (float): A parameter controlling the concentration of the distribution around the identity matrix. Higher values indicate greater concentration. Must be greater than 1.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\nAttributes: concentration (float): The concentration parameter.\n\n\n\n\nLog-Normal\nThe LogNormal distribution is a probability distribution defined for positive real-valued random variables, parameterized by a location parameter (loc) and a scale parameter (scale). It arises when the logarithm of a random variable is normally distributed.\n   f(x) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-\\frac{(log(x) - \\mu)^2}{2\\sigma^2}}\n\n\nArgs:\nbi.dist.log_normal(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (float): Location parameter.\nsample (float): Scale parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI LogNormal distribution object (for model building). JAX array of samples drawn from the LogNormal distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.log_normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#lognormal\n\n\n\n\nLog-Uniform\nSamples from a LogUniform distribution.\nThe LogUniform distribution is defined over the positive real numbers and is the result of applying an exponential transformation to a uniform distribution over the interval [low, high]. It is often used when modeling parameters that must be positive.\n   f(x) = \\frac{1}{(high - low) \\log(high / low)}\n\\text{ for } low \\le x \\le high\n\n\nArgs:\nbi.dist.log_uniform(\nlow,\nhigh,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlow (jnp.ndarray): The lower bound of the uniform distribution’s log-space. Must be positive.\nhigh (jnp.ndarray): The upper bound of the uniform distribution’s log-space. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI LogUniform distribution object (for model building) when sample=False.\nJAX array of samples drawn from the LogUniform distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.log_uniform(low=0.1, high=10.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#loguniform\n\n\n\n\nLogistic\nSamples from a Logistic distribution.\nThe Logistic distribution is a continuous probability distribution defined by two parameters: location and scale. It is often used to model growth processes and is closely related to the normal distribution.\n   f(x) = \\frac{1}{s} \\exp\\left(-\\frac{(x - \\mu)}{s}\\right)\n\n\nArgs:\nbi.dist.logistic(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray or float): The location parameter, specifying the median of the distribution. Defaults to 0.0.\nsample (jnp.ndarray or float): The scale parameter, which determines the spread of the distribution. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Logistic distribution object (for model building) when sample=False. JAX array of samples drawn from the Logistic distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.logistic(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#logistic\n\n\n\n\nLow Rank Multivariate Normal\nRepresents a multivariate normal distribution with a low-rank covariance structure.\n\np(x) = \\frac{1}{\\sqrt{(2\\pi)^K |\\Sigma|}}\n\\exp\\left(-\\tfrac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)\n\nwhere:\n\nx is a vector of observations.\n\\mu is the mean vector.\n\\Sigma is the covariance matrix, represented in a low-rank form.\n\nParameters: - loc (jnp.ndarray): Mean vector.\ncov_factor (jnp.ndarray): Matrix used to construct the covariance matrix.\ncov_diag (jnp.ndarray): Diagonal elements of the covariance matrix.\n\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\nExample Usage:\nfrom BI import bi m = bi(‘cpu’) event_size = 100 # Our distribution has 100 dimensions rank = 5\nm.dist.low_rank_multivariate_normal( - loc=m.dist.normal(0,1, shape = (event_size,), sample=True)2, cov_factor=m.dist.normal(0,1, shape = (event_size, rank), sample=True), cov_diag=jnp.exp(m.dist.normal(0,1, shape = (event_size,), sample=True))  0.1, sample=True )\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lowrankmultivariatenormal\nbi.dist.low_rank_multivariate_normal(\nloc,\ncov_factor,\ncov_diag,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\n\n\n\nLower Truncated Power Law\nLower truncated power law distribution with \\alpha index.\nThe probability density function (PDF) is given by:\n\nf(x; \\alpha, a) = (-\\alpha-1)a^{-\\alpha - 1}x^{-\\alpha},\n\\qquad x \\geq a, \\qquad \\alpha &lt; -1,\n\nwhere a is the lower bound.\n\nArgs:\nbi.dist.lower_truncated_power_law(\nalpha,\nlow,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nalpha (jnp.ndarray): index of the power law distribution. Must be less than -1. low (jnp.ndarray): lower bound of the distribution. Must be greater than 0.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI LowerTruncatedPowerLaw distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LowerTruncatedPowerLaw distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.lower_truncated_power_law(alpha=-2.0, low=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lowertruncatedpowerlaw\n\n\n\n\nMatrix Normal\nSamples from a Matrix Normal distribution, which is a multivariate normal distribution over matrices. The distribution is characterized by a location matrix and two lower triangular matrices that define the correlation structure. The distribution is related to the multivariate normal distribution in the following way. If X ~ MN(loc,U,V) then vec(X) ~ MVN(vec(loc), kron(V,U) ).\n\np(x) = \\frac{1}{2\\pi^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)\n\n\nArgs:\nbi.dist.matrix_normal(\nloc,\nscale_tril_row,\nscale_tril_column,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (array_like): Location of the distribution.\n\nscale_tril_row (array_like): Lower cholesky of rows correlation matrix.\nscale_tril_column (array_like): Lower cholesky of columns correlation matrix.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI MatrixNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MatrixNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')            \nn_rows, n_cols = 3, 4\n\n- *loc* = jnp.zeros((n_rows, n_cols))\nU_row_cov = jnp.array([[1.0, 0.5, 0.2],\n[0.5, 1.0, 0.3],\n[0.2, 0.3, 1.0]])\nscale_tril_row = jnp.linalg.cholesky(U_row_cov)\n\nV_col_cov = jnp.array([[2.0, -0.8, 0.1, 0.4],\n[-0.8, 2.0, 0.2, -0.2],\n[0.1, 0.2, 2.0, 0.0],\n[0.4, -0.2, 0.0, 2.0]])\n\n# The argument passed to the distribution is its Cholesky factor\nscale_tril_column = jnp.linalg.cholesky(V_col_cov)\n\nm.dist.matrix_normal(\noc=loc, \nscale_tril_row=scale_tril_row, \nscale_tril_column=scale_tril_column, \nsample=True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#matrixnormal_lowercase\n\n\n\n\nA marginalized finite mixture of component distributions.\nThis distribution represents a mixture of component distributions, where the mixing weights are determined by a Categorical distribution. The resulting distribution can be either a MixtureGeneral (when component distributions are a list) or a MixtureSameFamily (when component distributions are a single distribution).\n   p(x) = \\sum_{i=1}^{K} w_i p_i(x)\n\n\nArgs:\nbi.dist.mixture(\nmixing_distribution,\ncomponent_distributions,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Mixture distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Mixture distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom jax import random\nimport BI as pyro\nm = pyro.distributions.Mixture(\npyro.distributions.Categorical(torch.ones(2)),\n[pyro.distributions.Normal(0, 1), pyro.distributions.Normal(2, 1)]\n)\nsamples = m.sample(sample_shape=(10,))\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixture\n\n\n\n\nMixture General\nA finite mixture of component distributions from different families.\n\nmixing_distribution: A :class:~numpyro.distributions.Categorical specifying the weights for each mixture component. The size of this distribution specifies the number of components in the mixture.\ncomponent_distributions: A list of mixture_size :class:~numpyro.distributions.Distribution objects.\nsupport: A :class:~numpyro.distributions.constraints.Constraint object specifying the support of the mixture distribution. If not provided, the support will be inferred from the component distributions.\n\nThe probability density function (PDF) of a MixtureGeneral distribution is given by:\np(x) = \\sum_{i=1}^{K} \\pi_i p_i(x)\n where:\n\nK is the number of components in the mixture.\n\\pi_i is the mixing weight for the i-th component, such that \\sum_{i=1}^{K} \\pi_i = 1.\np_i(x) is the probability density function of the i-th component distribution.\n\nParameters:\n\nmixing_distribution: A Categorical distribution representing the mixing weights.\ncomponent_distributions: A list of distributions representing the components of the mixture.\n**sample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n#### Returns:\n\nWhen sample=False: A BI MixtureGeneral distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MixtureGeneral distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n#### Example Usage:\nfrom BI import bi m = bi(‘cpu’) m.dist.mixture_general( mixing_distribution=m.dist.categorical(probs=jnp.array([0.3, 0.7]), create_obj = True),\ncomponent_distributions=[m.dist.normal(loc=0.0, scale=1.0, create_obj=True),m.dist.normal (loc=0.0, scale=1.0, create_obj=True)], sample = True )\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixturegeneral\nbi.dist.mixture_general(\nmixing_distribution,\ncomponent_distributions,\nsupport=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\n\n\n\nFinite mixture of component distributions from the same family.\nThis mixture only supports a mixture of component distributions that are all of the same family. The different components are specified along the last batch dimension of the input component_distribution. If you need a mixture of distributions from different families, use the more general implementation in :class:~numpyro.distributions.MixtureGeneral.\n   p(x) = \\sum_{k=1}^{K} w_k p_k(x)\n\nwhere:\n\nK is the number of mixture components.\nw_k is the mixing weight for component k.\np_k(x) is the probability density function (PDF) of the k-th component distribution.\n\n\nArgs:\nbi.dist.mixture_same_family(\nmixing_distribution,\ncomponent_distribution,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nDistribution Args:\nmixing_distribution: A Categorical distribution representing the mixing weights.\ncomponent_distributions: A list of distributions representing the components of the mixture.\nSampling / Modeling Args:\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\n**sample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI MixtureSameFamily distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MixtureSameFamily distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.mixture_same_family(\nmixing_distribution=m.dist.categorical(probs=jnp.array([0.3, 0.7]), create_obj = True), \ncomponent_distribution=m.dist.normal(loc=0.0, scale=1.0, shape = (2,), create_obj=True),\nsample = True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixture-same-family\n\n\n\n\nMultinomial\nSamples from a Multinomial distribution, which models the probability of different outcomes in a sequence of independent trials, each with a fixed number of trials and a fixed set of possible outcomes. It generalizes the binomial distribution to multiple categories.\n   P(X = x) = \\frac{n!}{x_1! x_2! \\cdots x_k!} p_1^{x_1} p_2^{x_2} \\cdots p_k^{x_k}\n\n\nArgs:\nbi.dist.multinomial(\ntotal_count=1,\nprobs=None,\nlogits=None,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntotal_count (int or jnp.ndarray): The number of trials.\n\nprobs (jnp.ndarray, optional): Event probabilities. Must sum to 1.\n\nlogits (jnp.ndarray, optional): Event log probabilities.\ntotal_count_max (int, optional): An optional integer providing an upper bound on total_count. This is used for performance optimization with lax.scan when total_count is a dynamic JAX tracer, helping to avoid recompilation.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Multinomial distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Multinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial(total_count=10, probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#multinomial\n\n\n\n\nMultinomial Logits\nSamples from a MultinomialLogits distribution.\nThis distribution represents the probability of observing a specific outcome from a multinomial experiment, given the logits for each outcome. The logits are the natural logarithm of the odds of each outcome.\n   P(k | \\mathbf{\\pi}) = \\frac{n!}{k! (n-k)!} \\prod_{i=1}^k \\pi_i\n\n\nArgs:\nbi.dist.multinomial_logits(\nlogits,\ntotal_count=1,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlogits (jnp.ndarray): Logits for each outcome. Must be at least one-dimensional.\ntotal_count (jnp.ndarray): The total number of trials.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultinomialLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultinomialLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial_logits(logits=jnp.array([1.0, 0.5], dtype=jnp.float32), total_count=jnp.array(5, dtype=jnp.int32), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multinomiallogits\n\n\n\n\nMultinomial Probs\nSamples from a Multinomial distribution.\nThe Multinomial distribution models the number of times each of several discrete outcomes occurs in a fixed number of trials. Each trial independently results in one of several outcomes, and each outcome has a probability of occurring.\n   P(X = x) = \\frac{n!}{x_1! x_2! \\cdots x_k!} p_1^{x_1} p_2^{x_2} \\cdots p_k^{x_k}\n\nwhere:\n\nn is the total number of trials.\nx is a vector of counts for each outcome.\np is a vector of probabilities for each outcome. $$\n\n\nArgs:\nbi.dist.multinomial_probs(\nprobs,\ntotal_count=1,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nprobs (jnp.ndarray): Vector of probabilities for each outcome. Must sum to 1. total_count (jnp.ndarray): The number of trials.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI MultinomialProbs distribution object (for model building). JAX array of samples drawn from the MultinomialProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial_probs(probs=jnp.array([0.2, 0.3, 0.5]), total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multinomialprobs\n\n\n\n\nMultivariate Normal\nThe Multivariate Normal distribution, also known as the Gaussian distribution in multiple dimensions, is a probability distribution that arises frequently in statistics and machine learning. It is defined by its mean vector and covariance matrix, which describe the central tendency and spread of the distribution, respectively.\n\np(x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)\n\nwhere: - x is a n-dimensional vector of random variables. - \\mu is the mean vector. - \\Sigma is the covariance matrix.\n\nArgs:\nbi.dist.multivariate_normal(\nloc=0.0,\ncovariance_matrix=None,\nprecision_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (tuple): The mean vector of the distribution.\n\ncovariance_matrix (jnp.ndarray, optional): The covariance matrix of the distribution. Must be positive definite.\nprecision_matrix (jnp.ndarray, optional): The precision matrix (inverse of the covariance matrix) of the distribution. Must be positive definite.\nscale_tril (jnp.ndarray, optional): The lower triangular Cholesky decomposition of the covariance matrix.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultivariateNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultivariateNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multivariate_normal(\n- *loc*=jnp.array([1.0, 0.0, -2.0]), \ncovariance_matrix=jnp.array([[ 2.0,  0.7, -0.3],\n[ 0.7,  1.0,  0.5],\n[-0.3,  0.5,  1.5]]), \nsample=True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#multivariate-normal\n\n\n\n\nMultivariate Student’s t\nThe Multivariate Student’s t distribution is a generalization of the Student’s t distribution to multiple dimensions. It is a heavy-tailed distribution that is often used to model data that is not normally distributed.\n\np(x) = \\frac{1}{B(df/2, n/2)} \\frac{\\Gamma(df/2 + n/2)}{\\Gamma(df/2)}\n\\left(1 + \\frac{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}{df}\\right)^{-(df + n)/2}\n\n\nArgs:\nbi.dist.multivariate_student_t(\ndf,\nloc=0.0,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\ndf (jnp.ndarray): Degrees of freedom, must be positive.\nloc (jnp.ndarray): Location vector, representing the mean of the distribution. scale_tril (jnp.ndarray): Lower triangular matrix defining the scale.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultivariateStudentT distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultivariateStudentT distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\nm.dist.multivariate_student_t(\ndf = 2,\n- *loc*=jnp.array([1.0, 0.0, -2.0]), \nscale_tril=jnp.linalg.cholesky(\njnp.array([[ 2.0,  0.7, -0.3],\n[ 0.7,  1.0,  0.5],\n[-0.3,  0.5,  1.5]])), \nsample=True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multivariatestudentt\n\n\n\n\nNegative Binomial\nThe NegativeBinomial distribution models the number of failures before the first success in a sequence of independent Bernoulli trials. It is characterized by two parameters: ‘total_count’ (r) and ‘probs’ or ‘logits’ (p).\nP(k) = \\binom{k+r-1}{r-1} p^r (1-p)^k\n\n\nArgs:\nbi.dist.negative_binomial(\ntotal_count,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntotal_count (jnp.ndarray): The total number of events.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI NegativeBinomial distribution object (for model building). JAX array of samples drawn from the NegativeBinomial distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial(total_count=5.0,probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#negativebinomial\n\n\n\n\nNegative Binomial Logits\nSamples from a Negative Binomial Logits distribution.\nThe Negative Binomial Logits distribution is a generalization of the Negative Binomial distribution where the parameter ‘r’ (number of successes) is expressed as a function of a logit parameter. This allows for more flexible modeling of count data.\n\nP(k) = \\frac{e^{-n \\cdot \\text{softplus}(x)} \\cdot \\text{softplus}(-x)^k}{k!}\n\n\nArgs:\nbi.dist.negative_binomial_logits(\ntotal_count,\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntotal_count (jnp.ndarray): The parameter controlling the shape of the distribution. Represents the total number of trials.\nlogits (jnp.ndarray): The log-odds parameter. Related to the probability of success.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nNegative Binomial Logits: A BI Negative Binomial Logits distribution object (for model building).\njnp.ndarray: A JAX array of samples drawn from the Negative Binomial Logits distribution (for direct sampling).\nNegative Binomial Logits: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial_logits(total_count=5.0, logits=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#Negative Binomial Logits\n\n\n\n\nNegative Binomial with probabilities.\nThe Negative Binomial distribution models the number of failures before the first success in a sequence of independent Bernoulli trials. It is characterized by two parameters: ‘concentration’ (r) and ‘rate’ (p). In this implementation, the ‘concentration’ parameter is derived from ‘total_count’ and the ‘rate’ parameter is derived from ‘probs’.\n\nP(k) = \\binom{k+r-1}{r-1} p^r (1-p)^k\n\n\nArgs:\nbi.dist.negative_binomial_probs(\ntotal_count,\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntotal_count (jnp.ndarray): A numeric vector, matrix, or array representing the parameter.\n\nprobs (jnp.ndarray): A numeric vector representing event probabilities. Must sum to 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI NegativeBinomialProbs distribution object (for model building). JAX array of samples drawn from the NegativeBinomialProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial_probs(total_count=10.0, probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#negativebinomialprobs\n\n\n\n\nNormal\nSamples from a Normal (Gaussian) distribution.\nThe Normal distribution is characterized by its mean (loc) and standard deviation (scale). It’s a continuous probability distribution that arises frequently in statistics and probability theory.\n   p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\n\nArgs:\nbi.dist.normal(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (jnp.ndarray): The mean of the distribution.\nsample (jnp.ndarray): The standard deviation of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Normal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Normal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#normal\n\n\n\n\nOrdered Logistic\nA categorical distribution with ordered outcomes. This distribution represents the probability of an event falling into one of several ordered categories, based on a predictor variable and a set of cutpoints. The probability of an event falling into a particular category is determined by the number of categories above it.\n   P(Y = k) = \\begin{cases}\n1 & \\text{if } k = 0 \\\\\n\\frac{1}{k} & \\text{if } k &gt; 0\n\\end{cases}\n\n\nArgs:\nbi.dist.ordered_logistic(\npredictor,\ncutpoints,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\npredictor (jnp.ndarray): Prediction in real domain; typically this is output of a linear model.\ncutpoints (jnp.ndarray): Positions in real domain to separate categories.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI OrderedLogistic distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the OrderedLogistic distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.ordered_logistic(predictor=jnp.array([0.2, 0.5, 0.8]), cutpoints=jnp.array([-1.0, 0.0, 1.0]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#orderedlogistic\n\n\n\n\nPareto\nSamples from a Pareto distribution.\nThe Pareto distribution is a power-law probability distribution that is often used to model income, wealth, and the size of cities. It is defined by two parameters: alpha (shape) and scale.\n\nf(x) = \\frac{\\alpha \\cdot \\text{scale}^{\\alpha}}{x^{\\alpha + 1}}\n\\text{ for } x \\geq \\text{scale}\n\n\nArgs:\nbi.dist.pareto(\nscale,\nalpha,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (jnp.ndarray or float): Scale parameter of the Pareto distribution. Must be positive.\n\nalpha (jnp.ndarray or float): Shape parameter of the Pareto distribution. Must be positive.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Pareto distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Pareto distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.pareto(scale=2.0, alpha=3.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#pareto\n\n\n\n\nPoisson\nCreates a Poisson distribution, a discrete probability distribution that models the number of events occurring in a fixed interval of time or space if these events occur with a known average rate and independently of the time since the last event.\n  \\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}\n\n\nArgs:\nbi.dist.poisson(\nrate,\nis_sparse=False,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nrate (jnp.ndarray): The rate parameter, representing the average number of events. is_sparse (bool, optional): Indicates whether the rate parameter is sparse. If True, a specialized sparse sampling implementation is used, which can be more efficient for models with many zero-rate components (e.g., zero-inflated models). Defaults to False.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Poisson distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Poisson distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.poisson(rate=2.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#poisson\n\n\n\n\nProjected Normal\nThis distribution over directional data is qualitatively similar to the von Mises and von Mises-Fisher distributions, but permits tractable variational inference via reparametrized gradients.\n   p(x) = \\frac{1}{Z} \\exp\\left(-\\frac{1}{2\\sigma^2} ||x - \\mu||^2\\right)\n\n\nArgs:\nbi.dist.projected_normal(\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): The concentration parameter, representing the direction towards which the samples are concentrated. Must be a JAX array with at least one dimension.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI ProjectedNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ProjectedNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.projected_normal(concentration=jnp.array([1.0, 3.0, 2.0]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#projectednormal\n\n\n\n\nRelaxed Bernoulli\nThe Relaxed Bernoulli distribution is a continuous relaxation of the discrete Bernoulli distribution. It’s useful for variational inference and other applications where a differentiable approximation of the Bernoulli is needed. The probability density function (PDF) is defined as:\n\np(x) = \\frac{1}{2} \\left( 1 + \\tanh\\left(\\frac{x - \\beta \\log(\\frac{p}{1-p})}{1}\\right) \\right)\n\n\nArgs:\nbi.dist.relaxed_bernoulli(\ntemperature,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntemperature (float): The temperature parameter.\n\nprobs (jnp.ndarray, optional): The probability of success. Must be in the interval [0, 1]. Only one of probs or logits can be specified.\n\nlogits (jnp.ndarray, optional): The log-odds of success. Only one of probs or logits can be specified.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI RelaxedBernoulli distribution object (for model building) when sample=False. A JAX array of samples drawn from the RelaxedBernoulli distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.relaxed_bernoulli(temperature=1.0, probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#relaxedbernoulli\n\n\n\n\nRelaxed Bernoulli Logits\nRepresents a relaxed version of the Bernoulli distribution, parameterized by logits and a temperature. The temperature parameter controls the sharpness of the distribution. The distribution is defined by transforming the output of a Logistic distribution through a sigmoid function.\n\nP(x) = \\sigma\\left(\\frac{x}{\\text{temperature}}\\right)\n\n\nArgs:\nbi.dist.relaxed_bernoulli_logits(\ntemperature,\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ntemperature (jnp.ndarray): The temperature parameter, must be positive. logits (jnp.ndarray): The logits parameter.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nRelaxedBernoulliLogits: A BI RelaxedBernoulliLogits distribution object (for model building). jnp.ndarray: A JAX array of samples drawn from the RelaxedBernoulliLogits distribution (for direct sampling). RelaxedBernoulliLogits: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.relaxed_bernoulli_logits(temperature=1.0, logits=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#relaxed-bernoulli-logits\n\n\n\n\nRight Truncated\nSamples from a right-truncated distribution.\nThis distribution truncates the base distribution at a specified high value. Values greater than high are discarded, effectively creating a distribution that is only supported up to that point. This is useful for modeling data where observations are only possible within a certain range.\nThe probability density function (PDF) of the truncated distribution is:\n\nf_{\\text{trunc}}(x) = \\frac{f_{\\text{base}}(x)}{F_{\\text{base}}(\\text{high})} \\quad \\text{for } x \\le \\text{high}\n\nwhere f_{\\text{base}}(x) is the PDF of the base distribution and F_{\\text{base}}(\\text{high}) is the cumulative distribution function (CDF) of the base distribution evaluated at high.\nwhere f(x) is the probability density function (PDF) of the base distribution and P(X \\le high) is the cumulative distribution function (CDF) of the base distribution evaluated at high. $$\n\nArgs:\nbi.dist.right_truncated_distribution(\nbase_dist,\nhigh=0.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbase_dist: The base distribution to truncate. Must be a univariate distribution with real support.\nhigh (float, jnp.ndarray, optional): The upper truncation point. The support of the new distribution is (-\\infty, \\text{high}]. Defaults to 0.0.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI RightTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the RightTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.right_truncated_distribution(base_dist = m.dist.normal(0,1, create_obj = True), high=0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#righttruncateddistribution\n\n\n\n\nSine Bivariate Von Mises\nA unimodal distribution for two dependent angles on the 2-torus (S^1 \\otimes S^1), which is useful for modeling coupled angles like torsion angles in peptide chains. [1]\nThe probability density function is given by:\n\nC^{-1}\\exp(\\kappa_1\\cos(x_1-\\mu_1) + \\kappa_2\\cos(x_2 -\\mu_2) + \\rho\\sin(x_1 - \\mu_1)\\sin(x_2 - \\mu_2))\n\nwhere the normalization constant C is:\n\nC = (2\\pi)^2 \\sum_{i=0}^{\\infty} \\binom{2i}{i} \\left(\\frac{\\rho^2}{4\\kappa_1\\kappa_2}\\right)^i I_i(\\kappa_1)I_i(\\kappa_2)\n\nHere, I_i(\\cdot) is the modified Bessel function of the first kind, \\mu’s are the locations, \\kappa’s are the concentrations, and \\rho represents the correlation between the angles x_1 and x_2.\n\nArgs:\nbi.dist.sine_bivariate_vonmises(\nphi_loc,\npsi_loc,\nphi_concentration,\npsi_concentration,\ncorrelation=None,\nweighted_correlation=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nphi_loc (jnp.ndarray): The location parameter for the first angle (phi).\npsi_loc (jnp.ndarray): The location parameter for the second angle (psi).\nphi_concentration (jnp.ndarray): The concentration parameter for the first angle (phi). Must be positive.\npsi_concentration (jnp.ndarray): The concentration parameter for the second angle (psi). Must be positive.\ncorrelation (jnp.ndarray, optional): The correlation parameter between the two angles. One of correlation or weighted_correlation must be specified.\nweighted_correlation (jnp.ndarray, optional): An alternative correlation parameter. One of correlation or weighted_correlation must be specified.\nvalidate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\nBI.primitives.Messenger: A BI sample site object when used in a model context (sample=False). jnp.ndarray: A JAX array of samples drawn from the SineBivariateVonMises distribution (for direct sampling, sample=True). numpyro.distributions.Distribution: The raw BI distribution object (if create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling\nsamples = m.dist.sine_bivariate_vonmises(\nphi_loc=0.0,\npsi_loc=jnp.pi,\nphi_concentration=1.0,\npsi_concentration=1.0,\ncorrelation=0.5,\nsample=True,\n- *shape*=(10,)\n)\n\n# Usage within a model\ndef my_model():\nangles = m.dist.sine_bivariate_vonmises(\nphi_loc=0.0,\npsi_loc=0.0,\nphi_concentration=2.0,\npsi_concentration=2.0,\nweighted_correlation=0.9,\nname='angles'\n)\n# ... rest of the model\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#sinebivariatevonmises\n\n\n\n\nSine-skewing\nSine-skewing [1] is a procedure for producing a distribution that breaks pointwise symmetry on a torus distribution. The new distribution is called the Sine Skewed X distribution, where X is the name of the (symmetric) base distribution. Torus distributions are distributions with support on products of circles (i.e., \\otimes S^1 where S^1 = [-pi,pi)). So, a 0-torus is a point, the 1-torus is a circle, and the 2-torus is commonly associated with the donut shape.\n.. note: This distribution is available in BI: https://num.pyro.ai/en/stable/distributions.html#sineskewed\nParameters:\n\nbase_dist: Base density on a d-dimensional torus. Supported base distributions include: 1D :class:~numpyro.distributions.VonMises, :class:~numnumpyro.distributions.SineBivariateVonMises, 1D :class:~numpyro.distributions.ProjectedNormal, and :class:~numpyro.distributions.Uniform (-pi, pi).\nskewness: Skewness of the distribution.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\nPDF:\nThe probability density function (PDF) of the Sine Skewed X distribution is not explicitly defined here, but it is derived from the base distribution and the skewness parameter.\n\nExample Usage:\nfrom num.pyro import distributions as dist import num.pyro as pyro import num.numpy as np\nm = pyro.distributions.Normal(loc=0.0, scale=1.0) skewness = np.array([0.5, 0.5]) sine_skewed = dist.SineSkewed(base_dist=m, skewness=skewness) samples = sine_skewed.sample((1000,))\nbi.dist.sine_skewed(\nbase_dist: numpyro.distributions.distribution.Distribution,\nskewness,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\n\n\n\nSoftLaplace\nSamples from a SoftLaplace distribution.\nThis distribution is a smooth approximation of a Laplace distribution, characterized by its log-convex density. It offers Laplace-like tails while being infinitely differentiable, making it suitable for HMC and Laplace approximation.\n\nf(x) = \\log\\!\\left(\\tfrac{2}{\\pi}\\right) - \\log(\\text{scale})\n- \\log\\!\\left( e^{\\tfrac{x - \\text{loc}}{\\text{scale}}} + e^{-\\tfrac{x - \\text{loc}}{\\text{scale}}} \\right)\n\n\nArgs:\nbi.dist.soft_laplace(\nloc,\nscale,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc: Location parameter. scale: Scale parameter. $$\n\n\n\nArgs:\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI SoftLaplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the SoftLaplace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.soft_laplace(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#softlaplace\n\n\n\n\nStudent’s t\nThe Student’s t-distribution is a probability distribution that arises in hypothesis testing involving the mean of a normally distributed population when the population standard deviation is unknown. It is similar to the normal distribution, but has heavier tails, making it more robust to outliers.\n\nf(x) = \\frac{1}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}\n\n\nArgs:\nbi.dist.student_t(\ndf,\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\ndf (jnp.ndarray): Degrees of freedom, must be positive. - loc (jnp.ndarray): Location parameter, defaults to 0.0. - sample (jnp.ndarray): Scale parameter, defaults to 1.0.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI StudentT distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the StudentT distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.student_t(df = 2, loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#studentt\n\n\n\n\nTruncated Cauchy\nThe Cauchy distribution, also known as the Lorentz distribution, is a continuous probability distribution that appears frequently in various areas of mathematics and physics. It is characterized by its heavy tails, which extend to infinity. The truncated version limits the support of the Cauchy distribution to a specified interval.\n\nf(x) = \\frac{1}{\\pi \\cdot c \\cdot (1 + ((x - b) / c)^2)}  \\text{ for } a &lt; x &lt; b\n\n\nArgs:\nbi.dist.truncated_cauchy(\nloc=0.0,\nscale=1.0,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (float): Location parameter of the Cauchy distribution.\nsample (float): Scale parameter of the Cauchy distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI TruncatedCauchy distribution object (for model building) when sample=False.\nJAX array of samples drawn from the TruncatedCauchy distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_cauchy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncatedcauchy\n\n\n\n\nTruncated\nSamples from a Truncated Distribution.\nThis distribution represents a base distribution truncated between specified lower and upper bounds. The truncation modifies the probability density function (PDF) of the base distribution, effectively removing observations outside the defined interval.\n   p(x) = \\frac{p(x)}{P(\\text{lower} \\le x \\le \\text{upper})}\n\n\nArgs:\nbi.dist.truncated_distribution(\nbase_dist,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbase_dist: The base distribution to be truncated. This should be a univariate distribution. Currently, only the following distributions are supported: Cauchy, Laplace, Logistic, Normal, and StudentT.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI TruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the TruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_distribution(base_dist = m.dist.normal(0,1, create_obj = True), high=1, low = 0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncateddistribution\n\n\n\n\nTruncated Normal\nThe Truncated Normal distribution is a normal distribution truncated to a specified interval. It is defined by its location (loc), scale (scale), lower bound (low), and upper bound (high).\n\nf(x) = \\frac{p(x)}{\\alpha}, \\quad x \\in [\\text{low}, \\text{high}]\n\nwhere\n\np(x) = \\frac{1}{\\text{scale}\\,\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\tfrac{1}{2}\\left(\\tfrac{x - \\text{loc}}{\\text{scale}}\\right)^2\\right),\n\nand\n\n\\alpha = \\int_{\\text{low}}^{\\text{high}} p(x)\\,dx.\n\n\nArgs:\nbi.dist.truncated_normal(\nloc=0.0,\nscale=1.0,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nloc (float): The location parameter of the normal distribution.\nsample (float): The scale parameter of the normal distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI TruncatedNormal distribution object (for model building). JAX array of samples drawn from the TruncatedNormal distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncatednormal_lowercase\n\n\n\n\nTruncated PolyaGamma\nSamples from a Truncated PolyaGamma distribution.\nThis distribution is a truncated version of the PolyaGamma distribution, defined over the interval [0, truncation_point]. It is often used in Bayesian non-parametric models.\n   p(x) = \\frac{1}{Z} \\exp\\left( \\sum_{n=0}^{N} \\left( \\log(2n+1) - 1.5 \\log(x) - \\frac{(2n+1)^2}{4x} \\right) \\right)\n\n\nArgs:\nbi.dist.truncated_polya_gamma(\nbatch_shape=(),\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbatch_shape (tuple): The shape of the batch dimension.\n\nevent (int): The number of batch dimensions to reinterpret as event dimensions.\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Truncated PolyaGamma distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Truncated PolyaGamma distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_polya_gamma(batch_shape=(), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#truncatedpolygammadistribution\n\n\n\n\nTwo Sided Truncated\nThis distribution truncates a base distribution between two specified lower and upper bounds.\n\nf(x) =\n\\begin{cases}\n\\dfrac{p(x)}{P(\\text{low} \\le X \\le \\text{high})}, & \\text{if } \\text{low} \\le x \\le \\text{high}, \\\\[6pt]\n0, & \\text{otherwise}.\n\\end{cases}\n\nwhere p(x) is the probability density function of the base distribution.\n\nArgs:\nbi.dist.two_sided_truncated_distribution(\nbase_dist,\nlow=0.0,\nhigh=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbase_dist: The base distribution to truncate.\nlow: The lower bound for truncation.\nhigh: The upper bound for truncation.\n\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI TwoSidedTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the TwoSidedTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#twosidedtruncateddistribution\n\n\n\n\nUniform\nSamples from a Uniform distribution, which is a continuous probability distribution where all values within a given interval are equally likely.\n   f(x) = \\frac{1}{b - a}, \\text{ for } a \\le x \\le b\n\n\nArgs:\nbi.dist.uniform(\nlow=0.0,\nhigh=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlow (jnp.ndarray): The lower bound of the uniform interval.\nhigh (jnp.ndarray): The upper bound of the uniform interval.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Uniform distribution object (for model building) when sample=False.\nJAX array of samples drawn from the Uniform distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.uniform(low=0.0, high=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#uniform\n\n\n\n\nUnit\nThe Unit distribution is a trivial, non-normalized distribution representing the unit type. It has a single value with no data, effectively a placeholder often used in probabilistic programming for situations where no actual data is involved.\n\np(x) = 1\n\n\nArgs:\nbi.dist.unit(\nlog_factor,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nlog_factor (jnp.ndarray): Log factor for the unit distribution. This parameter determines the - shape and batch size of the distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Unit distribution object: When sample=False (for model building). jnp.ndarray: A JAX array of samples drawn from the Unit distribution (for direct sampling). BI Unit distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.unit(log_factor=jnp.ones(5), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#unit\n\n\n\n\nWeibull\nSamples from a Weibull distribution.\nThe Weibull distribution is a versatile distribution often used to model failure rates in engineering and reliability studies. It is characterized by its shape and scale parameters.\n\nf(x) = \\frac{\\beta}{\\alpha} \\left(\\frac{x}{\\alpha}\\right)^{\\beta - 1} e^{-\\left(\\frac{x}{\\alpha}\\right)^{\\beta}} \\text{ for } x \\ge 0\n\nwhere \\alpha is the scale parameter and \\beta is the shape parameter.\n\nArgs:\nbi.dist.weibull(\nscale,\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (jnp.ndarray): The scale parameter of the Weibull distribution. Must be positive. concentration (jnp.ndarray): The shape parameter of the Weibull distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI Weibull distribution object (for model building) when sample=False. JAX array of samples drawn from the Weibull distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.weibull(scale=1.0, concentration=2.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#weibull\n\n\n\n\nWishart\nThe Wishart distribution is a multivariate distribution used to model positive definite matrices, often representing covariance matrices. It’s commonly used in Bayesian statistics and machine learning, particularly in models involving covariance estimation.\n   p(X) = \\frac{1}{W^{p/2} \\Gamma_p(concentration/2)} \\left|X\\right|^{-concentration/2} \\exp\\left(-\\frac{1}{2} \\text{tr}(X^{-1} X)\\right)\n\n\nArgs:\nbi.dist.wishart(\nconcentration,\nscale_matrix=None,\nrate_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nconcentration (jnp.ndarray): Positive concentration parameter analogous to the concentration of a :class:Gamma distribution. The concentration must be larger than the dimensionality of the scale matrix.\nscale_matrix (jnp.ndarray, optional): Scale matrix analogous to the inverse rate of a :class:Gamma distribution.\nrate_matrix (jnp.ndarray, optional): Rate matrix anaologous to the rate of a :class:Gamma distribution.\nscale_tril (jnp.ndarray, optional): Cholesky decomposition of the :code:scale_matrix.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI Wishart distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Wishart distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.wishart(concentration=5.0, scale_matrix=jnp.eye(2), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#wishart\n\n\n\n\nWishart Cholesky\nThe Wishart distribution is a multivariate distribution used as a prior distribution for covariance matrices. This implementation represents the distribution in terms of its Cholesky decomposition.\n.. rubric:: Probability Density Function\nThe probability density function (PDF) is given by:\nPDF = (1 / ((2 * pi)^(k * (k - 1) / 2) * Gamma(k/2))  (concentration^(k/2)  det(scale_matrix))^(-1/2)  exp(-1/2  trace(rate_matrix @ scale_matrix)))\nwhere:\n\nk is the dimensionality of the covariance matrix.\nconcentration is a positive concentration parameter.\nscale_matrix is the scale matrix.\nrate_matrix is the rate matrix.\nGamma is the gamma function.\n\n.. rubric:: Parameters\n\nconcentration: (Tensor) Positive concentration parameter analogous to the concentration of a :class:Gamma distribution. The concentration must be larger than the dimensionality of the scale matrix.\nscale_matrix: (Tensor, optional) Scale matrix analogous to the inverse rate of a :class:Gamma distribution. If not provided, rate_matrix or scale_tril must be.\nrate_matrix: (Tensor, optional) Rate matrix anaologous to the rate of a :class:Gamma distribution. If not provided, scale_matrix or scale_tril must be.\nscale_tril: (Tensor, optional) Cholesky decomposition of the :code:scale_matrix. If not provided, scale_matrix or rate_matrix must be.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\nbi.dist.wishart_cholesky(\nconcentration,\nscale_matrix=None,\nrate_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\n\n\nGeneric Zero Inflated\nA Zero-Inflated distribution combines a base distribution with a Bernoulli distribution to model data with an excess of zero values. It assumes that each observation is either drawn from the base distribution or is a zero with probability determined by the Bernoulli distribution (the “gate”). This is useful for modeling data where zeros are more frequent than expected under a single distribution, often due to a different underlying process.\n\nP(x) = \\pi \\cdot I(x=0) + (1 - \\pi) \\cdot P_{base}(x)\nwhere: - P_{base}(x) is the probability density function (PDF) or probability mass function (PMF) of the base distribution. - \\pi is the probability of generating a zero, governed by the Bernoulli gate. - I(x=0) is an indicator function that equals 1 if x=0 and 0 otherwise.\n\nArgs:\nbi.dist.zero_inflated_distribution(\nbase_dist,\ngate=None,\ngate_logits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nbase_dist (Distribution): The base distribution to be zero-inflated (e.g., Poisson, NegativeBinomial).\ngate (jnp.ndarray, optional): Probability of extra zeros (between 0 and 1).\ngate_logits (jnp.ndarray, optional): Log-odds of extra zeros.\nvalidate_args (bool, optional): Whether to validate parameter values. Defaults to None.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroInflatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroInflatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_distribution(base_dist=m.dist.poisson(rate=5, create_obj = True), gate = 0.3, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflateddistribution\n\n\n\n\nZero-Inflated Negative Binomial\nThis distribution combines a Negative Binomial distribution with a binary gate variable. Observations are either drawn from the Negative Binomial distribution with probability (1 - gate) or are treated as zero with probability ‘gate’. This models data with excess zeros compared to what a standard Negative Binomial distribution would predict.\n\nP(X = x) = (1 - gate) \\cdot \\frac{\\Gamma(x + \\alpha)}{\\Gamma(x + \\alpha + \\beta) \\Gamma(\\alpha)} \\left(\\frac{\\beta}{\\alpha + \\beta}\\right)^x + gate \\cdot \\delta_{x, 0}\n\n\nArgs:\nbi.dist.zero_inflated_negative_binomial2(\nmean,\nconcentration,\ngate=None,\ngate_logits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nmean (jnp.ndarray or float): The mean of the Negative Binomial 2 distribution. concentration (jnp.ndarray or float): The concentration parameter of the Negative Binomial 2 distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroInflatedNegativeBinomial2 distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroInflatedNegativeBinomial2 distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_negative_binomial2(mean=2.0, concentration=3.0, gate = 0.3, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflatednegativebinomial2\n\n\n\n\nA Zero Inflated Poisson\nThis distribution combines two Poisson processes: one with a rate parameter and another that generates only zeros. The probability of observing a zero is determined by the ‘gate’ parameter, while the probability of observing a non-zero value is governed by the ‘rate’ parameter of the underlying Poisson distribution.\n\nP(X = k) = (1 - gate) * \\frac{e^{-rate} rate^k}{k!} + gate\n\n\nArgs:\nbi.dist.zero_inflated_poisson(\ngate,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\nrate (jnp.ndarray): The rate parameter of the underlying Poisson distribution.\n\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\nBI ZeroInflatedPoisson distribution object (when sample=False). JAX array of samples drawn from the ZeroInflatedPoisson distribution (when sample=True). The raw BI distribution object (when create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_poisson(gate = 0.3, rate=2.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflatedpoisson\n\n\n\n\nZero Sum Normal\nSamples from a ZeroSumNormal distribution, which is a Normal distribution where one or more axes are constrained to sum to zero.\n\nZSN(\\sigma) = N(0, \\sigma^2 (I - \\tfrac{1}{n}J)) \\\\\n\\text{where} \\ ~ J_{ij} = 1 \\ ~ \\text{and} \\\\\nn = \\text{number of zero-sum axes}\n\n\nArgs:\nbi.dist.zero_sum_normal(\nscale,\nevent_shape,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nsample (array_like): Standard deviation of the underlying normal distribution before the zerosum constraint is enforced.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution’s batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. [7] This argument has no effect when sample=False, as randomness is handled by BI’s inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‘x’.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroSumNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroSumNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_sum_normal(scale=1.0, event_shape = (2,), sample = True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#zerosumnormal",
    "crumbs": [
      "Get started",
      "Distributions"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "Univariate Linear Regression",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\alpha, which represents the origin of the line—the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\beta, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA standard deviation term \\sigma, which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#general-principles",
    "href": "1. Linear Regression for continuous variable.html#general-principles",
    "title": "Univariate Linear Regression",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\alpha, which represents the origin of the line—the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\beta, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA standard deviation term \\sigma, which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#considerations",
    "href": "1. Linear Regression for continuous variable.html#considerations",
    "title": "Univariate Linear Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nBayesian models allow us to update our understanding of parameters conditional on an observed data set. This allow us to consider model parameter uncertainty 🛈, which quantifies our confidence or uncertainty in the parameters in a form of a posterior distribution 🛈. Therefore, we need to declare prior distributions 🛈 for each model parameter, in this case for: \\alpha, \\beta, and \\sigma.\nPrior distributions are built following these considerations:\n\nAs the data are normalized🛈 (see introduction), we can use a Normal distribution for \\alpha and \\beta, with a mean of 0 and a standard deviation of 1. This tends to be a weakly regularizing prior, and weaker priors like a Normal(0,10) are also possible.\nSince \\sigma must be strictly positive, we must use a distribution with supprt on the positive reals, such as the Exponential or Folded-Normal distribution.\n\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without depending on a non linear link function 🛈 (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "Univariate Linear Regression",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Subset data to adults\nm.scale(['weight']) # Normalize\n\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.lognormal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# Filter data frame\nm$df = m$df[m$df$age &gt; 18,] # Subset data to adults\n\n# Scale\nm$scale(list('weight'))  # Normalize\n\n# Convert data to JAX arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Parameter prior distributions\n  s = bi.dist.uniform(0, 50, name = 's')\n  a = bi.dist.normal(178, 20,  name = 'a')\n  b = bi.dist.normal(0, 1, name = 'b')\n  \n  # Likelihood\n  m$normal(a + b * weight, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "Univariate Linear Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist Formulation\nThe following equation describe the frequentist formulation of linear regression: \nY_i = \\alpha + \\beta  X_i + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\n\\beta is the regression coefficient.\nX_i is the input variable for observation i.\n\\epsilon_i is the error term for observation i, and the vector of the error terms, \\epsilon, are assumed to be independent and identically distributed.\n\n\n\nBayesian Formulation\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express a Bayesian version of this regression model using the following model:\n\nY_i \\sim \\text{Normal}(\\alpha + \\beta   X_i, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Uniform}(0, 50)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha and \\beta are the intercept and regression coefficient, respectively.\nX_i is the indepenent variable for observation i.\n\\sigma is the standard deviation of the Normal distribution, which describes the variance in the relationship between the dependent variable Y and the independent variable X.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#notes",
    "href": "1. Linear Regression for continuous variable.html#notes",
    "title": "Univariate Linear Regression",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nWe observe a difference between the Frequentist and the Bayesian formulation regarding the error term. Indeed, in the Frequentist formulation, the error terms \\epsilon represents residual fluctuations around the predicted values. This assumption leads to point estimates for \\alpha and \\beta. In contrast, the Bayesian formulation treats \\sigma as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "Univariate Linear Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "23. Network with block model.html",
    "href": "23. Network with block model.html",
    "title": "Stochastic Block Models",
    "section": "",
    "text": "Within networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#considerations",
    "href": "23. Network with block model.html#considerations",
    "title": "Stochastic Block Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\nIn addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (Any in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#example",
    "href": "23. Network with block model.html#example",
    "title": "Stochastic Block Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the Network model example, with the addition of covariates Any, Merica, and Quantum, representing the block membership of each node. This example is based on Ross, McElreath, and Redhead (2024).\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#mathematical-details",
    "href": "23. Network with block model.html#mathematical-details",
    "title": "Stochastic Block Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe model’s block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the Network model chapter:\n\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\n\n\\log(Y_{ij}) = B_{k(i), k(j)}\n\nwhere:\n\nB is a matrix of intercept parameters unique to the interaction of categories. For example, if there are three groups, then B will be a 3x3 matrix where each element give the rate an individual in group k interacting with an individual in group l.\nWe use the function k, to return the group identity (i.e., the block) of individual i.\n\n\n\nDefining formula sub-equations and prior distributions\nTo account for all link rates between categories, we can define a square matrix B as follows: the off-diagonal elements represent the link rates between categories i and j, while the diagonal elements represent the link rates within category i.\n\nB_{i,j} =\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j}\n\\end{bmatrix}\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks. Basic priors could be:\n\na_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n\n\na_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n\nwhere:\n\nk \\rightarrow k indicates a diagonal element.\nk \\rightarrow \\tilde{k} indicates an off-diagonal element.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#notes",
    "href": "23. Network with block model.html#notes",
    "title": "Stochastic Block Models",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nBy defining this block model within our network model, we are estimating assortativity 🛈 and disassortativity 🛈 for categorical variables.\nSimilarly, for continuous variables, we can generate a block model that includes all continuous variables.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#references",
    "href": "23. Network with block model.html#references",
    "title": "Stochastic Block Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nRoss, Cody T, Richard McElreath, and Daniel Redhead. 2024. “Modelling Animal Network Data in r Using STRAND.” Journal of Animal Ecology 93 (3): 254–66.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html",
    "href": "6. Beta binomial model.html",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion 🛈, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#general-principles",
    "href": "6. Beta binomial model.html#general-principles",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion 🛈, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#considerations",
    "href": "6. Beta binomial model.html#considerations",
    "title": "Beta-Binomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Binomial regression.\nA Beta-Binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.\nA Beta distribution is a continuous probability distribution defined on the interval. It is characterized by two positive shape parameters, commonly denoted as α and β, which control the shape of the distribution. In the context of the provided equations, \\gamma and \\eta serve as these shape parameters. These parameters determine the shape of the distribution, allowing it to model a wide variety of random variables representing proportions or probabilities. How the \\gamma and \\eta parameters influence the distribution’s shape can be summarized as follows:\nWhen \\gamma &gt; 1 and \\eta &gt; 1, the distribution is unimodal (bell-shaped), with the peak becoming sharper as the values of \\gamma and \\eta increase. If \\gamma and \\eta are equal and greater than 1, the distribution is symmetrical and centered around 0.5.\nIf \\gamma &lt; 1 and \\eta &lt; 1, the distribution is U-shaped, with peaks at both 0 and 1.\nThe skewness of the distribution is determined by the relative values of \\gamma and \\eta. If \\gamma &gt; \\eta, the distribution is skewed toward 1 (left-skewed), meaning more of the probability mass is concentrated on higher values. Conversely, if \\eta &gt; \\gamma, the distribution is skewed toward 0 (right-skewed), with more probability mass on lower values. The mean of the distribution is given by \\gamma / (\\gamma + \\eta).\n\nTherefore, by adjusting the shape parameters \\gamma and \\eta, the Beta distribution offers significant flexibility in modeling various types of prior beliefs about probabilities.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#example",
    "href": "6. Beta binomial model.html#example",
    "title": "Beta-Binomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression using the Bayesian Inference (BI) package. The data consist of:\n\nOne binary dependent variable (admit), which represents candidates’ admission status.\nOne independent categorical variable representing individuals’ gender (gid).\nAdditionally, we have the number of applications (applications) per gender, which will be used to account for independent rates.\n\nThe goal is to evaluate whether the probability of admission is different between genders, while accounting for differences in the number of applications between genders. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'UCBadmit.csv'\nm.data(data_path, sep=';') \nm.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n\n# Define model ------------------------------------------------\ndef model(gid, applications, admit):\n    # Prior for overall concentration scaling (positive, via exponential)\n    phi = m.dist.exponential(1, name='phi')\n    \n    # Priors for group-level intercepts (two groups, normal-distributed)\n    alpha = m.dist.normal(0., 1.5, shape=(2,), name='alpha')\n    \n    # Shifted concentration scale (avoids too small values)\n    theta = phi + 2\n    \n    # Group-specific mean success probability (mapped to [0,1] with sigmoid)\n    pbar = jax.nn.sigmoid(alpha[gid])\n    \n    # Beta distribution parameter for \"successes\"\n    concentration1 = pbar * theta\n    \n    # Beta distribution parameter for \"failures\"\n    concentration0 = (1 - pbar) * theta\n    \n    # Likelihood: admissions modeled with Beta-Binomial\n    m.dist.betabinomial(\n        total_count=applications,\n        concentration1=concentration1,\n        concentration0=concentration0,\n        obs=admit\n    )\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\nm$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\nm$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(gid, applications, admit){\n  # Parameter prior distributions\n  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n  t = phi + 2\n  pbar = jax$nn$sigmoid(alpha[gid])\n  gamma = pbar * t\n  eta = (1 - pbar) * t\n  # Likelihood\n  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#mathematical-details",
    "href": "6. Beta binomial model.html#mathematical-details",
    "title": "Beta-Binomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian Model\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{BetaBinomial}(N_i, \\gamma_i, \\eta_i)\n\n\n\\gamma_i = p_i   \\tau\n\n\n\\eta_i = (1 - p_i ) \\tau\n\n\np_i = \\text{logit}^{-1}(\\alpha + \\beta * X_i)\n\n\n\\tau = \\phi + 2\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\n\n\\phi \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the count of successes for the i-th observation, which follows a Beta-binomial distribution with N_i trials.\n\\gamma_i represents the concentration parameter for the number of successes, derived from the probability of success, p_i, and scaled by \\tau.\n\\eta_i represents the concentration parameter for failures, derived from the probability of failure (1 - p_i) and also scaled by \\tau.\np_i is the probability of success for the i-th observation. The logit function transforms the linear predictor (which can take any real value) into a probability value between 0 and 1.\n\\tau is derived from 𝜙 and is used as a scaling factor for the shape parameters 𝛾 and 𝜂.\n\\beta and \\alpha are the regression coefficient and intercept, respectively.\nϕ is a random variable following an Exponential distribution with a rate of 1.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#references",
    "href": "6. Beta binomial model.html#references",
    "title": "Beta-Binomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "11. Zero inflated.html",
    "href": "11. Zero inflated.html",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#general-principles",
    "href": "11. Zero inflated.html#general-principles",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#example",
    "href": "11. Zero inflated.html#example",
    "title": "Zero-Inflated Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Zero-Inflated Poisson regression using the Bayesian Inference (BI) package. The data represent the production of books in a monastery (y), which is affected by the number of days that individuals work, as well as the number of days individuals drink. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\nfrom jax.scipy.special import expit\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Simulated data------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n\n# Sample one year of production\nN = 365\ndrink = m.dist.binomial(1, prob_drink, shape = (N,), sample = True)\ny = (1 - drink) * m.dist.poisson(rate_work, shape = (N,), sample = True)\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.data_on_model = dict(\n    y=jnp.array(y)\n)\n\n# Define model ------------------------------------------------\ndef model(y):\n    al = dist.normal(1, 0.5, name='al')\n    ap = dist.normal(-1.5, 1, name='ap')\n    p = m.link.inv_logit(ap)\n    lambda_ = jnp.exp(al)\n    m.dist.zero_inflated_poisson(p, lambda_, obs=y)\n\n# Run MCMC ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Simulate data ------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n# sample one year of production\nN = as.integer(365)\ndrink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\ny = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\ndata = list()\ndata$y = y\nm$data_on_model = data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(y){\n  al = bi.dist.normal(1, 0.5, name='al', shape=c(1))\n  ap = bi.dist.normal(-1, 1, name='ap', shape=c(1))\n  p = m$link$invert_logit(ap)\n  lambda_ = jnp$exp(al)\n  m$zeroinflatedpoisson(p, lambda_, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#references",
    "href": "11. Zero inflated.html#references",
    "title": "Zero-Inflated Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "start/Import_data.html",
    "href": "start/Import_data.html",
    "title": "Import Data and handle it",
    "section": "",
    "text": "The BI class can import data from a csv file or from a dictionary for data that can’t be stored in a tabular format.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#import-tabular-data-from-a-csv-file",
    "href": "start/Import_data.html#import-tabular-data-from-a-csv-file",
    "title": "Import Data and handle it",
    "section": "Import tabular data from a csv file",
    "text": "Import tabular data from a csv file\n\nPythonR\n\n\nm.data(data_path, sep=';') \n\n\nm$data(data_path,  sep=';')",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#import-non-tabular-data",
    "href": "start/Import_data.html#import-non-tabular-data",
    "title": "Import Data and handle it",
    "section": "Import non tabular data",
    "text": "Import non tabular data\nFirst you need to create our own dictionary with the data.\n\nPythonR\n\n\nm.data_on_model = dict(\n    ID1 = Value1,\n    ID2 = Value2, \n)\n\n\nkeys &lt;- c(\"ID1\",\"ID2\")\nvalues &lt;- list(Value1,Value2)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#handle-data",
    "href": "start/Import_data.html#handle-data",
    "title": "Import Data and handle it",
    "section": "Handle data",
    "text": "Handle data\nFor tabular data, you ca use some functions to manipulate the data:\n\nPerform one-hot encoding OHE: One-hot encoding is a technique that converts categorical variables into binary variables. This is useful when you have a large number of categories and want to use them as features in a model.\nCreate index encoding for categorical columns index: Index encoding is a technique that assigns a unique integer value to each category in a categorical variable. This is useful when you have a large number of categories and want to use them as features in a model.\nScale scale: Standardize the data by subtracting the mean and dividing by the standard deviation.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Fit model.html",
    "href": "start/Fit model.html",
    "title": "Fit model",
    "section": "",
    "text": "Once a function declaring the model is defined, you can fit the model to the data using the fit function.\n\nPythonR\n\n\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n\nm$fit(model) # Optimize model parameters through MCMC sampling",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Fit model"
    ]
  },
  {
    "objectID": "start/Diagnostics.html",
    "href": "start/Diagnostics.html",
    "title": "Model Diagnostic",
    "section": "",
    "text": "The BI class can compute model diagnostics for a given model.\nLets concider the following model for a linear regression:\nY_i \\sim \\text{Normal}(\\alpha + \\beta   X_i, \\sigma)\n\\alpha \\sim \\text{Normal}(0, 1)\n\\beta \\sim \\text{Normal}(0, 1)\n\\sigma \\sim \\text{Uniform}(0, 50)\nfrom BI import bi\nimport jax.numpy as jnp\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\nm.data('Howell1.csv', sep=';') \nm.df = m.df[m.df.age &gt; 18]\nm.scale(data=['weight'])\n\n\n # define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal( 178, 20, name = 'a')\n    b = m.dist.log_normal(  0, 1, name = 'b')   \n    s = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(a + b * weight , s, obs=height, shape=(weight.shape[0],))\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500,num_chains=4) \nm.summary()\n\nWARNING:2025-09-25 08:49:37,825:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\njax.local_device_count 32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na\n154.66\n0.28\n154.22\n155.11\n0.01\n0.01\n1912.82\n1630.66\n1.0\n\n\nb\n5.80\n0.28\n5.38\n6.25\n0.01\n0.01\n1779.03\n1519.74\n1.0\n\n\ns\n5.14\n0.20\n4.81\n5.47\n0.00\n0.00\n2005.82\n1068.71\n1.0",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#predictions-from-model-base-on-specific-data-value",
    "href": "start/Diagnostics.html#predictions-from-model-base-on-specific-data-value",
    "title": "Model Diagnostic",
    "section": "Predictions from model base on specific data value",
    "text": "Predictions from model base on specific data value\n\nm.sample() # Predictions from model base on data in data_on_model\nm.sample(data=dict(weight=jnp.array([0.4])), remove_obs=False)# Predictions from a given value\n\n{'x': Array([149.77945571], dtype=float64)}",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#forest-plot-of-estimated-values",
    "href": "start/Diagnostics.html#forest-plot-of-estimated-values",
    "title": "Model Diagnostic",
    "section": "Forest plot of estimated values",
    "text": "Forest plot of estimated values\n\nm.diag.forest()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#density-plots-of-the-posterior-distribution",
    "href": "start/Diagnostics.html#density-plots-of-the-posterior-distribution",
    "title": "Model Diagnostic",
    "section": "Density plots of the posterior distribution",
    "text": "Density plots of the posterior distribution\n\nm.diag.density()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#posterior-distribution-plots",
    "href": "start/Diagnostics.html#posterior-distribution-plots",
    "title": "Model Diagnostic",
    "section": "Posterior distribution plots",
    "text": "Posterior distribution plots\n\nm.diag.posterior()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#trace-plots-for-mcmc-chains",
    "href": "start/Diagnostics.html#trace-plots-for-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Trace plots for MCMC chains",
    "text": "Trace plots for MCMC chains\n\nm.diag.plot_trace()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#pairwise-plots-of-the-posterior-distribution",
    "href": "start/Diagnostics.html#pairwise-plots-of-the-posterior-distribution",
    "title": "Model Diagnostic",
    "section": "Pairwise plots of the posterior distribution",
    "text": "Pairwise plots of the posterior distribution\n\nm.diag.pair()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#plot-autocorrelation-of-the-mcmc-chains",
    "href": "start/Diagnostics.html#plot-autocorrelation-of-the-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Plot autocorrelation of the MCMC chains",
    "text": "Plot autocorrelation of the MCMC chains\n\nm.diag.autocor()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#create-rank-plots-for-mcmc-chains",
    "href": "start/Diagnostics.html#create-rank-plots-for-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Create rank plots for MCMC chains",
    "text": "Create rank plots for MCMC chains\n\nm.diag.rank()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#evolution-of-effective-sample-size-across-iterations",
    "href": "start/Diagnostics.html#evolution-of-effective-sample-size-across-iterations",
    "title": "Model Diagnostic",
    "section": "Evolution of effective sample size across iterations",
    "text": "Evolution of effective sample size across iterations\n\nm.diag.plot_ess()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#pareto-smoothed",
    "href": "start/Diagnostics.html#pareto-smoothed",
    "title": "Model Diagnostic",
    "section": "Pareto-smoothed",
    "text": "Pareto-smoothed\n\nm.diag.loo()\n\nComputed from 2000 posterior samples and 346 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1058.52    14.68\np_loo        3.23        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      346  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#widely-applicable-information-criterion",
    "href": "start/Diagnostics.html#widely-applicable-information-criterion",
    "title": "Model Diagnostic",
    "section": "Widely applicable information criterion",
    "text": "Widely applicable information criterion\n\nm.diag.WAIC()\n\nComputed from 2000 posterior samples and 346 observations log-likelihood matrix.\n\n          Estimate       SE\nelpd_waic -1058.51    14.68\np_waic        3.21        -",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "api_diag.html",
    "href": "api_diag.html",
    "title": "Diagnostics",
    "section": "",
    "text": "diag.diag is a class to unify various diagnostics methods and provide a consistent interface for diagnostics.\nCompute the widely applicable information criterion.\nEstimates the expected log pointwise predictive density (elpd) using WAIC. Also calculates the WAIC’s standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1004.2316\n\n\npointwise: bool If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for waic computation. scale: str Output scale for WAIC. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy. dask_kwargs : dict, optional Dask related kwargs passed to :func:~arviz.wrap_xarray_ufunc.\n\n\n\nELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_waic: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_waic: effective number parameters n_samples: number of samples n_data_points: number of data points warning: bool True if posterior variance of the log predictive densities exceeds 0.4 waic_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True scale: scale of the elpd\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.WAIC(\nself,\npointwise=None,\nvar_name=None,\nscale=None,\ndask_kwargs=None,\n)\n\nPlot autocorrelation of the MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_autocorr\nReturns: fig: Autocorrelation plot\nbi.dist.autocor(\nself,\n*args,\n**kwargs,\n)\n\nCompare models based on their expected log pointwise predictive density (ELPD).\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353\n\n\n\ncompare_dict: dict of {str: InferenceData or ELPDData} A dictionary of model names and :class:arviz.InferenceData or ELPDData. ic: str, optional Method to estimate the ELPD, available options are “loo” or “waic”. Defaults to rcParams[\"stats.information_criterion\"]. method: str, optional Method used to estimate the weights for each model. Available options are:\n\n‘stacking’ : stacking of predictive distributions.\n‘BB-pseudo-BMA’ : pseudo-Bayesian Model averaging using Akaike-type weighting. The weights are stabilized using the Bayesian bootstrap.\n‘pseudo-BMA’: pseudo-Bayesian Model averaging using Akaike-type weighting, without Bootstrap stabilization (not recommended).\n\nFor more information read https://arxiv.org/abs/1704.02030 b_samples: int, optional default = 1000 Number of samples taken by the Bayesian bootstrap estimation. Only useful when method = ‘BB-pseudo-BMA’. Defaults to rcParams[\"stats.ic_compare_method\"]. alpha: float, optional The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only useful when method = ‘BB-pseudo-BMA’. When alpha=1 (default), the distribution is uniform on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1. seed: int or np.random.RandomState instance, optional If int or RandomState, use it for seeding Bayesian bootstrap. Only useful when method = ‘BB-pseudo-BMA’. Default None the global :mod:numpy.random state is used. scale: str, optional Output scale for IC. Available options are:\n\nlog : (default) log-score (after Vehtari et al. (2017))\nnegative_log : -1 * (log-score)\ndeviance : -2 * (log-score)\n\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy. var_name: str, optional If there is more than a single observed variable in the InferenceData, which should be used as the basis for comparison.\n\n\n\nA DataFrame, ordered from best to worst model (measured by the ELPD). The index reflects the key with which the models are passed to this function. The columns are: rank: The rank-order of the models. 0 is the best. elpd: ELPD estimated either using (PSIS-LOO-CV elpd_loo or WAIC elpd_waic). Higher ELPD indicates higher out-of-sample predictive fit (“better” model). If scale is deviance or negative_log smaller values indicates higher out-of-sample predictive fit (“better” model). pIC: Estimated effective number of parameters. elpd_diff: The difference in ELPD between two models. If more than two models are compared, the difference is computed relative to the top-ranked model, that always has a elpd_diff of 0. weight: Relative weight for each model. This can be loosely interpreted as the probability of each model (among the compared model) given the data. By default the uncertainty in the weights estimation is considered using Bayesian bootstrap. SE: Standard error of the ELPD estimate. If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap. dSE: Standard error of the difference in ELPD between each model and the top-ranked model. It’s always 0 for the top-ranked model. warning: A value of 1 indicates that the computation of the ELPD may not be reliable. This could be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details. scale: Scale used for the ELPD.\n\n\n\n.. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017) see https://doi.org/10.1007/s11222-016-9696-4\nbi.dist.compare(\ncompare_dict,\nic=None,\nmethod='stacking',\nb_samples=1000,\nalpha=1,\nseed=None,\nscale=None,\nvar_name=None,\n)\n\nPlot density plots of the posterior distribution.\nArgs: var_names (list): Variables to include shade (float): Transparency of the filled area *args, **kwargs: Additional arguments for arviz.plot_density\nReturns: fig: Density plots\nbi.dist.density(\nself,\nvar_names=None,\nshade=0.2,\n*args,\n**kwargs,\n)\n\nCalculate effective sample size (ESS).\nArgs: *args, **kwargs: Additional arguments for arviz.ess\nReturns: ess: Effective sample sizes\nbi.dist.ess(\nself,\n*args,\n**kwargs,\n)\n\nCreate a forest plot of estimated values.\nArgs: list: Data to plot (default: self.trace) kind (str): Type of plot (default: “ridgeplot”) ess (bool): Include effective sample size var_names (list): Variables to include *args, **kwargs: Additional arguments for arviz.plot_forest\nReturns: fig: Forest plot\nbi.dist.forest(\nself,\nlist=None,\nkind='ridgeplot',\ness=True,\nvar_names=None,\n*args,\n**kwargs,\n)\n\nCompute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\nEstimates the expected log pointwise predictive density (elpd) using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO’s standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646\n\n\n\npointwise: bool, optional If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for loo computation. reff: float, optional Relative MCMC efficiency, ess / n i.e. number of effective samples divided by the number of actual samples. Computed from trace by default. scale: str Output scale for loo. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy.\n\n\n\nELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_loo: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_loo: effective number of parameters n_samples: number of samples n_data_points: number of data points warning: bool True if the estimated shape parameter of Pareto distribution is greater than good_k. loo_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True pareto_k: array of Pareto shape values, only if pointwise True scale: scale of the elpd good_k: For a sample size S, the thresold is compute as min(1 - 1/log10(S), 0.7)\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.loo(\nself,\npointwise=None,\nvar_name=None,\nreff=None,\nscale=None,\n)\n\nPerform comprehensive model diagnostics.\nCreates various diagnostic plots including: - Posterior distributions - Autocorrelation plots - Trace plots - Rank plots - Forest plots\nStores plots in instance variables: self.plot_posterior, self.autocor, self.traces, self.rank, self.forest\nbi.dist.model_checks(\nself,\n)\n\nCreate pairwise plots of the posterior distribution.\nArgs: var_names (list): Variables to include kind (list): Type of plots (“scatter” and/or “kde”) kde_kwargs (dict): Additional arguments for KDE plots marginals (bool): Include marginal distributions point_estimate (str): Point estimate to plot figsize (tuple): Size of the figure *args, **kwargs: Additional arguments for arviz.plot_pair\nReturns: fig: Pair plot\nbi.dist.pair(\nself,\nvar_names=None,\nkind=['scatter', 'kde'],\nkde_kwargs={'fill_last': False},\nmarginals=True,\npoint_estimate='median',\nfigsize=(11.5, 5),\n*args,\n**kwargs,\n)\n\nSummary plot for model comparison.\nModels are compared based on their expected log pointwise predictive density (ELPD). This plot is in the style of the one used in [2]_. Chapter 6 in the first edition or 7 in the second.\n\n\n\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend LOO in line with the work presented by [1]_.\n\n\n\ncomp_df : pandas.DataFrame Result of the :func:arviz.compare method. insample_dev : bool, default False Plot in-sample ELPD, that is the value of the information criteria without the penalization given by the effective number of parameters (p_loo or p_waic). plot_standard_error : bool, default True Plot the standard error of the ELPD. plot_ic_diff : bool, default False Plot standard error of the difference in ELPD between each model and the top-ranked model. order_by_rank : bool, default True If True ensure the best model is used as reference. legend : bool, default False Add legend to figure. figsize : (float, float), optional If None, size is (6, num of models) inches. title : bool, default True Show a tittle with a description of how to interpret the plot. textsize : float, optional Text size scaling factor for labels, titles and lines. If None it will be autoscaled based on figsize. labeller : Labeller, optional Class providing the method make_label_vert to generate the labels in the plot titles. Read the :ref:label_guide for more details and usage examples. plot_kwargs : dict, optional Optional arguments for plot elements. Currently accepts ‘color_ic’, ‘marker_ic’, ‘color_insample_dev’, ‘marker_insample_dev’, ‘color_dse’, ‘marker_dse’, ‘ls_min_ic’ ‘color_ls_min_ic’, ‘fontsize’ ax : matplotlib_axes or bokeh_figure, optional Matplotlib axes or bokeh figure. backend : {“matplotlib”, “bokeh”}, default “matplotlib” Select plotting backend. backend_kwargs : bool, optional These are kwargs specific to the backend being used, passed to :func:matplotlib.pyplot.subplots or :class:bokeh.plotting.figure. For additional documentation check the plotting method of the backend. show : bool, optional Call backend show function.\n\n\n\naxes : matplotlib_axes or bokeh_figure\n\n\n\nplot_elpd : Plot pointwise elpd differences between two or more models. compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation. loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). waic : Compute the widely applicable information criterion.\n\n\n\n.. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC https://arxiv.org/abs/1507.04544\n.. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in R and Stan, Second edition, CRC Press.\nbi.dist.plot_compare(\ncomp_df,\ninsample_dev=False,\nplot_standard_error=True,\nplot_ic_diff=False,\norder_by_rank=True,\nlegend=False,\ntitle=True,\nfigsize=None,\ntextsize=None,\nlabeller=None,\nplot_kwargs=None,\nax=None,\nbackend=None,\nbackend_kwargs=None,\nshow=None,\n)\n\nPlot evolution of effective sample size across iterations.\nReturns: fig: ESS evolution plot\nbi.dist.plot_ess(\nself,\n)\n\nCreate a trace plot for visualizing MCMC diagnostics.\nArgs: var_names (list): List of variable names to include kind (str): Type of plot (default: “rank_bars”) *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: plot: The trace plot object\nbi.dist.plot_trace(\nself,\nvar_names=None,\nkind='rank_bars',\n*args,\n**kwargs,\n)\n\nCreate posterior distribution plots.\nArgs: figsize (tuple): Size of the figure (width, height)\nReturns: fig: Matplotlib figure containing posterior plots\nbi.dist.posterior(\nself,\nfigsize=(8, 4),\n)\n\nVisualize prior distributions compared to log probability.\nArgs: N (int): Number of samples to draw from priors\nReturns: fig: Matplotlib figure containing the prior distribution plots\nbi.dist.prior_dist(\nself,\nN=100,\n)\n\nCreate rank plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_rank\nReturns: fig: Rank plots\nbi.dist.rank(\nself,\n*args,\n**kwargs,\n)\n\nCalculate R-hat statistics for convergence.\nArgs: *args, **kwargs: Additional arguments for arviz.rhat\nReturns: rhat: R-hat values\nbi.dist.rhat(\nself,\n*args,\n**kwargs,\n)\n\nCalculate summary statistics for the posterior distribution.\nArgs: round_to (int): Number of decimal places to round results kind (str): Type of statistics to compute (default: “stats”) hdi_prob (float): Probability for highest posterior density interval *args, **kwargs: Additional arguments for arviz.summary\nReturns: pd.DataFrame: Summary statistics of the posterior distribution\nbi.dist.summary(\nself,\nround_to=2,\nkind='stats',\nhdi_prob=0.89,\n*args,\n**kwargs,\n)\n\nConvert the sampler output to an arviz trace object.\nThis method prepares the trace for use with arviz diagnostic tools.\nReturns: self.trace: The arviz trace object containing the diagnostic data\nbi.dist.to_az(\nself,\nbackend='numpyro',\nsample_stats_name=['target_log_prob', 'log_accept_ratio', 'has_divergence', 'energy'],\n)\n\nCreate trace plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: fig: Trace plots\nbi.dist.traces(\nself,\n*args,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters",
    "href": "api_diag.html#parameters",
    "title": "Diagnostics",
    "section": "",
    "text": "pointwise: bool If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for waic computation. scale: str Output scale for WAIC. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy. dask_kwargs : dict, optional Dask related kwargs passed to :func:~arviz.wrap_xarray_ufunc.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns",
    "href": "api_diag.html#returns",
    "title": "Diagnostics",
    "section": "",
    "text": "ELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_waic: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_waic: effective number parameters n_samples: number of samples n_data_points: number of data points warning: bool True if posterior variance of the log predictive densities exceeds 0.4 waic_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True scale: scale of the elpd\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.WAIC(\nself,\npointwise=None,\nvar_name=None,\nscale=None,\ndask_kwargs=None,\n)\n\nPlot autocorrelation of the MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_autocorr\nReturns: fig: Autocorrelation plot\nbi.dist.autocor(\nself,\n*args,\n**kwargs,\n)\n\nCompare models based on their expected log pointwise predictive density (ELPD).\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-1",
    "href": "api_diag.html#parameters-1",
    "title": "Diagnostics",
    "section": "",
    "text": "compare_dict: dict of {str: InferenceData or ELPDData} A dictionary of model names and :class:arviz.InferenceData or ELPDData. ic: str, optional Method to estimate the ELPD, available options are “loo” or “waic”. Defaults to rcParams[\"stats.information_criterion\"]. method: str, optional Method used to estimate the weights for each model. Available options are:\n\n‘stacking’ : stacking of predictive distributions.\n‘BB-pseudo-BMA’ : pseudo-Bayesian Model averaging using Akaike-type weighting. The weights are stabilized using the Bayesian bootstrap.\n‘pseudo-BMA’: pseudo-Bayesian Model averaging using Akaike-type weighting, without Bootstrap stabilization (not recommended).\n\nFor more information read https://arxiv.org/abs/1704.02030 b_samples: int, optional default = 1000 Number of samples taken by the Bayesian bootstrap estimation. Only useful when method = ‘BB-pseudo-BMA’. Defaults to rcParams[\"stats.ic_compare_method\"]. alpha: float, optional The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only useful when method = ‘BB-pseudo-BMA’. When alpha=1 (default), the distribution is uniform on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1. seed: int or np.random.RandomState instance, optional If int or RandomState, use it for seeding Bayesian bootstrap. Only useful when method = ‘BB-pseudo-BMA’. Default None the global :mod:numpy.random state is used. scale: str, optional Output scale for IC. Available options are:\n\nlog : (default) log-score (after Vehtari et al. (2017))\nnegative_log : -1 * (log-score)\ndeviance : -2 * (log-score)\n\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy. var_name: str, optional If there is more than a single observed variable in the InferenceData, which should be used as the basis for comparison.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-1",
    "href": "api_diag.html#returns-1",
    "title": "Diagnostics",
    "section": "",
    "text": "A DataFrame, ordered from best to worst model (measured by the ELPD). The index reflects the key with which the models are passed to this function. The columns are: rank: The rank-order of the models. 0 is the best. elpd: ELPD estimated either using (PSIS-LOO-CV elpd_loo or WAIC elpd_waic). Higher ELPD indicates higher out-of-sample predictive fit (“better” model). If scale is deviance or negative_log smaller values indicates higher out-of-sample predictive fit (“better” model). pIC: Estimated effective number of parameters. elpd_diff: The difference in ELPD between two models. If more than two models are compared, the difference is computed relative to the top-ranked model, that always has a elpd_diff of 0. weight: Relative weight for each model. This can be loosely interpreted as the probability of each model (among the compared model) given the data. By default the uncertainty in the weights estimation is considered using Bayesian bootstrap. SE: Standard error of the ELPD estimate. If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap. dSE: Standard error of the difference in ELPD between each model and the top-ranked model. It’s always 0 for the top-ranked model. warning: A value of 1 indicates that the computation of the ELPD may not be reliable. This could be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details. scale: Scale used for the ELPD.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#references",
    "href": "api_diag.html#references",
    "title": "Diagnostics",
    "section": "",
    "text": ".. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017) see https://doi.org/10.1007/s11222-016-9696-4\nbi.dist.compare(\ncompare_dict,\nic=None,\nmethod='stacking',\nb_samples=1000,\nalpha=1,\nseed=None,\nscale=None,\nvar_name=None,\n)\n\nPlot density plots of the posterior distribution.\nArgs: var_names (list): Variables to include shade (float): Transparency of the filled area *args, **kwargs: Additional arguments for arviz.plot_density\nReturns: fig: Density plots\nbi.dist.density(\nself,\nvar_names=None,\nshade=0.2,\n*args,\n**kwargs,\n)\n\nCalculate effective sample size (ESS).\nArgs: *args, **kwargs: Additional arguments for arviz.ess\nReturns: ess: Effective sample sizes\nbi.dist.ess(\nself,\n*args,\n**kwargs,\n)\n\nCreate a forest plot of estimated values.\nArgs: list: Data to plot (default: self.trace) kind (str): Type of plot (default: “ridgeplot”) ess (bool): Include effective sample size var_names (list): Variables to include *args, **kwargs: Additional arguments for arviz.plot_forest\nReturns: fig: Forest plot\nbi.dist.forest(\nself,\nlist=None,\nkind='ridgeplot',\ness=True,\nvar_names=None,\n*args,\n**kwargs,\n)\n\nCompute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\nEstimates the expected log pointwise predictive density (elpd) using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO’s standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-2",
    "href": "api_diag.html#parameters-2",
    "title": "Diagnostics",
    "section": "",
    "text": "pointwise: bool, optional If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for loo computation. reff: float, optional Relative MCMC efficiency, ess / n i.e. number of effective samples divided by the number of actual samples. Computed from trace by default. scale: str Output scale for loo. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-2",
    "href": "api_diag.html#returns-2",
    "title": "Diagnostics",
    "section": "",
    "text": "ELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_loo: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_loo: effective number of parameters n_samples: number of samples n_data_points: number of data points warning: bool True if the estimated shape parameter of Pareto distribution is greater than good_k. loo_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True pareto_k: array of Pareto shape values, only if pointwise True scale: scale of the elpd good_k: For a sample size S, the thresold is compute as min(1 - 1/log10(S), 0.7)\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.loo(\nself,\npointwise=None,\nvar_name=None,\nreff=None,\nscale=None,\n)\n\nPerform comprehensive model diagnostics.\nCreates various diagnostic plots including: - Posterior distributions - Autocorrelation plots - Trace plots - Rank plots - Forest plots\nStores plots in instance variables: self.plot_posterior, self.autocor, self.traces, self.rank, self.forest\nbi.dist.model_checks(\nself,\n)\n\nCreate pairwise plots of the posterior distribution.\nArgs: var_names (list): Variables to include kind (list): Type of plots (“scatter” and/or “kde”) kde_kwargs (dict): Additional arguments for KDE plots marginals (bool): Include marginal distributions point_estimate (str): Point estimate to plot figsize (tuple): Size of the figure *args, **kwargs: Additional arguments for arviz.plot_pair\nReturns: fig: Pair plot\nbi.dist.pair(\nself,\nvar_names=None,\nkind=['scatter', 'kde'],\nkde_kwargs={'fill_last': False},\nmarginals=True,\npoint_estimate='median',\nfigsize=(11.5, 5),\n*args,\n**kwargs,\n)\n\nSummary plot for model comparison.\nModels are compared based on their expected log pointwise predictive density (ELPD). This plot is in the style of the one used in [2]_. Chapter 6 in the first edition or 7 in the second.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#notes",
    "href": "api_diag.html#notes",
    "title": "Diagnostics",
    "section": "",
    "text": "The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend LOO in line with the work presented by [1]_.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-3",
    "href": "api_diag.html#parameters-3",
    "title": "Diagnostics",
    "section": "",
    "text": "comp_df : pandas.DataFrame Result of the :func:arviz.compare method. insample_dev : bool, default False Plot in-sample ELPD, that is the value of the information criteria without the penalization given by the effective number of parameters (p_loo or p_waic). plot_standard_error : bool, default True Plot the standard error of the ELPD. plot_ic_diff : bool, default False Plot standard error of the difference in ELPD between each model and the top-ranked model. order_by_rank : bool, default True If True ensure the best model is used as reference. legend : bool, default False Add legend to figure. figsize : (float, float), optional If None, size is (6, num of models) inches. title : bool, default True Show a tittle with a description of how to interpret the plot. textsize : float, optional Text size scaling factor for labels, titles and lines. If None it will be autoscaled based on figsize. labeller : Labeller, optional Class providing the method make_label_vert to generate the labels in the plot titles. Read the :ref:label_guide for more details and usage examples. plot_kwargs : dict, optional Optional arguments for plot elements. Currently accepts ‘color_ic’, ‘marker_ic’, ‘color_insample_dev’, ‘marker_insample_dev’, ‘color_dse’, ‘marker_dse’, ‘ls_min_ic’ ‘color_ls_min_ic’, ‘fontsize’ ax : matplotlib_axes or bokeh_figure, optional Matplotlib axes or bokeh figure. backend : {“matplotlib”, “bokeh”}, default “matplotlib” Select plotting backend. backend_kwargs : bool, optional These are kwargs specific to the backend being used, passed to :func:matplotlib.pyplot.subplots or :class:bokeh.plotting.figure. For additional documentation check the plotting method of the backend. show : bool, optional Call backend show function.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-3",
    "href": "api_diag.html#returns-3",
    "title": "Diagnostics",
    "section": "",
    "text": "axes : matplotlib_axes or bokeh_figure",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#see-also",
    "href": "api_diag.html#see-also",
    "title": "Diagnostics",
    "section": "",
    "text": "plot_elpd : Plot pointwise elpd differences between two or more models. compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation. loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). waic : Compute the widely applicable information criterion.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#references-1",
    "href": "api_diag.html#references-1",
    "title": "Diagnostics",
    "section": "",
    "text": ".. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC https://arxiv.org/abs/1507.04544\n.. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in R and Stan, Second edition, CRC Press.\nbi.dist.plot_compare(\ncomp_df,\ninsample_dev=False,\nplot_standard_error=True,\nplot_ic_diff=False,\norder_by_rank=True,\nlegend=False,\ntitle=True,\nfigsize=None,\ntextsize=None,\nlabeller=None,\nplot_kwargs=None,\nax=None,\nbackend=None,\nbackend_kwargs=None,\nshow=None,\n)\n\nPlot evolution of effective sample size across iterations.\nReturns: fig: ESS evolution plot\nbi.dist.plot_ess(\nself,\n)\n\nCreate a trace plot for visualizing MCMC diagnostics.\nArgs: var_names (list): List of variable names to include kind (str): Type of plot (default: “rank_bars”) *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: plot: The trace plot object\nbi.dist.plot_trace(\nself,\nvar_names=None,\nkind='rank_bars',\n*args,\n**kwargs,\n)\n\nCreate posterior distribution plots.\nArgs: figsize (tuple): Size of the figure (width, height)\nReturns: fig: Matplotlib figure containing posterior plots\nbi.dist.posterior(\nself,\nfigsize=(8, 4),\n)\n\nVisualize prior distributions compared to log probability.\nArgs: N (int): Number of samples to draw from priors\nReturns: fig: Matplotlib figure containing the prior distribution plots\nbi.dist.prior_dist(\nself,\nN=100,\n)\n\nCreate rank plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_rank\nReturns: fig: Rank plots\nbi.dist.rank(\nself,\n*args,\n**kwargs,\n)\n\nCalculate R-hat statistics for convergence.\nArgs: *args, **kwargs: Additional arguments for arviz.rhat\nReturns: rhat: R-hat values\nbi.dist.rhat(\nself,\n*args,\n**kwargs,\n)\n\nCalculate summary statistics for the posterior distribution.\nArgs: round_to (int): Number of decimal places to round results kind (str): Type of statistics to compute (default: “stats”) hdi_prob (float): Probability for highest posterior density interval *args, **kwargs: Additional arguments for arviz.summary\nReturns: pd.DataFrame: Summary statistics of the posterior distribution\nbi.dist.summary(\nself,\nround_to=2,\nkind='stats',\nhdi_prob=0.89,\n*args,\n**kwargs,\n)\n\nConvert the sampler output to an arviz trace object.\nThis method prepares the trace for use with arviz diagnostic tools.\nReturns: self.trace: The arviz trace object containing the diagnostic data\nbi.dist.to_az(\nself,\nbackend='numpyro',\nsample_stats_name=['target_log_prob', 'log_accept_ratio', 'has_divergence', 'energy'],\n)\n\nCreate trace plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: fig: Trace plots\nbi.dist.traces(\nself,\n*args,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "Advance/Build in models.html",
    "href": "Advance/Build in models.html",
    "title": "Build in models",
    "section": "",
    "text": "from BI import bi, jnp\n\nm=bi()\nm.data('iris.csv', sep=',') # Data is already scaled\nm.data_on_model = dict(\n    X=jnp.array(m.df.iloc[:,0:-2].values)\n)\nm.fit(m.models.pca(type=\"ARD\"), progress_bar=False) # or robust, sparse, classic, sparse_robust_ard\n\nm.models.pca.plot(\n    X=m.df.iloc[:,0:-2].values,\n    y=m.df.iloc[:,-2].values, \n    feature_names=m.df.columns[0:-2], \n    target_names=m.df.iloc[:,-1].unique(),\n    color_var=m.df.iloc[:,0].values,\n    shape_var=m.df.iloc[:,-2].values\n)\n\njax.local_device_count 16",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in models"
    ]
  },
  {
    "objectID": "Advance/Build in models.html#python",
    "href": "Advance/Build in models.html#python",
    "title": "Build in models",
    "section": "Python",
    "text": "Python\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\nm.data_on_model = {\"data\": data, \"T\": 10 } # T = Number of maximum cluster to test for\nm.fit(m.models.dpmm) \nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:04&lt;1:07:36,  4.06s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|▏         | 15/1000 [00:04&lt;03:26,  4.77it/s, 511 steps of size 9.88e-03. acc. prob=0.65]warmup:   2%|▏         | 19/1000 [00:04&lt;02:43,  6.01it/s, 511 steps of size 5.50e-03. acc. prob=0.67]warmup:   2%|▏         | 22/1000 [00:04&lt;02:24,  6.75it/s, 127 steps of size 1.59e-02. acc. prob=0.70]warmup:   2%|▎         | 25/1000 [00:04&lt;02:00,  8.08it/s, 63 steps of size 5.95e-02. acc. prob=0.73] warmup:   3%|▎         | 28/1000 [00:05&lt;01:39,  9.79it/s, 127 steps of size 2.94e-02. acc. prob=0.73]warmup:   3%|▎         | 31/1000 [00:05&lt;01:21, 11.94it/s, 255 steps of size 1.59e-02. acc. prob=0.72]warmup:   4%|▎         | 37/1000 [00:05&lt;01:01, 15.55it/s, 511 steps of size 2.80e-02. acc. prob=0.74]warmup:   4%|▍         | 40/1000 [00:05&lt;01:03, 15.21it/s, 31 steps of size 3.96e-02. acc. prob=0.75] warmup:   4%|▍         | 45/1000 [00:05&lt;00:47, 19.98it/s, 159 steps of size 8.74e-02. acc. prob=0.76]warmup:   5%|▍         | 48/1000 [00:05&lt;00:53, 17.73it/s, 383 steps of size 3.51e-02. acc. prob=0.75]warmup:   5%|▌         | 53/1000 [00:06&lt;00:41, 22.84it/s, 31 steps of size 1.22e-01. acc. prob=0.76] warmup:   6%|▌         | 57/1000 [00:06&lt;00:39, 23.69it/s, 127 steps of size 7.05e-02. acc. prob=0.76]warmup:   6%|▋         | 64/1000 [00:06&lt;00:28, 32.71it/s, 55 steps of size 4.99e-02. acc. prob=0.76] warmup:   7%|▋         | 70/1000 [00:06&lt;00:26, 35.76it/s, 127 steps of size 3.14e-02. acc. prob=0.76]warmup:   8%|▊         | 76/1000 [00:06&lt;00:23, 39.16it/s, 127 steps of size 8.55e-02. acc. prob=0.77]warmup:   8%|▊         | 81/1000 [00:06&lt;00:23, 39.16it/s, 71 steps of size 8.10e-02. acc. prob=0.77] warmup:   9%|▉         | 88/1000 [00:06&lt;00:19, 45.69it/s, 23 steps of size 9.41e-02. acc. prob=0.77]warmup:  10%|▉         | 95/1000 [00:06&lt;00:17, 50.39it/s, 95 steps of size 5.95e-02. acc. prob=0.77]warmup:  10%|█         | 101/1000 [00:07&lt;00:17, 51.60it/s, 63 steps of size 1.04e+00. acc. prob=0.78]warmup:  11%|█         | 107/1000 [00:07&lt;00:20, 42.55it/s, 127 steps of size 1.08e-01. acc. prob=0.77]warmup:  11%|█         | 112/1000 [00:07&lt;00:20, 42.30it/s, 31 steps of size 1.23e-02. acc. prob=0.76] warmup:  12%|█▏        | 117/1000 [00:07&lt;00:25, 34.46it/s, 31 steps of size 1.84e-02. acc. prob=0.77]warmup:  12%|█▏        | 121/1000 [00:07&lt;00:27, 32.04it/s, 63 steps of size 1.35e-01. acc. prob=0.77]warmup:  12%|█▎        | 125/1000 [00:07&lt;00:27, 31.48it/s, 127 steps of size 5.72e-02. acc. prob=0.77]warmup:  13%|█▎        | 129/1000 [00:08&lt;00:31, 27.91it/s, 255 steps of size 4.65e-02. acc. prob=0.77]warmup:  14%|█▎        | 135/1000 [00:08&lt;00:25, 34.19it/s, 63 steps of size 1.61e-01. acc. prob=0.78] warmup:  14%|█▍        | 139/1000 [00:08&lt;00:26, 32.81it/s, 63 steps of size 8.10e-02. acc. prob=0.77]warmup:  14%|█▍        | 143/1000 [00:08&lt;00:27, 31.15it/s, 127 steps of size 6.67e-02. acc. prob=0.77]warmup:  15%|█▌        | 150/1000 [00:08&lt;00:22, 38.42it/s, 63 steps of size 8.31e-02. acc. prob=0.78] warmup:  16%|█▌        | 156/1000 [00:08&lt;00:19, 42.50it/s, 63 steps of size 1.53e-01. acc. prob=0.77]warmup:  16%|█▋        | 163/1000 [00:08&lt;00:17, 49.09it/s, 31 steps of size 2.88e-02. acc. prob=0.77]warmup:  17%|█▋        | 169/1000 [00:08&lt;00:17, 46.63it/s, 127 steps of size 4.86e-02. acc. prob=0.77]warmup:  17%|█▋        | 174/1000 [00:09&lt;00:24, 33.19it/s, 127 steps of size 7.51e-02. acc. prob=0.77]warmup:  18%|█▊        | 180/1000 [00:09&lt;00:21, 38.08it/s, 63 steps of size 1.57e-01. acc. prob=0.78] warmup:  19%|█▉        | 188/1000 [00:09&lt;00:17, 45.65it/s, 63 steps of size 1.08e-01. acc. prob=0.78]warmup:  20%|█▉        | 196/1000 [00:09&lt;00:14, 53.61it/s, 31 steps of size 2.67e-01. acc. prob=0.78]warmup:  21%|██        | 207/1000 [00:09&lt;00:11, 66.48it/s, 63 steps of size 1.53e-01. acc. prob=0.78]warmup:  22%|██▏       | 215/1000 [00:09&lt;00:12, 64.06it/s, 15 steps of size 7.63e-02. acc. prob=0.78]warmup:  22%|██▎       | 225/1000 [00:09&lt;00:10, 73.01it/s, 31 steps of size 2.22e-01. acc. prob=0.78]warmup:  23%|██▎       | 233/1000 [00:09&lt;00:10, 73.46it/s, 15 steps of size 2.23e-01. acc. prob=0.78]warmup:  24%|██▍       | 242/1000 [00:10&lt;00:10, 74.40it/s, 63 steps of size 1.26e-01. acc. prob=0.78]warmup:  25%|██▌       | 251/1000 [00:10&lt;00:09, 78.15it/s, 31 steps of size 1.21e+00. acc. prob=0.78]warmup:  26%|██▌       | 260/1000 [00:10&lt;00:10, 69.48it/s, 63 steps of size 8.54e-03. acc. prob=0.78]warmup:  27%|██▋       | 268/1000 [00:10&lt;00:18, 39.32it/s, 127 steps of size 5.43e-02. acc. prob=0.78]warmup:  27%|██▋       | 274/1000 [00:10&lt;00:17, 42.25it/s, 31 steps of size 1.37e-01. acc. prob=0.78] warmup:  28%|██▊       | 281/1000 [00:10&lt;00:15, 45.67it/s, 63 steps of size 8.61e-02. acc. prob=0.78]warmup:  29%|██▊       | 287/1000 [00:11&lt;00:15, 45.68it/s, 127 steps of size 5.90e-02. acc. prob=0.78]warmup:  29%|██▉       | 293/1000 [00:11&lt;00:17, 40.39it/s, 31 steps of size 1.70e-01. acc. prob=0.78] warmup:  30%|███       | 301/1000 [00:11&lt;00:14, 47.96it/s, 31 steps of size 9.80e-02. acc. prob=0.78]warmup:  31%|███       | 307/1000 [00:11&lt;00:16, 40.96it/s, 127 steps of size 7.81e-03. acc. prob=0.78]warmup:  31%|███       | 312/1000 [00:12&lt;00:25, 26.77it/s, 127 steps of size 5.03e-02. acc. prob=0.78]warmup:  32%|███▏      | 321/1000 [00:12&lt;00:18, 36.37it/s, 63 steps of size 9.57e-02. acc. prob=0.78] warmup:  33%|███▎      | 328/1000 [00:12&lt;00:16, 41.50it/s, 63 steps of size 1.32e-01. acc. prob=0.78]warmup:  34%|███▎      | 335/1000 [00:12&lt;00:14, 45.99it/s, 63 steps of size 6.82e-02. acc. prob=0.78]warmup:  34%|███▍      | 341/1000 [00:12&lt;00:14, 46.74it/s, 63 steps of size 2.19e-02. acc. prob=0.78]warmup:  35%|███▍      | 347/1000 [00:12&lt;00:15, 41.48it/s, 31 steps of size 1.53e-01. acc. prob=0.78]warmup:  35%|███▌      | 352/1000 [00:12&lt;00:15, 42.04it/s, 63 steps of size 5.96e-02. acc. prob=0.78]warmup:  36%|███▌      | 358/1000 [00:12&lt;00:14, 44.03it/s, 95 steps of size 1.07e-01. acc. prob=0.78]warmup:  36%|███▋      | 365/1000 [00:13&lt;00:13, 48.50it/s, 63 steps of size 7.18e-02. acc. prob=0.78]warmup:  37%|███▋      | 371/1000 [00:13&lt;00:12, 48.69it/s, 63 steps of size 7.15e-02. acc. prob=0.78]warmup:  38%|███▊      | 377/1000 [00:13&lt;00:12, 48.25it/s, 127 steps of size 5.55e-02. acc. prob=0.78]warmup:  38%|███▊      | 383/1000 [00:13&lt;00:12, 50.14it/s, 63 steps of size 9.71e-02. acc. prob=0.78] warmup:  39%|███▉      | 389/1000 [00:13&lt;00:12, 48.92it/s, 127 steps of size 5.07e-02. acc. prob=0.78]warmup:  39%|███▉      | 394/1000 [00:13&lt;00:12, 47.39it/s, 127 steps of size 4.60e-02. acc. prob=0.78]warmup:  40%|███▉      | 399/1000 [00:13&lt;00:13, 44.14it/s, 63 steps of size 6.21e-02. acc. prob=0.78] warmup:  40%|████      | 404/1000 [00:13&lt;00:13, 43.63it/s, 63 steps of size 7.85e-02. acc. prob=0.78]warmup:  41%|████      | 409/1000 [00:14&lt;00:17, 32.84it/s, 127 steps of size 4.02e-02. acc. prob=0.78]warmup:  41%|████▏     | 413/1000 [00:14&lt;00:18, 31.86it/s, 63 steps of size 8.14e-02. acc. prob=0.78] warmup:  42%|████▏     | 420/1000 [00:14&lt;00:15, 38.46it/s, 63 steps of size 1.17e-01. acc. prob=0.79]warmup:  43%|████▎     | 429/1000 [00:14&lt;00:11, 48.74it/s, 63 steps of size 1.02e-01. acc. prob=0.79]warmup:  44%|████▎     | 435/1000 [00:14&lt;00:11, 49.66it/s, 63 steps of size 9.54e-02. acc. prob=0.79]warmup:  44%|████▍     | 441/1000 [00:14&lt;00:10, 50.96it/s, 31 steps of size 1.28e-01. acc. prob=0.79]warmup:  45%|████▍     | 447/1000 [00:14&lt;00:11, 49.07it/s, 63 steps of size 5.09e-02. acc. prob=0.78]warmup:  45%|████▌     | 453/1000 [00:15&lt;00:14, 38.32it/s, 63 steps of size 1.02e-02. acc. prob=0.78]warmup:  46%|████▌     | 458/1000 [00:15&lt;00:17, 31.71it/s, 63 steps of size 5.24e-02. acc. prob=0.78]warmup:  46%|████▋     | 465/1000 [00:15&lt;00:15, 34.17it/s, 255 steps of size 2.61e-02. acc. prob=0.78]warmup:  47%|████▋     | 469/1000 [00:15&lt;00:15, 34.34it/s, 63 steps of size 1.63e-01. acc. prob=0.78] warmup:  47%|████▋     | 473/1000 [00:15&lt;00:15, 32.98it/s, 63 steps of size 9.14e-02. acc. prob=0.78]warmup:  48%|████▊     | 477/1000 [00:15&lt;00:16, 31.99it/s, 127 steps of size 4.32e-02. acc. prob=0.78]warmup:  48%|████▊     | 481/1000 [00:16&lt;00:26, 19.90it/s, 255 steps of size 2.60e-02. acc. prob=0.78]warmup:  48%|████▊     | 484/1000 [00:16&lt;00:25, 20.54it/s, 63 steps of size 8.64e-02. acc. prob=0.78] warmup:  49%|████▉     | 488/1000 [00:16&lt;00:21, 23.93it/s, 63 steps of size 8.34e-02. acc. prob=0.78]warmup:  49%|████▉     | 491/1000 [00:16&lt;00:20, 25.05it/s, 63 steps of size 1.05e-01. acc. prob=0.78]warmup:  50%|████▉     | 496/1000 [00:16&lt;00:16, 30.31it/s, 63 steps of size 2.10e-02. acc. prob=0.78]warmup:  50%|█████     | 500/1000 [00:16&lt;00:17, 27.80it/s, 63 steps of size 4.79e-02. acc. prob=0.78]sample:  50%|█████     | 505/1000 [00:17&lt;00:15, 31.54it/s, 63 steps of size 4.79e-02. acc. prob=0.97]sample:  51%|█████     | 509/1000 [00:17&lt;00:15, 32.26it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  52%|█████▏    | 515/1000 [00:17&lt;00:12, 37.64it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  52%|█████▏    | 521/1000 [00:17&lt;00:11, 41.81it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  53%|█████▎    | 526/1000 [00:17&lt;00:11, 41.86it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  53%|█████▎    | 532/1000 [00:17&lt;00:10, 44.53it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  54%|█████▎    | 537/1000 [00:17&lt;00:11, 41.48it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  54%|█████▍    | 542/1000 [00:17&lt;00:11, 39.04it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  55%|█████▍    | 546/1000 [00:17&lt;00:11, 38.09it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  55%|█████▌    | 550/1000 [00:18&lt;00:11, 38.37it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  56%|█████▌    | 555/1000 [00:18&lt;00:11, 39.97it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  56%|█████▌    | 561/1000 [00:18&lt;00:10, 43.79it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  57%|█████▋    | 566/1000 [00:18&lt;00:10, 43.35it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  57%|█████▋    | 571/1000 [00:18&lt;00:09, 42.97it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  58%|█████▊    | 576/1000 [00:18&lt;00:09, 42.92it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  58%|█████▊    | 581/1000 [00:18&lt;00:10, 41.51it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  59%|█████▊    | 587/1000 [00:18&lt;00:09, 44.30it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  59%|█████▉    | 592/1000 [00:19&lt;00:08, 45.78it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  60%|█████▉    | 598/1000 [00:19&lt;00:08, 47.80it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  60%|██████    | 603/1000 [00:19&lt;00:08, 46.55it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  61%|██████    | 608/1000 [00:19&lt;00:08, 47.40it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  61%|██████▏   | 613/1000 [00:19&lt;00:08, 43.80it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  62%|██████▏   | 619/1000 [00:19&lt;00:08, 45.66it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  62%|██████▏   | 624/1000 [00:19&lt;00:08, 46.77it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  63%|██████▎   | 630/1000 [00:19&lt;00:07, 47.68it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|██████▎   | 635/1000 [00:19&lt;00:07, 47.65it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|██████▍   | 640/1000 [00:20&lt;00:07, 46.02it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|██████▍   | 645/1000 [00:20&lt;00:07, 45.85it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  65%|██████▌   | 650/1000 [00:20&lt;00:07, 44.93it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  66%|██████▌   | 655/1000 [00:20&lt;00:07, 43.89it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  66%|██████▌   | 660/1000 [00:20&lt;00:08, 38.23it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  66%|██████▋   | 664/1000 [00:20&lt;00:08, 38.46it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  67%|██████▋   | 670/1000 [00:20&lt;00:08, 39.84it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  68%|██████▊   | 675/1000 [00:20&lt;00:07, 41.03it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  68%|██████▊   | 680/1000 [00:21&lt;00:07, 41.55it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  69%|██████▊   | 686/1000 [00:21&lt;00:06, 45.01it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  69%|██████▉   | 692/1000 [00:21&lt;00:06, 46.92it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  70%|██████▉   | 697/1000 [00:21&lt;00:06, 46.10it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  70%|███████   | 703/1000 [00:21&lt;00:06, 47.49it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  71%|███████   | 709/1000 [00:21&lt;00:05, 48.80it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  71%|███████▏  | 714/1000 [00:21&lt;00:06, 43.34it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  72%|███████▏  | 719/1000 [00:21&lt;00:06, 42.94it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  72%|███████▎  | 725/1000 [00:22&lt;00:06, 45.12it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  73%|███████▎  | 731/1000 [00:22&lt;00:05, 46.76it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  74%|███████▎  | 736/1000 [00:22&lt;00:05, 45.40it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  74%|███████▍  | 741/1000 [00:22&lt;00:05, 44.14it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  75%|███████▍  | 746/1000 [00:22&lt;00:05, 44.27it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  75%|███████▌  | 751/1000 [00:22&lt;00:05, 44.01it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  76%|███████▌  | 757/1000 [00:22&lt;00:05, 45.87it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  76%|███████▌  | 762/1000 [00:22&lt;00:05, 42.77it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  77%|███████▋  | 768/1000 [00:22&lt;00:05, 45.08it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  77%|███████▋  | 774/1000 [00:23&lt;00:04, 46.79it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  78%|███████▊  | 779/1000 [00:23&lt;00:12, 17.13it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  78%|███████▊  | 784/1000 [00:24&lt;00:10, 20.18it/s, 191 steps of size 4.79e-02. acc. prob=0.95]sample:  79%|███████▉  | 788/1000 [00:24&lt;00:09, 21.81it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  79%|███████▉  | 793/1000 [00:24&lt;00:07, 26.30it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  80%|███████▉  | 798/1000 [00:24&lt;00:06, 30.14it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  80%|████████  | 803/1000 [00:24&lt;00:06, 31.67it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  81%|████████  | 807/1000 [00:24&lt;00:05, 32.27it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  81%|████████  | 812/1000 [00:24&lt;00:05, 35.41it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  82%|████████▏ | 816/1000 [00:24&lt;00:05, 33.68it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  82%|████████▏ | 820/1000 [00:25&lt;00:05, 30.83it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  82%|████████▏ | 824/1000 [00:25&lt;00:05, 32.45it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  83%|████████▎ | 828/1000 [00:25&lt;00:05, 31.97it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  83%|████████▎ | 832/1000 [00:25&lt;00:05, 31.57it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  84%|████████▎ | 837/1000 [00:25&lt;00:04, 33.69it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  84%|████████▍ | 842/1000 [00:25&lt;00:04, 36.85it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  85%|████████▍ | 846/1000 [00:25&lt;00:04, 36.84it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  85%|████████▌ | 850/1000 [00:25&lt;00:04, 34.42it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  85%|████████▌ | 854/1000 [00:25&lt;00:04, 35.85it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  86%|████████▌ | 859/1000 [00:26&lt;00:03, 38.27it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  86%|████████▋ | 865/1000 [00:26&lt;00:03, 42.00it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  87%|████████▋ | 870/1000 [00:26&lt;00:02, 43.62it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  88%|████████▊ | 875/1000 [00:26&lt;00:02, 44.90it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  88%|████████▊ | 881/1000 [00:26&lt;00:02, 44.17it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  89%|████████▊ | 886/1000 [00:26&lt;00:02, 43.50it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  89%|████████▉ | 891/1000 [00:26&lt;00:02, 39.59it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  90%|████████▉ | 896/1000 [00:26&lt;00:02, 41.93it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  90%|█████████ | 902/1000 [00:27&lt;00:02, 44.79it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  91%|█████████ | 907/1000 [00:27&lt;00:02, 46.08it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  91%|█████████ | 912/1000 [00:27&lt;00:01, 44.96it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  92%|█████████▏| 917/1000 [00:27&lt;00:02, 40.46it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  92%|█████████▏| 922/1000 [00:27&lt;00:01, 41.19it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  93%|█████████▎| 927/1000 [00:27&lt;00:01, 40.55it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  93%|█████████▎| 932/1000 [00:27&lt;00:01, 40.55it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  94%|█████████▎| 937/1000 [00:27&lt;00:01, 39.47it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  94%|█████████▍| 942/1000 [00:28&lt;00:01, 40.30it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  95%|█████████▍| 947/1000 [00:28&lt;00:01, 41.09it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  95%|█████████▌| 952/1000 [00:28&lt;00:01, 43.28it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  96%|█████████▌| 957/1000 [00:28&lt;00:01, 39.55it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  96%|█████████▌| 962/1000 [00:28&lt;00:00, 40.93it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  97%|█████████▋| 968/1000 [00:28&lt;00:00, 44.07it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  97%|█████████▋| 973/1000 [00:28&lt;00:00, 43.97it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  98%|█████████▊| 979/1000 [00:28&lt;00:00, 45.74it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  98%|█████████▊| 985/1000 [00:28&lt;00:00, 45.65it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  99%|█████████▉| 990/1000 [00:29&lt;00:00, 44.97it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample: 100%|█████████▉| 995/1000 [00:29&lt;00:00, 42.45it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:29&lt;00:00, 38.89it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:29&lt;00:00, 34.01it/s, 63 steps of size 4.79e-02. acc. prob=0.94]\n\n\nModel found 9 clusters.",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in models"
    ]
  },
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "Bayesian analysis with BI",
    "section": "",
    "text": "This document is a guide to Bayesian analysis and the implementation of Bayesian Inference (BI) package. It is intended for users ranging from those with little or no experience to advanced practitioners. In this introduction, we outline the main steps of Bayesian analysis. Each of the subsequent chapters present increasingly complex models. Each chapter will have the same structure in order to allow users to easily find the information they are looking for. The structure is as follows:\nWe recommend reading the introduction first since some key concepts here will not be revisited in later chapters.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#modeling-likelihood",
    "href": "0. Introduction.html#modeling-likelihood",
    "title": "Bayesian analysis with BI",
    "section": "Modeling Likelihood",
    "text": "Modeling Likelihood\nOnce the likelihood is defined, we can now define the mathematical equations that describe our parameters (\\mu and \\sigma) and their relationship with the dependent variable Y. We can express this relationship in the form of a linear function:\n\n\\mu = \\alpha + \\beta X\n\nWhere \\alpha is the intercept 🛈 and \\beta is the slope 🛈 of the regression line. These parameters are the unknowns that we want to estimate to evaluate the strength and direction of the relationship between the independent variable X and the dependent variable Y.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions",
    "href": "0. Introduction.html#link-functions",
    "title": "Bayesian analysis with BI",
    "section": "Link functions",
    "text": "Link functions\nDepending on the type of problem you are trying to solve (classification, regression, etc.) and the type of data you are working with (continuous, discrete, binomial, etc.), you will need to choose the appropriate distribution to describe the relationships in the data. For each different outcome distribution, you will need to use an appropriate link function 🛈.\nFor the moment, we just need to know that these different distributions require a link function (for each specific family we will discuss the corresponding link function in their respective chapters); however, below is a table summarizing some of the most common link functions, the mathematical form of each, their typical applications, and how to interpret them. Link functions in BI can be accessed through the class bi.link.XXX where XXX is the name of the link function. Let \\mu be a real number output from a linear model, and g(\\mu) be the corresponding link function. Then the table shows the most common link functions and their interpretations.\n\n\n\n\n\n\n\n\n\nLink Function\nMathematical Form of Inverse Link\nTypical Use / Model\nInterpretation & Range\n\n\n\n\nIdentity\ng(\\mu) = \\mu\nLinear regression (Normal)\nDirectly models \\mu; the outputs space is also the set of real numbers.\n\n\nLogit\ng(\\mu) = \\frac{1}{1+\\exp(-\\mu)}\nLogistic regression (Binomial)\nThe output space is the unit interval [0, 1] ; coefficients reflect log-odds.\n\n\nProbit\ng(\\mu) = \\Phi(\\mu)\nProbit regression (Binomial)\nSimilar to logit; uses the standard normal CDF, \\Phi.\n\n\nLog\ng(\\mu) = \\exp(\\mu)\nPoisson, Gamma regression (Count data)\nThe output space is the set of positive reals.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#the-prior-distributions",
    "href": "0. Introduction.html#the-prior-distributions",
    "title": "Bayesian analysis with BI",
    "section": "The Prior Distributions",
    "text": "The Prior Distributions\nFor each parameter in our model, we need to define a prior distribution 🛈 that encodes our initial beliefs about the parameter. In the case of the linear regression model, we need to specify prior distributions for the intercept, \\alpha, the slope, \\beta, and the standard deviation, \\sigma.\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nAnd with this, we can write our entire model as:\n\nY \\sim \\text{Normal}(\\mu, \\sigma)\n \n\\mu = \\alpha + \\beta X\n \n\\alpha \\sim \\text{Normal}(0, 1)\n \n\\beta \\sim \\text{Normal}(0, 1)\n \n\\sigma \\sim \\text{Exponential}(1)\n\nIn BI, you code all this statements within a single function. In that function you can use any probability distribution, link function, and mathematical operations required for your model (if they are supported by JAX). BI has been designed to allow you to declare your model as close as possible to the mathematical notation. For example, the model above can be written in BI as:\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\nbeta = 2.5\nalpha = 0.5\nsigma = 1.0\nX = m.dist.normal(0, 1, sample = True)\nY = m.dist.normal(alpha + beta * X, sigma, sample = True)\n\nm.data_on_model = dict(X=X, Y=Y)\n\ndef model(X, Y):    \n    alpha = m.dist.normal( 0, 1, name = 'alpha', shape= (1,))\n    beta = m.dist.normal( 0, 1, name = 'beta', shape= (1,))   \n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    m.normal(alpha + beta * X, sigma, obs=Y)\nThe code snippet provides several key features of the BI package:\n\nFirst, you need to initialize a bi object.\nThen, you can store data as a JAX array dictionary using the m.data_on_model function. If all the data can be stored in a data frame (e.g., Linear Regression for continuous variable), you do not need to use m.data_on_model, as the BI object automatically detects the data provided in the model arguments. However, sometimes you may need different data structures such as vectors and 2D arrays (e.g., Network model).\nRegarding distribution parameters, note the difference depending on whether you are generating data outside a function (e.g., for simulation purposes) or specifying priors inside a model function. In the former case, the argument sample should be set to True. However, if you are specifying priors within a model function, this argument is False by default.\nFinally, note that each parameter declared in the model must have a unique name as well as a shape. The shape refers to the number of parameters you want to estimate. For example, if you want to estimate a different \\beta for each independent variable, you would declare \\beta with a shape equal to the number of independent variables. By default, the shape is one, so technically you don’t need to specify it. In this example, we highlight this feature explicitly.\n\n\nWhich prior distribution range to use?\nThe choice of prior ranges can significantly affect Bayesian analysis results. There are several approaches to selecting them:\n\nExpert Knowledge: The prior distributions can be based on expert knowledge or historical data. This approach is useful when there is a lot of information available about the parameters.\nNoninformative Priors: When there is little or no information about the parameters, noninformative priors can be used. These priors are designed to have minimal influence on the posterior distribution, allowing the data to dominate the inference process.\nScaled data: If the data are scaled 🛈, the prior distributions can be chosen to reflect this. For example, if the data are scaled, the prior distributions for the intercept and slope can be centered around 0 and 1, respectively. By scaling the independent variable, we obtain a unit of change based on variance; that is, the effect represents a one–standard–deviation change in X on Y. Scaling the data improves both numerical stability. When all data are scaled to the same range, it leads to more stable numerical behavior during estimation. Additionally, it facilitates setting priors that are both meaningful and relatively uninformative. By aligning the scale of the data with the scale assumed in the priors, we ensure that the posterior distributions exhibit reasonable spread and that our uncertainty quantification is consistent with the data’s scale. For the remainder of this document, we will assume that the data are scaled. However, users should be aware that it is often necessary to rescale parameters estimates by the standard deviation of the data in order to get parameters that are interpretable on the natural scale.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fit-and-posterior-distribution",
    "href": "0. Introduction.html#model-fit-and-posterior-distribution",
    "title": "Bayesian analysis with BI",
    "section": "Model fit and posterior distribution",
    "text": "Model fit and posterior distribution\nOnce data are observed, Bayes’ Theorem 🛈 is used to evaluate how well a given set of parameter values fits the data:\n\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\nWhere:\n\n\\theta represents the unknown parameters we are interested in.\nP(\\theta) is the prior distribution, representing our beliefs about \\theta before seeing the data.\nP(\\text{data} \\mid \\theta) is the likelihood, representing the model of how the data are generated given \\theta.\nP(\\theta \\mid \\text{data}) is the posterior distribution, representing our updated beliefs after observing the data. It tells us not only the most likely value of \\theta (e.g., \\alpha, \\beta, and \\sigma in our case) but also quantifies the uncertainty in these estimates.\n\nVarious techniques can be used to approximate the mathematical definition of Bayes’ theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Descriptions of these algorithms are out of the scope of this document. For more information, please refer to Wikipedia. In BI, we use MCMC and it can be called as m.fit(model) where model is the function that describes the model.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "Bayesian analysis with BI",
    "section": "Model diagnostic",
    "text": "Model diagnostic\nOnce a Bayesian model has been fit, it is crucial to evaluate how well it captures the observed data and to assess whether the Markov chain Monte Carlo (MCMC) sampling has converged. Bayesian model diagnostics help us answer questions like: “Are our uncertainty estimates reliable?”, “Does the model generate data similar to what we observed?”, and “Have the chains mixed well?” Multiple diagnostics approaches can be used to assess the model’s performance. Below are some key diagnostic tools and techniques available in BI within the class BI.diag.XXX where XXX is the name of the diagnostic tool.\n\n\n\n\n\n\n\n\n\nDiagnostic Tool\nPurpose\nKey Indicator\nInterpretation\n\n\n\n\nposterior predictive checks (PPCs) 🛈\nAssess if the model can reproduce observed data\nGraphs, p-values, summary stats\nGood fit if simulated data resemble observed data\n\n\nCredible Interval (CI)\nQuantify uncertainty in parameter estimates\n95% CI or other percentage\n95% probability the parameter lies within the interval\n\n\nhighest posterior density intervals (HPDI) 🛈\nIdentify the narrowest interval containing a given probability mass density\n95% HPDI\nSmallest interval capturing 95% of the posterior density\n\n\neffective sample size (ESS) 🛈\nMeasure independent information in the chain\nESS value (ideally high)\nLow ESS indicates high autocorrelation (poor mixing) ot to few samples correlations\n\n\npotential scale reduction factor (Rhat) 🛈\nCheck convergence across multiple chains\nRhat ≈ 1 (typically &lt;1.01)\nValues near 1 indicate convergence; &gt;1 suggests non-convergence\n\n\nTrace plots 🛈\nVisualize the sampling path to check convergence and mixing\nPlot showing parameter values over iterations\nStationary, ‘hairy caterpillar’ pattern suggests convergence\n\n\nautocorrelation plots 🛈\nAssess dependency between samples over lags\nAutocorrelation values across lags\nRapid decay to zero suggests good mixing; slow decay indicates poor mixing\n\n\ndensity plots 🛈\nVisualize the posterior distribution of a parameter\nSmoothness and shape of the curve\nUnimodal and smooth suggests convergence; multimodal or irregular may suggest poor mixing",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-comparison",
    "href": "0. Introduction.html#model-comparison",
    "title": "Bayesian analysis with BI",
    "section": "Model comparison",
    "text": "Model comparison\nModel comparison is performed by evaluating how well different models explain the observed data while accounting for model complexity. Multiple criteria can be used to compare models, and are summarized in the table below. In BI, we can compare models using Watanabe-Akaike Information Criterion (WAIC) with the function m.diag.waic(model1, model2).\n\n\n\n\n\n\n\n\n\n\nCriterion\nPurpose\nInterpretation\nStrengths\nWeaknesses\n\n\n\n\nDIC (Deviance Information Criterion)\nMeasures model fit while penalizing complexity\nLower values indicate better model fit\nSimple to compute, useful for hierarchical models\nSensitive to the number of parameters, not always reliable in complex models\n\n\nWAIC (Watanabe-Akaike Information Criterion)\nEstimates out-of-sample predictive accuracy while penalizing complexity\nLower values indicate better models\nMore robust than DIC, accounts for overfitting\nComputationally intensive for large models\n\n\nBF (Bayes Factor)\nQuantifies relative support for two models based on marginal likelihoods\nBF &gt; 1 favors the numerator model, BF &lt; 1 favors the denominator\nProvides direct evidence comparison, works with different model types\nSensitive to prior choices, requires good model specification",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html",
    "href": "8. Gamma-Poisson.html",
    "title": "Gamma-Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion 🛈, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#general-principles",
    "href": "8. Gamma-Poisson.html#general-principles",
    "title": "Gamma-Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion 🛈, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#considerations",
    "href": "8. Gamma-Poisson.html#considerations",
    "title": "Gamma-Poisson Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Poisson model.\nOverdispersion is handled because the Gamma-Poisson model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is log_days).",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#example",
    "href": "8. Gamma-Poisson.html#example",
    "title": "Gamma-Poisson Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package.\n\nPythonR\n\n\nfrom BI import bi\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim dat Gamma poisson.csv'\nm.data(data_path, sep=',') \nm.data_to_model(['log_days', 'monastery', 'y']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    phi = m.dist.exponential(1, name = 'phi', shape=(1,))\n    mu = m.jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n    m.dist.poisson(rate = Lambda, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  phi = bi.dist.exponential(1, name='phi', shape=c(1))\n  mu = jnp$exp(log_days + alpha + beta * monastery)\n  Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n  # Likelihood\n  m$poisson(rate=Lambda, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#mathematical-details",
    "href": "8. Gamma-Poisson.html#mathematical-details",
    "title": "Gamma-Poisson Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\\lambda_i \\sim \\text{Gamma}(\\mu_i \\phi, \\phi)\n\n\n\\log(\\mu_i) = \\text{rates}_i + \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\n\n\\phi \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\lambda_i is the rate parameter of the Poisson distribution for observation i, assuming that each Poisson count observation has its own rate_i.\n\\mu_i is the mean rate parameter.\n\n\\phi controls the level of overdispersion in the rates.\n\\alpha is the intercept term.\n\\beta is the regression coefficient.\nX_i is the value of the predictor variable for observation i.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#notes",
    "href": "8. Gamma-Poisson.html#notes",
    "title": "Gamma-Poisson Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in chapter 2.\nWe can apply interaction terms similarly as in chapter 3.\nWe can apply categorical variables similarly as in chapter 4.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#references",
    "href": "8. Gamma-Poisson.html#references",
    "title": "Gamma-Poisson Model",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "10. Multinomial model (wip).html",
    "href": "10. Multinomial model (wip).html",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Multinomial model.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "10. Multinomial model (wip).html#general-principles",
    "href": "10. Multinomial model (wip).html#general-principles",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Multinomial model.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "10. Multinomial model (wip).html#considerations",
    "href": "10. Multinomial model (wip).html#considerations",
    "title": "Multinomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Categorical model.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "10. Multinomial model (wip).html#example",
    "href": "10. Multinomial model (wip).html#example",
    "title": "Multinomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = 0\n    p = jax.nn.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(p[career], obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "10. Multinomial model (wip).html#mathematical-details",
    "href": "10. Multinomial model (wip).html#mathematical-details",
    "title": "Multinomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable Y_i with K categories, the Dirichlet likelihood function is:\n\nY_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the outcome (i.e. the vector of frequencies for each k categories) for observation i.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex 🛈.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.",
    "crumbs": [
      "Models",
      "Multinomial Model"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "If you have a case where you believe the effect of one independent variable depends on the value of another independent variable, you can use regression analysis with interaction terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two independent variables (see note on how this multiplication arises).",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#general-principles",
    "href": "3. Interaction between continuous variables.html#general-principles",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "If you have a case where you believe the effect of one independent variable depends on the value of another independent variable, you can use regression analysis with interaction terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two independent variables (see note on how this multiplication arises).",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "Interaction Terms in Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same assumptions as for Regression for continuous variable.\nWe wish to model the relationship between a dependent variable, Y, and an independent variable, X_1, whose effect varies as a function of a second independent variable X_2. To do this, we explicitly model the hypothesis that the slope between Y and X_1 depends on (i.e., is conditional on) X_2.\nFor continuous interactions with normalized data, the intercept becomes the grand mean 🛈 of the outcome variable.\nThe interpretation of slopes estimates is more complex. The coefficient for a non-interaction term reflects the expected change in Y when X_1 increases by one unit, holding X_2 constant at its average value. The coefficient for the interaction term represents how the effect of X_1 on Y changes depending on the value of X_2, and vice versa, showing how the relationship between the two variables influences the outcome Y.\nTriptych 🛈 plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "Interaction Terms in Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. The data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'tulips.csv'\nm.data(data_path, sep=';')\nm.scale(['blooms', 'water', 'shade']) # Normalize\n\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.normal(mu, sigma, obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Normalize\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(blooms, water,shade){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')\n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint')\n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "Interaction Terms in Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#frequentist-formulation",
    "href": "3. Interaction between continuous variables.html#frequentist-formulation",
    "title": "Interaction Terms in Regression",
    "section": "Frequentist formulation",
    "text": "Frequentist formulation\nWe model the relationship between the input features (X_1 and X_2) and the target variable (Y) using the following equation: \n𝑌_i = \\alpha + \\beta_1 𝑋_{[1,i]} + \\beta_2 𝑋_{[2,i]} + \\beta_3 𝑋_{[1,i]} 𝑋_{[2,i]} + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\nX_{[1,i]} and X_{[2,i]} are the values of the two independent variables for observation i.\n\\beta_1 and \\beta_2 are the coefficients for X_{1} and X_{2}, respectively, when the other variable has value 0.\n\\beta_3 is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\n\\epsilon_i is the error term, assumed to be independent and normally distributed.\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian regression model as follows:\n\nY_i \\sim \\text{Normal}(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_1 \\sim \\text{Normal}(0,1)\n\n\n\\beta_2 \\sim \\text{Normal}(0,1)\n\n\n\\beta_{3} \\sim \\text{Normal}(0,1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_1 and \\beta_2 are the coefficients for X_{1} and X_{2}, respectively, when the other variable has value 0.\n\\beta_3 is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\nX_{[1,i]} and X_{[2,i]} are the two values of the independent continuous variables for observation i.\n\\sigma is a standard deviation parameter, which here has an Exponential prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#Notes",
    "href": "3. Interaction between continuous variables.html#Notes",
    "title": "Interaction Terms in Regression",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe interaction term equation: \nY_i \\sim Normal(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n\ncan be re-written as: \nY_i \\sim Normal(\\alpha + (\\beta_1 + \\beta_{3} X_{[2,i]}) X_{[1,i]} + \\beta_2 X_{[2,i]}, \\sigma)\n\nsimply by factoring the terms with X_{[1,i]} in them. The result is that the coefficient on X_{[1,i]} is written specifically as a linear regression model of X_{[2,i]}.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "Interaction Terms in Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "14. Varying slopes.html",
    "href": "14. Varying slopes.html",
    "title": "Varying Slopes Models",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#general-principles",
    "href": "14. Varying slopes.html#general-principles",
    "title": "Varying Slopes Models",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#considerations",
    "href": "14. Varying slopes.html#considerations",
    "title": "Varying Slopes Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for 12. Varying intercepts.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance 🛈.\nTo construct the covariance matrix, we use an SRS decomposition where S is a diagonal matrix of standard deviations and R is a correlation matrix. To model the correlation matrix, we use an LKJcorr distribution parametrized with a single control parameter η that controls the amount of regularization. η is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near −1 or 1. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\nThe standard deviations in S are model with a prior that constrains them to strictly positive values.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#example",
    "href": "14. Varying slopes.html#example",
    "title": "Varying Slopes Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects. This example is based on McElreath (2018).\n\nSimulated data\n\nPython (Raw)Python (Build in function)R\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    Rho = m.dist.lkj(2, 2, name = 'Rho')\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n        N_group = N_cafes,\n        group = cafe,\n        global_intercept= a,\n        global_slope= b,\n        group_name = 'cafe')\n    \n\n    mu = varying_intercept + varying_slope* afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariatenormal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  m$normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details",
    "href": "14. Varying slopes.html#mathematical-details",
    "title": "Varying Slopes Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nCentered parameterization\nWe can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_{i} \\sim \\text{Normal}(\\mu_{i} , \\sigma)\n \n\\mu_{i} = \\alpha_{k(i)} + \\beta_{k(i)} X_{i}\n\nThe varying intercepts (\\alpha_k) and slopes (\\beta_k) are modeled using a Multivariate Normal distribution:\n\n\\begin{pmatrix}\n\\alpha_k \\\\\n\\beta_k\n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left(\n\\begin{pmatrix}\n\\bar{\\alpha} \\\\\n\\bar{\\beta}\n\\end{pmatrix},\n\\text{diag}(\\varsigma) ~ \\Omega ~ \\text{diag}(\\varsigma)\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim  \\text{Exponential}(1)\n \n\\Omega \\sim \\text{LKJ}(\\eta)\n\nWhere:\n\n\\left(\\begin{array}{cc} \\bar{\\alpha} \\\\ \\bar{\\beta} \\end{array}\\right) is a vector composed from concatenating a parameter for the global intercept and a parameter vector of the global slopes.\n\\varsigma is a vector giving the standard deviation of the random effects for the intercept and slopes across groups.\n\\Omega is the correlation matrix.\n\n\n\nNon-centered parameterization\nFor computational reasons, it is often better to implement a non-centered parameterization 🛈 that is equivalent to the Multivariate Normal distribution approach:\n\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n=\n\\left(\\begin{array}{cc}\n\\bar{\\alpha} \\\\\n\\bar{\\beta}\n\\end{array}\\right) + \\varsigma\\circ\n\\left(\nL \\cdot\n\\left(\n\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\n\\right)\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim  \\text{Exponential}(1)\n \nL \\sim \\text{LKJ Cholesky}(\\eta)\n\n\n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n\n\nWhere:\n\n\\sigma_\\alpha \\sim \\text{Exponential}(1) is the prior standard deviation among intercepts.\n\\sigma_\\beta \\sim \\text{Exponential}(1) is the prior standard deviation among slopes.\nL \\sim \\text{LKJcorr}(\\eta) is the a cholesky factor of the correlation matrix matrix using the Cholesky Factor 🛈\n\n\n\n\nMultivariate Model with One Random Slope for Each Variable\nWe can apply a multivariate model similarly to Chapter 2. In this case, we apply the same principle, but with a covariance matrix with a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for i observations in a model with two independent variables X_1 and X_2, we can define the formula as follows:\n\nY_{i}  \\sim \\text{Normal}(\\mu_i , \\sigma)\n\n\n\\mu_i =   \\alpha_i + \\beta_{k(i)} X_{1i}  + \\gamma_{k(i)} X_{2i}\n\n\n\n\\begin{pmatrix}\n\\alpha\\\\\n\\beta\\\\\n\\gamma\n\\end{pmatrix}\n\\sim \\begin{pmatrix}\n\\bar{\\alpha}\\\\\n\\bar{\\beta}\\\\\n\\bar{\\gamma}\n\\end{pmatrix} + \\varsigma \\circ\n\\left(\nL \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{k} \\\\\n\\widehat{\\beta}_{k} \\\\\n\\widehat{\\gamma}_{k}\n\\end{pmatrix}\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n\n  \n\\bar{\\gamma} \\sim \\text{Normal}(0, 1)\n\n\n\\varsigma \\sim  \\text{Exponential}(1)\n \nL \\sim \\text{LKJ Cholesky}(2)\n\n\n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\gamma}_k \\sim \\text{Exponential}(1)",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#notes",
    "href": "14. Varying slopes.html#notes",
    "title": "Varying Slopes Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#references",
    "href": "14. Varying slopes.html#references",
    "title": "Varying Slopes Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Varying Slopes Models"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html",
    "href": "24. Network control for data collection biases (wip).html",
    "title": "Controlling for Network Biases",
    "section": "",
    "text": "Data collection biases are a persistent issue in studies of social networks. Two main types of biases can be considered: exposure biases 🛈 and censoring biases 🛈.\nTo account for exposure biases, we can switch the network link probability model from a Poisson distribution to a Binomial distribution, as the binomial distribution allows us to account for the number of trials for each data estimation.\nTo address censoring biases, we need to add an additional equation to account for the probability of missing an interaction during observation when modeling the interaction between individuals i and j.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#considerations",
    "href": "24. Network control for data collection biases (wip).html#considerations",
    "title": "Controlling for Network Biases",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-1",
    "href": "24. Network control for data collection biases (wip).html#example-1",
    "title": "Controlling for Network Biases",
    "section": "Example 1",
    "text": "Example 1\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases. This example is based on Sosa et al. (n.d.).\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure_mat,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.binomial(total_count = m.net.mat_to_edgl(exposure_mat), logits = jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-2",
    "href": "24. Network control for data collection biases (wip).html#example-2",
    "title": "Controlling for Network Biases",
    "section": "Example 2",
    "text": "Example 2\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases and censoring biases:",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#mathematical-details",
    "href": "24. Network control for data collection biases (wip).html#mathematical-details",
    "title": "Controlling for Network Biases",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\n\nY_{[i,j]} \\sim \\text{Binomial}\\Big(E_{[i,j]}, Q_{[i,j]}  \\Big)\n\n\nQ_{[i,j]} = \\phi_{[i,j]}\\eta_{[i]}\\eta_{[j]}\n\nWhere:\n\nE_{[i,j]} is the number of trials for each observation (i.e., the sampling effort).\nQ_{[i,j]} is the indicator of a true tie between i and j, defined as: \nQ_{[i,j]} \\sim \\begin{cases}\n0 & \\text{if no interaction occurs or if } i \\text{ or } j \\text{ is not detectable} \\\\\n1 & \\text{if } i \\text{ and } j \\text{ are both detectable}\n\\end{cases}\n\n\\phi_{[i,j]} is the probability of a true tie between i and j.\n\\eta_{[i]} is the probability of individual i being detectable.\n\\eta_{[j]} is the probability of individual j being detectable.\n\n\n\nDefining formula sub-equations and prior distributions\nWe can let \\eta_{[i]} depend on individual-specific covariates. To model the probability of censoring, we can model 1-\\eta_{[i]}: \n\\text{logit}(1-\\eta_{[i]}) = \\mu_\\psi + \\hat\\psi_{[i]}  \\sigma_\\psi + \\dots\n\nWhere:\n\n\\mu_\\psi is the intercept term.\n\\sigma_\\psi is a scalar for the variance of random effects.\n\\hat\\psi_{[i]}\\sim \\text{Normal}(0,1), and the ellipsis signifies any linear model of coefficients and individual-level covariates. For example, if C is an animal-specific measure, like a binary variable for cryptic coloration, then the ellipsis may be replaced with \\kappa_{[5]}C_{[i]} to give the effects of coloration on censoring probability.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#notes",
    "href": "24. Network control for data collection biases (wip).html#notes",
    "title": "Controlling for Network Biases",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nOne major limitation of this model is the necessity of having an estimation of the censoring bias for each individual.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html",
    "href": "10. Dirichlet model (wip).html",
    "title": "Dirichlet Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Dirichlet model.",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#general-principles",
    "href": "10. Dirichlet model (wip).html#general-principles",
    "title": "Dirichlet Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Dirichlet model.",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#considerations",
    "href": "10. Dirichlet model (wip).html#considerations",
    "title": "Dirichlet Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Multinomial model.",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#example",
    "href": "10. Dirichlet model (wip).html#example",
    "title": "Dirichlet Model",
    "section": "Example",
    "text": "Example\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = 0\n    p = jax.nn.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dirichlet(p[career], lambda_, obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#mathematical-details",
    "href": "10. Dirichlet model (wip).html#mathematical-details",
    "title": "Dirichlet Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable Y_i with 𝐾 categories, the Dirichlet likelihood function is:\n\nY_i \\sim \\text{Dirichlet}(\\theta_i  \\kappa) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\kappa \\sim \\text{Exponential}(1) \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the outcome simplex 🛈 for observation i.\n\\kappa is the concentration parameter, it controls the prior weight on each category.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex 🛈.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#references",
    "href": "10. Dirichlet model (wip).html#references",
    "title": "Dirichlet Model",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Dirichlet Model"
    ]
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "Regression with a Categorical Independent Variables",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding 🛈 or by converting categories to indices 🛈.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#general-principles",
    "href": "4. Categorical variable.html#general-principles",
    "title": "Regression with a Categorical Independent Variables",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding 🛈 or by converting categories to indices 🛈.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "Regression with a Categorical Independent Variables",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for a Continuous Variable.\nAs we generate regression coefficients for each k category, we need to specify a prior with a shape equal to the number of categories k in the code (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare confidence intervals or p-values directly.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "Regression with a Categorical Independent Variables",
    "section": "Example",
    "text": "Example\nBelow is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (kcal_per_g), representing the caloric value of milk per gram, a categorical independent variable (index_clade), representing species clade membership, and a continuous independent variable (mass), representing the mass of individuals in the clade. The goal is to estimate the differences in milk calories between clades. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'milk.csv'\nm.data(data_path, sep=';') \nm.index([\"clade\"]) # Manipulate\nm.scale(['kcal_per_g']) # Scale\nm.data_to_model(['kcal_per_g', \"index_clade\"]) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade):\n    a = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    b = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    s = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(kcal_per_g, index_clade){\n  # Parameter prior distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma =bi.dist.exponential(1, name = 's')\n  # Likelihood\n  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor R users, when working with indices you have to ensure 1) that indices are intergers (i.e. as.integer(index_clade)) and, 2) that indices start at 0 (i.e. as.integer(index_clade)-1).",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "Regression with a Categorical Independent Variables",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\nY_i = \\alpha + \\beta_k X_i + \\sigma\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\n\\beta_k are the regression coefficients for each k category.\nX_i is the encoded categorical input variable for observation i.\n\\sigma is the error term.\n\nWe can interpret \\beta_i as the effect of each category on Y relative to the baseline (usually one of the categories or the intercept).\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY \\sim \\text{Normal}(\\alpha +  \\beta_K X, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_K \\sim \\text{Normal}(0,1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_K are slope coefficients for the K distinct independent variables categories, which also have unit-normal priors.\nX_i is the encoded categorical input variable for observation i.\n\\sigma is a standard deviation parameter, which here has a Exponential prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "Regression with a Categorical Independent Variables",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms similarly to Chapter 3: Interaction between Continuous Variables.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "Regression with a Categorical Independent Variables",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variables"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html",
    "href": "26. Network Based Diffusion analysis (wip).html",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links Hasenjager, Leadbeater, and Hoppitt (2021). The basic model underlying NBDA states that at time t an individual, i, learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g. the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e. individuals that acquired the behavior of interest at time t-1).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "href": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links Hasenjager, Leadbeater, and Hoppitt (2021). The basic model underlying NBDA states that at time t an individual, i, learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g. the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e. individuals that acquired the behavior of interest at time t-1).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#considerations",
    "href": "26. Network Based Diffusion analysis (wip).html#considerations",
    "title": "Network-Based Diffusion Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThere are two main NBDA variants: order-of-acquisition diffusion analysis (OADA), which takes as data the order in which individuals acquired the target behaviour, and time-of-acquisition diffusion analysis (TADA), which uses the times of acquisition of the target behaviour.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#example",
    "href": "26. Network Based Diffusion analysis (wip).html#example",
    "title": "Network-Based Diffusion Analysis",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Multiplex network model using the Bayesian Inference (BI) package Nightingale et al. (2015):\n\nPython",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "href": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "title": "Network-Based Diffusion Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormulation\nThere are two parameters of interest in the basic time of acquisition diffusion analysis model: the rate of social transmission be-tween individuals per unit of network connection,s, and the baseline rate of trait performance in the absence of social transmission, λ_0.\n\n\\lambda_i(t) = \\lambda_0(t) (1- z_i(t))  \\left[ s \\sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \\right]\n\nWhere:\n\n\\lambda_i(t) is the rate at which individuals i acquire the task solution at time t.\n\\lambda_0(t) is a baseline acquisition function determining the distribution of latencies to acquisition in the absence of social transmission (that is, through asocial learning). It can be specify by an exponential or Weibull distrbution.\nz_i(t) gives the status (1 = informed, 0 = naïve) of individual i at time t.\ns is the regression coefficients capturing the effect of x on the hazard have an assigned a normal prior.\n(1- z_i(t)) and z_j (-1) terms ensure that the task solution is only transmitted from informed to uninformed individuals:\n\n\nz_j(t) =  Y_i \\sim \\begin{cases}\n0, & \\text{if j is naive} \\\\\n1, & \\text{if j is informed}\n\\end{cases}",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#notes",
    "href": "26. Network Based Diffusion analysis (wip).html#notes",
    "title": "Network-Based Diffusion Analysis",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#references",
    "href": "26. Network Based Diffusion analysis (wip).html#references",
    "title": "Network-Based Diffusion Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nHasenjager, Matthew J., Ellouise Leadbeater, and William Hoppitt. 2021. “Detecting and Quantifying Social Transmission Using Network-Based Diffusion Analysis.” Journal of Animal Ecology 90 (1): 8–26. https://doi.org/https://doi.org/10.1111/1365-2656.13307.\n\n\nNightingale, Glenna, Neeltje J Boogert, Kevin N Laland, and Will Hoppitt. 2015. “Quantifying Diffusion in Social Networks: A Bayesian Approach.” Animal Social Networks, 38–52.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "21. DPMM.html",
    "href": "21. DPMM.html",
    "title": "Dirichlet Process Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a non-parametric 🛈 clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its center (mean \\mu) and its shape/spread (covariance \\sigma).\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#general-principles",
    "href": "21. DPMM.html#general-principles",
    "title": "Dirichlet Process Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a non-parametric 🛈 clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its center (mean \\mu) and its shape/spread (covariance \\sigma).\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#considerations",
    "href": "21. DPMM.html#considerations",
    "title": "Dirichlet Process Mixture Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA DPMM is a Bayesian model 🛈 that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation called the Stick-Breaking Process 🛈.\nThe key parameters and their priors are:\n\nConcentration \\alpha: This single parameter controls the tendency to create new clusters. A low α favors fewer, larger clusters, while a high α allows for many smaller clusters. We typically place a Gamma prior on \\alpha to learn its value from the data.\n\n\nCluster Weights w: Generated via the Stick-Breaking process from \\alpha. These are the probabilities of drawing a data point from any given cluster.\nCluster Parameters (\\mu, \\sigma): Each potential cluster has a mean \\mu and a covariance matrix \\sigma. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, 14). Howver, if the data is one-dimensional, we use a univariate normal distribution.\n\nThe model is often implemented in its marginalized form 🛈. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#example",
    "href": "21. DPMM.html#example",
    "title": "Dirichlet Process Mixture Models",
    "section": "Example",
    "text": "Example\nBelow is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\nPythonR\n\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n#  The model\ndef dpmm(data, T=10):\n    N, D = data.shape  # Number of features\n    data_mean = jnp.mean(data, axis=0)\n    data_std = jnp.std(data, axis=0)*2\n\n    # 1) stick-breaking weights\n    alpha = dist.gamma(1.0, 10.0,name='alpha')\n\n    with m.plate(\"beta_plate\", T - 1):\n        beta = m.dist.Beta(1, alpha))\n\n    w = numpyro.deterministic(\"w\",mix_weights(beta))\n\n\n    # 2) component parameters\n    with m.plate(\"components\", T):\n        mu = m.dist.multivariatenormal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n        sigma = m.dist.lognormal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n        Lcorr = m.dist.lkjcholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n\n        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n\n    # 3) Latent cluster assignments for each data point\n    with m.plate(\"data\", N):\n        # Sample the assignment for each data point\n        z = m.dist.Categorical(w) # shape (N,)  \n\n        # Sample the data point from the assigned component\n        m.dist.MultivariateNormal(loc=mu[z], scale_tril=scale_tril[z],\n            obs=data\n        )  \n\nm.data_on_model = dict(data=data)\nm.fit(dpmm)  # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#mathematical-details",
    "href": "21. DPMM.html#mathematical-details",
    "title": "Dirichlet Process Mixture Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThis level describes how any single data point, x_i, is generated. The process involves two steps: first, assigning the data point to a cluster, and second, drawing it from that cluster’s specific distribution. We use a truncation level T as a finite approximation for the infinite number of possible clusters in a true Dirichlet Process.\n\nx_i \\mid z_i=k \\sim \\text{MultivariateNormal}(\\mu_k, \\Sigma_{\\text{obs}}) \\\\\nz_i \\sim \\text{Categorical}(w) \\\\\nw = \\text{StickBreaking}(\\beta_1, ..., \\beta_{T-1}) \\\\\n\\beta_k \\sim \\text{Beta}(1, \\alpha) \\\\\n\\alpha \\sim \\text{Gamma}(1, 10)\\\\\n\\mu_k \\sim \\text{MultivariateNormal}(\\mu_0, \\Sigma_0)  \\\\\n\\Sigma_{\\text{obs}} = I_D \\\\\n\nParameter Definitions: * Observed Data: * x_i: The i-th observed D-dimensional data point.\n\nLatent Variables (Inferred):\n\nz_i: The integer cluster assignment for the i-th data point.\nw: The vector of mixture weights, where w_k is the probability of belonging to cluster k.\n\\beta_k: The set of Beta-distributed random variables for the stick-breaking process.\n\\alpha: The concentration parameter, controlling the effective number of clusters.\n\n\\mu_k: The D-dimensional mean vector of the k-th cluster.\n\nHyperparameters (Fixed):\n\n\\mu_0: The prior mean for the cluster centers (e.g., mean(data)).\n\\Sigma_0: The prior covariance for the cluster centers (e.g., 10 * I_D).",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#notes",
    "href": "21. DPMM.html#notes",
    "title": "Dirichlet Process Mixture Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary advantage of the DPMM over methods like K-Means or a GMM is the automatic inference of the number of clusters. Instead of running the model multiple times with different values of K and comparing them, the DPMM explores different numbers of clusters as part of its fitting process. The posterior distribution of the weights w reveals which components are “active” (have significant weight) and thus gives a probabilistic estimate of the number of clusters supported by the data.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "21. DPMM.html#references",
    "href": "21. DPMM.html#references",
    "title": "Dirichlet Process Mixture Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nGershman and Blei (2012)",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models"
    ]
  },
  {
    "objectID": "12. Survival analysis.html",
    "href": "12. Survival analysis.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving beyond a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#general-principles",
    "href": "12. Survival analysis.html#general-principles",
    "title": "Survival Analysis",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving beyond a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#considerations",
    "href": "12. Survival analysis.html#considerations",
    "title": "Survival Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nBayesian models provide a framework to account for uncertainty 🛈 in parameter estimates through posterior distributions. You will need to define prior distributions 🛈 for all model parameters, such as baseline hazard, covariate effects, and variance terms.\nIn survival analysis:\n\nThe baseline hazard can follow distributions like Exponential, Weibull, or Gompertz, depending on the data.\nCensoring (when the event is not observed for some subjects) must be accounted for in the likelihood function. Proper handling is essential for unbiased results.\n\nBayesian survival models allow flexible handling of time-dependent covariates, random effects, and incorporate uncertainty more naturally than Frequentist methods.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#example",
    "href": "12. Survival analysis.html#example",
    "title": "Survival Analysis",
    "section": "Example",
    "text": "Example\nHere’s an example of a Bayesian survival analysis using the Bayesian Inference (BI) package. The data come from a clinical trial of mastectomy for breast cancer. The goal is to estimate the effect of the metastasized covariate, coded as 0 (no metastasis) and 1 (metastasis), on the survival outcome event for each patient. Time is continuous and censoring is indicated by the event variable.\n\nPython\n\n\n\nfrom BI import bi\nimport numpy as np\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'mastectomy.csv'\nm.data(data_path, sep=',') \n\nm.df.metastasized = (m.df.metastasized == \"yes\").astype(np.int64)\nm.df.event = jnp.array(m.df.event.values, dtype=jnp.int32)\n\n## Create survival object\nm.models.survival.surv_object(time='time', event='event', cov='metastasized', interval_length=3)\n\n# Plot censoring ------------------------------------------------\nm.models.survival.plot_censoring(cov='metastasized')\n\n# Model ------------------------------------------------\ndef model(intervals, death, metastasized, exposure):\n    # Parameter prior distributions-------------------------\n    ## Base hazard distribution\n    lambda0 = m.dist.gamma(0.01, 0.01, shape= intervals.shape, name = 'lambda0')\n    ## Covariate effect distribution\n    beta = m.dist.normal(0, 1000, shape = (1,),  name='beta')\n    ### Likelihood\n    #### Compute hazard rate based on covariate effect\n    lambda_ = m.models.survival.hazard_rate(cov = metastasized, beta = beta, lambda0 = lambda0)\n    #### Compute exposure rates\n    mu = exposure * lambda_\n\n    # Likelihood calculation\n    y = m.dist.poisson(mu + jnp.finfo(mu.dtype).tiny, obs = death)\n\n# Run mcmc ------------------------------------------------\nm.fit(model, num_samples=500) \n\n# Summary ------------------------------------------------\nprint(m.summary())\n\n# Plot hazards and survival function ------------------------------------------------\nm.models.survival.plot_surv()\n\nWARNING:2025-09-25 08:48:43,267:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;08:28,  1.96it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|▏         | 20/1000 [00:00&lt;00:22, 42.73it/s, 1023 steps of size 2.66e-01. acc. prob=0.74]warmup:   3%|▎         | 31/1000 [00:00&lt;00:18, 51.62it/s, 1023 steps of size 1.65e-01. acc. prob=0.75]warmup:   4%|▍         | 41/1000 [00:00&lt;00:17, 55.20it/s, 1023 steps of size 1.34e-01. acc. prob=0.76]warmup:   5%|▍         | 49/1000 [00:01&lt;00:16, 56.51it/s, 511 steps of size 2.34e-01. acc. prob=0.77] warmup:   6%|▌         | 57/1000 [00:01&lt;00:15, 61.05it/s, 1023 steps of size 1.38e-01. acc. prob=0.77]warmup:   6%|▋         | 65/1000 [00:01&lt;00:14, 64.21it/s, 1023 steps of size 3.22e-01. acc. prob=0.78]warmup:   7%|▋         | 73/1000 [00:01&lt;00:14, 61.97it/s, 619 steps of size 1.54e-01. acc. prob=0.77] warmup:   8%|▊         | 80/1000 [00:01&lt;00:14, 61.77it/s, 1023 steps of size 2.87e-01. acc. prob=0.78]warmup:   9%|▊         | 87/1000 [00:01&lt;00:14, 62.76it/s, 1023 steps of size 1.90e-01. acc. prob=0.78]warmup:   9%|▉         | 94/1000 [00:01&lt;00:14, 64.19it/s, 150 steps of size 2.51e-01. acc. prob=0.78] warmup:  10%|█         | 105/1000 [00:01&lt;00:11, 76.50it/s, 511 steps of size 7.33e-03. acc. prob=0.76]warmup:  11%|█▏        | 113/1000 [00:01&lt;00:12, 72.69it/s, 511 steps of size 1.37e-02. acc. prob=0.77]warmup:  12%|█▏        | 123/1000 [00:02&lt;00:11, 77.16it/s, 1023 steps of size 8.42e-03. acc. prob=0.77]warmup:  13%|█▎        | 132/1000 [00:02&lt;00:11, 76.69it/s, 1023 steps of size 9.63e-03. acc. prob=0.77]warmup:  14%|█▍        | 143/1000 [00:02&lt;00:10, 85.15it/s, 511 steps of size 4.42e-03. acc. prob=0.77] warmup:  15%|█▌        | 152/1000 [00:02&lt;00:10, 78.01it/s, 1 steps of size 1.15e-02. acc. prob=0.77]  warmup:  16%|█▌        | 161/1000 [00:02&lt;00:11, 71.02it/s, 511 steps of size 3.70e-03. acc. prob=0.77]warmup:  17%|█▋        | 169/1000 [00:02&lt;00:12, 67.21it/s, 1023 steps of size 1.03e-02. acc. prob=0.77]warmup:  18%|█▊        | 178/1000 [00:02&lt;00:11, 69.66it/s, 1023 steps of size 8.95e-03. acc. prob=0.77]warmup:  19%|█▊        | 186/1000 [00:02&lt;00:11, 68.12it/s, 511 steps of size 1.15e-02. acc. prob=0.77] warmup:  19%|█▉        | 194/1000 [00:03&lt;00:11, 70.98it/s, 511 steps of size 5.34e-03. acc. prob=0.77]warmup:  20%|██        | 202/1000 [00:03&lt;00:11, 69.45it/s, 1023 steps of size 5.75e-03. acc. prob=0.77]warmup:  21%|██        | 210/1000 [00:03&lt;00:11, 66.16it/s, 1023 steps of size 7.70e-03. acc. prob=0.77]warmup:  22%|██▏       | 218/1000 [00:03&lt;00:11, 67.35it/s, 1023 steps of size 4.88e-03. acc. prob=0.77]warmup:  23%|██▎       | 228/1000 [00:03&lt;00:10, 75.41it/s, 1023 steps of size 8.55e-03. acc. prob=0.78]warmup:  24%|██▍       | 238/1000 [00:03&lt;00:09, 81.37it/s, 144 steps of size 5.19e-03. acc. prob=0.78] warmup:  25%|██▍       | 248/1000 [00:03&lt;00:08, 85.62it/s, 511 steps of size 3.11e-03. acc. prob=0.78]warmup:  26%|██▌       | 258/1000 [00:03&lt;00:08, 87.81it/s, 511 steps of size 6.43e-03. acc. prob=0.77]warmup:  27%|██▋       | 267/1000 [00:03&lt;00:09, 80.70it/s, 511 steps of size 1.04e-02. acc. prob=0.78]warmup:  28%|██▊       | 276/1000 [00:04&lt;00:09, 77.06it/s, 255 steps of size 3.76e-03. acc. prob=0.78]warmup:  28%|██▊       | 284/1000 [00:04&lt;00:09, 73.34it/s, 1023 steps of size 6.26e-03. acc. prob=0.78]warmup:  29%|██▉       | 292/1000 [00:04&lt;00:09, 74.69it/s, 511 steps of size 1.00e-02. acc. prob=0.78] warmup:  30%|███       | 300/1000 [00:04&lt;00:09, 71.29it/s, 1023 steps of size 6.19e-03. acc. prob=0.78]warmup:  31%|███       | 308/1000 [00:04&lt;00:10, 68.29it/s, 1023 steps of size 3.10e-03. acc. prob=0.78]warmup:  32%|███▏      | 317/1000 [00:04&lt;00:09, 73.16it/s, 380 steps of size 8.10e-03. acc. prob=0.78] warmup:  32%|███▎      | 325/1000 [00:04&lt;00:09, 74.70it/s, 1023 steps of size 3.88e-03. acc. prob=0.78]warmup:  33%|███▎      | 334/1000 [00:04&lt;00:08, 78.77it/s, 511 steps of size 5.63e-03. acc. prob=0.78] warmup:  34%|███▍      | 342/1000 [00:05&lt;00:08, 76.21it/s, 511 steps of size 4.10e-03. acc. prob=0.78]warmup:  35%|███▌      | 351/1000 [00:05&lt;00:08, 77.97it/s, 1023 steps of size 5.46e-03. acc. prob=0.78]warmup:  36%|███▌      | 360/1000 [00:05&lt;00:08, 79.96it/s, 511 steps of size 7.81e-03. acc. prob=0.78] warmup:  37%|███▋      | 369/1000 [00:05&lt;00:08, 75.50it/s, 698 steps of size 4.54e-03. acc. prob=0.78]warmup:  38%|███▊      | 377/1000 [00:05&lt;00:08, 69.77it/s, 157 steps of size 5.97e-03. acc. prob=0.78]warmup:  39%|███▊      | 386/1000 [00:05&lt;00:08, 72.21it/s, 1023 steps of size 6.02e-03. acc. prob=0.78]warmup:  40%|███▉      | 395/1000 [00:05&lt;00:07, 76.19it/s, 1023 steps of size 4.95e-03. acc. prob=0.78]warmup:  40%|████      | 403/1000 [00:05&lt;00:07, 75.23it/s, 373 steps of size 4.64e-03. acc. prob=0.78] warmup:  41%|████      | 411/1000 [00:05&lt;00:07, 75.49it/s, 511 steps of size 9.35e-03. acc. prob=0.78]warmup:  42%|████▏     | 423/1000 [00:06&lt;00:06, 87.70it/s, 322 steps of size 6.29e-03. acc. prob=0.78]warmup:  43%|████▎     | 433/1000 [00:06&lt;00:06, 89.30it/s, 511 steps of size 9.96e-03. acc. prob=0.78]warmup:  44%|████▍     | 443/1000 [00:06&lt;00:06, 86.64it/s, 511 steps of size 4.88e-03. acc. prob=0.78]warmup:  45%|████▌     | 452/1000 [00:06&lt;00:06, 80.83it/s, 2 steps of size 1.04e-02. acc. prob=0.78]  warmup:  46%|████▌     | 461/1000 [00:06&lt;00:06, 80.02it/s, 1023 steps of size 6.29e-03. acc. prob=0.78]warmup:  47%|████▋     | 470/1000 [00:06&lt;00:06, 78.34it/s, 1023 steps of size 8.06e-03. acc. prob=0.78]warmup:  48%|████▊     | 481/1000 [00:06&lt;00:06, 84.96it/s, 1023 steps of size 6.14e-03. acc. prob=0.78]warmup:  49%|████▉     | 490/1000 [00:06&lt;00:06, 84.88it/s, 433 steps of size 5.52e-03. acc. prob=0.78] sample:  50%|█████     | 501/1000 [00:06&lt;00:05, 89.19it/s, 511 steps of size 6.52e-03. acc. prob=0.97]sample:  51%|█████     | 511/1000 [00:07&lt;00:05, 91.61it/s, 511 steps of size 6.52e-03. acc. prob=0.96]sample:  52%|█████▏    | 521/1000 [00:07&lt;00:05, 91.47it/s, 511 steps of size 6.52e-03. acc. prob=0.97]sample:  53%|█████▎    | 531/1000 [00:07&lt;00:05, 91.13it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  54%|█████▍    | 541/1000 [00:07&lt;00:04, 92.02it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  55%|█████▌    | 551/1000 [00:07&lt;00:04, 94.12it/s, 511 steps of size 6.52e-03. acc. prob=0.96]sample:  56%|█████▌    | 561/1000 [00:07&lt;00:04, 91.73it/s, 511 steps of size 6.52e-03. acc. prob=0.96]sample:  57%|█████▋    | 571/1000 [00:07&lt;00:04, 88.56it/s, 1023 steps of size 6.52e-03. acc. prob=0.96]sample:  58%|█████▊    | 582/1000 [00:07&lt;00:04, 92.62it/s, 511 steps of size 6.52e-03. acc. prob=0.96] sample:  59%|█████▉    | 594/1000 [00:07&lt;00:04, 98.49it/s, 353 steps of size 6.52e-03. acc. prob=0.95]sample:  60%|██████    | 604/1000 [00:08&lt;00:04, 97.52it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  61%|██████▏   | 614/1000 [00:08&lt;00:03, 98.02it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  62%|██████▏   | 624/1000 [00:08&lt;00:03, 96.84it/s, 1023 steps of size 6.52e-03. acc. prob=0.95]sample:  63%|██████▎   | 634/1000 [00:08&lt;00:03, 96.84it/s, 511 steps of size 6.52e-03. acc. prob=0.95] sample:  64%|██████▍   | 645/1000 [00:08&lt;00:03, 99.94it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  66%|██████▌   | 656/1000 [00:08&lt;00:03, 100.59it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  67%|██████▋   | 667/1000 [00:08&lt;00:03, 102.68it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  68%|██████▊   | 678/1000 [00:08&lt;00:03, 104.69it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  69%|██████▉   | 689/1000 [00:08&lt;00:02, 105.59it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  70%|███████   | 700/1000 [00:08&lt;00:02, 105.48it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  71%|███████   | 711/1000 [00:09&lt;00:02, 101.17it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  72%|███████▏  | 722/1000 [00:09&lt;00:02, 95.89it/s, 511 steps of size 6.52e-03. acc. prob=0.95] sample:  73%|███████▎  | 732/1000 [00:09&lt;00:02, 96.69it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  74%|███████▍  | 743/1000 [00:09&lt;00:02, 97.77it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  75%|███████▌  | 754/1000 [00:09&lt;00:02, 97.69it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  76%|███████▋  | 765/1000 [00:09&lt;00:02, 99.89it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  78%|███████▊  | 776/1000 [00:09&lt;00:02, 98.15it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  79%|███████▊  | 786/1000 [00:09&lt;00:02, 95.43it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  80%|███████▉  | 796/1000 [00:09&lt;00:02, 96.00it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  81%|████████  | 806/1000 [00:10&lt;00:02, 93.31it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  82%|████████▏ | 817/1000 [00:10&lt;00:01, 97.16it/s, 511 steps of size 6.52e-03. acc. prob=0.94]sample:  83%|████████▎ | 828/1000 [00:10&lt;00:01, 99.92it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  84%|████████▍ | 839/1000 [00:10&lt;00:01, 102.19it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  85%|████████▌ | 850/1000 [00:10&lt;00:01, 102.83it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  86%|████████▌ | 861/1000 [00:10&lt;00:01, 102.61it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  87%|████████▋ | 872/1000 [00:10&lt;00:01, 103.06it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  88%|████████▊ | 883/1000 [00:10&lt;00:01, 104.96it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  89%|████████▉ | 894/1000 [00:10&lt;00:01, 103.98it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  90%|█████████ | 905/1000 [00:11&lt;00:00, 99.10it/s, 1023 steps of size 6.52e-03. acc. prob=0.95]sample:  92%|█████████▏| 915/1000 [00:11&lt;00:00, 94.66it/s, 1023 steps of size 6.52e-03. acc. prob=0.95]sample:  92%|█████████▎| 925/1000 [00:11&lt;00:00, 92.35it/s, 415 steps of size 6.52e-03. acc. prob=0.95] sample:  94%|█████████▎| 936/1000 [00:11&lt;00:00, 95.56it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  95%|█████████▍| 947/1000 [00:11&lt;00:00, 98.51it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  96%|█████████▌| 957/1000 [00:11&lt;00:00, 93.20it/s, 767 steps of size 6.52e-03. acc. prob=0.95]sample:  97%|█████████▋| 968/1000 [00:11&lt;00:00, 95.69it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  98%|█████████▊| 978/1000 [00:11&lt;00:00, 95.03it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample:  99%|█████████▉| 988/1000 [00:11&lt;00:00, 94.78it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample: 100%|█████████▉| 998/1000 [00:12&lt;00:00, 89.76it/s, 511 steps of size 6.52e-03. acc. prob=0.95]sample: 100%|██████████| 1000/1000 [00:12&lt;00:00, 82.66it/s, 511 steps of size 6.52e-03. acc. prob=0.95]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n                 mean    sd  hdi_5.5%  hdi_94.5%  mcse_mean  mcse_sd  \\\nbeta[0]          0.77  0.48       0.0       1.49       0.04     0.03   \nlambda0[0]       0.00  0.00       0.0       0.00       0.00     0.00   \nlambda0[1]       0.00  0.00       0.0       0.01       0.00     0.00   \nlambda0[2]       0.00  0.01       0.0       0.01       0.00     0.00   \nlambda0[3]       0.00  0.01       0.0       0.01       0.00     0.00   \n...               ...   ...       ...        ...        ...      ...   \nlambda_[43, 71]  0.00  0.01       0.0       0.00       0.00     0.00   \nlambda_[43, 72]  0.00  0.02       0.0       0.00       0.00     0.00   \nlambda_[43, 73]  0.00  0.02       0.0       0.00       0.00     0.00   \nlambda_[43, 74]  0.00  0.01       0.0       0.00       0.00     0.01   \nlambda_[43, 75]  0.00  0.03       0.0       0.00       0.00     0.01   \n\n                 ess_bulk  ess_tail  r_hat  \nbeta[0]            171.68    206.44    NaN  \nlambda0[0]         143.46    113.83    NaN  \nlambda0[1]         306.90    219.76    NaN  \nlambda0[2]         337.30    325.35    NaN  \nlambda0[3]         219.38    227.65    NaN  \n...                   ...       ...    ...  \nlambda_[43, 71]    167.71    235.17    NaN  \nlambda_[43, 72]    157.28    192.96    NaN  \nlambda_[43, 73]     82.53     84.53    NaN  \nlambda_[43, 74]    156.31    204.17    NaN  \nlambda_[43, 75]     71.75     81.01    NaN  \n\n[3421 rows x 9 columns]\n\n\n/home/sosa/work/test/lib/python3.10/site-packages/arviz/plots/hdiplot.py:166: FutureWarning:\n\nhdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n\n/home/sosa/work/test/lib/python3.10/site-packages/arviz/plots/hdiplot.py:166: FutureWarning:\n\nhdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n\n/home/sosa/work/test/lib/python3.10/site-packages/arviz/plots/hdiplot.py:166: FutureWarning:\n\nhdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n\n/home/sosa/work/test/lib/python3.10/site-packages/arviz/plots/hdiplot.py:166: FutureWarning:\n\nhdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#mathematical-details",
    "href": "12. Survival analysis.html#mathematical-details",
    "title": "Survival Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThe model is defined as follows:\n\ndN_i(t) \\sim \\text{Poisson}(\\lambda_i(t)Y_i(t)dt)\n\n\n\\lambda_i(t) = \\lambda_{0}(t)\\exp(X_i\\beta)\n\nThe hierarchical priors for the regression coefficients are:\n\n\\beta \\sim \\text{Normal}(\\mu_\\beta, \\sigma^2_\\beta)\n\n\n\\mu_\\beta \\sim \\text{Normal}(0, 100)\n\n\n\\sigma^2_\\beta \\sim \\text{InverseGamma}(0.1, 0.1)\n\n\nWhere:\n\nN_i(t) is the counting process for subject i, which counts the number of observed events up to time t. For survival analysis, this is typically 0 or 1.\ndN_i(t) is the increment of the process over a small interval dt, indicating if an event occurred for subject i at time t.\nY_i(t) is the at-risk indicator, taking a value of 1 if subject i is under observation and has not yet experienced an event just prior to time t, and 0 otherwise. This indicator is the mechanism that handles censoring:\n\nIf a subject is censored at time t', their at-risk indicator Y_i(t) remains 1 up to t', signifying they were at risk during this period.\nAt the moment of censoring t', no event is recorded (the counting process N_i(t) does not increment).\nFor all subsequent times t &gt; t', the indicator Y_i(t) switches to 0, effectively removing the individual from the risk set for any future calculations.\n\n\\lambda_i(t) is the hazard rate for subject i at time t.\n\\lambda_{0}(t) is the baseline hazard rate function at time t. A key assumption is that this baseline hazard is the same for all subjects.\nX_i is the covariates for subject i.\n\\beta is the regression coefficients.\n\nPriors:\n\nWe assign prior distributions to the unknown parameters. The regression coefficients \\beta are given a Normal prior.\nThe hyperparameters of the \\beta prior, \\mu_\\beta and \\sigma^2_\\beta, are themselves given vague priors to be learned from the data.\nThe baseline hazard \\lambda_0(t) is also treated as an unknown parameter and is often modeled non-parametrically, for instance, using a gamma process or as a piecewise constant function, where priors are placed on the hazard level in each time interval.\n\n\nThe key assumption of this model is that the hazard ratios are constant over time. Censoring is typically handled in the likelihood function used for estimation, not by multiplying the hazard function by a factor. The data for each subject i is represented by a tuple (t_i, \\delta_i, X_i), where t_i is the observed time (either event or censoring time), \\delta_i is an event indicator (1 if the event was observed, 0 if censored), and X_i is the vector of covariates.",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#references",
    "href": "12. Survival analysis.html#references",
    "title": "Survival Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\nhttps://en.wikipedia.org/wiki/Proportional_hazards_model https://www.mathworks.com/help/stats/cox-proportional-hazard-regression.html https://www.pymc.io/projects/examples/en/latest/survival_analysis/survival_analysis.html https://vflores-io.github.io/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/",
    "crumbs": [
      "Models",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "22. Network model.html",
    "href": "22. Network model.html",
    "title": "Network Models",
    "section": "",
    "text": "A network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random intercepts and slopes. By adding such parameters to the model, we can account for the correlations between node link relationships.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#considerations",
    "href": "22. Network model.html#considerations",
    "title": "Network Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThe particularity here is that varying intercepts and slopes are generated for both nodal effects 🛈 and dyadic effects 🛈. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#example",
    "href": "22. Network model.html#example",
    "title": "Network Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect. This example is based on Ross, McElreath, and Redhead (2024).\n\nPythonR\n\n\n\n# Setup device------------------------------------------------\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\n\ndef model(network, dyadic_predictors, sender_predictors, receiver_predictors, Merica, Quantum, Any):\n    N_id = network.shape[0]\n    # Block ---------------------------------------\n    B=m.net.block_model2(jnp.full(N_id,0),1,N_id, name='intecept') # BLock model intercept\n   \n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(sender_predictors,receiver_predictors,\n    s_sd=2.5, r_sd=2.5, sr_sd = 2.5 )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(shape = sr.shape[0], d_sd=2.5) # Diadic effect intercept only \n\n   m.dist.bernoulli(logits = B + sr + dr, obs=result_outcomes)\n\n\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1, progress_bar = True)\nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\n\nlibrary(BI)\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\n\nload(paste(system.file(package = \"BI\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nids = 0:(model_dat$N_id-1)\nidx = m$net$vec_node_to_edgle(jnp$stack(jnp$array(list(ids, ids)), axis = -as.integer(1)))\n\nkeys &lt;- c(\"idx\",\n          'idxShape',\n          \"result_outcomes\",\n          'focal_individual_predictors',\n          'target_individual_predictors')\n\nvalues &lt;- list(\n  idx,\n  idx$shape[[1]],\n  m$net$mat_to_edgl(model_dat$outcomes[,,1]),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50)),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50))\n)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(idx, idxShape, result_outcomes,focal_individual_predictors, target_individual_predictors){\n  N_id = 50\n  x=0.1/jnp$sqrt(N_id)\n  tmp=jnp$log(x / (1 - x))\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(tmp, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(idxShape))\n\n  ## SR ---------------------------------------                                                      \n  m$poisson(jnp$exp(B + sr + dr), obs=result_outcomes)  \n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\nsummary[rownames(summary) %in% c('focal_effects[0]', 'target_effects[0]', 'block[0]'),]\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nEvent if you don’t have dyadic effect, or block model effect, they need to be define to create intercepts (means) for those effects",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#mathematical-details",
    "href": "22. Network model.html#mathematical-details",
    "title": "Network Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe simple model that can be built to model link weights between nodes i and j can be defined using a Poisson distribution:\n\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\n\nlog(Y_{ij}) = \\alpha +  \\lambda_i + \\pi_j + \\delta_{ij}  + \\beta_1 X_i + \\beta_2 X_j + \\beta_3 Q_{ij}\n\nwhere:\n\nY_{ij} is the weight of the link between i and j.\n\\lambda_i is the sender random effect 🛈.\n\\pi_j is the receiver random effect 🛈.\n\\delta_{ij} is the dyadic random effect 🛈.\n\\beta_1 is the effect of an individuals i level feature on the emission of a link (i.e., out-strength).\n\\beta_2 is the effect of an individuals j level feature on the receiving a link (i.e., in-strength).\n\\beta_3 is the effect of an dyadic characteristic between i and j on the likelihood of a tie.\n\n\n\nDefining formula sub-equations and prior distributions\nThe sender and receiver random effects are similar to those described in chapter 13: Varying intercepts, but they are defined here using a joint prior so as to estimate the correlation within individuals to emit and receive a link:\n\n\\left(\\begin{array}{cc}\n\\lambda_i \\\\\n\\pi_i\n\\end{array}\\right)\n=\n\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL\n\\left(\\begin{array}{cc}\n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n\n\n\\sigma_\\lambda \\sim \\text{Exponential}(1)\n\n\n\\sigma_\\pi \\sim \\text{Exponential}(1)\n\n\nL \\sim \\text{LKJ}(2)\n\n\n\\hat{\\lambda}_i \\sim \\text{Normal}(0,1)\n\n\n\\hat{\\pi}_i \\sim \\text{Normal}(0,1)\n\nSimilarly, for each dyad we can define a joint prior to estimate correlation between i–j links and j–i links:\n\n\\left(\\begin{array}{cc}\n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right)\n=\n\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL_\\delta\n\\left(\\begin{array}{cc}\n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n\n\n\\sigma_\\delta \\sim \\text{Exponential}(1)\n\n\nL_\\delta \\sim \\text{LKJ}(2)\n\n\n\\hat{\\delta}_{ij}  \\sim \\text{Normal}(0,1)",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#notes",
    "href": "22. Network model.html#notes",
    "title": "Network Models",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nNote that any additional covariates can be summed with a regression coefficient to \\lambda_i, \\pi_j and \\delta_{ij}. Of course, for \\lambda_i and \\pi_j, as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for \\delta_{ij}, as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, individual_predictors, and a matrix of dyadic characteristics, kinship, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively.\n\nWe can apply multiple variables as in chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms as in chapter 3: Interaction Between Continuous Variables.\nNetwork links can be modeled using Bernoulli (for proportions), Binomial (for unweighted network), Poisson or zero-inflated Poisson distributions (for count). In BI, you just need to set the correct likelihood distributions. For example, if you want to model the number of interactions between nodes, you can use the Poisson distribution. If you want to model the existence or absence of a link, you can use the Bernoulli distribution.\nIf the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms \\lambda_i, \\pi_j, and \\delta_{ij} are no longer required. (Is it correct?)\nTo account for exposure on a poisson model treat exposure as a nodal characteristic with its own parameter effect (i.e., regression coefficient). Their is several function that will help you to convert vectors or matrices in edge list format to have compatible data structure for the model (see API reference for bi.net.vec_to_edgl and bi.net.mat_to_edgl). F\n\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#references",
    "href": "22. Network model.html#references",
    "title": "Network Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nRoss, Cody T, Richard McElreath, and Daniel Redhead. 2024. “Modelling Animal Network Data in r Using STRAND.” Journal of Animal Ecology 93 (3): 254–66.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "27. BNN.html",
    "href": "27. BNN.html",
    "title": "Bayesian Neural Networks",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of “neurons.” Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our “knobs.”\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\beta). In a neural network, which can have thousands or millions of weights, we don’t define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network’s predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\sigma that quantifies the data’s noise around the model’s predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#general-principles",
    "href": "27. BNN.html#general-principles",
    "title": "Bayesian Neural Networks",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of “neurons.” Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our “knobs.”\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\beta). In a neural network, which can have thousands or millions of weights, we don’t define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network’s predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\sigma that quantifies the data’s noise around the model’s predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#considerations",
    "href": "27. BNN.html#considerations",
    "title": "Bayesian Neural Networks",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nLike all Bayesian models, BNNs consider model parameter uncertainty 🛈. The parameters here are the network’s weights (W) and biases (b). We quantify our uncertainty about them through their posterior distribution 🛈. Therefore, we must declare prior distributions 🛈 for all weights and biases, as well as for the output variance \\sigma.\nUnlike in a linear regression where the coefficient β has a direct interpretation (e.g., the effect of weight on height), the individual weights and biases in a BNN are not directly interpretable. A single weight’s influence is entangled with thousands of other parameters through non-linear functions. Consequently, BNNs are best viewed as powerful predictive tools rather than explanatory ones. They excel at learning complex patterns and quantifying predictive uncertainty, but if the goal is to isolate and interpret the effect of a specific variable, a simpler model is often more appropriate.\nPrior distributions are built following these considerations:\n\nAs the data is typically scaled 🛈 (see introduction), we can use a standard Normal distribution (mean 0, standard deviation 1) as a weakly-informative prior for all weights and biases. This acts as a form of regularization.\nSince the output variance \\sigma must be positive, we can use a positively-defined distribution, such as the Exponential or Half-Normal.\n\nBNNs can be used for both regression and classification. The final layer’s activation and the chosen likelihood distribution depend on the task. For binary classification, a sigmoid activation is paired with a Bernoulli likelihood, which requires a link function 🛈 (logit) to connect the linear output of the network to the probability space [0, 1]. For regression, the identity activation is often used with a Gaussian likelihood.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#example",
    "href": "27. BNN.html#example",
    "title": "Bayesian Neural Networks",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Neural Network for regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to predict height from weight using a non-linear model.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):\n    # Define the BNN architecture and get its output (mu)\n    # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n    # Priors for weights/biases are Normal(0,1) by default\n    mu = m.bnn(x=weight, n_neurons=[10, 1], activations=['tanh', 'identity'], name='bnn')\n\n    # Prior for the output standard deviation\n    s = m.dist.exponential(1, name='s')\n    \n    # Likelihood\n    m.normal(mu, s, obs=height)\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Approximate posterior distributions for weights, biases, and sigma\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# Filter data frame\nm$df = m$df[m$df$age &gt; 18,]\n\n# Scale\nm$scale(list('weight')) \n\n# Convert data to JAX arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Define the BNN architecture\n  # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n  # Priors for weights/biases are Normal(0,1) by default\n  mu &lt;- bi$bnn(x = weight, n_neurons = list(10, 1), activations = list('tanh', 'identity'), name = 'bnn')\n\n  # Prior for the output standard deviation\n  s = bi$dist$exponential(1, name = 's')\n  \n  # Likelihood\n  m$normal(mu, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Approximate posterior distributions\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#mathematical-details",
    "href": "27. BNN.html#mathematical-details",
    "title": "Bayesian Neural Networks",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist Formulation\nA standard (non-Bayesian) neural network with one hidden layer is defined by forward propagation:\n\nh_i = f(X_i W_1 + b_1)\n\n\n\\hat{Y}_i = g(h_i W_2 + b_2)\n\nWhere: - Y_i is the predicted output for observation i. - X_i is the input vector for observation i. - W_1, b_1 are the weight matrix and bias vector for the hidden layer. - W_2, b_2 are the weight matrix and bias vector for the output layer. - h_i is the activation of the hidden layer. - f and g are activation functions (e.g., ReLU, tanh, sigmoid, identity). - Parameters W_1, b_1, W_2, b_2 are learned as single optimal values via optimization.\n\n\nBayesian Formulation\nIn the Bayesian formulation, we place priors 🛈 on all weights and biases and define a likelihood for the output. For a regression task with a one-layer BNN:\n\nY_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\n\n\\mu_i = g(h_i W_2 + b_2)\n\n\nh_i = f(X_i W_1 + b_1)\n\nThe parameters are now distributions: \nW_1 \\sim \\text{Normal}(0, 1)\n \nb_1 \\sim \\text{Normal}(0, 1)\n \nW_2 \\sim \\text{Normal}(0, 1)\n \nb_2 \\sim \\text{Normal}(0, 1)\n \n\\sigma \\sim \\text{Exponential}(1)\n\nWhere: - Y_i is the observed dependent variable for observation i. - \\mu_i is the mean predicted by the network, which is itself a distribution because it is a function of the distributions of weights and biases. - W_1, b_1, W_2, b_2 are the weights and biases, treated as random variables. - \\sigma is the standard deviation of the normal distribution, quantifying observation noise.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#notes",
    "href": "27. BNN.html#notes",
    "title": "Bayesian Neural Networks",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe primary difference between a Frequentist and Bayesian neural network lies in how parameters are treated. In the frequentist approach, weights and biases are point estimates found by minimizing a loss function (e.g., via gradient descent). Techniques like Dropout or L2 regularization are often used to prevent overfitting, which can be interpreted as approximations to a Bayesian treatment. In contrast, the Bayesian formulation does not seek a single best set of weights. Instead, it uses methods like MCMC or Variational Inference to approximate the entire posterior distribution for every weight and bias. This provides a principled and direct way to quantify model uncertainty.\nWhile present an example of non-linear regression, the Bayesian Neural Network can be used for linear regressions as well (keeping in mind that interpretation of the weights are impossible).\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):\n    # Define the BNN architecture and get its output (mu)\n    # 1 input -&gt; 10 hidden neurons (tanh) -&gt; 1 output neuron (identity)\n    # Priors for weights/biases are Normal(0,1) by default\n    mu = m.bnn(x=weight, n_neurons=[10, 1], activations=['tanh', 'identity'], name='bnn')\n\n    # Prior for the output standard deviation\n    s = m.dist.exponential(1, name='s')\n    \n    # Likelihood\n    m.normal(mu, s, obs=height)\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Approximate posterior distributions for weights, biases, and sigma\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "27. BNN.html#references",
    "href": "27. BNN.html#references",
    "title": "Bayesian Neural Networks",
    "section": "Reference(s)",
    "text": "Reference(s)\n(neal1995bayesian?)",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "9. Categorical model.html",
    "href": "9. Categorical model.html",
    "title": "Categorical Model",
    "section": "",
    "text": "To model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a Categorical model.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#general-principles",
    "href": "9. Categorical model.html#general-principles",
    "title": "Categorical Model",
    "section": "",
    "text": "To model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a Categorical model.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#considerations",
    "href": "9. Categorical model.html#considerations",
    "title": "Categorical Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nOne way to interpret a Categorical model is to consider that we need to build K - 1 linear models, where K is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a simplex 🛈. To do this, we convert the regression outputs using the softmax function 🛈 (see the “jax.nn.softmax” line in the code).\nThe intercept \\alpha captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\nOn the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients \\beta are shared across categories.\nThe relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#example",
    "href": "9. Categorical model.html#example",
    "title": "Categorical Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Categorical model using the Bayesian Inference (BI) package. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(career, income):\n    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n    b = m.dist.halfnormal(0.5, shape=(1,), name = 'b')\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0] #pivot\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    m.dist.categorical(probs=p, obs=career)\n\n# Run sampler ------------------------------------------------ \nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multinomial.csv\", sep = ''), sep=',')\nkeys &lt;- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues &lt;- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n\n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n\n  # Likelihood\n  bi.dist.categorical(probs=p[career], obs=career)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#mathematical-details",
    "href": "9. Categorical model.html#mathematical-details",
    "title": "Categorical Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a Categorical model using a Categorical distribution. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable 𝑦 with 𝐾 categories, the multinomial likelihood function is:\n\nY_i \\sim \\text{Categorical}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the dependent categorical variable for observation i indicating the category of the observation.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex 🛈.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#references",
    "href": "9. Categorical model.html#references",
    "title": "Categorical Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html",
    "href": "18. Latent variable (wip).html",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables—variables that are not directly observed but are inferred from the data—can help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\nY = f(X, Z) + \\epsilon\n\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#general-principles",
    "href": "18. Latent variable (wip).html#general-principles",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables—variables that are not directly observed but are inferred from the data—can help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\nY = f(X, Z) + \\epsilon\n\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#considerations",
    "href": "18. Latent variable (wip).html#considerations",
    "title": "Latent Variable Models (WIP)",
    "section": "Considerations",
    "text": "Considerations\nIn Bayesian regression with latent variables, we consider the uncertainty in both the observed and latent variables. We declare prior distributions for the latent variables, in addition to the usual priors for regression coefficients and intercepts. These latent variables are often modeled using Gaussian distributions (Normal priors) or more flexible distributions such as Multivariate Normal for correlations among the latent variables.\nThe goal is to infer the posterior distribution over both the parameters and the latent variables, given the observed data.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#example",
    "href": "18. Latent variable (wip).html#example",
    "title": "Latent Variable Models (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with latent variables using TensorFlow Probability:\nfrom BI import bi\nimport numpy as np\nimport jax.numpy as jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Data Simulation ------------------------------------------------\nNY = 4  # Number of dependent variables or outcomes (e.g., dimensions for latent variables)\nNV = 8  # Number of observations or individual-level data points (e.g., subjects)\n\n# Initialize the matrix Y2 with shape (NV, NY) filled with NaN values, to be filled later\nY2 = np.full((NV, NY), np.nan)\n\n# Generate the means and offsets for the data\n# means: Generate random normal means for each of the NY outcomes\n# offsets: Generate random normal offsets for each of the NV observations\nmeans = m.dist.normal(0, 1, shape=(NY,), sample=True, seed=10)\noffsets = m.dist.normal(0, 1, shape=(NV, 1), sample=True, seed=20)\n\n# Fill the matrix Y2 with simulated data based on the generated means and offsets\n# Each observation (i) is the sum of an individual-specific offset and an outcome-specific mean\nfor i in range(NV):\n    for k in range(NY):\n        Y2[i, k] = means[k] + offsets[i]\n\n# Simulate individual-level random effects (e.g., random slopes or intercepts)\n# b_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates\nb_individual = BI.distribution.normal(0, 1, shape=(N, K), sample=True, seed=0)\n\n# mu: Add an additional effect 'a' to the individual-level random effects 'b_individual'\n# 'a' could represent a population-level effect or a baseline\nmu = b_individual + a\n\n# Convert Y2 to a JAX array for further computation in a JAX-based framework\nY2 = jnp.array(Y2)\n\n\n# Set data ------------------------------------------------\ndat = dict(\n    NY = NY,\n    NV = NV,\n    Y2 = Y2\n)\nm.data_on_model = dat\n\n# Define model ------------------------------------------------\ndef model(NY, NV, Y2):\n    means = m.dist.normal(0, 1, shape=(NY,), name='means')\n    offset = m.dist.normal(0, 1, shape=(NV, 1), name='offset')\n    sigma = m.dist.exponential(1, shape=(NY,), name='sigma')\n    tmp = jnp.tile(means, (NV, 1)).reshape(NV, NY)\n    mu_l = tmp + offset\n    m.normal(mu_l, jnp.tile(sigma, [NV, 1]), obs=Y2)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#mathematical-details",
    "href": "18. Latent variable (wip).html#mathematical-details",
    "title": "Latent Variable Models (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can express the Bayesian latent variable model using probability distributions as follows:\n\n\\begin{aligned}\n& p(Y | X, Z, W, \\sigma) = \\text{Normal}(X \\cdot W + Z, \\sigma^2) \\\\\n& p(Z) = \\text{Normal}(0, \\tau^2) \\\\\n& p(W) = \\text{Normal}(0, \\alpha^2) \\\\\n\\end{aligned}\n\nWhere: - p(Y | X, Z, W, ) is the likelihood function for the observed outcome variable, which depends on both the observed predictor X and the latent variable Z. - p(Z) is the prior distribution for the latent variable Z, often modeled as Normal with a mean of 0 and variance ^2. - p(W) is the prior distribution for the regression coefficient(s) W, typically assumed to follow a Normal distribution with mean 0 and variance ^2.\nThe latent variable Z introduces additional flexibility to the model, capturing unobserved influences on the outcome Y.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "href": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "title": "Latent Variable Models (WIP)",
    "section": "Interpretation of Latent Variables",
    "text": "Interpretation of Latent Variables\n\nLatent Variable (Z): Represents hidden factors not captured by the observed variables, allowing the model to explain more of the variance in the outcome. For instance, in a psychological model, Z might represent a latent trait such as intelligence or anxiety that influences the outcome.\nPosterior Inference: The posterior distribution of the latent variable Z can give insights into how much the unobserved factors contribute to the outcome.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#use-cases",
    "href": "18. Latent variable (wip).html#use-cases",
    "title": "Latent Variable Models (WIP)",
    "section": "Use Cases",
    "text": "Use Cases\n\nLatent Factors in Psychometrics: In psychometric models, latent variables represent traits or abilities that are not directly observed, such as cognitive ability or personality traits.\nTime-Varying Effects: Latent variables can represent unobserved time trends or individual-specific effects in time-series or longitudinal models.\nMixed Models: In hierarchical or mixed models, latent variables can represent group-specific intercepts or slopes.",
    "crumbs": [
      "Models",
      "Latent Variable Models (WIP)"
    ]
  },
  {
    "objectID": "25. Network Metrics.html",
    "href": "25. Network Metrics.html",
    "title": "Network Metrics",
    "section": "",
    "text": "This overview is from Sosa, Sueur, and Puga-Gonzalez (2021).",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#general-principles",
    "href": "25. Network Metrics.html#general-principles",
    "title": "Network Metrics",
    "section": "General Principles",
    "text": "General Principles\nNetwork metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#nodal-metrics",
    "href": "25. Network Metrics.html#nodal-metrics",
    "title": "Network Metrics",
    "section": "Nodal metrics",
    "text": "Nodal metrics\nNodal metrics* enable the assessment of nodes’ social heterogeneity and the understanding of underlying mechanisms such as individual characteristics (e.g., the ageing process), ecological factors (e.g., demographic variation), and evolutionary processes (e.g., differences in social styles). Node measures are calculated at a nodal level and assess, in different ways and with different meanings, how an individual is connected. Connections can be ego’s* direct links only (e.g., degree, strength), its alters’* links as well (e.g., eigenvector, clustering coefficient), or even all the links in the network (e.g., betweenness). Node measures can also be used to describe the overall network structure through distributions, means, and coefficients of variation.\n\nDegree and strength\nThe degree m.net.degree measures the number of links of a node. When computed on an undirected network, the degree represents the number of alters of an ego. When the network is directed, it represents the number of either incoming or outgoing* links of an ego, and it is then called in-degree m.net.indegree or out-degree m.net.outdegree, respectively. Note that degree can also be computed in directed networks; in this case, it represents the sum of incoming and outgoing links and not the number of alters.\n\nD_i = \\sum_{j=1}^N a_{ij}\n\nWhere a_{ij} is the value of the link between nodes i and j. Isolated node(s) can be considered as zero(s).\nStrength (or weighted degree) m.net.strength is the sum of the links’ weights in a weighted network*. When the network comprises directed links, then it is also possible to differentiate between in-strength m.net.instrength (the sum of weights of incoming links) and out-strength m.net.outstrength (the sum of weights of outgoing links). While degree and strength can be considered correlated, it may not always be the case, as individuals can interact frequently with a few social partners or vice versa (Liao, Sosa, Wu, & Zhang, 2018). Therefore, it is necessary to test their correlation prior to the analysis.\n\nS_i = \\sum_{j=1}^N a_{ij} w_{ij}\n\nWhere a_{ij} is the value of the link between nodes i and j. Isolated node(s) can be considered as zero(s).\n\n\nEigenvector centrality\nEigenvector centrality m.net.eigenvector is the first non-negative eigenvector value obtained by transforming an adjacency matrix linearly. It can be computed on weighted, binary, directed, or undirected networks. It measures centrality by examining the connectedness of an ego as well as that of its alters. Thus, a node’s eigenvector value can be linked either to its own degree or strength or to the degrees or strengths of the nodes to which it is connected. Eigenvector may be interpreted as the social support or social capital of an individual (Brent, Semple, Dubuc, Heistermann, & MacLarnon, 2011), that is, the real or perceived availability of social resources.\n\n\\lambda c = W c\n\nWhere \\lambda is the largest eigenvalue of the adjacency matrix W. Isolated node(s) can be considered as zero(s).\n\n\nLocal clustering coefficient\nThe local clustering coefficient m.net.cc measures the number of closed triplets* over the total theoretical number of triplets (i.e., open and closed), where a triplet is a set of three nodes that are connected by either two (open triplet) or three (closed triplet) edges. This measure aims to examine the links that may exist between the alters of an ego and measures the cohesion of the network. The main topological effect of closed triplets is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity (see corresponding section). The local clustering coefficient can be computed in a binary network by measuring the proportion of links between the nodes of an ego-network* divided by the number of potential links between them. In weighted networks, several versions exist, such as those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or Opsahl and Panzarasa (2009).\n\nBinary Local Clustering Coefficient\n\nC_i^b = \\frac{2L}{N_i (N_i - 1)}\n Where L is the number of links in the ego-network of node i.\n\n\nBarrat’s Local Clustering Coefficient\n\nC_i^W = \\frac{1}{S_i (D_i - 1)} \\sum_{j \\neq h \\in N} \\frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}\n\nWhere S_i and D_i are the strength and the degree of node i, respectively. w_{ij} and w_{ih} are the weights of the links, and a_{ij}, a_{ih}, a_{jh} are the links between the nodes.\n\n\nOpsahl’s Local Clustering Coefficient\n\nC^W(G) = \\frac{\\sum_{\\tau_\\Delta} w}{\\sum_\\tau w}\n Where \\tau_\\Delta represents closed triplets, and w is the chosen weighting scheme (maximum, minimum, arithmetic, or geometric mean).\n\n\n\nBetweenness\nBetweenness (WIP) is the number of times a node is included in the shortest paths (geodesic distances) generated by every combination of two nodes. The value of the betweenness indicates the theoretical role of a node in social transmission (information, disease, etc., see Figure 1), as it indicates to what extent a node connects subgroups, as a bridge, and thus is likely to spread an entity across the whole network (Newman, 2005).\n\nb = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\nWhere \\sigma_{st} is the total number of shortest paths from node s to node t, and \\sigma_{st}(v) is the number of those paths that pass through v. As no paths go through isolated nodes, their betweenness value can be considered zero.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#polyadic-metrics",
    "href": "25. Network Metrics.html#polyadic-metrics",
    "title": "Network Metrics",
    "section": "Polyadic metrics",
    "text": "Polyadic metrics\nPatterns of interactions (how and with whom individuals interact) can be examined using specific network measures* that analyse local-scale interactions within a network and make it possible to test hypotheses about the mechanisms underlying network connectivity. These types of measures are generally used to test mechanistic biological questions, such as what factors (e.g., ecological as well as sociodemographic) affect individuals’ interactions/associations.\n\nAssortativity\nAssortativity (Newman, 2003) (WIP) is probably the most used measure to study homophily (preferential associations or interactions among individuals sharing the same characteristics; Lazarsfeld & Merton, 1954). Assortativity values range from −1 (total disassortativity, i.e., all the nodes associate or interact with those with the opposite characteristic, such as males interacting exclusively with females) to 1 (total assortativity, i.e., all the nodes associate or interact with those with the same characteristic, such as males interacting only with males). The assortativity coefficient measures the proportion of links between and within clusters of nodes with the same characteristics. Individuals’ characteristics can be continuous (e.g., age, individual network measure, personality) or categorical features (e.g., sex, matriline belonging; Figure 2). Assortativity does not consider directionality* and can be measured in weighted (Leung & Chau, 2007) or binary (Newman, 2003) networks using categorical or continuous characteristics (Figure 2). The use of one or the other assortativity variant depends on the type of characteristics being examined and, whenever possible, the weighted version should be preferred since it is more reliable than the binary version (Farine, 2014).\n\nBinary Assortativity\n\nr = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1 - \\sum_i a_i b_i}\n\nWhere e_{ii} is the proportion of specific links, a_i is the proportion of outgoing links, and b_i is the proportion of incoming links.\n\n\nWeighted Continuous Assortativity\n\nr = \\frac{\\sum_i e_{ii}^w - \\sum_i a_i^w b_i^w}{1 - \\sum_i a_i^w b_i^w}\n Where e_{ii}^w is the proportion of weighted links, and a_i^w, b_i^w are the proportions of weighted outgoing and incoming links.\n\n\n\nTransitive triplets\nTransitive triplets (WIP) are closed triplets where the links among the nodes follow a specific temporal pattern of creation, that is, when the establishment of links between nodes A and B and between nodes A and C is followed by the establishment of a link between nodes B and C. This network measure can be computed in directed, binary, or weighted networks. These types of connections can be studied over time based on the creation of links. From a static perspective, directionality can be considered by calculating the number of transitive triplets divided by the number of potential transitive triplets, and weights can also be considered by using Opsahl’s variants, which are discussed in the section on local clustering coefficient (Opsahl & Panzarasa, 2009). While transitivity is importantly related to the clustering coefficient (the clustering coefficient includes transitive triplets), not all closed triplets are transitive. Transitive triplets are one of the 16 possible configurations of a triplet considering open and closed triplets as well as link directionality (i.e., triad census).",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#global-metrics",
    "href": "25. Network Metrics.html#global-metrics",
    "title": "Network Metrics",
    "section": "Global metrics",
    "text": "Global metrics\nThe structure of this section is based on the distinction between network connectivity and social diffusion (information or disease spread). However, the social diffusion section contains measures specifically designed to study theoretical (i.e., considering the diffusion is perfectly related to network links and link weights) social diffusion features based on geodesic distances (see corresponding section). Aspects of the structure and properties of a group (e.g., cohesion, sub-grouping) can be quantified using global network measures. For instance, one may quantify properties such as network resilience (see Diameter), network clustering* (see Modularity) through network connectivity analysis, or network transmission efficiency* (see Global efficiency) through network theoretical social diffusion analysis.\n\nDensity\nThe density m.net.density is the ratio of existing links to all potential links in a network. This measure is easy to interpret; it assesses how fully connected a network is. Density considers neither directionality nor link weights.\n\nD = \\frac{2|L|}{|N|(|N| - 1)}\n\nWhere L is the number of links and N is the number of nodes. Isolated node(s) can be considered as zero(s).\n\n\nGeodesic Distance\nGeodesic distance m.net.geodesic_distance is the shortest path considering all potential dyads in a network. This measure thereby indicates the fastest path of diffusion. Geodesic distance can be calculated in binary, weighted*, directed, or undirected networks. In weighted networks, it can be normalized (by dividing all links by the network’s mean weight), and the strongest or the weakest links can be considered as the fastest route between two nodes. This great number of variants of geodesic distance can greatly affect the results and interpretations. Researchers must thus have knowledge of the variants and know which one is the most appropriate according to their research question (Opsahl, Agneessens, & Skvoretz, 2010).\nThe computation uses algorithms like breadth-first search, depth-first search, or Dijkstra’s algorithm. None handle isolated nodes.\n\n\nDiameter\nThe diameter m.net.diameter of a network represents the longest of the shortest paths in the network. The diameter is used in ASNA to examine aspects such as network cohesion and the rapidness of information or disease transmission. While global efficiency measures the theoretical social diffusion spread, diameter informs on the maximum path length of diffusion required to reach all nodes.\n\n\nGlobal efficiency\nGlobal efficiency (WIP) is the ratio between the number of individuals and the number of connections multiplied by the network diameter. It provides a quantitative measure of how efficiently information is exchanged among the nodes of the network. As global efficiency gives a probability of social diffusion, it may help to better understand social transmission phenomena in the short and long term (Migliano et al., 2017). Pasquaretta et al. (2014) found a positive correlation between the neocortex ratio and global efficiency in primate species with a higher neocortex ratio. By drawing a parallel between cognitive capacities and social network efficiency, this study showed that in species with a higher neocortex ratio, individuals may adjust their social relationships to gain better access to social information and thus optimize network efficiency. Alternatively, studies on epidemiology in ant colonies showed that ants adapt their interaction rate to decrease network efficiency when infected by a pathogen (Stroeymeyt et al., 2018).\n\n\nModularity\nModularity (WIP) is a measure designed to quantify the degree to which a network can be divided into different groups or clusters, and its value ranges from 0 to 1. Networks with high modularity have dense connections within the modules but sparse connections between them. Modularity can be computed in weighted, binary, directed, or undirected networks.\n\nQ = \\sum_{s=1}^m \\left[ \\frac{l_s}{|E|} - \\left(\\frac{d_s}{2|E|}\\right)^2 \\right]\n\nWhere l_s is the number of edges in the s-th community, and d_s is the sum of the degrees of the nodes in the community.\n\n\nGlobal Clustering Coefficient\nThe global clustering coefficient (WIP), like the local clustering coefficient, evaluates how well the alters of an ego are interconnected and measures the cohesion of the network. Its main topological effect is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity. However, it becomes highly correlated with density and less so with modularity as density grows. Several variants of the global clustering coefficient can be found: (a) the ratio of closed triplets to all triplets (open and closed), and (b) the binary local mean clustering coefficient derived from the node level (see Local clustering coefficient). The binary local mean clustering coefficient allows us to consider node heterogeneity and thus should be preferred over the first variant. Weighted versions also exist and are based on the same variants described in the section on the local clustering coefficient and require the same considerations.\n\nC^b(G) = \\frac{\\sum \\tau_\\Delta}{\\sum \\tau}\n\nWhere \\tau is the total number of triplets and \\tau_\\Delta represents closed triplets.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#references",
    "href": "25. Network Metrics.html#references",
    "title": "Network Metrics",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nSosa, Sebastian, Cédric Sueur, and Ivan Puga-Gonzalez. 2021. “Network Measures in Animal Social Network Analysis: Their Strengths, Limits, Interpretations and Uses.” Methods in Ecology and Evolution 12 (1): 10–21. https://doi.org/https://doi.org/10.1111/2041-210X.13366.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "5. Binomial model.html",
    "href": "5. Binomial model.html",
    "title": "Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary dependent variable—e.g., counts of successes/failures, yes/no, or 1/0—and one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#general-principles",
    "href": "5. Binomial model.html#general-principles",
    "title": "Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary dependent variable—e.g., counts of successes/failures, yes/no, or 1/0—and one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#considerations",
    "href": "5. Binomial model.html#considerations",
    "title": "Binomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nThis is the first model for which we need a link function: e.g., the logit function. The logit link function converts the linear combination of predictor variables into probabilities, making it suitable for modeling the probability of binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring that model predictions fall within the bounds of the binomial distribution’s success parameter \\in(0,1).",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#example",
    "href": "5. Binomial model.html#example",
    "title": "Binomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which lever each chimpanzee pulled in an experimental setup. The goal is to evaluate the probability of pulling the left side. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'chimpanzees.csv'\nm.data(data_path, sep=';')\nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.binomial(total_count = 1, logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  m$binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#mathematical-details",
    "href": "5. Binomial model.html#mathematical-details",
    "title": "Binomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian formulation\nWe can express the Bayesian Binomial regression model including prior distributions as follows:\n\nY_i \\sim \\text{Binomial}(N_i, p_i)\n\n\nlogit(p_i) = \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\nWhere:\n\nY_i is the count of successes for observation i (often a binary 0 or 1).\nN_i is the count of trials for observation i (1 in the case of binary outcomes, as in the example for total_count above).\np_i is the probability of success (0 &lt; p_i &lt; 1) for observation i, the probability of a success.\nlogit(p_i) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.\n\\beta and \\alpha are the regression coefficient and intercept, respectively.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#notes",
    "href": "5. Binomial model.html#notes",
    "title": "Binomial Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\nBelow is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled, and three independent variables (actor, side, cond). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as for the different conditions.\n\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';')\nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.dist.binomial(total_count = 1, logits=p, obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n# Diagnostic ------------------------------------------------\nm.summary()\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  a = bi.dist.normal( 0, 1.5, name = 'a')\n  b = bi.dist.normal( 0, 0.5, name = 'b')\n  p = a[actor] + b[treatment]\n  # Likelihood\n  m$binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#references",
    "href": "5. Binomial model.html#references",
    "title": "Binomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "Scalable - Built on top of Numpyro, TensorFlow Probability and Jax, for cpu, gpu and tpu vectorization and parallelization.\nFlexibility trade-offs - low-level abstraction coding available but also pre-build function for high-level of abstraction.\nUnified - One framework for both Python and R.\nAccessibility - 30 documented models.\nIntuitive - model-building syntax.\n\n\n\n\n\n\n\n\nPiplineFull Model definitionPrebuilt functionModel to LatexDistributions visualization\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu') # cpu, gpu or : tuple\n\n# Import Data ------------------------------------------------\nm.data(data_path) \n\n# Define model ------------------------------------------------\ndef model(arg1,argb2):\n   pass\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\n# Diagnostics ------------------------------------------------\nm.diag()\n\n\n\ndef model(kcal_per_g, index_clade):\n    alpha = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    beta = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    sigma = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.normal(mu, s, obs=kcal_per_g)\n\n\ndef model(kcal_per_g, index_clade):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n      N_group = N_cafes,\n      group = cafe,\n      global_intercept= a,\n      global_slope= b,\n      group_name = 'cafe'\n    )\n\n\n#| label: model-to-latex\n#| results: hold\n#| echo: true\nfrom BI import bi\nm = bi(platform='cpu')\n\n# define model ------------------------------------------------\ndef model(weight, height):    \n    alpha = m.dist.normal( 178, 20, name = 'a')\n    beta = m.dist.log_normal( 0, 1, name = 'b')   \n    sigma = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(alpha + beta * weight , sigma, obs=height)\n\n# Run sampler ------------------------------------------------\nm.model = model\nm.latex()\n\n\\begin{aligned}\nheight &\\sim \\text{Normal}(\\alpha + \\beta * weight, \\sigma) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 50) \\\\\n\\beta &\\sim \\text{LogNormal}(0, 1) \\\\\n\\alpha &\\sim \\text{Normal}(178, 20)\n\\end{aligned}\n\n\n\n\nfrom BI import bi\nm = bi()\nm.dist.normal( 0, 1, name = 'a', shape=(100,2, 4), sample = True, to_jax = False).hist()\n\njax.local_device_count 16"
  },
  {
    "objectID": "index.html#an-open-source-library-for-python-and-r",
    "href": "index.html#an-open-source-library-for-python-and-r",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "Scalable - Built on top of Numpyro, TensorFlow Probability and Jax, for cpu, gpu and tpu vectorization and parallelization.\nFlexibility trade-offs - low-level abstraction coding available but also pre-build function for high-level of abstraction.\nUnified - One framework for both Python and R.\nAccessibility - 30 documented models.\nIntuitive - model-building syntax."
  },
  {
    "objectID": "index.html#a-unified-scalable-and-accessible-framework",
    "href": "index.html#a-unified-scalable-and-accessible-framework",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "PiplineFull Model definitionPrebuilt functionModel to LatexDistributions visualization\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu') # cpu, gpu or : tuple\n\n# Import Data ------------------------------------------------\nm.data(data_path) \n\n# Define model ------------------------------------------------\ndef model(arg1,argb2):\n   pass\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\n# Diagnostics ------------------------------------------------\nm.diag()\n\n\n\ndef model(kcal_per_g, index_clade):\n    alpha = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    beta = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    sigma = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.normal(mu, s, obs=kcal_per_g)\n\n\ndef model(kcal_per_g, index_clade):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n      N_group = N_cafes,\n      group = cafe,\n      global_intercept= a,\n      global_slope= b,\n      group_name = 'cafe'\n    )\n\n\n#| label: model-to-latex\n#| results: hold\n#| echo: true\nfrom BI import bi\nm = bi(platform='cpu')\n\n# define model ------------------------------------------------\ndef model(weight, height):    \n    alpha = m.dist.normal( 178, 20, name = 'a')\n    beta = m.dist.log_normal( 0, 1, name = 'b')   \n    sigma = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(alpha + beta * weight , sigma, obs=height)\n\n# Run sampler ------------------------------------------------\nm.model = model\nm.latex()\n\n\\begin{aligned}\nheight &\\sim \\text{Normal}(\\alpha + \\beta * weight, \\sigma) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 50) \\\\\n\\beta &\\sim \\text{LogNormal}(0, 1) \\\\\n\\alpha &\\sim \\text{Normal}(178, 20)\n\\end{aligned}\n\n\n\n\nfrom BI import bi\nm = bi()\nm.dist.normal( 0, 1, name = 'a', shape=(100,2, 4), sample = True, to_jax = False).hist()\n\njax.local_device_count 16"
  },
  {
    "objectID": "8. Poisson mode with offset.html",
    "href": "8. Poisson mode with offset.html",
    "title": "Poisson Model with an Offset",
    "section": "",
    "text": "When we want to model count data, where the counts are observed over different periods or areas of exposure, we use a Poisson model with an offset. This is a type of generalized linear model used for modeling count data and contingency tables.\nAn offset is a predictor variable with a coefficient that is fixed at 1. It is used to account for the “exposure” variable, which represents the opportunity for an event to occur. For instance, if we are counting the number of sick individuals in different cities, the population of each city would be the exposure variable. A city with a larger population is expected to have more sick individuals. The offset accounts for this by essentially modeling the rate of events per unit of exposure.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#general-principles",
    "href": "8. Poisson mode with offset.html#general-principles",
    "title": "Poisson Model with an Offset",
    "section": "",
    "text": "When we want to model count data, where the counts are observed over different periods or areas of exposure, we use a Poisson model with an offset. This is a type of generalized linear model used for modeling count data and contingency tables.\nAn offset is a predictor variable with a coefficient that is fixed at 1. It is used to account for the “exposure” variable, which represents the opportunity for an event to occur. For instance, if we are counting the number of sick individuals in different cities, the population of each city would be the exposure variable. A city with a larger population is expected to have more sick individuals. The offset accounts for this by essentially modeling the rate of events per unit of exposure.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#considerations",
    "href": "8. Poisson mode with offset.html#considerations",
    "title": "Poisson Model with an Offset",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nThe dependent variable in a Poisson regression must be a non-negative count.\nThe exposure variable used as an offset cannot contain zeros.\nA key assumption of the Poisson distribution is that the mean and variance of the count variable are equal. If the variance is greater than the mean, a condition known as overdispersion, a Negative Binomial regression might be more appropriate.\nThe logarithm of the exposure variable is typically used as the offset. This is because Poisson regression models the logarithm of the expected count. By including the log of the exposure as an offset, we are effectively modeling the rate.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#example",
    "href": "8. Poisson mode with offset.html#example",
    "title": "Poisson Model with an Offset",
    "section": "Example",
    "text": "Example\nBelow is an example of code that demonstrates a Bayesian Poisson regression with an offset. The data consists of the number of elephant aggressions (agressions), the age of the elephants (age), and the number of years they have been observed (years_obs). The goal is to model the rate of aggressions per year, accounting for the age of the elephants.\n\nPythonR\n\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'elephants.csv'\nm.data(data_path, sep=',') \nm.log(['years_obs']) # Log transform the exposure variable\nm.data_to_model(['agressions', 'age', \"log_years_obs\"]) # Send to model\n\n# Define model ------------------------------------------------\ndef model(agressions, age, log_years_obs):\n    a = m.bi.dist.normal(0, 1, name = 'a') \n    b = m.bi.dist.normal(0, 0.5, name = 'b')\n    log_lambda = a + b * age + log_years_obs # Add offset to the linear model\n    m.poisson(m.bi.numpy.exp(log_lambda), obs=agressions)\n\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/elephants.csv\", sep = ''), sep=',')\nm$log(list('years_obs')) # Log transform the exposure variable\nm$data_to_model(list('agressions', 'age', 'log_years_obs')) # Send to model\n\n# Define model ------------------------------------------------\nmodel &lt;- function(agressions, age, log_years_obs){\n  # Parameter prior distributions\n  a = bi.dist.normal(0, 1, name = 'a')\n  b = bi.dist.normal(0, 0.5, name = 'b')\n  # Likelihood\n  log_lambda = a + b * age + log_years_obs # Add offset to the linear model\n  m$poisson(exp(log_lambda), obs=agressions)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#mathematical-details",
    "href": "8. Poisson mode with offset.html#mathematical-details",
    "title": "Poisson Model with an Offset",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variables (X) and the expected count (λ) using the following equation:\n\n\\log(\\lambda_i) = \\alpha + \\beta X_i + \\log(\\text{exposure}_i)\n\nWhere:\n\n\\lambda_i is the expected count for observation i.\n\\alpha is the intercept term.\n\\beta is the regression coefficient for the independent variable.\nX_i is the value of the independent variable for observation i.\n\\log(\\text{exposure}_i) is the offset, which is the natural logarithm of the exposure for observation i.\n\nThe number of observed counts Y_i is assumed to follow a Poisson distribution with mean \\lambda_i:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\nBayesian formulation\nIn the Bayesian framework, we assign prior distributions to the model parameters. The model can be expressed as:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\\log(\\lambda_i) = \\alpha + \\beta X_i + \\log(\\text{exposure}_i)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\nWhere:\n\nY_i is the observed count for observation i.\n\\lambda_i is the expected count.\n\\alpha is the intercept with a unit-normal prior.\n\\beta is the slope coefficient with a unit-normal prior.\nX_i is the independent variable.\n\\log(\\text{exposure}_i) is the offset.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#notes",
    "href": "8. Poisson mode with offset.html#notes",
    "title": "Poisson Model with an Offset",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe use of an offset is crucial when the goal is to compare rates of events rather than absolute counts.\nIt is a common practice to use the natural logarithm of the exposure variable as the offset.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#references",
    "href": "8. Poisson mode with offset.html#references",
    "title": "Poisson Model with an Offset",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#considerations",
    "href": "17. Missing data (wip).html#considerations",
    "title": "Handling Missing Data (WIP)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#example",
    "href": "17. Missing data (wip).html#example",
    "title": "Handling Missing Data (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Missing data model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#mathematical-details",
    "href": "17. Missing data (wip).html#mathematical-details",
    "title": "Handling Missing Data (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\n\n\nBayesian formulation",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#notes",
    "href": "17. Missing data (wip).html#notes",
    "title": "Handling Missing Data (WIP)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#references",
    "href": "17. Missing data (wip).html#references",
    "title": "Handling Missing Data (WIP)",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html",
    "href": "13. Varying intercepts.html",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data are grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#general-principles",
    "href": "13. Varying intercepts.html#general-principles",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data are grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#considerations",
    "href": "13. Varying intercepts.html#considerations",
    "title": "Varying Intercepts Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nThe main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept \\alpha_k is defined uniquely for each of the K declared groups.\nIn the code below, the intercept alpha for each of the k declared groups shares two priors, a_bar and sigma, which are respectively modeled by a Normal and an Exponential distribution.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#example",
    "href": "13. Varying intercepts.html#example",
    "title": "Varying Intercepts Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying intercepts using the Bayesian Inference (BI) package. The data consists of a dependent variable representing individuals’ survival (surv) and an independent categorical variable (tank), which indicates the tank where the individual was born, with a total of 48 tanks. This example is based on McElreath (2018).\n\nPython (Raw)Python (Build in function)R\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n    p = alpha[tank]\n    m.dist.binomial(total_count = density, logits = p, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    alpha = m.effects.varying_intercept(N_groups=48,group_id=tank, group_name = 'tank')\n    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/reedfrogs.csv\", sep = ''), sep=';')\nm$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\nm$data_to_model(list('tank', 'surv', 'density')) # Manipulate\nm$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\nm$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(tank, surv, density){\n  # Parameter prior distributions\n  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n  a_bar =  bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n  p = alpha[tank]\n  # Likelihood\n  m$binomial(total_count = density, logits = p, obs=surv)\n} \n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#mathematical-details",
    "href": "13. Varying intercepts.html#mathematical-details",
    "title": "Varying Intercepts Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe model the relationship between the independent variable X and the outcome variable Y while accounting for varying intercepts \\alpha for each group where k(i) give us group belonging for observation i, using the following equation:\n\nY_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\n \n\\mu_{i} = \\alpha_{[k(i)]} + \\beta X_{i}\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\n\n\\alpha_{[k]} \\sim \\text{Normal}(\\bar{\\alpha}, \\varsigma)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_{i} is the outcome variable for observation i.\n\\alpha_{[k(i)]} is the varying intercept corresponding to the group k of observation i%.\n\\beta is the regression coefficient.\n\\sigma is a standard deviation parameter, which here has an Exponential prior that constrains it to be positive.\n\\bar{\\alpha} is the overall mean intercept.\n\\varsigma is the variance of the intercepts across groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#notes",
    "href": "13. Varying intercepts.html#notes",
    "title": "Varying Intercepts Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2.\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying intercepts with any distribution developed in previous chapters.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#references",
    "href": "13. Varying intercepts.html#references",
    "title": "Varying Intercepts Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "api_manip.html",
    "href": "api_manip.html",
    "title": "Handle data",
    "section": "",
    "text": "manip is a class to unify various diagnostics methods and provide a consistent interface for diagnostics.",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#perform-one-hot-encoding-on-specified-columns",
    "href": "api_manip.html#perform-one-hot-encoding-on-specified-columns",
    "title": "Handle data",
    "section": "Perform one-hot encoding on specified columns",
    "text": "Perform one-hot encoding on specified columns\n\nArgs:\n\ncols (str or list): Columns to encode. Use ‘all’ for all object-type columns\n\n\n\nReturns:\n\npd.DataFrame: DataFrame with encoded columns\n\nbi.dist.OHE(\nself,\ncols='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#load-data-from-csv-file",
    "href": "api_manip.html#load-data-from-csv-file",
    "title": "Handle data",
    "section": "Load data from CSV file",
    "text": "Load data from CSV file\n\nArgs:\n\npath (str): Path to the CSV file\n\n**kwargs*: Additional arguments for pd.read_csv\n\n\n\n\nReturns:\npd.DataFrame: Loaded dataframe\nbi.dist.data(\nself,\npath,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#prepare-data-for-model-input-in-jax-format",
    "href": "api_manip.html#prepare-data-for-model-input-in-jax-format",
    "title": "Handle data",
    "section": "Prepare data for model input in JAX format",
    "text": "Prepare data for model input in JAX format\n\nArgs:\n\ncols (list): List of columns to include in model data\n\n\n\nReturns:\n\ndict: JAX formatted dictionary\n\nbi.dist.data_to_model(\nself,\ncols,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#create-index-encoding-for-categorical-columns",
    "href": "api_manip.html#create-index-encoding-for-categorical-columns",
    "title": "Handle data",
    "section": "Create index encoding for categorical columns",
    "text": "Create index encoding for categorical columns\n\nArgs:\n\ncols (str or list): Columns to encode. Use ‘all’ for all object-type columns\n\n\n\nReturns:\n\npd.DataFrame: DataFrame with encoded columns\n\nbi.dist.index(\nself,\ncols='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-pandas-dataframe-to-jax-compatible-format-for-a-model",
    "href": "api_manip.html#convert-pandas-dataframe-to-jax-compatible-format-for-a-model",
    "title": "Handle data",
    "section": "Convert pandas dataframe to JAX compatible format for a model",
    "text": "Convert pandas dataframe to JAX compatible format for a model\n\nArgs:\n\nmodel: JAX model to prepare data for\nbit (str): Bit precision for numbers (default: 32)\n\n\n\nReturns:\n\ndict: JAX formatted dictionary\n\nbi.dist.pd_to_jax(\nself,\nmodel,\nbit=None,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#standardize-specified-columns",
    "href": "api_manip.html#standardize-specified-columns",
    "title": "Handle data",
    "section": "Standardize specified columns",
    "text": "Standardize specified columns\n\nArgs:\n\ndata (str or list): Columns to standardize. Use ‘all’ for all columns\n\n\n\nReturns:\n\npd.DataFrame: Standardized dataframe\n\nbi.dist.scale(\nself,\ndata='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#jax-jitted-function-to-scalestandardize-a-single-variable",
    "href": "api_manip.html#jax-jitted-function-to-scalestandardize-a-single-variable",
    "title": "Handle data",
    "section": "JAX-jitted function to scale/standardize a single variable",
    "text": "JAX-jitted function to scale/standardize a single variable\nbi.dist.scale_var(\nself,\nx,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-specified-columns-to-float-type",
    "href": "api_manip.html#convert-specified-columns-to-float-type",
    "title": "Handle data",
    "section": "Convert specified columns to float type",
    "text": "Convert specified columns to float type\n\nArgs:\n\ncols (str or list): Columns to convert. Use ‘all’ for all columns\ntype (str): Float type to convert to (default: float32)\n\n\n\nReturns:\n\npd.DataFrame: Converted dataframe\n\nbi.dist.to_float(\nself,\ncols='all',\ntype='float32',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-specified-columns-to-integer-type",
    "href": "api_manip.html#convert-specified-columns-to-integer-type",
    "title": "Handle data",
    "section": "Convert specified columns to integer type",
    "text": "Convert specified columns to integer type\n\nArgs:\n\ncols (str or list): Columns to convert. Use ‘all’ for all columns\ntype (str): Integer type to convert to (default: int32)\n\n\n\nReturns:\n\npd.DataFrame: Converted dataframe\n\nbi.dist.to_int(\nself,\ncols='all',\ntype='int32',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "Advance/build in functions.html",
    "href": "Advance/build in functions.html",
    "title": "Build in functions",
    "section": "",
    "text": "Varying effects\n\nVarying interceptsVarying slopesVarying intercepts and slopes\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    alpha = m.effects.varying_intercept(group=tank,group_name = 'tank')\n    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\n\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n        N_group = N_cafes,\n        group = cafe,\n        global_intercept= a,\n        global_slope= b,\n        group_name = 'cafe')\n    \n\n    mu = varying_intercept + varying_slope* afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\n\n\n\nGaussian processes\n\nSquared Exponential KernelPeriodic KernelLocally Periodic Kernel\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_sq_exp\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_periodic\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_periodic_local\n\n\n\n\n\nNetworks effects\n\nSender receiverDyadicBlock model\n\n\nsr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n\nm.net.dyadic_effect(dyadic_predictors)\n\n\nm.net.block_model(Merica: vector[integer],3)\n\n\n\n\n\nNetwork metrics\n\nDegree (undirected, out-degree, in-degree)Strength (undirected, out-strength , in-strength)EigenvectorClustering coefficientDensityGeodesic_distanceDiameter\n\n\nm.net.degree(adj_matrix_jax)\nm.net.indegree(adj_matrix_jax)\nm.net.outdegree(adj_matrix_jax)\n\n\nm.net.strength(adj_matrix_jax)\nm.net.instrength(adj_matrix_jax)\nm.net.outstrength(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in functions"
    ]
  },
  {
    "objectID": "start/Installation.html",
    "href": "start/Installation.html",
    "title": "Installation",
    "section": "",
    "text": "You can run BI on python or R. For R users you need to have installed python and the R reticulate.\n\nPythonR\n\n\n#| code-fold: false\npip install BayesInference # For CPU support\npip install BayesInference[gpu] # For GPU support \n\n\n#| code-fold: false\npackage.install(BayesianInference)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAs reticulate use it’s own python environment, you need to install the python version of BI through reticulate. You can do this by running the following command in R:\nreticulate::py_install(\"BayesianInference\", pip = TRUE)\nGPU support is only available for linux and WSL2 systems.",
    "crumbs": [
      "Get started",
      "Installation",
      "Installation"
    ]
  },
  {
    "objectID": "start/Define model.html",
    "href": "start/Define model.html",
    "title": "Define model",
    "section": "",
    "text": "You can define your model by declaring a new function. The function should take the data as input and return the predicted values.\n\nPythonR\n\n\ndef model(Y, X):\n    # Define the model here\n    a = m.dist.normal(0, 1, name = 'a')\n    b = m.dist.normal(1, 1, name = 'b')\n    s = m.dist.exponential(1 name = 's')\n    \n    # Return the predicted values\n    m.dist.normal(a + b * X, obs = Y)\n\n\nmodel &lt;- function(Y, X){\n  # Define the model here\n    a = m.dist.normal(0, 1, name = 'a')\n    b = m.dist.normal(1, 1, name = 'b')\n    s = m.dist.exponential(1 name = 's')\n    m.dist.normal(a + b * X, obs = Y)\n}\n\n\n\nNote the additional obs argument when declaring the likelihood function. This argument is used to specify the observed data.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Define model"
    ]
  },
  {
    "objectID": "start/Import_class.html",
    "href": "start/Import_class.html",
    "title": "Import BI class",
    "section": "",
    "text": "Before anything, you need to import the BI class. This will allow you to create a BI object that will be used to : 1) import data, 2) define the model, 3) fit the model, 4) summarize the results, and 5) plot the results.\n\nPythonR\n\n\nfrom BI import bi\nm = bi()\n\n\nlibrary(BayesInference)\nm=importBI(platform='cpu')\n\n\n\n\nArguments:\n\nplatform: (str, optional). The hardware platform to use for computation. Options include:\n\n‘cpu’: Use CPU(s) for computation\n‘gpu’: Use GPU(s) for computation\n‘tpu’: Use TPU(s) for computation\n\nDefaults to ‘cpu’.\ncores: (int, optional). Number of CPU cores to allocate for computation. If None, all available CPU cores will be used. Only applicable when platform is ‘cpu’.\ndeallocate: (bool, optional). Whether to deallocate any existing devices before setting up new configuration. Defaults to False.\n\n\n\nExamples\nsetup_device(platform='cpu')\n\nsetup_device(platform='gpu') # Only for BayesInference[gpu]\n\nsetup_device(platform='cpu', cores=4) # Specifying CPU cores",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import BI class"
    ]
  },
  {
    "objectID": "19. PCA.html",
    "href": "19. PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) (Tipping and Bishop 1999) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\nPCA is employed for dimensionality reduction, particularly in scenarios involving high-dimensional datasets, as it effectively reduces complexity while explicitly accounting for uncertainty in the underlying latent structure. This approach also plays a crucial role in data visualization, enabling the projection of intricate, high-dimensional data into more interpretable 2D or 3D representations. Additionally, PCA excels in feature extraction, where the latent variables it identifies can be repurposed as informative features for subsequent tasks, such as classification or clustering. By modeling latent variables, it further enhances the interpretability and utility of the data for a variety of analytical applications.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#general-principles",
    "href": "19. PCA.html#general-principles",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) (Tipping and Bishop 1999) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\nPCA is employed for dimensionality reduction, particularly in scenarios involving high-dimensional datasets, as it effectively reduces complexity while explicitly accounting for uncertainty in the underlying latent structure. This approach also plays a crucial role in data visualization, enabling the projection of intricate, high-dimensional data into more interpretable 2D or 3D representations. Additionally, PCA excels in feature extraction, where the latent variables it identifies can be repurposed as informative features for subsequent tasks, such as classification or clustering. By modeling latent variables, it further enhances the interpretability and utility of the data for a variety of analytical applications.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#considerations",
    "href": "19. PCA.html#considerations",
    "title": "Principal Component Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nIn Bayesian PCA, we assume prior distributions for the latent variables Z and the principal component loadings W. We place Gaussian priors on both Z and W and learn their posterior distributions using the observed data X.\nRobustness to Outliers: Standard PCA are sensitive to outliers due to the assumption of Gaussian noise. Robust variants of Robust Bayesian PCA address this by employing heavy-tailed distributions for the noise model, such as the Student’s t-distribution, which reduces the influence of outliers (Archambeau, Delannay, and Verleysen 2006; Bouwmans and Zahzah 2014).\nAutomatic Dimensionality Selection: Through techniques like Automatic Relevance Determination (ARD), Bayesian PCA can automatically determine the effective dimensionality of the latent space. Priors are placed on the relevance of each principal component, and components that are not supported by the data are effectively “switched off’ (C. Bishop 1998; C. M. Bishop and Nasrabadi 2006).\nSparsity for High-Dimensional Data: In high-dimensional settings, it is often desirable for the principal components to be influenced by only a subset of the original features, leading to more interpretable results. Sparse Bayesian PCA achieves this by placing sparsity-inducing priors (e.g., Laplacian or spike-and-slab priors) on the loading (Sigg and Buhmann 2008; Zou, Hastie, and Tibshirani 2006).",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#example",
    "href": "19. PCA.html#example",
    "title": "Principal Component Analysis",
    "section": "Example",
    "text": "Example\nHere is an example code snippet demonstrating Bayesian PCA using BI:\n\nBuild in functionsStandardWith ARDRobustSparseSparse robust with ARD\n\n\n\nfrom BI import bi,jnp\nm=bi()\nm.data('iris.csv', sep=',') # Data is already scaled\nm.data_on_model = dict(\n    X=jnp.array(m.df.iloc[:,0:-2].values)\n)\nm.fit(m.models.pca(type=\"ARD\"), progress_bar=False) # or robust, sparse, classic, sparse_robust_ard\nm.models.pca.plot(\n    X=m.df.iloc[:,0:-2].values,\n    y=m.df.iloc[:,-2].values, \n    feature_names=m.df.columns[0:-2], \n    target_names=m.df.iloc[:,-1].unique(),\n    color_var=m.df.iloc[:,0].values,\n    shape_var=m.df.iloc[:,-2].values\n)\n\n\ndef model(x_train, data_dim, latent_dim, num_datapoints): \n    # Gaussian prior for the principal component 'W'.\n    w = m.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')\n\n    # Gaussian prior on the latent variables 'Z'\n    z = m.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')\n\n    # Exponential prior on the noise variance 'epsilon'\n    epsilon = m.dist.exponential(1, name='epsilon')\n\n    # Likelihood\n    m.dist.normal(w @ z , epsilon, obs = x_train)  \n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model)\n\n\n\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\n\ndef model_ARD(X, data_dim, latent_dim, num_datapoints):\n    # --- Automatic Dimensionality Selection using an ARD Prior ---\n    # There is one 'alpha' for each latent dimension.\n    # This 'alpha' acts as a prior on the \"relevance\" of each principal component. A large alpha will signal low relevance.\n    alpha = m.dist.gamma(.05, 1e-3, shape=(latent_dim,), name='alpha')\n\n    # Gaussian prior for the principal component 'W'.\n    # The precision (1 / variance) of the weights for each component is controlled by its corresponding 'alpha'.\n    w = m.dist.normal(0, 1. / jnp.sqrt(alpha)[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # Gaussian prior on the latent variables 'Z'\n    z = m.dist.normal(0, 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # Noise model\n    # We assume the noise is Gaussian, but we don't know its variance. So, we place a Gamma prior on the precision (1 / variance) to learn it from the data.\n    precision = m.dist.gamma(1.0, 1.0, name='precision')\n    # We convert the learned precision into a standard deviation for use in the likelihood.\n    stddv = 1. / jnp.sqrt(precision)\n\n    # Likelihood\n    # 'X' is modeled as a linear projection of the latents (w @ z) plus Gaussian noise.\n    m.dist.normal(w @ z, stddv, obs=X)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_ARD)\nm.summary()\n\n\ndef model_robust(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    Robust Bayesian PCA model using a Student's t-distribution for the likelihood.\n    \"\"\"\n    # --- Standard Priors for W and Z ---\n    w = m.dist.normal(0, 1., shape=(data_dim, latent_dim), name='w')\n    z = m.dist.normal(0, 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # --- Robustness to Outliers via a Heavy-Tailed Noise Model ---\n    # This defines the prior on the scale (similar to standard deviation) of the noise.\n    sigma = m.dist.halfcauchy(1.0, name='sigma')\n\n    # This is a prior on the \"degrees of freedom\" ('nu') of the Student's t-distribution.\n    # This parameter controls the \"heaviness\" of the tails. A small 'nu' means\n    # heavier tails, making the model more robust to outliers. By learning this\n    # parameter, the model can adapt its robustness to the data.\n    nu = m.dist.gamma(2.0, 0.1, name='nu')\n\n    # This is the key line for this model. The likelihood is a Student's t-distribution.\n    # As your description states, this \"heavy-tailed distribution... reduces the\n    # influence of outliers\" by treating them as more plausible events than a\n    # Gaussian distribution would, thus preventing them from skewing the results.\n    m.dist.studentt(df=nu, loc=w @ z, scale=sigma, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_robust)\nm.summary()\n\n\ndef model_sparse(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    Sparse Bayesian PCA model using a Laplace prior on the weights (w).\n    \"\"\"\n    # --- Sparsity for High-Dimensional Data via a Sparsity-Inducing Prior ---\n\n    # This is the first part of a hierarchical prior (known as the Bayesian Lasso).\n    # We place a prior on 'lambda_', which will control the scale of our Laplace prior.\n    # This allows the model to learn the appropriate level of sparsity from the data.\n    lambda_ = m.dist.gamma(1.0, 1.0, shape=(latent_dim,), name='lambda')\n\n    # This is the key line for this model. We place a Laplace prior on the loadings 'W'.\n    # As your description states, this is a \"sparsity-inducing prior\". The Laplace\n    # distribution is sharply peaked at zero, which encourages many of the weight\n    # values in 'W' to be exactly zero, leading to \"more interpretable results\".\n    w = m.dist.laplace(0., 1. / lambda_[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # --- Standard Model Components ---\n    \n    # We place a standard Gaussian prior on the latent variables 'Z', as described\n    # in the note: \"We place Gaussian priors on both Z and W...\".\n    z = m.dist.normal(0., 1., shape=(latent_dim, num_datapoints), name='z')\n    \n    # This section defines the standard Gaussian noise model, which is separate\n    # from the sparsity-inducing prior on the weights.\n    precision = m.dist.gamma(1.0, 1.0, name='precision')\n    stddv = 1. / jnp.sqrt(precision)\n\n    # This is the standard likelihood, same as in the ARD model. The generative story\n    # is unchanged, but the prior on 'W' now enforces the desired sparsity property.\n    m.dist.normal(w @ z, stddv, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_sparse)\nm.summary()\n\n\n\ndef model_sparse_robust_ard(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    A combined Sparse, Robust Bayesian PCA model with Automatic Relevance Determination (ARD).\n\n    - Sparsity: Achieved with a Laplace prior on the weights 'w'.\n    - Robustness: Achieved with a Student's t-distribution for the likelihood.\n    - ARD: Achieved by placing a hierarchical prior on the scale of the Laplace\n      distribution, allowing entire latent dimensions to be pruned.\n    \"\"\"\n    # --- ARD and Sparsity-Inducing Prior on w ---\n    \n    # This is the ARD component. We define a relevance parameter 'lambda_' for each\n    # latent dimension. A large lambda will signal that the corresponding\n    # component is not relevant and should be shrunk away.\n    # The Gamma prior ensures lambda_ is positive.\n    lambda_ = m.dist.gamma(1.0, 1.0, shape=(latent_dim,), name='lambda')\n\n    # This is the Sparsity component. We use a Laplace prior for the weights 'w'.\n    # The COMBINED effect happens here: the scale of the Laplace distribution is\n    # controlled by the ARD parameter 'lambda_'. If a component is irrelevant\n    # (large lambda_), the scale becomes small, and the Laplace prior aggressively\n    # forces the weights in that column of 'w' to zero.\n    # This gives us both sparsity and automatic dimensionality selection.\n    w = m.dist.laplace(0., 1. / lambda_[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # --- Standard Prior for Z ---\n    \n    # The prior on the latent variables 'Z' remains a standard Gaussian.\n    z = m.dist.normal(0., 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # --- Robustness to Outliers via a Heavy-Tailed Noise Model ---\n    \n    # Prior on the scale of the Student's t-distribution.\n    sigma = m.dist.halfcauchy(1.0, name='sigma')\n\n    # Prior on the degrees of freedom 'nu', which controls the robustness.\n    nu = m.dist.gamma(2.0, 0.1, name='nu')\n\n    # The likelihood is the Student's t-distribution, which makes the entire\n    # model robust to outliers in the observed data 'x_train'.\n    m.dist.studentt(df=nu, loc=w @ z, scale=sigma, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_sparse_robust_ard)\nm.summary()",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#mathematical-details",
    "href": "19. PCA.html#mathematical-details",
    "title": "Principal Component Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe assume the observed data matrix X is centered and arranged with features as rows and samples as columns, X \\in \\mathbb{R}^{N \\times V} where N is the number of observations and V the number of variables. The generative model projects the data into a lower-dimensional space with K latent variables, K \\leq V, using the following equation: \nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n\nwhere :\n\nX is the observed data matrix.\nZ \\in \\mathbb{R}^{N \\times K} is the latent variable matrix (latent features) with K \\ll D. Z is defined by a Normal distribution with mean 0 and variance 1.\nW \\in \\mathbb{R}^{K \\times V} is the matrix of principal components (projection matrix). W is defined by a Normal distribution with mean 0 and variance 1.\n\\sigma is the standard deviation of the normal distribution.\n\nThe likelihood and priors are defined element-wise for v=1...V, n=1...N, and k=1...K. for the following models:\n\nStandard PCA\n\nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n \nZ \\sim \\text{Normal}(0, 1)\n \nW \\sim \\text{Normal}(0, 1)\n \n\\sigma \\sim \\text{Exponential}(1)",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#note",
    "href": "19. PCA.html#note",
    "title": "Principal Component Analysis",
    "section": "Note",
    "text": "Note\n\nTo account for sign ambiguity 🛈 in PCA, we can set the number of latent dimensions K to be equal to the number of variables V. Then, we can calculate the dot product between the estimated parameters and the data. If it is negative, we multiply the estimated parameters by -1 to align them with the data. Below, a code snippet highlights how to do this:\n\n\ntrue_params = jnp.array(real_data)      \nestimated_params = jnp.array(m.posteriors) \n\n# Compute dot product\ndot_product = jnp.dot(true_params, estimated_params)\n\n# Align signs if necessary\nif dot_product &lt; 0:\n    estimated_params = -estimated_params\n\n# Plot the aligned parameters\nplt.scatter(true_params, estimated_params, alpha=0.7)\nplt.plot([min(true_params), max(true_params)], [min(true_params), max(true_params)], 'r--')\nplt.xlabel('True Parameters')\nplt.ylabel('Estimated Parameters')\nplt.title('True vs. Estimated Parameters After Sign Alignment')\nplt.show()",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#references",
    "href": "19. PCA.html#references",
    "title": "Principal Component Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nArchambeau, Cédric, Nicolas Delannay, and Michel Verleysen. 2006. “Robust Probabilistic Projections.” In Proceedings of the 23rd International Conference on Machine Learning, 33–40.\n\n\nBishop, Christopher. 1998. “Bayesian Pca.” Advances in Neural Information Processing Systems 11.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer. https://doi.org/https://link.springer.com/book/9780387310732.\n\n\nBouwmans, Thierry, and El Hadi Zahzah. 2014. “Robust PCA via Principal Component Pursuit: A Review for a Comparative Evaluation in Video Surveillance.” Computer Vision and Image Understanding 122: 22–34. https://doi.org/https://doi.org/10.1016/j.cviu.2013.11.009.\n\n\nSigg, Christian D, and Joachim M Buhmann. 2008. “Expectation-Maximization for Sparse and Non-Negative PCA.” In Proceedings of the 25th International Conference on Machine Learning, 960–67.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society Series B: Statistical Methodology 61 (3): 611–22.\n\n\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "7. Poisson model.html",
    "href": "7. Poisson model.html",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable—e.g., counts of events occurring in a fixed interval of time or space—and one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials n is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#general-principles",
    "href": "7. Poisson model.html#general-principles",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable—e.g., counts of events occurring in a fixed interval of time or space—and one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials n is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#considerations",
    "href": "7. Poisson model.html#considerations",
    "title": "Poisson Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nWe have the second link function 🛈: log. The log link ensures that λ is always positive.\nThe dependent variable in a Poisson regression must be a non-negative count.\nTo invert the log link function and linearly model the relationship between the predictor variables and the log of the mean rate parameter, we can apply the exponential function (see comment in code).\nA key assumption of the Poisson distribution is that the mean and variance of the count variable are equal. If the variance is greater than the mean, a condition known as overdispersion, a Gamma-Poisson model might be more appropriate.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#example",
    "href": "7. Poisson model.html#example",
    "title": "Poisson Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Poisson model using the Bayesian Inference (BI) package. Data consist of:\n\nA continuous dependent variable total_tools, which represents the number of tools produced by a civilization.\nA continuous independent variable population representing population size.\nA categorical independent variable cid representing different civilizations.\n\nThe goal is to estimate the production of tools based on population size, accounting for each civilization. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline.csv'\nm.data(data_path, sep=';')\nm.scale(['population'])\nm.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n#m.data_to_model(['total_tools', 'population', 'cid'])\ndef model(cid, population, total_tools):\n    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n    l = jnp.exp(a[cid] + b[cid]*population)\n    m.dist.poisson(l, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline.csv\", sep = ''), sep=';')\nm$scale(list('population'))# Scale\nm$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\nm$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(total_tools, population, cid){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n  l = jnp$exp(alpha[cid] + beta[cid]*population)\n  # Likelihood\n  m.dist.poisson(l, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#mathematical-details",
    "href": "7. Poisson model.html#mathematical-details",
    "title": "Poisson Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\nlog(\\lambda_i) = \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\log() is the log link function. This function links the log of the mean of the response variable, \\lambda_i, to the linear predictor, \\alpha + \\beta X_i. The logarithm is the canonical link function for the Poisson distribution. It ensures that the predicted mean, \\lambda_i = \\exp(\\alpha + \\beta X_i), will always be positive, as required for a Poisson rate parameter.\n\\alpha and \\beta are the intercept and regression coefficient, respectively, with their associated prior distributions.\nX_i is the value of the independent variable for observation i.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#notes",
    "href": "7. Poisson model.html#notes",
    "title": "Poisson Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\n\n\nReference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#references",
    "href": "7. Poisson model.html#references",
    "title": "Poisson Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "20. GMM.html",
    "href": "20. GMM.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric 🛈 clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model’s goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\mu) and its shape/spread (covariance \\Sigma).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#general-principles",
    "href": "20. GMM.html#general-principles",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric 🛈 clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model’s goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\mu) and its shape/spread (covariance \\Sigma).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#considerations",
    "href": "20. GMM.html#considerations",
    "title": "Gaussian Mixture Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA GMM is a Bayesian model 🛈 that considers uncertainty in all its parameters, except for the number of clusters, K, which must be fixed in advance.\nThe key parameters and their priors are:\n\nNumber of Clusters K: This is a fixed hyperparameter that you must choose before running the model. Choosing the right K often involves running the model multiple times and using model comparison criteria (like cross-validation, AIC, or BIC).\nCluster Weights w: These are the probabilities of drawing a data point from any given cluster. Since there are a fixed number K of them and they must sum to 1, they are typically given a Dirichlet prior. A symmetric Dirichlet prior (e.g., Dirichlet(1, 1, ..., 1)) represents an initial belief that all clusters are equally likely.\nCluster Parameters (\\boldsymbol{\\mu}, \\Sigma): Each of the K clusters has a mean \\boldsymbol{\\mu} and a covariance matrix \\Sigma. We place priors on these to define our beliefs about their plausible values.\n\nLike the DPMM, the model is often implemented in its marginalized form 🛈. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\nTo increase accuracy we run a k-means algorithm to initialize the cluster mean priors.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#example",
    "href": "20. GMM.html#example",
    "title": "Gaussian Mixture Models",
    "section": "Example",
    "text": "Example\nBelow is an example of a GMM implemented in BI. The goal is to cluster a synthetic dataset into a pre-specified K=4 groups.\n\nPythonR\n\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n\n#  The model\ndef gmm(data, K, initial_means): # Here K is the *exact* number of clusters\n    D = data.shape[1]  # Number of features\n    alpha_prior = 0.5 * jnp.ones(K)\n    w = dist.dirichlet(concentration=alpha_prior, name='weights') \n\n    with dist.plate(\"components\", K): # Use fixed K\n        mu = dist.multivariatenormal(loc=initial_means, covariance_matrix=0.1*jnp.eye(D), name='mu')        \n        sigma = dist.halfcauchy(1, shape=(D,), event=1, name='sigma')\n        Lcorr = dist.lkjcholesky(dimension=D, concentration=1.0, name='Lcorr')\n\n        scale_tril = sigma[..., None] * Lcorr\n\n    dist.mixturesamefamily(\n        mixing_distribution=dist.categorical(probs=w, create_obj=True),\n        component_distribution=dist.multivariatenormal(loc=mu, scale_tril=scale_tril, create_obj=True),\n        name=\"obs\",\n        obs=data\n    )\n\nm.data_on_model = {\"data\": data,\"K\": 4 }\nm.fit(gmm) # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#mathematical-details",
    "href": "20. GMM.html#mathematical-details",
    "title": "Gaussian Mixture Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThis section describes the generative process for a GMM. For each data point x_i, the model first selects one of the K clusters according to the weights w, and then draws the point from that cluster’s Gaussian distribution. \nz_i \\sim \\text{Categorical}(w)  \\text{for } i=1, \\dots, N \\\\\nx_i \\mid z_i=k \\sim \\text{MultivariateNormal}(\\mu_k, \\Sigma_k)  \\text{for } i=1, \\dots, N\n\n\nw \\sim \\text{Dirichlet}(\\alpha_0) \\quad \\text{(Mixture weights vector for K clusters)}\n\n\n\\mu_k \\sim \\text{MultivariateNormal}(\\mu_0, \\Sigma_0)  \\text{for } k=1, \\dots, K \\\\\n\\sigma_k \\sim \\text{HalfCauchy}(1)  \\text{for } k=1, \\dots, K \\\\\nL_{\\text{corr}, k} \\sim \\text{LKJCholesky}(D, 1.0)  \\text{for } k=1, \\dots, K \\\\\n\\Sigma_k = \\text{diag}(\\sigma_k) \\cdot L_{\\text{corr}, k} \\cdot L_{\\text{corr}, k}^T \\cdot \\text{diag}(\\sigma_k)\n\nParameter Definitions: * Observed Data: * x_i: The i-th observed D-dimensional data point.\n\nLatent Variables (Inferred):\n\nz_i: The integer cluster assignment for the i-th data point.\nw: The K-dimensional vector of mixture weights.\n\\mu_k: The D-dimensional mean vector of the k-th cluster.\n\\Sigma_k: The DxD covariance matrix of the k-th cluster (composed of \\sigma_k and L_{\\text{corr},k}).\n\nHyperparameters (Fixed):\n\nK: The total number of clusters.\n\\alpha_0: The concentration parameter vector for the Dirichlet prior on weights (e.g., [1, 1, ..., 1]).\n\\mu_0: The prior mean for the cluster centers.\n\\Sigma_0: The prior covariance for the cluster centers.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#notes",
    "href": "20. GMM.html#notes",
    "title": "Gaussian Mixture Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary challenge of the GMM compared to the DPMM is the need to manually specify the number of clusters K. If the chosen K is too small, the model may merge distinct clusters. If K is too large, it may split natural clusters into meaningless sub-groups. Therefore, applying a GMM often involves an outer loop of model selection where one fits the model for a range of K values and uses a scoring metric to select the best one.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#references",
    "href": "20. GMM.html#references",
    "title": "Gaussian Mixture Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nC. M. Bishop (2006). Pattern Recognition and Machine Learning. Springer. (Chapter 9)",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "get started.html",
    "href": "get started.html",
    "title": "Installation",
    "section": "",
    "text": "You can run BI on python or R. For R users you need to have installed python and the R reticulate package.\n\nPythonR\n\n\npip install BayesInference\n\n\npackage.install(BayesInference)"
  },
  {
    "objectID": "get started.html#import-tabular-data-from-a-csv-file",
    "href": "get started.html#import-tabular-data-from-a-csv-file",
    "title": "Installation",
    "section": "Import tabular data from a csv file",
    "text": "Import tabular data from a csv file\n\nPythonR\n\n\nm.data(data_path, sep=';') \n\n\nm$data(data_path,  sep=';')"
  },
  {
    "objectID": "get started.html#import-non-tabular-data",
    "href": "get started.html#import-non-tabular-data",
    "title": "Installation",
    "section": "Import non tabular data",
    "text": "Import non tabular data\nFirst you need to create our own dictionary with the data.\n\nPythonR\n\n\nm.data_on_model = dict(\n    ID1 = Value1,\n    ID2 = Value2, \n)\n\n\nkeys &lt;- c(\"ID1\",\"ID2\")\nvalues &lt;- list(Value1,Value2)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data"
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\beta_x for each continuous variable (e.g., \\beta_{weight} and \\beta_{age}).",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#general-principles",
    "href": "2. Multiple continuous Variables.html#general-principles",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\beta_x for each continuous variable (e.g., \\beta_{weight} and \\beta_{age}).",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "Multivariate Linear Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Regression for continuous variable.\nThe model interpretation of the regression coefficients \\beta_x is considered for fixed values of the other independent variable(s)’ regression coefficients—i.e., for a given age, \\beta_{weight} represents the expected change in the dependent variable (height) for each one-unit increase in weight, holding all other variables (e.g., age) constant.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "Multivariate Linear Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (height, weight, age), and the goal is to estimate the effect of weight and age on height. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nfrom importlib.resources import files\n# Import\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Subset data to adults\nm.scale(['weight', 'age']) # Normalize\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0, 50, name = 'sigma')\n    # Likelihood\n    m.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')# Import\nm$df = m$df[m$df$age &gt; 18,] # Subset data to adults\nm$scale(list('weight', 'age')) # Normalize\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight, age){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 0.5, name = 'a')\n  beta1 = bi.dist.normal(0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor R users, if you create the regression coefficient in a single call:\nbetas = bi.dist.normal(0, 0.5, name = 'regression_coefficients', shape = (2,))\nyou need to index them starting by 0:\n m$normal(alpha + betas[0] * weight + betas[1] * age, sigma, obs=height)",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "Multivariate Linear Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variables (X_{1i}, X_{2i}, ..., X_{[K,i]}) and the dependent variable Y using the following equation:\n\n𝑌_i = \\alpha +\\beta_1  𝑋_{[1,i]} + \\beta_2  𝑋_{[2,i]} + ... + \\beta_n  𝑋_{[K,i]} + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\nX_{[1,i]}, X_{[2,i]}, …, X_{[K,i]} are the values of the independent variables for observation i.\n\\beta_1, \\beta_2, …, \\beta_K are the regression coefficients.\n\\epsilon_i is the error term for observation i, and the vector of the error terms, \\epsilon, are assumed to be independent and identically distributed.\n\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express the Bayesian model as follows:\n\n𝑌_i \\sim \\text{Normal}(\\alpha + \\sum_{k=1}^K  \\beta_k  X_{[K,i]}, σ)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_k \\sim \\text{Normal}(0,1)\n\n\nσ \\sim \\text{Uniform}(0, 50)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_k are slope coefficients for the K distinct independent variables, which also have unit-normal priors.\nX_{[1,i]}, X_{[2,i]}, …, X_{[K,i]} are the values of the independent variables for observation i.\n\\sigma is a standard deviation parameter, which here has a Uniform prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "Multivariate Linear Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html",
    "href": "15. Gaussian processes.html",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. Basically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a kernel function 🛈.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#general-principles",
    "href": "15. Gaussian processes.html#general-principles",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. Basically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a kernel function 🛈.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#considerations",
    "href": "15. Gaussian processes.html#considerations",
    "title": "Gaussian Processes",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nTo capture complex, non-linear relationships in data where the underlying function is smooth but has an unknown functional form, GPs use a kernel 🛈.\nGPs assume normally distributed errors and may not be appropriate for all types of noise.\nThe choice of kernel hyperparameters can significantly impact results; thus, GPs require choosing an appropriate kernel function that captures the expected behavior of your data.\nThrough kernel definition, we can incorporate domain knowledge.\nThey scale poorly with dataset size (O(n³) complexity) due to matrix operations; thus, memory requirements can be substantial for large datasets, which has led to neural networks being used instead to resolve large non-linear problems.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#example",
    "href": "15. Gaussian processes.html#example",
    "title": "Gaussian Processes",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Gaussian Process regression using the Bayesian Inference (BI) package. Data consist of a continuous dependent variable (total_tools), representing the number of tools invented in the islands, and a continuous independent variable (population), representing the population of the islands. The goal is to estimate the effect of population on the total tools. We use the distance matrix of the islands for the kernel function in order to capture the spatial dependence of the relationship. This example is based on McElreath (2018).\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport numpyro\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline2.csv'\nm.data(data_path, sep=';') \n\ndata_path2 = files('BI.resources.data') / 'Kline2.csv'\nislandsDistMatrix = pd.read_csv(data_path2, index_col=0)\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    # non-centered Gaussian Process prior\n    etasq = m.dist.exponential(2, name = 'etasq')\n    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n    SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01)\n    k = m.dist.multivariatenormal(0, SIGMA, name = 'k')\n    #k = m.gaussian.gaussian_process(Dmat, etasq, rhosq, 0.01, shape = (10,))\n    k = m.gaussian.kernel_L2(Dmat, etasq, rhosq, 0.01)\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n\n\nlibrary(BI)\npd=import('pandas')\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline2.csv\", sep = ''), sep=';')\nislandsDistMatrix = pd$read_csv(paste(system.file(package = \"BI\"),\"/data/islandsDistMatrix.csv\", sep = ''), index_col=as.integer(0))\nm$data_to_model(list('total_tools', 'population'))\nm$data_on_model$society = jnp$arange(0,10, dtype='int64')\nm$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(Dmat, population, society, total_tools){\n  a = bi.dist.exponential(1, name = 'a')\n  b = bi.dist.exponential(1, name = 'b')\n  g = bi.dist.exponential(1, name = 'g')\n  \n  # non-centered Gaussian Process prior\n  etasq = bi.dist.exponential(2, name = 'etasq')\n  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n  z = bi.dist.normal(0,1, name = 'z', shape = c(10))\n  r = m$kernel_sq_exp(Dmat, z, etasq, rhosq, 0.01)\n  SIGMA = r[[1]]\n  L_SIGMA = r[[2]]\n  k = r[[3]]\n  lambda_ = a * population**b / g * jnp$exp(k[society])\n  m$poisson(lambda_, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#mathematical-details",
    "href": "15. Gaussian processes.html#mathematical-details",
    "title": "Gaussian Processes",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormula\nThe following equation allows us to evaluate the relationship between the dependent variable Y and the independent variable X while incorporating a GP for variable Z:\n\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\nwhere: - Y_i is the i-th value for the dependent variable Y.\n\n\\alpha is the intercept term.\n\\beta is the regression coefficient term.\nX_i is the i-th value for the independent variable X.\n\\gamma_{Z_i} is the Gaussian process i-th value for the independent variable Z.\n\nThe GP \\gamma_{Z_i} follows a multivariate normal distribution:\n\n\\begin{pmatrix}\n    Z_1 \\\\\n    \\vdots \\\\\n    Z_{n}\n\\end{pmatrix}\n\\sim \\text{MVNormal} \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\nwhere:\n\n(Z_1, ..., Z_n) represents a collection of all values of the random variable Z.\n(0, ..., 0) represents the mean vector of the multivariate normal distribution of the same size as the number of random variables and set to zero 🛈.\nK is the covariance matrix of the random variable Z. Each element K_{ij} of the matrix is given by the kernel function evaluated at the corresponding points: K_{ij} = k(Z_i, Z_j)\n\n\nK = \\begin{pmatrix}\n    k(Z_1, Z_1) & k(Z_1, Z_2) & \\cdots & k(Z_1, Z_{n}) \\\\\n    k(Z_2, Z_1) & k(Z_2, Z_2) & \\cdots & k(Z_2, Z_{n}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    k(Z_{n}, Z_1) & k(Z_{n}, Z_2) & \\cdots & k(Z_{n}, Z_{n})\n\\end{pmatrix}\n\n\nMultiple kernel functions exist and will be discussed in the Note(s) section. But the most common one is the quadratic kernel:\n\n\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\nWhere:\n\n\\eta is the signal variance, representing the overall variance of the outputs of the Gaussian process. It scales the influence of the kernel function. A larger \\eta^2 indicates a wider range of values the function can take.\np determines the rate of decline.\nD_{ij} is the distance between the i-th and j-th points.\n\\delta_{ij} is the Kronecker delta, taking a value of one when i = j and zero otherwise, allowing the self-covariance to be included in the calculation.\n\\sigma^2 is the noise variance, which accounts for the observation noise in the data. It represents the uncertainty or variability in the measurements or outputs at each point. The term effectively adds this noise variance only when i = j, ensuring that the diagonal elements of the covariance matrix represent the total variance at each input point.\n\n\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors 🛈. We can express a Bayesian version of this GP using the following model:\n\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\n\n\\gamma \\sim \\text{MVNormal} \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\n\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\eta^2 \\sim \\text{HalfCauchy}(0,1)\n\n\np^2 \\sim \\text{HalfCauchy}(0,1)\n\nwhere:\n\nY_i is the i-th value for the dependent variable Y.\n\\alpha is the intercept term with a prior of \\text{Normal}(0,1).\n\\beta is the regression coefficient term with a prior of \\text{Normal}(0,1).\nX_i is the i-th value for the independent variable X.\n\\gamma_{Z_i} is the Gaussian process i-th value for the independent variable Z.\n\\gamma is the latent function modeled by the GP.\nK_{ij} is the kernel function evaluated at the corresponding points, K_{ij} = k(Z_i, Z_j), with priors of HalfCauchy(0,1) for \\eta^2 and p^2 to ensure positive values.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#notes",
    "href": "15. Gaussian processes.html#notes",
    "title": "Gaussian Processes",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nCommon kernel functions include:\n\nRadial Basis Function (RBF) or Squared Exponential Kernel: k(x,x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\nRational Quadratic Kernel, this kernel is equivalent to adding together many RBF kernels with different length scales: k(x,x') = \\sigma^2 \\left(1 + \\frac{||x-x'||^2}{2l^2}\\right)^{-\\alpha}\nPeriodic kernel allows for modeling functions that repeat themselves exactly: k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)\nLocally Periodic Kernel:\n\nk(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right) \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\n\nGPs can be extended to classification problems using link functions.\nMulti-output problems can be addressed using matrix-valued kernels.\nDeep learning can be combined with GPs through Deep Kernel Learning.\nComputational tricks for large datasets include:\n\nSparse approximations (e.g., FITC, VFE)\nInducing points methods\nRandom Fourier features",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#references",
    "href": "15. Gaussian processes.html#references",
    "title": "Gaussian Processes",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.\n\n\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  }
]