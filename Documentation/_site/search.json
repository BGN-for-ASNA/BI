[
  {
    "objectID": "22. Network model.html",
    "href": "22. Network model.html",
    "title": "Network Models",
    "section": "",
    "text": "A network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random intercepts and slopes. By adding such parameters to the model, we can account for the correlations between node link relationships.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#considerations",
    "href": "22. Network model.html#considerations",
    "title": "Network Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThe particularity here is that varying intercepts and slopes are generated for both nodal effects üõà and dyadic effects üõà. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#example",
    "href": "22. Network model.html#example",
    "title": "Network Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect. This example is based on Ross, McElreath, and Redhead (2024).\n\nPythonR\n\n\n\n# Setup device------------------------------------------------\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = True)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = True)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\ndef sim_network(kinship, individual_predictor):\n  # Intercept\n  alpha = m.dist.normal(0,1, sample = True)\n\n  # SR\n  sr = m.net.sender_receiver(individual_predictor, individual_predictor, s_mu = 0.4, r_mu = -0.4, sample = True)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = True)\n\n  return m.dist.bernoulli(logits = alpha + sr + DR, sample = True)\n\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor)\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = dict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor\n)\n\n\ndef model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors):\n    N_id = network.shape[0]\n\n    # Block ---------------------------------------\n    alpha = m.dist.normal(0,1, sample = True)\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors,\n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n\n    m.dist.bernoulli(logits = alpha + sr + dr, obs=network)\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1)\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:02&lt;46:12,  2.78s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   0%|          | 5/1000 [00:02&lt;07:22,  2.25it/s, 1023 steps of size 6.62e-05. acc. prob=0.00]warmup:   1%|          | 7/1000 [00:03&lt;05:22,  3.08it/s, 1023 steps of size 2.69e-07. acc. prob=0.00]warmup:   1%|          | 9/1000 [00:03&lt;04:12,  3.92it/s, 1023 steps of size 1.34e-09. acc. prob=0.00]warmup:   1%|          | 11/1000 [00:03&lt;03:30,  4.71it/s, 1023 steps of size 8.48e-12. acc. prob=0.00]warmup:   1%|          | 12/1000 [00:03&lt;03:14,  5.09it/s, 1023 steps of size 7.42e-13. acc. prob=0.00]warmup:   1%|‚ñè         | 13/1000 [00:03&lt;02:58,  5.52it/s, 1023 steps of size 6.90e-14. acc. prob=0.00]warmup:   1%|‚ñè         | 14/1000 [00:04&lt;02:46,  5.92it/s, 1023 steps of size 6.82e-15. acc. prob=0.00]warmup:   2%|‚ñè         | 15/1000 [00:04&lt;02:35,  6.32it/s, 1023 steps of size 7.12e-16. acc. prob=0.00]warmup:   2%|‚ñè         | 16/1000 [00:04&lt;02:26,  6.70it/s, 1023 steps of size 7.86e-17. acc. prob=0.00]warmup:   2%|‚ñè         | 17/1000 [00:04&lt;02:20,  6.99it/s, 1023 steps of size 9.14e-18. acc. prob=0.00]warmup:   2%|‚ñè         | 18/1000 [00:04&lt;02:15,  7.23it/s, 1023 steps of size 1.12e-18. acc. prob=0.00]warmup:   2%|‚ñè         | 19/1000 [00:04&lt;02:11,  7.45it/s, 1023 steps of size 1.43e-19. acc. prob=0.00]warmup:   2%|‚ñè         | 20/1000 [00:04&lt;02:10,  7.53it/s, 1023 steps of size 1.92e-20. acc. prob=0.00]warmup:   2%|‚ñè         | 21/1000 [00:04&lt;02:07,  7.66it/s, 1023 steps of size 2.68e-21. acc. prob=0.00]warmup:   2%|‚ñè         | 22/1000 [00:05&lt;02:07,  7.70it/s, 1023 steps of size 3.92e-22. acc. prob=0.00]warmup:   2%|‚ñè         | 23/1000 [00:05&lt;02:05,  7.77it/s, 1023 steps of size 5.94e-23. acc. prob=0.00]warmup:   2%|‚ñè         | 24/1000 [00:05&lt;02:04,  7.84it/s, 1023 steps of size 9.35e-24. acc. prob=0.00]warmup:   2%|‚ñé         | 25/1000 [00:05&lt;02:04,  7.85it/s, 1023 steps of size 1.52e-24. acc. prob=0.00]warmup:   3%|‚ñé         | 26/1000 [00:05&lt;02:03,  7.89it/s, 1023 steps of size 2.57e-25. acc. prob=0.00]warmup:   3%|‚ñé         | 27/1000 [00:05&lt;02:04,  7.79it/s, 1023 steps of size 4.49e-26. acc. prob=0.00]warmup:   3%|‚ñé         | 28/1000 [00:05&lt;02:04,  7.79it/s, 1023 steps of size 8.07e-27. acc. prob=0.00]warmup:   3%|‚ñé         | 29/1000 [00:05&lt;02:04,  7.82it/s, 1023 steps of size 1.50e-27. acc. prob=0.00]warmup:   3%|‚ñé         | 30/1000 [00:06&lt;02:03,  7.83it/s, 1023 steps of size 2.85e-28. acc. prob=0.00]warmup:   3%|‚ñé         | 31/1000 [00:06&lt;02:02,  7.88it/s, 1023 steps of size 5.59e-29. acc. prob=0.00]warmup:   3%|‚ñé         | 32/1000 [00:06&lt;02:02,  7.91it/s, 1023 steps of size 1.13e-29. acc. prob=0.00]warmup:   3%|‚ñé         | 33/1000 [00:06&lt;02:03,  7.85it/s, 1023 steps of size 2.32e-30. acc. prob=0.00]warmup:   3%|‚ñé         | 34/1000 [00:06&lt;02:03,  7.82it/s, 1023 steps of size 4.91e-31. acc. prob=0.00]warmup:   4%|‚ñé         | 35/1000 [00:06&lt;02:02,  7.85it/s, 1023 steps of size 1.06e-31. acc. prob=0.00]warmup:   4%|‚ñé         | 36/1000 [00:06&lt;02:01,  7.93it/s, 1023 steps of size 2.35e-32. acc. prob=0.00]warmup:   4%|‚ñé         | 37/1000 [00:06&lt;02:01,  7.90it/s, 1023 steps of size 5.32e-33. acc. prob=0.00]warmup:   4%|‚ñç         | 38/1000 [00:07&lt;02:02,  7.87it/s, 1023 steps of size 1.23e-33. acc. prob=0.00]warmup:   4%|‚ñç         | 39/1000 [00:07&lt;02:02,  7.87it/s, 1023 steps of size 2.89e-34. acc. prob=0.00]warmup:   4%|‚ñç         | 40/1000 [00:07&lt;02:01,  7.87it/s, 1023 steps of size 6.95e-35. acc. prob=0.00]warmup:   4%|‚ñç         | 41/1000 [00:07&lt;02:01,  7.87it/s, 1023 steps of size 1.70e-35. acc. prob=0.00]warmup:   4%|‚ñç         | 42/1000 [00:07&lt;02:03,  7.77it/s, 1023 steps of size 4.24e-36. acc. prob=0.00]warmup:   4%|‚ñç         | 43/1000 [00:07&lt;02:02,  7.79it/s, 1023 steps of size 1.08e-36. acc. prob=0.00]warmup:   4%|‚ñç         | 44/1000 [00:07&lt;02:04,  7.70it/s, 1023 steps of size 2.77e-37. acc. prob=0.00]warmup:   4%|‚ñç         | 45/1000 [00:08&lt;02:03,  7.74it/s, 1023 steps of size 7.27e-38. acc. prob=0.00]warmup:   5%|‚ñç         | 46/1000 [00:08&lt;02:02,  7.77it/s, 1023 steps of size 1.94e-38. acc. prob=0.00]warmup:   5%|‚ñç         | 47/1000 [00:08&lt;02:04,  7.67it/s, 1023 steps of size 5.24e-39. acc. prob=0.00]warmup:   5%|‚ñç         | 48/1000 [00:08&lt;02:02,  7.76it/s, 1023 steps of size 1.44e-39. acc. prob=0.00]warmup:   5%|‚ñç         | 49/1000 [00:08&lt;02:02,  7.74it/s, 1023 steps of size 4.01e-40. acc. prob=0.00]warmup:   5%|‚ñå         | 50/1000 [00:08&lt;02:02,  7.78it/s, 1023 steps of size 1.13e-40. acc. prob=0.00]warmup:   5%|‚ñå         | 51/1000 [00:08&lt;02:01,  7.80it/s, 1023 steps of size 3.25e-41. acc. prob=0.00]warmup:   5%|‚ñå         | 52/1000 [00:08&lt;02:01,  7.81it/s, 1023 steps of size 9.42e-42. acc. prob=0.00]warmup:   5%|‚ñå         | 53/1000 [00:09&lt;02:00,  7.84it/s, 1023 steps of size 2.77e-42. acc. prob=0.00]warmup:   5%|‚ñå         | 54/1000 [00:09&lt;02:00,  7.85it/s, 1023 steps of size 8.24e-43. acc. prob=0.00]warmup:   6%|‚ñå         | 55/1000 [00:09&lt;02:00,  7.85it/s, 1023 steps of size 2.48e-43. acc. prob=0.00]warmup:   6%|‚ñå         | 56/1000 [00:09&lt;02:00,  7.83it/s, 1023 steps of size 7.57e-44. acc. prob=0.00]warmup:   6%|‚ñå         | 57/1000 [00:09&lt;02:00,  7.84it/s, 1023 steps of size 2.34e-44. acc. prob=0.00]warmup:   6%|‚ñå         | 58/1000 [00:13&lt;18:51,  1.20s/it, 1023 steps of size 7.29e-45. acc. prob=0.00]warmup:   6%|‚ñå         | 59/1000 [00:13&lt;13:46,  1.14it/s, 1023 steps of size 2.30e-45. acc. prob=0.00]warmup:   6%|‚ñå         | 60/1000 [00:13&lt;10:14,  1.53it/s, 1023 steps of size 7.32e-46. acc. prob=0.00]warmup:   6%|‚ñå         | 61/1000 [00:13&lt;07:44,  2.02it/s, 1023 steps of size 2.36e-46. acc. prob=0.00]warmup:   6%|‚ñå         | 62/1000 [00:13&lt;06:01,  2.60it/s, 1023 steps of size 7.67e-47. acc. prob=0.00]warmup:   6%|‚ñã         | 63/1000 [00:13&lt;04:48,  3.25it/s, 1023 steps of size 2.52e-47. acc. prob=0.00]warmup:   6%|‚ñã         | 64/1000 [00:14&lt;03:57,  3.95it/s, 1023 steps of size 8.36e-48. acc. prob=0.00]warmup:   6%|‚ñã         | 65/1000 [00:14&lt;03:21,  4.65it/s, 1023 steps of size 2.80e-48. acc. prob=0.00]warmup:   7%|‚ñã         | 66/1000 [00:14&lt;02:55,  5.33it/s, 1023 steps of size 9.47e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 67/1000 [00:14&lt;02:38,  5.90it/s, 1023 steps of size 3.23e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 68/1000 [00:14&lt;02:25,  6.40it/s, 1023 steps of size 1.11e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 69/1000 [00:14&lt;02:17,  6.75it/s, 1023 steps of size 3.86e-50. acc. prob=0.00]warmup:   7%|‚ñã         | 70/1000 [00:14&lt;02:12,  7.04it/s, 1023 steps of size 1.35e-50. acc. prob=0.00]warmup:   7%|‚ñã         | 71/1000 [00:14&lt;02:07,  7.30it/s, 1023 steps of size 4.76e-51. acc. prob=0.00]warmup:   7%|‚ñã         | 72/1000 [00:15&lt;02:04,  7.48it/s, 1023 steps of size 1.69e-51. acc. prob=0.00]warmup:   7%|‚ñã         | 73/1000 [00:15&lt;02:01,  7.63it/s, 1023 steps of size 6.07e-52. acc. prob=0.00]warmup:   7%|‚ñã         | 74/1000 [00:15&lt;02:00,  7.70it/s, 1023 steps of size 2.19e-52. acc. prob=0.00]warmup:   8%|‚ñä         | 75/1000 [00:15&lt;01:59,  7.77it/s, 1023 steps of size 7.98e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 76/1000 [00:15&lt;01:58,  7.83it/s, 1023 steps of size 2.93e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 77/1000 [00:15&lt;01:58,  7.77it/s, 1023 steps of size 1.08e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 78/1000 [00:15&lt;01:58,  7.76it/s, 1023 steps of size 4.02e-54. acc. prob=0.00]warmup:   8%|‚ñä         | 79/1000 [00:15&lt;01:57,  7.84it/s, 1023 steps of size 1.51e-54. acc. prob=0.00]warmup:   8%|‚ñä         | 80/1000 [00:16&lt;01:56,  7.88it/s, 1023 steps of size 5.68e-55. acc. prob=0.00]warmup:   8%|‚ñä         | 81/1000 [00:16&lt;01:56,  7.87it/s, 1023 steps of size 2.16e-55. acc. prob=0.00]warmup:   8%|‚ñä         | 82/1000 [00:16&lt;01:56,  7.88it/s, 1023 steps of size 8.25e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 83/1000 [00:16&lt;01:55,  7.93it/s, 1023 steps of size 3.17e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 84/1000 [00:16&lt;01:55,  7.91it/s, 1023 steps of size 1.23e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 85/1000 [00:16&lt;01:56,  7.88it/s, 1023 steps of size 4.78e-57. acc. prob=0.00]warmup:   9%|‚ñä         | 86/1000 [00:16&lt;01:55,  7.90it/s, 1023 steps of size 1.87e-57. acc. prob=0.00]warmup:   9%|‚ñä         | 87/1000 [00:16&lt;01:55,  7.89it/s, 1023 steps of size 7.39e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 88/1000 [00:17&lt;01:56,  7.81it/s, 1023 steps of size 2.93e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 89/1000 [00:17&lt;01:55,  7.86it/s, 1023 steps of size 1.17e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 90/1000 [00:17&lt;01:55,  7.85it/s, 1023 steps of size 4.69e-59. acc. prob=0.00]warmup:   9%|‚ñâ         | 91/1000 [00:17&lt;01:55,  7.87it/s, 1023 steps of size 1.89e-59. acc. prob=0.00]warmup:   9%|‚ñâ         | 92/1000 [00:17&lt;01:56,  7.78it/s, 1023 steps of size 7.67e-60. acc. prob=0.00]warmup:   9%|‚ñâ         | 93/1000 [00:17&lt;01:56,  7.78it/s, 1023 steps of size 3.13e-60. acc. prob=0.00]warmup:   9%|‚ñâ         | 94/1000 [00:17&lt;01:55,  7.82it/s, 1023 steps of size 1.28e-60. acc. prob=0.00]warmup:  10%|‚ñâ         | 95/1000 [00:17&lt;01:55,  7.86it/s, 1023 steps of size 5.28e-61. acc. prob=0.00]warmup:  10%|‚ñâ         | 96/1000 [00:18&lt;01:54,  7.89it/s, 1023 steps of size 2.19e-61. acc. prob=0.00]warmup:  10%|‚ñâ         | 97/1000 [00:18&lt;01:54,  7.89it/s, 1023 steps of size 9.10e-62. acc. prob=0.00]warmup:  10%|‚ñâ         | 98/1000 [00:18&lt;01:54,  7.87it/s, 1023 steps of size 3.81e-62. acc. prob=0.00]warmup:  10%|‚ñâ         | 99/1000 [00:18&lt;01:54,  7.87it/s, 1023 steps of size 1.60e-62. acc. prob=0.00]warmup:  10%|‚ñà         | 100/1000 [00:18&lt;01:53,  7.93it/s, 1023 steps of size 6.76e-63. acc. prob=0.00]warmup:  10%|‚ñà         | 101/1000 [00:18&lt;01:53,  7.92it/s, 1023 steps of size 1.58e-62. acc. prob=0.00]warmup:  10%|‚ñà         | 102/1000 [00:18&lt;01:53,  7.92it/s, 1023 steps of size 1.56e-63. acc. prob=0.00]warmup:  10%|‚ñà         | 103/1000 [00:18&lt;01:53,  7.92it/s, 1023 steps of size 1.13e-64. acc. prob=0.00]warmup:  10%|‚ñà         | 104/1000 [00:19&lt;01:53,  7.93it/s, 1023 steps of size 7.23e-66. acc. prob=0.00]warmup:  10%|‚ñà         | 105/1000 [00:19&lt;01:53,  7.90it/s, 1023 steps of size 4.47e-67. acc. prob=0.00]warmup:  11%|‚ñà         | 106/1000 [00:19&lt;01:53,  7.89it/s, 1023 steps of size 2.80e-68. acc. prob=0.00]warmup:  11%|‚ñà         | 107/1000 [00:19&lt;01:52,  7.91it/s, 1023 steps of size 1.82e-69. acc. prob=0.00]warmup:  11%|‚ñà         | 108/1000 [00:19&lt;01:52,  7.94it/s, 1023 steps of size 1.24e-70. acc. prob=0.00]warmup:  11%|‚ñà         | 109/1000 [00:19&lt;01:52,  7.94it/s, 1023 steps of size 9.02e-72. acc. prob=0.00]warmup:  11%|‚ñà         | 110/1000 [00:19&lt;01:53,  7.87it/s, 1023 steps of size 6.97e-73. acc. prob=0.00]warmup:  11%|‚ñà         | 111/1000 [00:19&lt;01:52,  7.90it/s, 1023 steps of size 5.73e-74. acc. prob=0.00]warmup:  11%|‚ñà         | 112/1000 [00:20&lt;01:52,  7.89it/s, 1023 steps of size 5.01e-75. acc. prob=0.00]warmup:  11%|‚ñà‚ñè        | 113/1000 [00:20&lt;01:53,  7.79it/s, 1023 steps of size 4.67e-76. acc. prob=0.00]warmup:  11%|‚ñà‚ñè        | 114/1000 [00:20&lt;01:54,  7.73it/s, 1023 steps of size 4.61e-77. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 115/1000 [00:20&lt;01:53,  7.82it/s, 1023 steps of size 4.81e-78. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 116/1000 [00:20&lt;01:53,  7.79it/s, 1023 steps of size 5.31e-79. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 117/1000 [00:20&lt;01:52,  7.83it/s, 1023 steps of size 6.18e-80. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 118/1000 [00:20&lt;01:52,  7.85it/s, 1023 steps of size 7.55e-81. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 119/1000 [00:20&lt;01:52,  7.85it/s, 1023 steps of size 9.67e-82. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 120/1000 [00:21&lt;01:51,  7.86it/s, 1023 steps of size 1.30e-82. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 121/1000 [00:21&lt;01:52,  7.85it/s, 1023 steps of size 1.81e-83. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 122/1000 [00:21&lt;01:52,  7.83it/s, 1023 steps of size 2.65e-84. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 123/1000 [00:21&lt;01:51,  7.90it/s, 1023 steps of size 4.01e-85. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 124/1000 [00:21&lt;01:50,  7.92it/s, 1023 steps of size 6.32e-86. acc. prob=0.00]warmup:  12%|‚ñà‚ñé        | 125/1000 [00:21&lt;01:50,  7.89it/s, 1023 steps of size 1.03e-86. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 126/1000 [00:21&lt;01:50,  7.92it/s, 1023 steps of size 1.74e-87. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 127/1000 [00:22&lt;01:50,  7.90it/s, 1023 steps of size 3.03e-88. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 128/1000 [00:22&lt;01:51,  7.82it/s, 1023 steps of size 5.46e-89. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 129/1000 [00:22&lt;01:50,  7.86it/s, 1023 steps of size 1.01e-89. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 130/1000 [00:22&lt;01:50,  7.87it/s, 1023 steps of size 1.93e-90. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 131/1000 [00:22&lt;01:49,  7.91it/s, 1023 steps of size 3.78e-91. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 132/1000 [00:22&lt;01:49,  7.91it/s, 1023 steps of size 7.60e-92. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 133/1000 [00:22&lt;01:49,  7.93it/s, 1023 steps of size 1.57e-92. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 134/1000 [00:22&lt;01:49,  7.91it/s, 1023 steps of size 3.32e-93. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 135/1000 [00:23&lt;01:49,  7.88it/s, 1023 steps of size 7.18e-94. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 136/1000 [00:23&lt;01:48,  7.93it/s, 1023 steps of size 1.59e-94. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 137/1000 [00:23&lt;01:48,  7.95it/s, 1023 steps of size 3.59e-95. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 138/1000 [00:23&lt;01:49,  7.86it/s, 1023 steps of size 8.30e-96. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 139/1000 [00:23&lt;01:50,  7.82it/s, 1023 steps of size 1.96e-96. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 140/1000 [00:23&lt;01:49,  7.84it/s, 1023 steps of size 4.70e-97. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 141/1000 [00:23&lt;01:49,  7.85it/s, 1023 steps of size 1.15e-97. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 142/1000 [00:23&lt;01:50,  7.80it/s, 1023 steps of size 2.87e-98. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 143/1000 [00:24&lt;01:51,  7.65it/s, 1023 steps of size 7.27e-99. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 144/1000 [00:24&lt;01:50,  7.78it/s, 1023 steps of size 1.88e-99. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 145/1000 [00:24&lt;01:49,  7.78it/s, 1023 steps of size 4.92e-100. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 146/1000 [00:24&lt;01:49,  7.83it/s, 1023 steps of size 1.31e-100. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 147/1000 [00:24&lt;01:48,  7.87it/s, 1023 steps of size 3.54e-101. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 148/1000 [00:24&lt;01:49,  7.81it/s, 1023 steps of size 9.73e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 149/1000 [00:24&lt;01:48,  7.82it/s, 1023 steps of size 2.71e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 150/1000 [00:24&lt;01:48,  7.83it/s, 1023 steps of size 7.66e-103. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 151/1000 [00:25&lt;01:47,  7.87it/s, 1023 steps of size 1.79e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 152/1000 [00:25&lt;01:47,  7.86it/s, 1023 steps of size 1.76e-103. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 153/1000 [00:25&lt;01:47,  7.91it/s, 1023 steps of size 1.28e-104. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 154/1000 [00:25&lt;01:46,  7.94it/s, 1023 steps of size 8.19e-106. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 155/1000 [00:25&lt;01:46,  7.94it/s, 1023 steps of size 5.07e-107. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 156/1000 [00:25&lt;01:47,  7.82it/s, 1023 steps of size 3.17e-108. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 157/1000 [00:25&lt;01:47,  7.88it/s, 1023 steps of size 2.06e-109. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 158/1000 [00:25&lt;01:45,  7.96it/s, 1023 steps of size 1.41e-110. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 159/1000 [00:26&lt;01:45,  7.99it/s, 1023 steps of size 1.02e-111. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 160/1000 [00:26&lt;01:45,  7.93it/s, 1023 steps of size 7.89e-113. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 161/1000 [00:26&lt;01:45,  7.96it/s, 1023 steps of size 6.49e-114. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 162/1000 [00:26&lt;01:45,  7.96it/s, 1023 steps of size 5.68e-115. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 163/1000 [00:26&lt;01:46,  7.84it/s, 1023 steps of size 5.29e-116. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 164/1000 [00:26&lt;01:46,  7.83it/s, 1023 steps of size 5.22e-117. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 165/1000 [00:26&lt;01:46,  7.82it/s, 1023 steps of size 5.46e-118. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 166/1000 [00:26&lt;01:45,  7.87it/s, 1023 steps of size 6.02e-119. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 167/1000 [00:27&lt;01:45,  7.87it/s, 1023 steps of size 7.00e-120. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 168/1000 [00:27&lt;01:45,  7.88it/s, 1023 steps of size 8.55e-121. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 169/1000 [00:27&lt;01:44,  7.93it/s, 1023 steps of size 1.10e-121. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 170/1000 [00:27&lt;01:45,  7.85it/s, 1023 steps of size 1.47e-122. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 171/1000 [00:27&lt;01:45,  7.89it/s, 1023 steps of size 2.06e-123. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 172/1000 [00:27&lt;01:45,  7.86it/s, 1023 steps of size 3.00e-124. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 173/1000 [00:27&lt;01:44,  7.89it/s, 1023 steps of size 4.55e-125. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 174/1000 [00:27&lt;01:44,  7.92it/s, 1023 steps of size 7.16e-126. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 175/1000 [00:28&lt;01:43,  7.96it/s, 1023 steps of size 1.17e-126. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 176/1000 [00:28&lt;01:43,  7.96it/s, 1023 steps of size 1.97e-127. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 177/1000 [00:28&lt;01:43,  7.95it/s, 1023 steps of size 3.44e-128. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 178/1000 [00:28&lt;01:43,  7.95it/s, 1023 steps of size 6.18e-129. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 179/1000 [00:28&lt;01:42,  7.99it/s, 1023 steps of size 1.15e-129. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 180/1000 [00:28&lt;01:42,  7.98it/s, 1023 steps of size 2.19e-130. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 181/1000 [00:28&lt;01:43,  7.94it/s, 1023 steps of size 4.28e-131. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 182/1000 [00:28&lt;01:42,  7.96it/s, 1023 steps of size 8.62e-132. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 183/1000 [00:29&lt;01:42,  7.94it/s, 1023 steps of size 1.78e-132. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 184/1000 [00:29&lt;01:43,  7.89it/s, 1023 steps of size 3.76e-133. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 185/1000 [00:29&lt;01:43,  7.84it/s, 1023 steps of size 8.14e-134. acc. prob=0.00]warmup:  19%|‚ñà‚ñä        | 186/1000 [00:29&lt;01:43,  7.89it/s, 1023 steps of size 1.80e-134. acc. prob=0.00]warmup:  19%|‚ñà‚ñä        | 187/1000 [00:29&lt;01:42,  7.93it/s, 1023 steps of size 4.07e-135. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 188/1000 [00:29&lt;01:42,  7.93it/s, 1023 steps of size 9.41e-136. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 189/1000 [00:29&lt;01:42,  7.89it/s, 1023 steps of size 2.22e-136. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 190/1000 [00:29&lt;01:41,  7.95it/s, 1023 steps of size 5.32e-137. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 191/1000 [00:30&lt;01:42,  7.87it/s, 1023 steps of size 1.30e-137. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 192/1000 [00:30&lt;01:42,  7.91it/s, 1023 steps of size 3.25e-138. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 193/1000 [00:30&lt;01:41,  7.93it/s, 1023 steps of size 8.24e-139. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 194/1000 [00:30&lt;01:41,  7.93it/s, 1023 steps of size 2.12e-139. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 195/1000 [00:30&lt;01:41,  7.94it/s, 1023 steps of size 5.57e-140. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 196/1000 [00:30&lt;01:41,  7.93it/s, 1023 steps of size 1.48e-140. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 197/1000 [00:30&lt;01:41,  7.92it/s, 1023 steps of size 4.02e-141. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 198/1000 [00:31&lt;01:42,  7.84it/s, 1023 steps of size 1.10e-141. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 199/1000 [00:31&lt;01:42,  7.85it/s, 1023 steps of size 3.07e-142. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 200/1000 [00:31&lt;01:41,  7.88it/s, 1023 steps of size 8.68e-143. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 201/1000 [00:31&lt;01:41,  7.90it/s, 1023 steps of size 2.49e-143. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 202/1000 [00:31&lt;01:40,  7.92it/s, 1023 steps of size 7.22e-144. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 203/1000 [00:31&lt;01:40,  7.94it/s, 1023 steps of size 2.12e-144. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 204/1000 [00:31&lt;01:40,  7.95it/s, 1023 steps of size 6.31e-145. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 205/1000 [00:31&lt;01:41,  7.84it/s, 1023 steps of size 1.90e-145. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 206/1000 [00:32&lt;01:41,  7.81it/s, 1023 steps of size 5.80e-146. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 207/1000 [00:32&lt;01:41,  7.84it/s, 1023 steps of size 1.79e-146. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 208/1000 [00:32&lt;01:41,  7.79it/s, 1023 steps of size 5.58e-147. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 209/1000 [00:32&lt;01:41,  7.79it/s, 1023 steps of size 1.76e-147. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 210/1000 [00:32&lt;01:41,  7.77it/s, 1023 steps of size 5.61e-148. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 211/1000 [00:32&lt;01:42,  7.73it/s, 1023 steps of size 1.81e-148. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 212/1000 [00:32&lt;01:41,  7.77it/s, 1023 steps of size 5.88e-149. acc. prob=0.00]warmup:  21%|‚ñà‚ñà‚ñè       | 213/1000 [00:32&lt;01:41,  7.77it/s, 1023 steps of size 1.93e-149. acc. prob=0.00]warmup:  21%|‚ñà‚ñà‚ñè       | 214/1000 [00:33&lt;01:40,  7.78it/s, 1023 steps of size 6.41e-150. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 215/1000 [00:33&lt;01:40,  7.84it/s, 1023 steps of size 2.15e-150. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 216/1000 [00:33&lt;01:40,  7.77it/s, 1023 steps of size 7.25e-151. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 217/1000 [00:33&lt;01:40,  7.83it/s, 1023 steps of size 2.47e-151. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 218/1000 [00:33&lt;01:38,  7.91it/s, 1023 steps of size 8.51e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 219/1000 [00:33&lt;01:39,  7.86it/s, 1023 steps of size 2.95e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 220/1000 [00:33&lt;01:40,  7.80it/s, 1023 steps of size 1.03e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 221/1000 [00:33&lt;01:40,  7.79it/s, 1023 steps of size 3.65e-153. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 222/1000 [00:34&lt;01:40,  7.74it/s, 1023 steps of size 1.30e-153. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 223/1000 [00:34&lt;01:41,  7.64it/s, 1023 steps of size 4.65e-154. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 224/1000 [00:34&lt;01:40,  7.69it/s, 1023 steps of size 1.68e-154. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñé       | 225/1000 [00:34&lt;01:40,  7.71it/s, 1023 steps of size 6.11e-155. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 226/1000 [00:34&lt;01:39,  7.79it/s, 1023 steps of size 2.24e-155. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 227/1000 [00:34&lt;01:38,  7.82it/s, 1023 steps of size 8.28e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 228/1000 [00:34&lt;01:38,  7.82it/s, 1023 steps of size 3.08e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 229/1000 [00:34&lt;01:38,  7.81it/s, 1023 steps of size 1.15e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 230/1000 [00:35&lt;01:38,  7.84it/s, 1023 steps of size 4.35e-157. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 231/1000 [00:35&lt;01:38,  7.84it/s, 1023 steps of size 1.65e-157. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 232/1000 [00:35&lt;01:39,  7.73it/s, 1023 steps of size 6.32e-158. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 233/1000 [00:35&lt;01:39,  7.74it/s, 1023 steps of size 2.43e-158. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 234/1000 [00:35&lt;01:38,  7.77it/s, 1023 steps of size 9.40e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 235/1000 [00:35&lt;01:37,  7.83it/s, 1023 steps of size 3.66e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 236/1000 [00:35&lt;01:37,  7.85it/s, 1023 steps of size 1.44e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 237/1000 [00:36&lt;01:36,  7.89it/s, 1023 steps of size 5.66e-160. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 238/1000 [00:36&lt;01:36,  7.89it/s, 1023 steps of size 2.24e-160. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 239/1000 [00:36&lt;01:36,  7.88it/s, 1023 steps of size 8.95e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 240/1000 [00:36&lt;01:37,  7.80it/s, 1023 steps of size 3.59e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 241/1000 [00:36&lt;01:36,  7.84it/s, 1023 steps of size 1.45e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 242/1000 [00:36&lt;01:36,  7.83it/s, 1023 steps of size 5.87e-162. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 243/1000 [00:36&lt;01:36,  7.81it/s, 1023 steps of size 2.39e-162. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 244/1000 [00:36&lt;01:36,  7.86it/s, 1023 steps of size 9.81e-163. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 245/1000 [00:37&lt;01:35,  7.91it/s, 1023 steps of size 4.04e-163. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 246/1000 [00:37&lt;01:35,  7.92it/s, 1023 steps of size 1.67e-163. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 247/1000 [00:37&lt;01:35,  7.93it/s, 1023 steps of size 6.97e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 248/1000 [00:37&lt;01:34,  7.92it/s, 1023 steps of size 2.92e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 249/1000 [00:37&lt;01:35,  7.90it/s, 1023 steps of size 1.23e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 250/1000 [00:37&lt;01:35,  7.87it/s, 1023 steps of size 5.18e-165. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 251/1000 [00:37&lt;01:35,  7.84it/s, 1023 steps of size 1.21e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 252/1000 [00:37&lt;01:35,  7.83it/s, 1023 steps of size 1.19e-165. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 253/1000 [00:38&lt;01:34,  7.87it/s, 1023 steps of size 8.64e-167. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 254/1000 [00:38&lt;01:34,  7.86it/s, 1023 steps of size 5.54e-168. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 255/1000 [00:38&lt;01:34,  7.87it/s, 1023 steps of size 3.43e-169. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 256/1000 [00:38&lt;01:34,  7.86it/s, 1023 steps of size 2.14e-170. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 257/1000 [00:38&lt;01:36,  7.72it/s, 1023 steps of size 1.39e-171. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 258/1000 [00:38&lt;01:35,  7.73it/s, 1023 steps of size 9.53e-173. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 259/1000 [00:38&lt;01:35,  7.78it/s, 1023 steps of size 6.91e-174. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 260/1000 [00:38&lt;01:34,  7.83it/s, 1023 steps of size 5.34e-175. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 261/1000 [00:39&lt;01:34,  7.85it/s, 1023 steps of size 4.39e-176. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 262/1000 [00:39&lt;01:34,  7.81it/s, 1023 steps of size 3.84e-177. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 263/1000 [00:39&lt;01:33,  7.86it/s, 1023 steps of size 3.57e-178. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 264/1000 [00:39&lt;01:33,  7.88it/s, 1023 steps of size 3.53e-179. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 265/1000 [00:39&lt;01:33,  7.85it/s, 1023 steps of size 3.69e-180. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 266/1000 [00:39&lt;01:32,  7.90it/s, 1023 steps of size 4.07e-181. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 267/1000 [00:39&lt;01:32,  7.92it/s, 1023 steps of size 4.73e-182. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 268/1000 [00:39&lt;01:32,  7.91it/s, 1023 steps of size 5.78e-183. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 269/1000 [00:40&lt;01:32,  7.89it/s, 1023 steps of size 7.41e-184. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 270/1000 [00:40&lt;01:32,  7.91it/s, 1023 steps of size 9.93e-185. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 271/1000 [00:40&lt;01:32,  7.91it/s, 1023 steps of size 1.39e-185. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 272/1000 [00:40&lt;01:31,  7.93it/s, 1023 steps of size 2.03e-186. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 273/1000 [00:40&lt;01:31,  7.91it/s, 1023 steps of size 3.07e-187. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 274/1000 [00:40&lt;01:31,  7.94it/s, 1023 steps of size 4.84e-188. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 275/1000 [00:40&lt;01:31,  7.94it/s, 1023 steps of size 7.89e-189. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 276/1000 [00:40&lt;01:32,  7.86it/s, 1023 steps of size 1.33e-189. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 277/1000 [00:41&lt;01:31,  7.88it/s, 1023 steps of size 2.32e-190. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 278/1000 [00:41&lt;01:31,  7.89it/s, 1023 steps of size 4.18e-191. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 279/1000 [00:41&lt;01:31,  7.89it/s, 1023 steps of size 7.74e-192. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 280/1000 [00:41&lt;01:31,  7.89it/s, 1023 steps of size 1.48e-192. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 281/1000 [00:41&lt;01:30,  7.94it/s, 1023 steps of size 2.89e-193. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 282/1000 [00:41&lt;01:30,  7.93it/s, 1023 steps of size 5.82e-194. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 283/1000 [00:41&lt;01:30,  7.92it/s, 1023 steps of size 1.20e-194. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 284/1000 [00:41&lt;01:30,  7.94it/s, 1023 steps of size 2.54e-195. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 285/1000 [00:42&lt;01:30,  7.93it/s, 1023 steps of size 5.50e-196. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñä       | 286/1000 [00:42&lt;01:29,  7.98it/s, 1023 steps of size 1.22e-196. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñä       | 287/1000 [00:42&lt;01:29,  7.92it/s, 1023 steps of size 2.75e-197. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 288/1000 [00:42&lt;01:30,  7.90it/s, 1023 steps of size 6.36e-198. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 289/1000 [00:42&lt;01:30,  7.84it/s, 1023 steps of size 1.50e-198. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 290/1000 [00:42&lt;01:30,  7.81it/s, 1023 steps of size 3.60e-199. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 291/1000 [00:42&lt;01:30,  7.86it/s, 1023 steps of size 8.81e-200. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 292/1000 [00:42&lt;01:30,  7.81it/s, 1023 steps of size 2.20e-200. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 293/1000 [00:43&lt;01:29,  7.89it/s, 1023 steps of size 5.57e-201. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 294/1000 [00:43&lt;01:29,  7.87it/s, 1023 steps of size 1.44e-201. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 295/1000 [00:43&lt;01:29,  7.84it/s, 1023 steps of size 3.77e-202. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 296/1000 [00:43&lt;01:29,  7.84it/s, 1023 steps of size 1.00e-202. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 297/1000 [00:43&lt;01:29,  7.84it/s, 1023 steps of size 2.71e-203. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 298/1000 [00:43&lt;01:29,  7.89it/s, 1023 steps of size 7.45e-204. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [00:43&lt;01:28,  7.88it/s, 1023 steps of size 2.08e-204. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 300/1000 [00:44&lt;01:28,  7.88it/s, 1023 steps of size 5.87e-205. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 301/1000 [00:44&lt;01:27,  7.95it/s, 1023 steps of size 1.68e-205. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 302/1000 [00:44&lt;01:27,  7.97it/s, 1023 steps of size 4.88e-206. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 303/1000 [00:44&lt;01:27,  7.97it/s, 1023 steps of size 1.43e-206. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 304/1000 [00:44&lt;01:28,  7.84it/s, 1023 steps of size 4.27e-207. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 305/1000 [00:44&lt;01:28,  7.89it/s, 1023 steps of size 1.29e-207. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 306/1000 [00:44&lt;01:28,  7.87it/s, 1023 steps of size 3.92e-208. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 307/1000 [00:44&lt;01:27,  7.93it/s, 1023 steps of size 1.21e-208. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 308/1000 [00:45&lt;01:27,  7.95it/s, 1023 steps of size 3.77e-209. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 309/1000 [00:45&lt;01:26,  7.97it/s, 1023 steps of size 1.19e-209. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 310/1000 [00:45&lt;01:26,  7.96it/s, 1023 steps of size 3.79e-210. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 311/1000 [00:45&lt;01:27,  7.89it/s, 1023 steps of size 1.22e-210. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 312/1000 [00:49&lt;13:47,  1.20s/it, 1023 steps of size 3.97e-211. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà‚ñè      | 313/1000 [00:49&lt;10:03,  1.14it/s, 1023 steps of size 1.31e-211. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà‚ñè      | 314/1000 [00:49&lt;07:28,  1.53it/s, 1023 steps of size 4.33e-212. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 315/1000 [00:49&lt;05:40,  2.01it/s, 1023 steps of size 1.45e-212. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 316/1000 [00:49&lt;04:23,  2.60it/s, 1023 steps of size 4.90e-213. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 317/1000 [00:49&lt;03:29,  3.25it/s, 1023 steps of size 1.67e-213. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 318/1000 [00:49&lt;02:53,  3.93it/s, 1023 steps of size 5.75e-214. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 319/1000 [00:49&lt;02:27,  4.62it/s, 1023 steps of size 2.00e-214. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 320/1000 [00:50&lt;02:08,  5.30it/s, 1023 steps of size 6.98e-215. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 321/1000 [00:50&lt;01:55,  5.89it/s, 1023 steps of size 2.46e-215. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 322/1000 [00:50&lt;01:45,  6.40it/s, 1023 steps of size 8.77e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 323/1000 [00:50&lt;01:39,  6.81it/s, 1023 steps of size 3.14e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 324/1000 [00:50&lt;01:35,  7.10it/s, 1023 steps of size 1.14e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñé      | 325/1000 [00:50&lt;01:32,  7.30it/s, 1023 steps of size 4.13e-217. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 326/1000 [00:50&lt;01:29,  7.49it/s, 1023 steps of size 1.52e-217. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 327/1000 [00:51&lt;01:28,  7.63it/s, 1023 steps of size 5.60e-218. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 328/1000 [00:51&lt;01:26,  7.74it/s, 1023 steps of size 2.08e-218. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 329/1000 [00:51&lt;01:26,  7.78it/s, 1023 steps of size 7.80e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 330/1000 [00:51&lt;01:25,  7.83it/s, 1023 steps of size 2.94e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 331/1000 [00:51&lt;01:25,  7.87it/s, 1023 steps of size 1.12e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 332/1000 [00:51&lt;01:25,  7.84it/s, 1023 steps of size 4.27e-220. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 333/1000 [00:51&lt;01:24,  7.89it/s, 1023 steps of size 1.64e-220. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 334/1000 [00:51&lt;01:24,  7.89it/s, 1023 steps of size 6.36e-221. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 335/1000 [00:52&lt;01:23,  7.92it/s, 1023 steps of size 2.48e-221. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 336/1000 [00:52&lt;01:23,  7.92it/s, 1023 steps of size 9.70e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 337/1000 [00:52&lt;01:23,  7.95it/s, 1023 steps of size 3.82e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 338/1000 [00:52&lt;01:23,  7.91it/s, 1023 steps of size 1.52e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 339/1000 [00:52&lt;01:23,  7.91it/s, 1023 steps of size 6.05e-223. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 340/1000 [00:52&lt;01:23,  7.87it/s, 1023 steps of size 2.43e-223. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 341/1000 [00:52&lt;01:23,  7.88it/s, 1023 steps of size 9.79e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 342/1000 [00:52&lt;01:23,  7.91it/s, 1023 steps of size 3.97e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 343/1000 [00:53&lt;01:23,  7.89it/s, 1023 steps of size 1.62e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 344/1000 [00:53&lt;01:22,  7.91it/s, 1023 steps of size 6.63e-225. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 345/1000 [00:53&lt;01:22,  7.90it/s, 1023 steps of size 2.73e-225. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 346/1000 [00:53&lt;01:22,  7.95it/s, 1023 steps of size 1.13e-225. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 347/1000 [00:53&lt;01:22,  7.91it/s, 1023 steps of size 4.71e-226. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 348/1000 [00:53&lt;01:22,  7.92it/s, 1023 steps of size 1.97e-226. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 349/1000 [00:53&lt;01:22,  7.86it/s, 1023 steps of size 8.29e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 350/1000 [00:53&lt;01:22,  7.92it/s, 1023 steps of size 3.50e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 351/1000 [00:54&lt;01:22,  7.83it/s, 1023 steps of size 1.49e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 352/1000 [00:54&lt;01:23,  7.76it/s, 1023 steps of size 6.33e-228. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 353/1000 [00:54&lt;01:23,  7.76it/s, 1023 steps of size 2.71e-228. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 354/1000 [00:54&lt;01:22,  7.79it/s, 1023 steps of size 1.17e-228. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 355/1000 [00:54&lt;01:22,  7.84it/s, 1023 steps of size 5.04e-229. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 356/1000 [00:54&lt;01:22,  7.83it/s, 1023 steps of size 2.19e-229. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 357/1000 [00:54&lt;01:22,  7.77it/s, 1023 steps of size 9.54e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 358/1000 [00:54&lt;01:22,  7.76it/s, 1023 steps of size 4.18e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 359/1000 [00:55&lt;01:22,  7.78it/s, 1023 steps of size 1.84e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 360/1000 [00:55&lt;01:24,  7.56it/s, 1023 steps of size 8.10e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 361/1000 [00:55&lt;01:23,  7.62it/s, 1023 steps of size 3.59e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 362/1000 [00:55&lt;01:22,  7.70it/s, 1023 steps of size 1.60e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 363/1000 [00:55&lt;01:21,  7.77it/s, 1023 steps of size 7.14e-232. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 364/1000 [00:55&lt;01:21,  7.84it/s, 1023 steps of size 3.20e-232. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 365/1000 [00:55&lt;01:21,  7.78it/s, 1023 steps of size 1.44e-232. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 366/1000 [00:55&lt;01:22,  7.73it/s, 1023 steps of size 6.51e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 367/1000 [00:56&lt;01:21,  7.75it/s, 1023 steps of size 2.95e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 368/1000 [00:56&lt;01:21,  7.79it/s, 1023 steps of size 1.35e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 369/1000 [00:56&lt;01:20,  7.84it/s, 1023 steps of size 6.15e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 370/1000 [00:56&lt;01:19,  7.89it/s, 1023 steps of size 2.82e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 371/1000 [00:56&lt;01:20,  7.82it/s, 1023 steps of size 1.30e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 372/1000 [00:56&lt;01:20,  7.77it/s, 1023 steps of size 5.99e-235. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 373/1000 [00:56&lt;01:20,  7.77it/s, 1023 steps of size 2.78e-235. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 374/1000 [00:57&lt;01:20,  7.79it/s, 1023 steps of size 1.29e-235. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 375/1000 [00:57&lt;01:20,  7.80it/s, 1023 steps of size 6.02e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 376/1000 [00:57&lt;01:20,  7.79it/s, 1023 steps of size 2.82e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 377/1000 [00:57&lt;01:19,  7.82it/s, 1023 steps of size 1.32e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 378/1000 [00:57&lt;01:19,  7.84it/s, 1023 steps of size 6.24e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 379/1000 [00:57&lt;01:18,  7.87it/s, 1023 steps of size 2.95e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 380/1000 [00:57&lt;01:18,  7.85it/s, 1023 steps of size 1.40e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 381/1000 [00:57&lt;01:19,  7.77it/s, 1023 steps of size 6.65e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 382/1000 [00:58&lt;01:19,  7.77it/s, 1023 steps of size 3.17e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 383/1000 [00:58&lt;01:18,  7.82it/s, 1023 steps of size 1.52e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 384/1000 [00:58&lt;01:18,  7.84it/s, 1023 steps of size 7.29e-239. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 385/1000 [00:58&lt;01:18,  7.88it/s, 1023 steps of size 3.51e-239. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 386/1000 [00:58&lt;01:17,  7.88it/s, 1023 steps of size 1.70e-239. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 387/1000 [00:58&lt;01:17,  7.92it/s, 1023 steps of size 8.21e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 388/1000 [00:58&lt;01:17,  7.90it/s, 1023 steps of size 3.99e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 389/1000 [00:58&lt;01:17,  7.92it/s, 1023 steps of size 1.94e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 390/1000 [00:59&lt;01:17,  7.87it/s, 1023 steps of size 9.49e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 391/1000 [00:59&lt;01:17,  7.86it/s, 1023 steps of size 4.65e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [00:59&lt;01:17,  7.88it/s, 1023 steps of size 2.28e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 393/1000 [00:59&lt;01:16,  7.90it/s, 1023 steps of size 1.12e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 394/1000 [00:59&lt;01:17,  7.86it/s, 1023 steps of size 5.55e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 395/1000 [00:59&lt;01:16,  7.90it/s, 1023 steps of size 2.75e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 396/1000 [00:59&lt;01:17,  7.83it/s, 1023 steps of size 1.36e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 397/1000 [00:59&lt;01:17,  7.74it/s, 1023 steps of size 6.79e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 398/1000 [01:00&lt;01:19,  7.58it/s, 1023 steps of size 3.39e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [01:00&lt;01:19,  7.59it/s, 1023 steps of size 1.69e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 400/1000 [01:00&lt;01:17,  7.70it/s, 1023 steps of size 8.49e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 401/1000 [01:00&lt;01:16,  7.79it/s, 1023 steps of size 4.27e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 402/1000 [01:00&lt;01:17,  7.74it/s, 1023 steps of size 2.15e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 403/1000 [01:00&lt;01:16,  7.78it/s, 1023 steps of size 1.09e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 404/1000 [01:00&lt;01:16,  7.83it/s, 1023 steps of size 5.51e-245. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 405/1000 [01:00&lt;01:15,  7.86it/s, 1023 steps of size 2.80e-245. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 406/1000 [01:01&lt;01:15,  7.86it/s, 1023 steps of size 1.42e-245. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 407/1000 [01:01&lt;01:15,  7.88it/s, 1023 steps of size 7.25e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 408/1000 [01:01&lt;01:15,  7.87it/s, 1023 steps of size 3.71e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 409/1000 [01:01&lt;01:15,  7.82it/s, 1023 steps of size 1.90e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 410/1000 [01:01&lt;01:15,  7.78it/s, 1023 steps of size 9.76e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 411/1000 [01:01&lt;01:15,  7.85it/s, 1023 steps of size 5.02e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 412/1000 [01:01&lt;01:14,  7.87it/s, 1023 steps of size 2.59e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 413/1000 [01:01&lt;01:14,  7.87it/s, 1023 steps of size 1.34e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 414/1000 [01:02&lt;01:14,  7.92it/s, 1023 steps of size 6.94e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 415/1000 [01:02&lt;01:14,  7.88it/s, 1023 steps of size 3.60e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/1000 [01:02&lt;01:14,  7.83it/s, 1023 steps of size 1.88e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/1000 [01:02&lt;01:14,  7.82it/s, 1023 steps of size 9.78e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 418/1000 [01:02&lt;01:14,  7.85it/s, 1023 steps of size 5.11e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 419/1000 [01:02&lt;01:13,  7.90it/s, 1023 steps of size 2.68e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/1000 [01:02&lt;01:14,  7.83it/s, 1023 steps of size 1.40e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 421/1000 [01:02&lt;01:13,  7.89it/s, 1023 steps of size 7.38e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 422/1000 [01:03&lt;01:12,  7.94it/s, 1023 steps of size 3.89e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/1000 [01:03&lt;01:12,  7.93it/s, 1023 steps of size 2.05e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/1000 [01:03&lt;01:13,  7.87it/s, 1023 steps of size 1.09e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/1000 [01:03&lt;01:13,  7.84it/s, 1023 steps of size 5.76e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 426/1000 [01:03&lt;01:12,  7.89it/s, 1023 steps of size 3.06e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/1000 [01:03&lt;01:12,  7.89it/s, 1023 steps of size 1.63e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 428/1000 [01:03&lt;01:12,  7.89it/s, 1023 steps of size 8.67e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/1000 [01:04&lt;01:11,  7.96it/s, 1023 steps of size 4.63e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 430/1000 [01:04&lt;01:11,  7.92it/s, 1023 steps of size 2.48e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 431/1000 [01:04&lt;01:12,  7.85it/s, 1023 steps of size 1.33e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/1000 [01:04&lt;01:11,  7.89it/s, 1023 steps of size 7.13e-253. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433/1000 [01:04&lt;01:12,  7.87it/s, 1023 steps of size 3.84e-253. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 434/1000 [01:04&lt;01:11,  7.89it/s, 1023 steps of size 2.07e-253. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 435/1000 [01:04&lt;01:11,  7.92it/s, 1023 steps of size 1.12e-253. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 436/1000 [01:04&lt;01:12,  7.82it/s, 1023 steps of size 6.05e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/1000 [01:05&lt;01:13,  7.71it/s, 1023 steps of size 3.28e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/1000 [01:05&lt;01:12,  7.79it/s, 1023 steps of size 1.78e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 439/1000 [01:05&lt;01:11,  7.84it/s, 1023 steps of size 9.67e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/1000 [01:05&lt;01:11,  7.83it/s, 1023 steps of size 5.27e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 441/1000 [01:05&lt;01:11,  7.84it/s, 1023 steps of size 2.88e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 442/1000 [01:05&lt;01:11,  7.81it/s, 1023 steps of size 1.57e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 443/1000 [01:05&lt;01:10,  7.88it/s, 1023 steps of size 8.61e-256. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/1000 [01:05&lt;01:10,  7.85it/s, 1023 steps of size 4.72e-256. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445/1000 [01:06&lt;01:10,  7.84it/s, 1023 steps of size 2.59e-256. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/1000 [01:06&lt;01:10,  7.83it/s, 1023 steps of size 1.43e-256. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 447/1000 [01:06&lt;01:10,  7.86it/s, 1023 steps of size 7.87e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/1000 [01:06&lt;01:10,  7.87it/s, 1023 steps of size 4.34e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/1000 [01:06&lt;01:09,  7.90it/s, 1023 steps of size 2.40e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 450/1000 [01:06&lt;01:10,  7.84it/s, 1023 steps of size 1.33e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/1000 [01:06&lt;01:09,  7.85it/s, 1023 steps of size 3.11e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/1000 [01:06&lt;01:09,  7.88it/s, 1023 steps of size 3.06e-258. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/1000 [01:07&lt;01:09,  7.89it/s, 1023 steps of size 2.22e-259. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/1000 [01:07&lt;01:08,  7.92it/s, 1023 steps of size 1.42e-260. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/1000 [01:07&lt;01:08,  7.95it/s, 1023 steps of size 8.80e-262. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 456/1000 [01:07&lt;01:09,  7.84it/s, 1023 steps of size 5.51e-263. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 457/1000 [01:07&lt;01:08,  7.90it/s, 1023 steps of size 3.58e-264. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/1000 [01:07&lt;01:08,  7.89it/s, 1023 steps of size 2.45e-265. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 459/1000 [01:07&lt;01:08,  7.88it/s, 1023 steps of size 1.78e-266. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/1000 [01:07&lt;01:08,  7.92it/s, 1023 steps of size 1.37e-267. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/1000 [01:08&lt;01:07,  7.95it/s, 1023 steps of size 1.13e-268. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 462/1000 [01:08&lt;01:07,  7.95it/s, 1023 steps of size 9.87e-270. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 463/1000 [01:08&lt;01:07,  7.94it/s, 1023 steps of size 9.18e-271. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/1000 [01:08&lt;01:07,  7.91it/s, 1023 steps of size 9.07e-272. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/1000 [01:08&lt;01:10,  7.56it/s, 1023 steps of size 9.48e-273. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/1000 [01:08&lt;01:13,  7.29it/s, 1023 steps of size 1.05e-273. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467/1000 [01:08&lt;01:13,  7.25it/s, 1023 steps of size 1.22e-274. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/1000 [01:09&lt;01:12,  7.34it/s, 1023 steps of size 1.49e-275. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/1000 [01:09&lt;01:11,  7.39it/s, 1023 steps of size 1.90e-276. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/1000 [01:09&lt;01:14,  7.15it/s, 1023 steps of size 2.55e-277. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/1000 [01:09&lt;01:13,  7.17it/s, 1023 steps of size 3.57e-278. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/1000 [01:09&lt;01:12,  7.28it/s, 1023 steps of size 5.21e-279. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/1000 [01:09&lt;01:10,  7.42it/s, 1023 steps of size 7.90e-280. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 474/1000 [01:09&lt;01:09,  7.58it/s, 1023 steps of size 1.24e-280. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475/1000 [01:09&lt;01:08,  7.69it/s, 1023 steps of size 2.03e-281. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/1000 [01:10&lt;01:07,  7.80it/s, 1023 steps of size 3.42e-282. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/1000 [01:10&lt;01:06,  7.82it/s, 1023 steps of size 5.97e-283. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/1000 [01:10&lt;01:06,  7.87it/s, 1023 steps of size 1.07e-283. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 479/1000 [01:10&lt;01:06,  7.85it/s, 1023 steps of size 1.99e-284. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/1000 [01:10&lt;01:05,  7.89it/s, 1023 steps of size 3.80e-285. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481/1000 [01:10&lt;01:05,  7.91it/s, 1023 steps of size 7.44e-286. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/1000 [01:10&lt;01:05,  7.86it/s, 1023 steps of size 1.50e-286. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 483/1000 [01:10&lt;01:05,  7.90it/s, 1023 steps of size 3.09e-287. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 484/1000 [01:11&lt;01:06,  7.80it/s, 1023 steps of size 6.53e-288. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 485/1000 [01:11&lt;01:05,  7.85it/s, 1023 steps of size 1.41e-288. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/1000 [01:11&lt;01:05,  7.90it/s, 1023 steps of size 3.13e-289. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/1000 [01:11&lt;01:05,  7.88it/s, 1023 steps of size 7.07e-290. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/1000 [01:11&lt;01:04,  7.89it/s, 1023 steps of size 1.63e-290. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 489/1000 [01:11&lt;01:04,  7.87it/s, 1023 steps of size 3.85e-291. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/1000 [01:11&lt;01:04,  7.86it/s, 1023 steps of size 9.25e-292. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491/1000 [01:11&lt;01:04,  7.85it/s, 1023 steps of size 2.26e-292. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 492/1000 [01:12&lt;01:05,  7.78it/s, 1023 steps of size 5.64e-293. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/1000 [01:12&lt;01:04,  7.83it/s, 1023 steps of size 1.43e-293. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 494/1000 [01:12&lt;01:04,  7.87it/s, 1023 steps of size 3.69e-294. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 495/1000 [01:12&lt;01:04,  7.88it/s, 1023 steps of size 9.68e-295. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/1000 [01:12&lt;01:04,  7.86it/s, 1023 steps of size 2.58e-295. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/1000 [01:12&lt;01:03,  7.89it/s, 1023 steps of size 6.98e-296. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 498/1000 [01:12&lt;01:03,  7.91it/s, 1023 steps of size 1.92e-296. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [01:13&lt;01:03,  7.91it/s, 1023 steps of size 5.34e-297. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/1000 [01:13&lt;01:03,  7.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/1000 [01:13&lt;01:02,  7.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 502/1000 [01:13&lt;01:02,  7.91it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/1000 [01:13&lt;01:03,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/1000 [01:13&lt;01:03,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/1000 [01:13&lt;01:03,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 506/1000 [01:13&lt;01:03,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/1000 [01:14&lt;01:03,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/1000 [01:14&lt;01:02,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 509/1000 [01:14&lt;01:02,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/1000 [01:14&lt;01:01,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 511/1000 [01:14&lt;01:01,  7.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 512/1000 [01:14&lt;01:01,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 513/1000 [01:14&lt;01:01,  7.91it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 514/1000 [01:14&lt;01:02,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 515/1000 [01:15&lt;01:02,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/1000 [01:15&lt;01:02,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/1000 [01:15&lt;01:01,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 518/1000 [01:15&lt;01:01,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 519/1000 [01:15&lt;01:01,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/1000 [01:15&lt;01:01,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/1000 [01:15&lt;01:01,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 522/1000 [01:15&lt;01:01,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 523/1000 [01:16&lt;01:00,  7.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/1000 [01:16&lt;01:00,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/1000 [01:16&lt;00:59,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 526/1000 [01:16&lt;00:59,  7.94it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 527/1000 [01:16&lt;00:59,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/1000 [01:16&lt;00:59,  7.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/1000 [01:16&lt;00:59,  7.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 530/1000 [01:16&lt;00:59,  7.94it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/1000 [01:17&lt;00:59,  7.89it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532/1000 [01:17&lt;00:59,  7.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [01:17&lt;00:59,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 534/1000 [01:17&lt;00:59,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 535/1000 [01:17&lt;00:58,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 536/1000 [01:17&lt;00:59,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 537/1000 [01:17&lt;00:59,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/1000 [01:17&lt;00:59,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 539/1000 [01:18&lt;00:58,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/1000 [01:18&lt;00:58,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/1000 [01:18&lt;00:58,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 542/1000 [01:18&lt;01:00,  7.56it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 543/1000 [01:18&lt;01:00,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/1000 [01:18&lt;01:00,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/1000 [01:18&lt;00:59,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 546/1000 [01:19&lt;01:01,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/1000 [01:19&lt;01:00,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/1000 [01:19&lt;01:00,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/1000 [01:19&lt;00:59,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 550/1000 [01:19&lt;00:58,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/1000 [01:19&lt;01:00,  7.48it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/1000 [01:19&lt;01:00,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 553/1000 [01:19&lt;00:59,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 554/1000 [01:20&lt;00:58,  7.56it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/1000 [01:20&lt;00:58,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/1000 [01:20&lt;01:04,  6.94it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/1000 [01:20&lt;01:04,  6.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 558/1000 [01:20&lt;01:02,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/1000 [01:20&lt;01:01,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/1000 [01:20&lt;01:00,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/1000 [01:21&lt;01:00,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 562/1000 [01:21&lt;00:59,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563/1000 [01:24&lt;08:49,  1.21s/it, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/1000 [01:25&lt;06:27,  1.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 565/1000 [01:25&lt;04:47,  1.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/1000 [01:25&lt;03:37,  1.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/1000 [01:25&lt;03:01,  2.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/1000 [01:25&lt;02:27,  2.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/1000 [01:25&lt;02:07,  3.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 570/1000 [01:26&lt;01:49,  3.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 571/1000 [01:26&lt;01:34,  4.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 572/1000 [01:26&lt;01:23,  5.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 573/1000 [01:26&lt;01:15,  5.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/1000 [01:26&lt;01:09,  6.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 575/1000 [01:26&lt;01:06,  6.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/1000 [01:26&lt;01:05,  6.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 577/1000 [01:27&lt;01:10,  5.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/1000 [01:27&lt;01:06,  6.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579/1000 [01:27&lt;01:04,  6.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/1000 [01:27&lt;01:04,  6.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 581/1000 [01:27&lt;01:02,  6.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/1000 [01:27&lt;01:00,  6.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583/1000 [01:27&lt;01:00,  6.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 584/1000 [01:28&lt;00:59,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/1000 [01:28&lt;00:59,  6.96it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/1000 [01:28&lt;00:59,  6.97it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 587/1000 [01:28&lt;00:59,  6.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/1000 [01:28&lt;00:58,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/1000 [01:28&lt;00:56,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/1000 [01:28&lt;00:55,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/1000 [01:29&lt;00:54,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/1000 [01:29&lt;00:58,  7.00it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 593/1000 [01:29&lt;00:57,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/1000 [01:29&lt;00:57,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/1000 [01:29&lt;00:58,  6.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 596/1000 [01:29&lt;00:57,  7.06it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 597/1000 [01:29&lt;00:57,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/1000 [01:30&lt;00:55,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [01:30&lt;00:56,  7.15it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/1000 [01:30&lt;00:55,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/1000 [01:30&lt;00:58,  6.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/1000 [01:30&lt;00:57,  6.95it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 603/1000 [01:30&lt;00:55,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/1000 [01:30&lt;00:55,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/1000 [01:31&lt;00:53,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/1000 [01:31&lt;00:52,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/1000 [01:31&lt;00:52,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/1000 [01:31&lt;00:52,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/1000 [01:31&lt;00:52,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/1000 [01:31&lt;00:52,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/1000 [01:31&lt;00:55,  6.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 612/1000 [01:32&lt;00:56,  6.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 613/1000 [01:32&lt;00:56,  6.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/1000 [01:32&lt;00:55,  6.94it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 615/1000 [01:32&lt;00:53,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 616/1000 [01:32&lt;00:51,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 617/1000 [01:32&lt;00:51,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 618/1000 [01:32&lt;00:52,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/1000 [01:32&lt;00:55,  6.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/1000 [01:33&lt;00:54,  6.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 621/1000 [01:33&lt;00:53,  7.15it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 622/1000 [01:33&lt;00:51,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 623/1000 [01:33&lt;00:50,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/1000 [01:33&lt;00:50,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/1000 [01:33&lt;00:49,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/1000 [01:33&lt;00:54,  6.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/1000 [01:34&lt;00:58,  6.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/1000 [01:34&lt;00:55,  6.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 629/1000 [01:34&lt;00:54,  6.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/1000 [01:34&lt;00:52,  7.01it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/1000 [01:34&lt;00:51,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/1000 [01:34&lt;00:54,  6.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/1000 [01:34&lt;00:54,  6.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 634/1000 [01:35&lt;00:53,  6.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/1000 [01:35&lt;00:51,  7.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/1000 [01:35&lt;00:50,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 637/1000 [01:35&lt;00:50,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 638/1000 [01:35&lt;00:49,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 639/1000 [01:35&lt;00:50,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [01:35&lt;00:48,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 641/1000 [01:36&lt;00:48,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 642/1000 [01:36&lt;00:47,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/1000 [01:36&lt;00:46,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 644/1000 [01:36&lt;00:46,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/1000 [01:36&lt;00:46,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 646/1000 [01:36&lt;00:45,  7.71it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 647/1000 [01:36&lt;00:46,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 648/1000 [01:36&lt;00:45,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 649/1000 [01:37&lt;00:45,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/1000 [01:37&lt;00:45,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/1000 [01:37&lt;00:45,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 652/1000 [01:37&lt;00:45,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 653/1000 [01:37&lt;00:46,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/1000 [01:37&lt;00:45,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/1000 [01:37&lt;00:45,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/1000 [01:38&lt;00:45,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 657/1000 [01:38&lt;00:44,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 658/1000 [01:38&lt;00:44,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 659/1000 [01:38&lt;00:47,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/1000 [01:38&lt;00:48,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 661/1000 [01:38&lt;00:46,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 662/1000 [01:38&lt;00:45,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/1000 [01:38&lt;00:45,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/1000 [01:39&lt;00:45,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 665/1000 [01:39&lt;00:46,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/1000 [01:39&lt;00:46,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/1000 [01:39&lt;00:45,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/1000 [01:39&lt;00:45,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 669/1000 [01:39&lt;00:44,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/1000 [01:39&lt;00:44,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 671/1000 [01:40&lt;00:44,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/1000 [01:40&lt;00:46,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 673/1000 [01:40&lt;00:46,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/1000 [01:40&lt;00:45,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/1000 [01:40&lt;00:44,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/1000 [01:40&lt;00:43,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 677/1000 [01:40&lt;00:43,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 678/1000 [01:41&lt;00:42,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/1000 [01:41&lt;00:43,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/1000 [01:41&lt;00:47,  6.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 681/1000 [01:41&lt;00:46,  6.91it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/1000 [01:41&lt;00:45,  6.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 683/1000 [01:41&lt;00:44,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 684/1000 [01:41&lt;00:43,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/1000 [01:42&lt;00:42,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/1000 [01:42&lt;00:42,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 687/1000 [01:42&lt;00:45,  6.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/1000 [01:42&lt;00:45,  6.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 689/1000 [01:42&lt;00:44,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/1000 [01:42&lt;00:43,  7.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691/1000 [01:42&lt;00:42,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 692/1000 [01:43&lt;00:42,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 693/1000 [01:43&lt;00:41,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/1000 [01:43&lt;00:40,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 695/1000 [01:43&lt;00:44,  6.89it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/1000 [01:43&lt;00:43,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/1000 [01:43&lt;00:42,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 698/1000 [01:43&lt;00:41,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [01:43&lt;00:40,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/1000 [01:44&lt;00:39,  7.56it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/1000 [01:44&lt;00:39,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702/1000 [01:44&lt;00:39,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/1000 [01:44&lt;00:38,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/1000 [01:44&lt;00:38,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 705/1000 [01:44&lt;00:37,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/1000 [01:44&lt;00:37,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/1000 [01:44&lt;00:38,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 708/1000 [01:45&lt;00:37,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/1000 [01:45&lt;00:37,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/1000 [01:45&lt;00:37,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 711/1000 [01:45&lt;00:37,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 712/1000 [01:45&lt;00:36,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 713/1000 [01:45&lt;00:36,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 714/1000 [01:45&lt;00:36,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 715/1000 [01:46&lt;00:36,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/1000 [01:46&lt;00:36,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/1000 [01:46&lt;00:36,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 718/1000 [01:46&lt;00:35,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/1000 [01:46&lt;00:35,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/1000 [01:46&lt;00:35,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/1000 [01:46&lt;00:35,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/1000 [01:46&lt;00:35,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 723/1000 [01:47&lt;00:35,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 724/1000 [01:47&lt;00:35,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/1000 [01:47&lt;00:35,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/1000 [01:47&lt;00:34,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 727/1000 [01:47&lt;00:34,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/1000 [01:47&lt;00:34,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 729/1000 [01:47&lt;00:34,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 730/1000 [01:47&lt;00:34,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/1000 [01:48&lt;00:34,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/1000 [01:48&lt;00:34,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/1000 [01:48&lt;00:34,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 734/1000 [01:48&lt;00:34,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 735/1000 [01:48&lt;00:33,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/1000 [01:48&lt;00:34,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 737/1000 [01:48&lt;00:33,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 738/1000 [01:48&lt;00:33,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739/1000 [01:49&lt;00:33,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/1000 [01:49&lt;00:33,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 741/1000 [01:49&lt;00:32,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 742/1000 [01:49&lt;00:32,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 743/1000 [01:49&lt;00:32,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/1000 [01:49&lt;00:32,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/1000 [01:49&lt;00:32,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/1000 [01:49&lt;00:32,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 747/1000 [01:50&lt;00:34,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/1000 [01:50&lt;00:34,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 749/1000 [01:50&lt;00:33,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750/1000 [01:50&lt;00:33,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/1000 [01:50&lt;00:32,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/1000 [01:50&lt;00:32,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 753/1000 [01:50&lt;00:31,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/1000 [01:51&lt;00:31,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/1000 [01:51&lt;00:31,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 756/1000 [01:51&lt;00:31,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/1000 [01:51&lt;00:31,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 758/1000 [01:51&lt;00:31,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 759/1000 [01:51&lt;00:31,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 760/1000 [01:51&lt;00:30,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/1000 [01:51&lt;00:30,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 762/1000 [01:52&lt;00:30,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 763/1000 [01:52&lt;00:30,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/1000 [01:52&lt;00:30,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/1000 [01:52&lt;00:30,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/1000 [01:52&lt;00:30,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/1000 [01:52&lt;00:29,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/1000 [01:52&lt;00:30,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/1000 [01:52&lt;00:29,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 770/1000 [01:53&lt;00:29,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/1000 [01:53&lt;00:29,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 772/1000 [01:53&lt;00:29,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 773/1000 [01:53&lt;00:28,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/1000 [01:53&lt;00:29,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/1000 [01:53&lt;00:29,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 776/1000 [01:53&lt;00:29,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 777/1000 [01:54&lt;00:29,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/1000 [01:54&lt;00:28,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/1000 [01:54&lt;00:28,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 780/1000 [01:54&lt;00:28,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/1000 [01:54&lt;00:28,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 782/1000 [01:54&lt;00:28,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/1000 [01:54&lt;00:27,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/1000 [01:54&lt;00:28,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 785/1000 [01:55&lt;00:28,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 786/1000 [01:55&lt;00:28,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 787/1000 [01:55&lt;00:28,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/1000 [01:55&lt;00:28,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/1000 [01:55&lt;00:28,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 790/1000 [01:55&lt;00:28,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 791/1000 [01:55&lt;00:28,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 792/1000 [01:55&lt;00:27,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 793/1000 [01:56&lt;00:27,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 794/1000 [01:56&lt;00:27,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/1000 [01:56&lt;00:27,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/1000 [01:56&lt;00:27,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 797/1000 [01:56&lt;00:27,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 798/1000 [01:56&lt;00:27,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [01:56&lt;00:26,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/1000 [01:57&lt;00:26,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/1000 [02:00&lt;03:59,  1.21s/it, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/1000 [02:00&lt;02:54,  1.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/1000 [02:01&lt;02:09,  1.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/1000 [02:01&lt;01:37,  2.00it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/1000 [02:01&lt;01:16,  2.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 806/1000 [02:01&lt;01:00,  3.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 807/1000 [02:01&lt;00:49,  3.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 808/1000 [02:01&lt;00:41,  4.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 809/1000 [02:01&lt;00:36,  5.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/1000 [02:01&lt;00:32,  5.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 811/1000 [02:02&lt;00:30,  6.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 812/1000 [02:02&lt;00:28,  6.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 813/1000 [02:02&lt;00:26,  6.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/1000 [02:02&lt;00:25,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/1000 [02:02&lt;00:25,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/1000 [02:02&lt;00:24,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/1000 [02:02&lt;00:24,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/1000 [02:02&lt;00:23,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 819/1000 [02:03&lt;00:23,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/1000 [02:03&lt;00:23,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 821/1000 [02:03&lt;00:22,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/1000 [02:03&lt;00:22,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/1000 [02:03&lt;00:22,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 824/1000 [02:03&lt;00:22,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/1000 [02:03&lt;00:22,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 826/1000 [02:03&lt;00:22,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/1000 [02:04&lt;00:22,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/1000 [02:04&lt;00:22,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 829/1000 [02:04&lt;00:22,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 830/1000 [02:04&lt;00:21,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/1000 [02:04&lt;00:21,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/1000 [02:04&lt;00:21,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 833/1000 [02:04&lt;00:21,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/1000 [02:05&lt;00:21,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/1000 [02:05&lt;00:21,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/1000 [02:05&lt;00:20,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 837/1000 [02:05&lt;00:21,  7.71it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 838/1000 [02:05&lt;00:20,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 839/1000 [02:05&lt;00:20,  7.71it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/1000 [02:05&lt;00:20,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/1000 [02:05&lt;00:20,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/1000 [02:06&lt;00:20,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/1000 [02:06&lt;00:20,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 844/1000 [02:06&lt;00:19,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 845/1000 [02:06&lt;00:19,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/1000 [02:06&lt;00:19,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 847/1000 [02:06&lt;00:19,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/1000 [02:06&lt;00:19,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/1000 [02:06&lt;00:19,  7.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 850/1000 [02:07&lt;00:19,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/1000 [02:07&lt;00:18,  7.86it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 852/1000 [02:07&lt;00:19,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 853/1000 [02:07&lt;00:19,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 854/1000 [02:07&lt;00:18,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/1000 [02:07&lt;00:18,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 856/1000 [02:07&lt;00:18,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 857/1000 [02:07&lt;00:18,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 858/1000 [02:08&lt;00:18,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 859/1000 [02:08&lt;00:18,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [02:08&lt;00:18,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 861/1000 [02:08&lt;00:18,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 862/1000 [02:08&lt;00:17,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/1000 [02:08&lt;00:17,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 864/1000 [02:08&lt;00:17,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/1000 [02:09&lt;00:17,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 866/1000 [02:09&lt;00:17,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/1000 [02:09&lt;00:17,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 868/1000 [02:09&lt;00:17,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 869/1000 [02:09&lt;00:17,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/1000 [02:09&lt;00:16,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 871/1000 [02:09&lt;00:16,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/1000 [02:09&lt;00:16,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 873/1000 [02:10&lt;00:16,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/1000 [02:10&lt;00:17,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/1000 [02:10&lt;00:18,  6.91it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 876/1000 [02:10&lt;00:17,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877/1000 [02:10&lt;00:16,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/1000 [02:10&lt;00:16,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 879/1000 [02:10&lt;00:16,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 880/1000 [02:11&lt;00:16,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/1000 [02:11&lt;00:15,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 882/1000 [02:11&lt;00:16,  7.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/1000 [02:11&lt;00:16,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 884/1000 [02:11&lt;00:15,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 885/1000 [02:11&lt;00:15,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/1000 [02:11&lt;00:15,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 887/1000 [02:12&lt;00:15,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/1000 [02:12&lt;00:15,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 889/1000 [02:12&lt;00:14,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 890/1000 [02:12&lt;00:15,  6.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/1000 [02:12&lt;00:15,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 892/1000 [02:12&lt;00:14,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 893/1000 [02:12&lt;00:14,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/1000 [02:12&lt;00:14,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/1000 [02:13&lt;00:14,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/1000 [02:13&lt;00:13,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 897/1000 [02:13&lt;00:14,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/1000 [02:13&lt;00:15,  6.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [02:13&lt;00:14,  6.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/1000 [02:13&lt;00:14,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/1000 [02:13&lt;00:13,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/1000 [02:14&lt;00:13,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 903/1000 [02:14&lt;00:13,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 904/1000 [02:14&lt;00:12,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/1000 [02:14&lt;00:13,  6.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/1000 [02:14&lt;00:13,  7.06it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 907/1000 [02:14&lt;00:12,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 908/1000 [02:14&lt;00:12,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/1000 [02:15&lt;00:12,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 910/1000 [02:15&lt;00:11,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/1000 [02:15&lt;00:11,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 912/1000 [02:15&lt;00:11,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/1000 [02:15&lt;00:11,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 914/1000 [02:15&lt;00:11,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 915/1000 [02:15&lt;00:10,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/1000 [02:15&lt;00:10,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/1000 [02:16&lt;00:10,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 918/1000 [02:16&lt;00:10,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/1000 [02:16&lt;00:10,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 920/1000 [02:16&lt;00:10,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 921/1000 [02:16&lt;00:10,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/1000 [02:16&lt;00:10,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/1000 [02:16&lt;00:10,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 924/1000 [02:17&lt;00:10,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 925/1000 [02:17&lt;00:10,  6.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/1000 [02:17&lt;00:10,  7.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/1000 [02:17&lt;00:09,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/1000 [02:17&lt;00:09,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 929/1000 [02:17&lt;00:09,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 930/1000 [02:17&lt;00:09,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 931/1000 [02:17&lt;00:09,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/1000 [02:18&lt;00:08,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 933/1000 [02:18&lt;00:08,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 934/1000 [02:18&lt;00:08,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 935/1000 [02:18&lt;00:08,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/1000 [02:18&lt;00:08,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 937/1000 [02:18&lt;00:08,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/1000 [02:18&lt;00:07,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/1000 [02:19&lt;00:07,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 940/1000 [02:19&lt;00:07,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/1000 [02:19&lt;00:07,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/1000 [02:19&lt;00:07,  7.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 943/1000 [02:19&lt;00:07,  7.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/1000 [02:19&lt;00:07,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 945/1000 [02:19&lt;00:07,  7.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/1000 [02:19&lt;00:06,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/1000 [02:20&lt;00:06,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 948/1000 [02:20&lt;00:06,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 949/1000 [02:20&lt;00:06,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/1000 [02:20&lt;00:06,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/1000 [02:20&lt;00:06,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 952/1000 [02:20&lt;00:06,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/1000 [02:20&lt;00:06,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/1000 [02:20&lt;00:05,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 955/1000 [02:21&lt;00:05,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 956/1000 [02:21&lt;00:05,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/1000 [02:21&lt;00:05,  7.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/1000 [02:21&lt;00:05,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 959/1000 [02:21&lt;00:05,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 960/1000 [02:21&lt;00:05,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/1000 [02:21&lt;00:04,  7.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 962/1000 [02:21&lt;00:04,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/1000 [02:22&lt;00:04,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964/1000 [02:22&lt;00:04,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/1000 [02:22&lt;00:04,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 966/1000 [02:22&lt;00:04,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 967/1000 [02:22&lt;00:04,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 968/1000 [02:22&lt;00:04,  7.85it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/1000 [02:22&lt;00:03,  7.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 970/1000 [02:22&lt;00:03,  7.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 971/1000 [02:23&lt;00:03,  7.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/1000 [02:23&lt;00:03,  7.89it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/1000 [02:23&lt;00:03,  7.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 974/1000 [02:23&lt;00:03,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/1000 [02:23&lt;00:03,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 976/1000 [02:23&lt;00:03,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/1000 [02:23&lt;00:02,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 978/1000 [02:24&lt;00:02,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/1000 [02:24&lt;00:02,  7.77it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 980/1000 [02:24&lt;00:02,  7.80it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/1000 [02:24&lt;00:02,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 982/1000 [02:24&lt;00:02,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 983/1000 [02:24&lt;00:02,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 984/1000 [02:24&lt;00:02,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/1000 [02:24&lt;00:01,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/1000 [02:25&lt;00:01,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 987/1000 [02:25&lt;00:01,  7.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 988/1000 [02:25&lt;00:01,  7.71it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 989/1000 [02:25&lt;00:01,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 990/1000 [02:25&lt;00:01,  7.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/1000 [02:25&lt;00:01,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 992/1000 [02:25&lt;00:01,  7.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/1000 [02:25&lt;00:00,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/1000 [02:26&lt;00:00,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 995/1000 [02:26&lt;00:00,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/1000 [02:26&lt;00:00,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 997/1000 [02:26&lt;00:00,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/1000 [02:26&lt;00:00,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [02:26&lt;00:00,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:26&lt;00:00,  7.78it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:26&lt;00:00,  6.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]\n\n\n\n\nlibrary(BayesianInference)\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nload(paste(system.file(package = \"BayesianInference\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nm$data_on_model = list()\nm$data_on_model$N_id = length(ids)\nm$data_on_model$network = m$net$mat_to_edgl(model_dat$outcomes[,,1])\nm$data_on_model$N_dyads = m$net$mat_to_edgl(model_dat$outcomes[,,1])$shape[[1]]\nm$data_on_model$focal_individual_predictors = jnp$array(model_dat$individual_predictors)\nm$data_on_model$target_individual_predictors =jnp$array(model_dat$individual_predictors)\n\n# Define model ------------------------------------------------\nmodel &lt;- function( N_id, N_dyads, network, focal_individual_predictors, target_individual_predictors){\n\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(0, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(N_dyads))\n  \n  ## SR ---------------------------------------                                                      \n  m$dist$poisson(jnp$exp(B + sr + dr), obs=network)  \n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nEvent if you don‚Äôt have dyadic effect, or block model effect, they need to be define to create intercepts (means) for those effects",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#mathematical-details",
    "href": "22. Network model.html#mathematical-details",
    "title": "Network Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe simple model that can be built to model link weights between nodes i and j can be defined using a Poisson distribution:\n\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\n\nlog(Y_{ij}) = \\alpha +  \\lambda_i + \\pi_j + \\delta_{ij}  + \\beta_1 X_i + \\beta_2 X_j + \\beta_3 Q_{ij}\n\nwhere:\n\nY_{ij} is the weight of the link between i and j.\n\\lambda_i is the sender random effect üõà.\n\\pi_j is the receiver random effect üõà.\n\\delta_{ij} is the dyadic random effect üõà.\n\\beta_1 is the effect of an individuals i level feature on the emission of a link (i.e., out-strength).\n\\beta_2 is the effect of an individuals j level feature on the receiving a link (i.e., in-strength).\n\\beta_3 is the effect of an dyadic characteristic between i and j on the likelihood of a tie.\n\n\n\nDefining formula sub-equations and prior distributions\nThe sender and receiver random effects are similar to those described in chapter 13: Varying intercepts, but they are defined here using a joint prior so as to estimate the correlation within individuals to emit and receive a link:\n\n\\left(\\begin{array}{cc}\n\\lambda_i \\\\\n\\pi_i\n\\end{array}\\right)\n=\n\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL\n\\left(\\begin{array}{cc}\n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n\n\n\\sigma_\\lambda \\sim \\text{Exponential}(1)\n\n\n\\sigma_\\pi \\sim \\text{Exponential}(1)\n\n\nL \\sim \\text{LKJ}(2)\n\n\n\\hat{\\lambda}_i \\sim \\text{Normal}(0,1)\n\n\n\\hat{\\pi}_i \\sim \\text{Normal}(0,1)\n\nSimilarly, for each dyad we can define a joint prior to estimate correlation between i‚Äìj links and j‚Äìi links:\n\n\\left(\\begin{array}{cc}\n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right)\n=\n\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL_\\delta\n\\left(\\begin{array}{cc}\n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n\n\n\\sigma_\\delta \\sim \\text{Exponential}(1)\n\n\nL_\\delta \\sim \\text{LKJ}(2)\n\n\n\\hat{\\delta}_{ij}  \\sim \\text{Normal}(0,1)",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#notes",
    "href": "22. Network model.html#notes",
    "title": "Network Models",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nNote that any additional covariates can be summed with a regression coefficient to \\lambda_i, \\pi_j and \\delta_{ij}. Of course, for \\lambda_i and \\pi_j, as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for \\delta_{ij}, as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, individual_predictors, and a matrix of dyadic characteristics, kinship, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively.\n\nWe can apply multiple variables as in chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms as in chapter 3: Interaction Between Continuous Variables.\nNetwork links can be modeled using Bernoulli (for proportions), Binomial (for unweighted network), Poisson or zero-inflated Poisson distributions (for count). In BI, you just need to set the correct likelihood distributions. For example, if you want to model the number of interactions between nodes, you can use the Poisson distribution. If you want to model the existence or absence of a link, you can use the Bernoulli distribution.\nIf the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms \\lambda_i, \\pi_j, and \\delta_{ij} are no longer required. (Is it correct?)\nTo account for exposure on a poisson model treat exposure as a nodal characteristic with its own parameter effect (i.e., regression coefficient). Their is several function that will help you to convert vectors or matrices in edge list format to have compatible data structure for the model (see API reference for bi.net.vec_to_edgl and bi.net.mat_to_edgl). F\n\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "22. Network model.html#references",
    "href": "22. Network model.html#references",
    "title": "Network Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nRoss, Cody T, Richard McElreath, and Daniel Redhead. 2024. ‚ÄúModelling Animal Network Data in r Using STRAND.‚Äù Journal of Animal Ecology 93 (3): 254‚Äì66.",
    "crumbs": [
      "Models",
      "Network Models"
    ]
  },
  {
    "objectID": "14. Varying slopes.html",
    "href": "14. Varying slopes.html",
    "title": "Varying Slopes Models üöß",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#general-principles",
    "href": "14. Varying slopes.html#general-principles",
    "title": "Varying Slopes Models üöß",
    "section": "",
    "text": "To model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#considerations",
    "href": "14. Varying slopes.html#considerations",
    "title": "Varying Slopes Models üöß",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for 12. Varying intercepts.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance üõà.\nTo construct the covariance matrix, we use an SRS decomposition where S is a diagonal matrix of standard deviations and R is a correlation matrix. To model the correlation matrix, we use an LKJcorr distribution parametrized with a single control parameter Œ∑ that controls the amount of regularization. Œ∑ is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near ‚àí1 or 1. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\nThe standard deviations in S are model with a prior that constrains them to strictly positive values.",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#example",
    "href": "14. Varying slopes.html#example",
    "title": "Varying Slopes Models üöß",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects. This example is based on McElreath (2018).\n\nSimulated data\n\nPython (Raw)Python (Build in function)R\n\n\n\nfrom BI import bi, jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multivariate_normal(only_path=True)\nm.data(data_path, sep=',') \nm.data_on_model = dict(\n    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n    N_cafes = len(m.df.cafe.unique()),\n    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n)\n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    Rho = m.dist.lkj(2, 2, name = 'Rho')\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariate_normal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;15:51,  1.05it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   6%|‚ñå         | 57/1000 [00:01&lt;00:12, 73.88it/s, 15 steps of size 1.18e-01. acc. prob=0.77]warmup:  14%|‚ñà‚ñé        | 137/1000 [00:01&lt;00:04, 187.59it/s, 63 steps of size 1.19e-01. acc. prob=0.77]warmup:  23%|‚ñà‚ñà‚ñé       | 227/1000 [00:01&lt;00:02, 317.71it/s, 15 steps of size 2.75e-01. acc. prob=0.78]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 319/1000 [00:01&lt;00:01, 442.30it/s, 15 steps of size 5.06e-01. acc. prob=0.78]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/1000 [00:01&lt;00:00, 590.75it/s, 31 steps of size 3.64e-01. acc. prob=0.79]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/1000 [00:01&lt;00:00, 671.45it/s, 15 steps of size 3.22e-01. acc. prob=0.88]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/1000 [00:01&lt;00:00, 768.58it/s, 15 steps of size 3.22e-01. acc. prob=0.88]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/1000 [00:01&lt;00:00, 817.70it/s, 15 steps of size 3.22e-01. acc. prob=0.87]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/1000 [00:01&lt;00:00, 820.14it/s, 15 steps of size 3.22e-01. acc. prob=0.87]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/1000 [00:01&lt;00:00, 882.68it/s, 15 steps of size 3.22e-01. acc. prob=0.87]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 485.41it/s, 15 steps of size 3.22e-01. acc. prob=0.87]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n/home/sosa/work/3.12venv/lib/python3.12/site-packages/arviz/stats/diagnostics.py:991: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nRho[0, 0]\n1.00\n0.00\n1.00\n1.00\n0.00\nNaN\n500.00\n500.00\nNaN\n\n\nRho[0, 1]\n-0.48\n0.19\n-0.78\n-0.19\n0.01\n0.01\n422.99\n367.44\nNaN\n\n\nRho[1, 0]\n-0.48\n0.19\n-0.78\n-0.19\n0.01\n0.01\n422.99\n367.44\nNaN\n\n\nRho[1, 1]\n1.00\n0.00\n1.00\n1.00\n0.00\n0.00\n454.62\n436.09\nNaN\n\n\na\n3.52\n0.20\n3.24\n3.87\n0.01\n0.01\n746.37\n288.40\nNaN\n\n\na_b_cafe[0, 0]\n3.55\n0.23\n3.21\n3.92\n0.01\n0.01\n452.61\n381.98\nNaN\n\n\na_b_cafe[0, 1]\n-1.53\n0.30\n-1.98\n-1.04\n0.01\n0.01\n509.28\n348.57\nNaN\n\n\na_b_cafe[1, 0]\n5.32\n0.23\n4.97\n5.69\n0.01\n0.01\n678.84\n388.59\nNaN\n\n\na_b_cafe[1, 1]\n-1.37\n0.32\n-1.91\n-0.91\n0.01\n0.02\n717.67\n407.82\nNaN\n\n\na_b_cafe[2, 0]\n3.50\n0.23\n3.19\n3.91\n0.01\n0.01\n584.24\n402.89\nNaN\n\n\na_b_cafe[2, 1]\n-1.24\n0.29\n-1.66\n-0.78\n0.01\n0.01\n565.87\n254.37\nNaN\n\n\na_b_cafe[3, 0]\n4.40\n0.21\n4.09\n4.71\n0.01\n0.01\n595.46\n337.76\nNaN\n\n\na_b_cafe[3, 1]\n-1.28\n0.28\n-1.73\n-0.85\n0.01\n0.01\n511.32\n319.38\nNaN\n\n\na_b_cafe[4, 0]\n3.56\n0.22\n3.22\n3.92\n0.01\n0.01\n651.57\n395.53\nNaN\n\n\na_b_cafe[4, 1]\n-1.58\n0.29\n-2.04\n-1.12\n0.01\n0.01\n580.06\n351.02\nNaN\n\n\na_b_cafe[5, 0]\n4.23\n0.21\n3.93\n4.60\n0.01\n0.01\n708.60\n355.22\nNaN\n\n\na_b_cafe[5, 1]\n-1.73\n0.29\n-2.13\n-1.21\n0.01\n0.01\n657.42\n399.55\nNaN\n\n\na_b_cafe[6, 0]\n4.08\n0.23\n3.70\n4.42\n0.01\n0.01\n536.86\n282.44\nNaN\n\n\na_b_cafe[6, 1]\n-0.19\n0.29\n-0.64\n0.25\n0.01\n0.01\n501.31\n399.55\nNaN\n\n\na_b_cafe[7, 0]\n3.78\n0.22\n3.45\n4.12\n0.01\n0.01\n842.35\n436.09\nNaN\n\n\na_b_cafe[7, 1]\n-1.08\n0.30\n-1.52\n-0.56\n0.01\n0.01\n741.73\n425.51\nNaN\n\n\na_b_cafe[8, 0]\n3.49\n0.22\n3.15\n3.82\n0.01\n0.01\n928.83\n448.99\nNaN\n\n\na_b_cafe[8, 1]\n-1.51\n0.29\n-1.95\n-1.01\n0.01\n0.01\n900.17\n323.82\nNaN\n\n\na_b_cafe[9, 0]\n3.25\n0.21\n2.93\n3.57\n0.01\n0.01\n633.95\n365.86\nNaN\n\n\na_b_cafe[9, 1]\n-0.34\n0.28\n-0.82\n0.04\n0.01\n0.01\n652.68\n393.66\nNaN\n\n\na_b_cafe[10, 0]\n3.29\n0.22\n2.92\n3.58\n0.01\n0.01\n810.20\n365.91\nNaN\n\n\na_b_cafe[10, 1]\n-0.60\n0.28\n-1.06\n-0.18\n0.01\n0.01\n659.36\n385.50\nNaN\n\n\na_b_cafe[11, 0]\n3.83\n0.20\n3.50\n4.13\n0.01\n0.01\n577.10\n304.11\nNaN\n\n\na_b_cafe[11, 1]\n-1.44\n0.27\n-1.82\n-0.96\n0.01\n0.01\n593.52\n297.44\nNaN\n\n\na_b_cafe[12, 0]\n1.92\n0.22\n1.57\n2.23\n0.01\n0.01\n600.90\n314.72\nNaN\n\n\na_b_cafe[12, 1]\n-0.77\n0.30\n-1.18\n-0.24\n0.01\n0.01\n724.12\n342.76\nNaN\n\n\na_b_cafe[13, 0]\n4.37\n0.22\n4.04\n4.71\n0.01\n0.01\n549.10\n369.49\nNaN\n\n\na_b_cafe[13, 1]\n-2.18\n0.30\n-2.64\n-1.67\n0.01\n0.01\n565.11\n332.48\nNaN\n\n\na_b_cafe[14, 0]\n2.88\n0.21\n2.55\n3.22\n0.01\n0.01\n708.31\n473.41\nNaN\n\n\na_b_cafe[14, 1]\n-0.73\n0.29\n-1.15\n-0.25\n0.01\n0.01\n644.40\n297.55\nNaN\n\n\na_b_cafe[15, 0]\n2.26\n0.22\n1.90\n2.59\n0.01\n0.01\n598.20\n408.34\nNaN\n\n\na_b_cafe[15, 1]\n-0.45\n0.29\n-0.90\n-0.01\n0.01\n0.01\n500.02\n219.35\nNaN\n\n\na_b_cafe[16, 0]\n4.68\n0.22\n4.28\n5.00\n0.01\n0.01\n601.60\n399.55\nNaN\n\n\na_b_cafe[16, 1]\n-2.08\n0.30\n-2.53\n-1.54\n0.01\n0.01\n584.76\n437.40\nNaN\n\n\na_b_cafe[17, 0]\n2.46\n0.21\n2.17\n2.84\n0.01\n0.01\n647.23\n279.36\nNaN\n\n\na_b_cafe[17, 1]\n-1.22\n0.28\n-1.67\n-0.80\n0.01\n0.02\n634.28\n211.75\nNaN\n\n\na_b_cafe[18, 0]\n2.51\n0.21\n2.20\n2.84\n0.01\n0.01\n518.14\n369.15\nNaN\n\n\na_b_cafe[18, 1]\n-0.16\n0.28\n-0.59\n0.31\n0.01\n0.01\n514.85\n364.46\nNaN\n\n\na_b_cafe[19, 0]\n2.82\n0.20\n2.48\n3.15\n0.01\n0.01\n570.10\n327.00\nNaN\n\n\na_b_cafe[19, 1]\n-0.57\n0.27\n-1.02\n-0.16\n0.01\n0.01\n766.08\n470.00\nNaN\n\n\nb\n-1.09\n0.15\n-1.34\n-0.86\n0.01\n0.01\n634.77\n375.64\nNaN\n\n\nsigma\n0.51\n0.03\n0.46\n0.54\n0.00\n0.00\n800.09\n512.95\nNaN\n\n\nsigma_cafe[0]\n0.92\n0.15\n0.67\n1.14\n0.01\n0.01\n541.45\n397.11\nNaN\n\n\nsigma_cafe[1]\n0.68\n0.13\n0.47\n0.87\n0.01\n0.01\n401.57\n358.18\nNaN\n\n\n\n\n\n\n\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m$load$sim_multivariate_normal(only_path=T)\nm.data(data_path, sep=',') \nm.data_on_model = dict(\n    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n    N_cafes = len(m.df.cafe.unique()),\n    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n)\n\n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n        N_vars = 1,\n        N_group = N_cafes,\n        group_id = cafe,\n        group_name = 'cafe',\n        centered = False)\n    \n\n    mu = varying_intercept + varying_slope* afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\nlibrary(BayesianInference)\njnp = reticulate::import('jax.numpy')\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\nm$data_on_model\n\n# Define model ------------------------------------------------\nmodel &lt;- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariate_normal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, \n    shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  bi.dist.normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details",
    "href": "14. Varying slopes.html#mathematical-details",
    "title": "Varying Slopes Models üöß",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nCentered parameterization\nThe Gaussian Mixture Model is a hierarchical model where each data point is generated from one of K distinct multivariate Gaussian distributions.\nThe varying intercepts (\\alpha_k) and slopes (\\beta_k) are modeled using a Multivariate Normal distribution:\n\n\\begin{pmatrix}\n\\alpha_k \\\\\n\\beta_k\n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left(\n\\begin{pmatrix}\n\\bar{\\alpha} \\\\\n\\bar{\\beta}\n\\end{pmatrix},\n\\text{diag}(\\varsigma) ~ \\Omega ~ \\text{diag}(\\varsigma)\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim  \\text{Exponential}(1)\n \n\\Omega \\sim \\text{LKJ}(\\eta)\n\nWhere:\n\n\\left(\\begin{array}{cc} \\bar{\\alpha} \\\\ \\bar{\\beta} \\end{array}\\right) is a vector composed from concatenating a parameter for the global intercept and a parameter vector of the global slopes.\n\\varsigma is a vector giving the standard deviation of the random effects for the intercept and slopes across groups.\n\\Omega is the correlation matrix.\n\n\n\nNon-centered parameterization\nFor computational reasons, it is often better to implement a non-centered parameterization üõà that is equivalent to the Multivariate Normal distribution approach:\n\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n=\n\\left(\\begin{array}{cc}\n\\bar{\\alpha} \\\\\n\\bar{\\beta}\n\\end{array}\\right) + \\varsigma\\circ\n\\left(\nL \\cdot\n\\left(\n\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\n\\right)\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim  \\text{Exponential}(1)\n \nL \\sim \\text{LKJ Cholesky}(\\eta)\n\n\n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n\n\nWhere:\n\n\\sigma_\\alpha \\sim \\text{Exponential}(1) is the prior standard deviation among intercepts.\n\\sigma_\\beta \\sim \\text{Exponential}(1) is the prior standard deviation among slopes.\nL \\sim \\text{LKJcorr}(\\eta) is the a cholesky factor of the correlation matrix matrix using the Cholesky Factor üõà\n\n\n\n\nMultivariate Model with One Random Slope for Each Variable\nWe can apply a multivariate model similarly to Chapter 2. In this case, we apply the same principle, but with a covariance matrix with a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for i observations in a model with two independent variables X_1 and X_2, we can define the formula as follows:\n\nY_{i}  \\sim \\text{Normal}(\\mu_i , \\sigma)\n\n\n\\mu_i =   \\alpha_i + \\beta_{k(i)} X_{1i}  + \\gamma_{k(i)} X_{2i}\n\n\n\n\\begin{pmatrix}\n\\alpha\\\\\n\\beta\\\\\n\\gamma\n\\end{pmatrix}\n\\sim \\begin{pmatrix}\n\\bar{\\alpha}\\\\\n\\bar{\\beta}\\\\\n\\bar{\\gamma}\n\\end{pmatrix} + \\varsigma \\circ\n\\left(\nL \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{k} \\\\\n\\widehat{\\beta}_{k} \\\\\n\\widehat{\\gamma}_{k}\n\\end{pmatrix}\n\\right)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n   \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n\n  \n\\bar{\\gamma} \\sim \\text{Normal}(0, 1)\n\n\n\\varsigma \\sim  \\text{Exponential}(1)\n \nL \\sim \\text{LKJ Cholesky}(2)\n\n\n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n\n\n\\widehat{\\gamma}_k \\sim \\text{Exponential}(1)",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#notes",
    "href": "14. Varying slopes.html#notes",
    "title": "Varying Slopes Models üöß",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#references",
    "href": "14. Varying slopes.html#references",
    "title": "Varying Slopes Models üöß",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Varying Slopes Models üöß"
    ]
  },
  {
    "objectID": "9. Categorical model.html",
    "href": "9. Categorical model.html",
    "title": "Categorical Model",
    "section": "",
    "text": "To model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a Categorical model.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#general-principles",
    "href": "9. Categorical model.html#general-principles",
    "title": "Categorical Model",
    "section": "",
    "text": "To model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a Categorical model.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#considerations",
    "href": "9. Categorical model.html#considerations",
    "title": "Categorical Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nOne way to interpret a Categorical model is to consider that we need to build K - 1 linear models, where K is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a simplex üõà. To do this, we convert the regression outputs using the softmax function üõà (see the ‚Äújax.nn.softmax‚Äù line in the code).\nThe intercept \\alpha captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\nOn the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients \\beta are shared across categories.\nThe relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#example",
    "href": "9. Categorical model.html#example",
    "title": "Categorical Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Categorical model using the Bayesian Inference (BI) package. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(career, income):\n    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n    b = m.dist.half_normal(0.5, shape=(1,), name = 'b')\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0] #pivot\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    m.dist.categorical(probs=p, obs=career)\n\n# Run sampler ------------------------------------------------ \nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;07:49,  2.13it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 240/1000 [00:00&lt;00:01, 558.41it/s, 127 steps of size 1.00e-01. acc. prob=0.78]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/1000 [00:00&lt;00:00, 997.95it/s, 3 steps of size 1.89e-01. acc. prob=0.79]  sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/1000 [00:00&lt;00:00, 1308.25it/s, 7 steps of size 1.86e-01. acc. prob=0.94]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/1000 [00:00&lt;00:00, 1626.87it/s, 15 steps of size 1.86e-01. acc. prob=0.92]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1116.50it/s, 7 steps of size 1.86e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n-2.09\n0.27\n-2.43\n-1.67\n0.03\n0.03\n85.36\n78.74\nNaN\n\n\na[1]\n-1.56\n0.16\n-1.80\n-1.30\n0.02\n0.01\n101.63\n28.52\nNaN\n\n\nb[0]\n0.05\n0.05\n0.00\n0.12\n0.01\n0.01\n66.53\n81.49\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys &lt;- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues &lt;- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n\n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n\n  # Likelihood\n  m$dist$categorical(probs=p[career], obs=career)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#mathematical-details",
    "href": "9. Categorical model.html#mathematical-details",
    "title": "Categorical Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a Categorical model using a Categorical distribution. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable ùë¶ with ùêæ categories, the multinomial likelihood function is:\n\nY_i \\sim \\text{Categorical}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the dependent categorical variable for observation i indicating the category of the observation.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex üõà.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "9. Categorical model.html#references",
    "href": "9. Categorical model.html#references",
    "title": "Categorical Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Categorical Model"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html",
    "href": "13. Varying intercepts.html",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data are grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#general-principles",
    "href": "13. Varying intercepts.html#general-principles",
    "title": "Varying Intercepts Models",
    "section": "",
    "text": "To model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data are grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#considerations",
    "href": "13. Varying intercepts.html#considerations",
    "title": "Varying Intercepts Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nThe main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept \\alpha_k is defined uniquely for each of the K declared groups.\nIn the code below, the intercept alpha for each of the k declared groups shares two priors, a_bar and sigma, which are respectively modeled by a Normal and an Exponential distribution.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#example",
    "href": "13. Varying intercepts.html#example",
    "title": "Varying Intercepts Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with varying intercepts using the Bayesian Inference (BI) package. The data consists of a dependent variable representing individuals‚Äô survival (surv) and an independent categorical variable (tank), which indicates the tank where the individual was born, with a total of 48 tanks. This example is based on McElreath (2018).\n\nPython (Raw)Python (Build in function)R\n\n\n\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.reedfrogs(only_path=True)\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = jnp.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n    p = alpha[tank]\n    m.dist.binomial(total_count = density, logits = p, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;09:26,  1.76it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 171/1000 [00:00&lt;00:02, 343.58it/s, 15 steps of size 5.17e-01. acc. prob=0.78]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 411/1000 [00:00&lt;00:00, 802.33it/s, 7 steps of size 5.08e-01. acc. prob=0.79] sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 662/1000 [00:00&lt;00:00, 1214.90it/s, 15 steps of size 4.21e-01. acc. prob=0.90]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 887/1000 [00:00&lt;00:00, 1480.33it/s, 7 steps of size 4.21e-01. acc. prob=0.90] sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01&lt;00:00, 983.37it/s, 7 steps of size 4.21e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na_bar\n1.35\n0.27\n0.95\n1.79\n0.01\n0.01\n467.13\n297.56\nNaN\n\n\nalpha[0]\n2.13\n0.85\n0.69\n3.34\n0.04\n0.04\n514.75\n388.20\nNaN\n\n\nalpha[1]\n3.09\n1.09\n1.30\n4.47\n0.05\n0.05\n565.53\n436.09\nNaN\n\n\nalpha[2]\n0.99\n0.68\n0.06\n2.14\n0.03\n0.04\n676.80\n327.00\nNaN\n\n\nalpha[3]\n3.12\n1.10\n1.27\n4.66\n0.05\n0.06\n594.03\n428.01\nNaN\n\n\nalpha[4]\n2.15\n0.89\n0.82\n3.57\n0.03\n0.04\n726.38\n463.33\nNaN\n\n\nalpha[5]\n2.19\n0.91\n0.87\n3.66\n0.04\n0.05\n771.96\n371.60\nNaN\n\n\nalpha[6]\n3.12\n1.07\n1.57\n4.72\n0.05\n0.04\n520.47\n347.62\nNaN\n\n\nalpha[7]\n2.14\n0.87\n0.73\n3.38\n0.03\n0.05\n1221.40\n424.56\nNaN\n\n\nalpha[8]\n-0.16\n0.62\n-1.02\n0.94\n0.02\n0.04\n811.97\n219.25\nNaN\n\n\nalpha[9]\n2.10\n0.86\n0.67\n3.33\n0.04\n0.04\n513.47\n370.85\nNaN\n\n\nalpha[10]\n0.99\n0.69\n-0.13\n2.03\n0.02\n0.03\n927.72\n411.04\nNaN\n\n\nalpha[11]\n0.62\n0.65\n-0.49\n1.61\n0.03\n0.03\n561.63\n290.42\nNaN\n\n\nalpha[12]\n1.05\n0.72\n-0.12\n2.05\n0.03\n0.04\n708.09\n317.39\nNaN\n\n\nalpha[13]\n0.18\n0.62\n-0.83\n1.18\n0.02\n0.03\n680.04\n406.36\nNaN\n\n\nalpha[14]\n2.16\n0.84\n0.69\n3.32\n0.03\n0.04\n843.95\n375.45\nNaN\n\n\nalpha[15]\n2.15\n0.91\n0.80\n3.65\n0.04\n0.04\n499.23\n283.02\nNaN\n\n\nalpha[16]\n2.91\n0.79\n1.42\n3.86\n0.03\n0.04\n716.71\n353.12\nNaN\n\n\nalpha[17]\n2.40\n0.67\n1.30\n3.38\n0.03\n0.04\n727.24\n247.66\nNaN\n\n\nalpha[18]\n2.03\n0.60\n1.10\n2.91\n0.03\n0.03\n642.62\n326.17\nNaN\n\n\nalpha[19]\n3.75\n0.95\n2.17\n5.10\n0.04\n0.04\n582.43\n361.94\nNaN\n\n\nalpha[20]\n2.35\n0.61\n1.41\n3.24\n0.02\n0.03\n824.43\n261.58\nNaN\n\n\nalpha[21]\n2.36\n0.66\n1.34\n3.37\n0.02\n0.03\n761.56\n320.99\nNaN\n\n\nalpha[22]\n2.40\n0.62\n1.37\n3.31\n0.03\n0.03\n573.35\n352.38\nNaN\n\n\nalpha[23]\n1.70\n0.53\n0.93\n2.56\n0.02\n0.02\n839.61\n423.04\nNaN\n\n\nalpha[24]\n-1.03\n0.45\n-1.69\n-0.33\n0.02\n0.02\n731.40\n365.09\nNaN\n\n\nalpha[25]\n0.16\n0.42\n-0.48\n0.83\n0.02\n0.02\n651.44\n369.19\nNaN\n\n\nalpha[26]\n-1.41\n0.45\n-2.15\n-0.71\n0.02\n0.02\n597.70\n314.86\nNaN\n\n\nalpha[27]\n-0.48\n0.42\n-1.08\n0.19\n0.02\n0.02\n537.11\n388.83\nNaN\n\n\nalpha[28]\n0.16\n0.40\n-0.61\n0.67\n0.02\n0.02\n431.36\n392.36\nNaN\n\n\nalpha[29]\n1.46\n0.48\n0.75\n2.27\n0.02\n0.02\n755.76\n404.80\nNaN\n\n\nalpha[30]\n-0.62\n0.43\n-1.18\n0.20\n0.02\n0.02\n706.26\n394.05\nNaN\n\n\nalpha[31]\n-0.30\n0.43\n-0.90\n0.41\n0.02\n0.02\n692.89\n408.34\nNaN\n\n\nalpha[32]\n3.20\n0.76\n2.04\n4.48\n0.03\n0.03\n563.22\n379.37\nNaN\n\n\nalpha[33]\n2.70\n0.64\n1.83\n3.88\n0.02\n0.03\n758.59\n428.71\nNaN\n\n\nalpha[34]\n2.74\n0.65\n1.65\n3.71\n0.03\n0.03\n545.98\n387.72\nNaN\n\n\nalpha[35]\n2.06\n0.49\n1.36\n2.94\n0.02\n0.02\n721.44\n392.52\nNaN\n\n\nalpha[36]\n2.09\n0.57\n1.25\n2.95\n0.02\n0.03\n732.34\n371.66\nNaN\n\n\nalpha[37]\n3.89\n0.95\n2.29\n5.11\n0.04\n0.05\n508.53\n294.22\nNaN\n\n\nalpha[38]\n2.70\n0.70\n1.66\n3.76\n0.03\n0.05\n696.43\n275.91\nNaN\n\n\nalpha[39]\n2.38\n0.54\n1.46\n3.19\n0.02\n0.03\n588.18\n352.03\nNaN\n\n\nalpha[40]\n-1.77\n0.47\n-2.50\n-1.03\n0.02\n0.02\n965.46\n269.98\nNaN\n\n\nalpha[41]\n-0.59\n0.31\n-1.09\n-0.13\n0.01\n0.01\n729.57\n401.86\nNaN\n\n\nalpha[42]\n-0.44\n0.37\n-1.04\n0.16\n0.01\n0.02\n744.12\n373.34\nNaN\n\n\nalpha[43]\n-0.33\n0.36\n-0.90\n0.23\n0.01\n0.02\n818.64\n313.40\nNaN\n\n\nalpha[44]\n0.57\n0.34\n0.07\n1.09\n0.02\n0.01\n487.25\n426.63\nNaN\n\n\nalpha[45]\n-0.58\n0.38\n-1.25\n-0.04\n0.01\n0.02\n1084.06\n421.32\nNaN\n\n\nalpha[46]\n2.06\n0.46\n1.34\n2.75\n0.02\n0.02\n569.86\n389.45\nNaN\n\n\nalpha[47]\n0.02\n0.35\n-0.50\n0.61\n0.01\n0.02\n710.85\n353.56\nNaN\n\n\nsigma\n1.62\n0.21\n1.30\n1.92\n0.01\n0.01\n339.24\n267.98\nNaN\n\n\n\n\n\n\n\n\n\n\nfrom BI import bi, jnp\n\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path =  m.load.reedfrogs(only_path=True)\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = jnp.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    alpha = m.effects.varying_intercept(N_groups=48,group_id=tank, group_name = 'tank')\n    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;07:53,  2.11it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 149/1000 [00:00&lt;00:02, 343.83it/s, 7 steps of size 1.89e-01. acc. prob=0.78]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 359/1000 [00:00&lt;00:00, 779.43it/s, 15 steps of size 5.64e-01. acc. prob=0.79]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/1000 [00:00&lt;00:00, 1192.31it/s, 7 steps of size 4.53e-01. acc. prob=0.88]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/1000 [00:00&lt;00:00, 1477.78it/s, 15 steps of size 4.53e-01. acc. prob=0.88]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1040.28it/s, 7 steps of size 4.53e-01. acc. prob=0.88]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nglobal_intercept_tank\n1.36\n0.26\n0.94\n1.77\n0.01\n0.01\n563.88\n182.64\nNaN\n\n\nintercept_tank[0]\n2.10\n0.86\n0.72\n3.38\n0.03\n0.04\n811.23\n369.49\nNaN\n\n\nintercept_tank[1]\n3.04\n1.11\n1.31\n4.69\n0.05\n0.06\n592.21\n290.26\nNaN\n\n\nintercept_tank[2]\n1.00\n0.65\n-0.06\n2.06\n0.03\n0.04\n647.60\n343.02\nNaN\n\n\nintercept_tank[3]\n3.06\n1.10\n1.16\n4.67\n0.05\n0.07\n547.75\n178.68\nNaN\n\n\nintercept_tank[4]\n2.14\n0.89\n0.80\n3.44\n0.03\n0.05\n953.17\n307.45\nNaN\n\n\nintercept_tank[5]\n2.15\n0.89\n0.91\n3.72\n0.03\n0.04\n850.84\n421.61\nNaN\n\n\nintercept_tank[6]\n3.13\n1.11\n1.40\n4.93\n0.06\n0.06\n455.95\n282.69\nNaN\n\n\nintercept_tank[7]\n2.18\n0.89\n1.00\n3.73\n0.04\n0.04\n504.63\n338.18\nNaN\n\n\nintercept_tank[8]\n-0.16\n0.62\n-1.02\n0.89\n0.02\n0.03\n602.28\n387.08\nNaN\n\n\nintercept_tank[9]\n2.15\n0.92\n0.81\n3.47\n0.04\n0.06\n588.68\n335.17\nNaN\n\n\nintercept_tank[10]\n1.00\n0.70\n0.03\n2.17\n0.02\n0.04\n921.27\n305.21\nNaN\n\n\nintercept_tank[11]\n0.58\n0.61\n-0.31\n1.64\n0.02\n0.03\n718.11\n320.40\nNaN\n\n\nintercept_tank[12]\n1.07\n0.69\n-0.06\n2.08\n0.03\n0.03\n511.78\n369.19\nNaN\n\n\nintercept_tank[13]\n0.20\n0.60\n-0.69\n1.20\n0.02\n0.02\n646.21\n339.77\nNaN\n\n\nintercept_tank[14]\n2.12\n0.83\n0.90\n3.52\n0.04\n0.04\n563.92\n353.53\nNaN\n\n\nintercept_tank[15]\n2.15\n0.91\n0.69\n3.50\n0.04\n0.05\n565.23\n355.31\nNaN\n\n\nintercept_tank[16]\n2.87\n0.76\n1.74\n4.10\n0.03\n0.03\n918.26\n411.31\nNaN\n\n\nintercept_tank[17]\n2.42\n0.68\n1.36\n3.40\n0.03\n0.05\n642.69\n209.59\nNaN\n\n\nintercept_tank[18]\n2.03\n0.60\n0.95\n2.83\n0.03\n0.02\n460.37\n346.04\nNaN\n\n\nintercept_tank[19]\n3.73\n1.04\n2.02\n5.18\n0.05\n0.05\n587.35\n381.98\nNaN\n\n\nintercept_tank[20]\n2.38\n0.64\n1.47\n3.46\n0.03\n0.03\n592.59\n386.77\nNaN\n\n\nintercept_tank[21]\n2.39\n0.69\n1.30\n3.37\n0.03\n0.03\n622.27\n322.91\nNaN\n\n\nintercept_tank[22]\n2.42\n0.65\n1.26\n3.35\n0.03\n0.04\n435.39\n247.89\nNaN\n\n\nintercept_tank[23]\n1.69\n0.53\n0.88\n2.48\n0.02\n0.03\n676.05\n393.95\nNaN\n\n\nintercept_tank[24]\n-1.00\n0.48\n-1.69\n-0.26\n0.02\n0.02\n792.69\n344.82\nNaN\n\n\nintercept_tank[25]\n0.14\n0.40\n-0.44\n0.81\n0.02\n0.02\n570.16\n385.30\nNaN\n\n\nintercept_tank[26]\n-1.42\n0.48\n-2.04\n-0.54\n0.02\n0.03\n699.69\n349.23\nNaN\n\n\nintercept_tank[27]\n-0.50\n0.42\n-1.12\n0.17\n0.02\n0.02\n482.27\n381.98\nNaN\n\n\nintercept_tank[28]\n0.15\n0.39\n-0.39\n0.82\n0.02\n0.02\n655.74\n295.17\nNaN\n\n\nintercept_tank[29]\n1.44\n0.48\n0.66\n2.19\n0.02\n0.02\n491.85\n336.45\nNaN\n\n\nintercept_tank[30]\n-0.62\n0.42\n-1.24\n0.11\n0.02\n0.02\n601.10\n369.15\nNaN\n\n\nintercept_tank[31]\n-0.28\n0.44\n-0.96\n0.37\n0.02\n0.02\n587.55\n285.26\nNaN\n\n\nintercept_tank[32]\n3.18\n0.72\n2.05\n4.19\n0.03\n0.03\n481.03\n336.93\nNaN\n\n\nintercept_tank[33]\n2.67\n0.62\n1.57\n3.49\n0.02\n0.03\n735.44\n289.00\nNaN\n\n\nintercept_tank[34]\n2.75\n0.62\n1.76\n3.73\n0.03\n0.03\n388.61\n347.85\nNaN\n\n\nintercept_tank[35]\n2.06\n0.47\n1.27\n2.80\n0.02\n0.02\n840.45\n343.02\nNaN\n\n\nintercept_tank[36]\n2.09\n0.53\n1.32\n2.91\n0.02\n0.02\n756.53\n393.66\nNaN\n\n\nintercept_tank[37]\n3.90\n0.95\n2.47\n5.27\n0.05\n0.04\n336.15\n304.76\nNaN\n\n\nintercept_tank[38]\n2.70\n0.67\n1.72\n3.72\n0.03\n0.04\n665.74\n311.83\nNaN\n\n\nintercept_tank[39]\n2.34\n0.53\n1.45\n3.15\n0.02\n0.03\n738.32\n281.88\nNaN\n\n\nintercept_tank[40]\n-1.78\n0.45\n-2.48\n-1.06\n0.01\n0.02\n913.76\n357.43\nNaN\n\n\nintercept_tank[41]\n-0.58\n0.34\n-1.14\n-0.07\n0.01\n0.01\n730.83\n425.09\nNaN\n\n\nintercept_tank[42]\n-0.44\n0.39\n-1.03\n0.21\n0.01\n0.02\n693.24\n438.47\nNaN\n\n\nintercept_tank[43]\n-0.33\n0.34\n-0.86\n0.20\n0.01\n0.02\n549.94\n352.19\nNaN\n\n\nintercept_tank[44]\n0.58\n0.33\n0.06\n1.09\n0.01\n0.01\n653.17\n339.77\nNaN\n\n\nintercept_tank[45]\n-0.59\n0.38\n-1.24\n-0.02\n0.02\n0.02\n620.34\n294.19\nNaN\n\n\nintercept_tank[46]\n2.03\n0.48\n1.30\n2.77\n0.02\n0.03\n657.69\n351.35\nNaN\n\n\nintercept_tank[47]\n0.01\n0.33\n-0.52\n0.54\n0.01\n0.01\n735.81\n303.89\nNaN\n\n\nsd_tank\n1.61\n0.20\n1.28\n1.92\n0.01\n0.01\n223.71\n348.72\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$reedfrogs(only_path=T), sep=';')\nm$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\nm$data_to_model(list('tank', 'surv', 'density')) # Manipulate\nm$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\nm$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(tank, surv, density){\n  # Parameter prior distributions\n  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n  a_bar =  bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n  p = alpha[tank]\n  # Likelihood\n  m$dist$binomial(total_count = density, logits = p, obs=surv)\n} \n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#mathematical-details",
    "href": "13. Varying intercepts.html#mathematical-details",
    "title": "Varying Intercepts Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe model the relationship between the independent variable X and the outcome variable Y while accounting for varying intercepts \\alpha for each group where k(i) give us group belonging for observation i, using the following equation:\n\nY_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\n \n\\mu_{i} = \\alpha_{[k(i)]} + \\beta X_{i}\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\n\n\\alpha_{[k]} \\sim \\text{Normal}(\\bar{\\alpha}, \\varsigma)\n\n\n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n \n\\varsigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_{i} is the outcome variable for observation i.\n\\alpha_{[k(i)]} is the varying intercept corresponding to the group k of observation i%.\n\\beta is the regression coefficient.\n\\sigma is a standard deviation parameter, which here has an Exponential prior that constrains it to be positive.\n\\bar{\\alpha} is the overall mean intercept.\n\\varsigma is the variance of the intercepts across groups.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#notes",
    "href": "13. Varying intercepts.html#notes",
    "title": "Varying Intercepts Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2.\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying intercepts with any distribution developed in previous chapters.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#references",
    "href": "13. Varying intercepts.html#references",
    "title": "Varying Intercepts Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Varying Intercepts Models"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html",
    "href": "18. Latent variable (wip).html",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables‚Äîvariables that are not directly observed but are inferred from the data‚Äîcan help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\nY = f(X, Z) + \\epsilon\n\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models üöß"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#general-principles",
    "href": "18. Latent variable (wip).html#general-principles",
    "title": "Latent Variable Models (WIP)",
    "section": "",
    "text": "In some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables‚Äîvariables that are not directly observed but are inferred from the data‚Äîcan help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\nY = f(X, Z) + \\epsilon\n\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "Models",
      "Latent Variable Models üöß"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#considerations",
    "href": "18. Latent variable (wip).html#considerations",
    "title": "Latent Variable Models (WIP)",
    "section": "Considerations",
    "text": "Considerations\nIn Bayesian regression with latent variables, we consider the uncertainty in both the observed and latent variables. We declare prior distributions for the latent variables, in addition to the usual priors for regression coefficients and intercepts. These latent variables are often modeled using Gaussian distributions (Normal priors) or more flexible distributions such as Multivariate Normal for correlations among the latent variables.\nThe goal is to infer the posterior distribution over both the parameters and the latent variables, given the observed data.",
    "crumbs": [
      "Models",
      "Latent Variable Models üöß"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#example",
    "href": "18. Latent variable (wip).html#example",
    "title": "Latent Variable Models (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian regression with latent variables using TensorFlow Probability:\n\nPythonR\n\n\n\nfrom BI import bi, jnp\n\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Data Simulation ------------------------------------------------\nNY = 4  # Number of dependent variables or outcomes (e.g., dimensions for latent variables)\nNV = 8  # Number of observations or individual-level data points (e.g., subjects)\nN = 100\nK = 5\na = 0.5\n# Generate the means and offsets for the data\n# means: Generate random normal means for each of the NY outcomes\n# offsets: Generate random normal offsets for each of the NV observations\nmeans = m.dist.normal(0, 1, shape=(NY,), sample=True, seed=10)\noffsets = m.dist.normal(0, 1, shape=(NV, 1), sample=True, seed=20)\n\nY2 = offsets + means\n\n# Simulate individual-level random effects (e.g., random slopes or intercepts)\n# b_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates\nb_individual = m.dist.normal(0, 1, shape=(N, K), sample=True, seed=0)\n\n# mu: Add an additional effect 'a' to the individual-level random effects 'b_individual'\n# 'a' could represent a population-level effect or a baseline\nmu = b_individual + a\n\n# Convert Y2 to a JAX array for further computation in a JAX-based framework\nY2 = jnp.array(Y2)\n\n\n# Set data ------------------------------------------------\ndat = dict(\n    NY = NY,\n    NV = NV,\n    Y2 = Y2\n)\nm.data_on_model = dat\n\n# Define model ------------------------------------------------\ndef model(NY, NV, Y2):\n    means = m.dist.normal(0, 1, shape=(NY,), name='means')\n    offset = m.dist.normal(0, 1, shape=(NV, 1), name='offset')\n    sigma = m.dist.exponential(1, shape=(NY,), name='sigma')\n    tmp = jnp.tile(means, (NV, 1)).reshape(NV, NY)\n    mu_l = tmp + offset\n    m.dist.normal(mu_l, jnp.tile(sigma, [NV, 1]), obs=Y2)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;18:07,  1.09s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   6%|‚ñå         | 60/1000 [00:01&lt;00:13, 68.76it/s, 1023 steps of size 4.55e-04. acc. prob=0.72]warmup:   9%|‚ñâ         | 93/1000 [00:01&lt;00:09, 94.60it/s, 7 steps of size 4.40e-04. acc. prob=0.74]   warmup:  12%|‚ñà‚ñè        | 120/1000 [00:01&lt;00:08, 107.96it/s, 1023 steps of size 9.35e-04. acc. prob=0.75]warmup:  14%|‚ñà‚ñç        | 142/1000 [00:01&lt;00:07, 118.36it/s, 15 steps of size 6.16e-04. acc. prob=0.75]  warmup:  16%|‚ñà‚ñå        | 162/1000 [00:01&lt;00:06, 130.15it/s, 1023 steps of size 9.77e-04. acc. prob=0.76]warmup:  18%|‚ñà‚ñä        | 181/1000 [00:01&lt;00:05, 139.42it/s, 915 steps of size 1.48e-03. acc. prob=0.76] warmup:  20%|‚ñà‚ñà        | 205/1000 [00:02&lt;00:05, 158.31it/s, 1023 steps of size 8.00e-04. acc. prob=0.76]warmup:  22%|‚ñà‚ñà‚ñé       | 225/1000 [00:02&lt;00:05, 144.31it/s, 1023 steps of size 4.95e-04. acc. prob=0.77]warmup:  25%|‚ñà‚ñà‚ñç       | 246/1000 [00:02&lt;00:04, 158.85it/s, 543 steps of size 5.48e-04. acc. prob=0.77] warmup:  26%|‚ñà‚ñà‚ñã       | 265/1000 [00:02&lt;00:04, 162.31it/s, 279 steps of size 1.12e-03. acc. prob=0.77]warmup:  28%|‚ñà‚ñà‚ñä       | 283/1000 [00:02&lt;00:04, 154.84it/s, 1023 steps of size 1.16e-03. acc. prob=0.77]warmup:  30%|‚ñà‚ñà‚ñà       | 300/1000 [00:02&lt;00:04, 151.17it/s, 351 steps of size 5.91e-04. acc. prob=0.77] warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 316/1000 [00:02&lt;00:04, 151.36it/s, 1023 steps of size 7.57e-04. acc. prob=0.77]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 332/1000 [00:02&lt;00:04, 152.28it/s, 1023 steps of size 1.36e-04. acc. prob=0.77]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 348/1000 [00:02&lt;00:04, 147.46it/s, 900 steps of size 2.77e-04. acc. prob=0.77] warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 364/1000 [00:03&lt;00:04, 148.45it/s, 983 steps of size 1.63e-04. acc. prob=0.77]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 380/1000 [00:03&lt;00:04, 142.19it/s, 711 steps of size 2.59e-04. acc. prob=0.77]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 396/1000 [00:03&lt;00:04, 143.13it/s, 1023 steps of size 1.26e-04. acc. prob=0.77]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 411/1000 [00:03&lt;00:04, 138.67it/s, 1023 steps of size 1.64e-04. acc. prob=0.78]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/1000 [00:03&lt;00:04, 138.03it/s, 1023 steps of size 2.12e-04. acc. prob=0.78]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/1000 [00:03&lt;00:04, 138.41it/s, 1023 steps of size 1.13e-04. acc. prob=0.78]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/1000 [00:03&lt;00:03, 139.39it/s, 1023 steps of size 7.86e-05. acc. prob=0.77]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/1000 [00:03&lt;00:04, 130.32it/s, 1023 steps of size 2.29e-04. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/1000 [00:03&lt;00:03, 143.87it/s, 1023 steps of size 1.52e-04. acc. prob=0.78]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/1000 [00:04&lt;00:03, 136.15it/s, 7 steps of size 1.66e-04. acc. prob=0.96]   sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/1000 [00:04&lt;00:03, 128.91it/s, 1023 steps of size 1.66e-04. acc. prob=0.94]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [00:04&lt;00:03, 134.93it/s, 1023 steps of size 1.66e-04. acc. prob=0.91]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/1000 [00:04&lt;00:03, 135.64it/s, 863 steps of size 1.66e-04. acc. prob=0.85] sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/1000 [00:04&lt;00:03, 130.06it/s, 1023 steps of size 1.66e-04. acc. prob=0.80]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/1000 [00:04&lt;00:03, 135.02it/s, 1023 steps of size 1.66e-04. acc. prob=0.71]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/1000 [00:04&lt;00:02, 136.83it/s, 722 steps of size 1.66e-04. acc. prob=0.68] sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/1000 [00:04&lt;00:03, 131.32it/s, 384 steps of size 1.66e-04. acc. prob=0.64]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/1000 [00:04&lt;00:02, 158.49it/s, 11 steps of size 1.66e-04. acc. prob=0.58] sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/1000 [00:05&lt;00:00, 345.40it/s, 39 steps of size 1.66e-04. acc. prob=0.39]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/1000 [00:05&lt;00:01, 230.55it/s, 1023 steps of size 1.66e-04. acc. prob=0.40]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/1000 [00:05&lt;00:01, 190.15it/s, 3 steps of size 1.66e-04. acc. prob=0.42]   sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [00:05&lt;00:01, 132.88it/s, 63 steps of size 1.66e-04. acc. prob=0.42]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/1000 [00:06&lt;00:01, 121.17it/s, 594 steps of size 1.66e-04. acc. prob=0.42]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/1000 [00:06&lt;00:01, 118.71it/s, 311 steps of size 1.66e-04. acc. prob=0.43]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/1000 [00:06&lt;00:01, 107.97it/s, 3 steps of size 1.66e-04. acc. prob=0.44]  sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 862/1000 [00:06&lt;00:01, 96.05it/s, 1023 steps of size 1.66e-04. acc. prob=0.45]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/1000 [00:06&lt;00:01, 115.66it/s, 1023 steps of size 1.66e-04. acc. prob=0.43]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [00:06&lt;00:00, 121.98it/s, 1023 steps of size 1.66e-04. acc. prob=0.42]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/1000 [00:07&lt;00:00, 123.84it/s, 1023 steps of size 1.66e-04. acc. prob=0.42]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/1000 [00:07&lt;00:00, 125.72it/s, 458 steps of size 1.66e-04. acc. prob=0.41] sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/1000 [00:07&lt;00:00, 116.93it/s, 3 steps of size 1.66e-04. acc. prob=0.41]  sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/1000 [00:07&lt;00:00, 101.22it/s, 1023 steps of size 1.66e-04. acc. prob=0.41]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/1000 [00:07&lt;00:00, 81.61it/s, 967 steps of size 1.66e-04. acc. prob=0.41]  sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/1000 [00:07&lt;00:00, 78.31it/s, 479 steps of size 1.66e-04. acc. prob=0.41]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/1000 [00:07&lt;00:00, 81.42it/s, 667 steps of size 1.66e-04. acc. prob=0.40]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/1000 [00:08&lt;00:00, 87.94it/s, 1023 steps of size 1.66e-04. acc. prob=0.40]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08&lt;00:00, 123.85it/s, 1023 steps of size 1.66e-04. acc. prob=0.40]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmeans[0]\n-1.45\n0.0\n-1.45\n-1.45\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\nmeans[1]\n-0.73\n0.0\n-0.73\n-0.72\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\nmeans[2]\n1.24\n0.0\n1.24\n1.25\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\nmeans[3]\n-0.60\n0.0\n-0.61\n-0.60\n0.0\n0.0\n1.34\n15.50\nNaN\n\n\noffset[0, 0]\n-0.64\n0.0\n-0.64\n-0.64\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\noffset[1, 0]\n0.75\n0.0\n0.75\n0.76\n0.0\n0.0\n1.34\n14.79\nNaN\n\n\noffset[2, 0]\n-1.14\n0.0\n-1.14\n-1.13\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\noffset[3, 0]\n0.10\n0.0\n0.09\n0.10\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\noffset[4, 0]\n-0.29\n0.0\n-0.30\n-0.29\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\noffset[5, 0]\n-0.80\n0.0\n-0.81\n-0.80\n0.0\n0.0\n1.34\n14.79\nNaN\n\n\noffset[6, 0]\n0.17\n0.0\n0.17\n0.17\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\noffset[7, 0]\n0.75\n0.0\n0.75\n0.75\n0.0\n0.0\n1.35\n14.79\nNaN\n\n\nsigma[0]\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n1.41\n10.90\nNaN\n\n\nsigma[1]\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n12.57\n10.86\nNaN\n\n\nsigma[2]\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n17.85\n12.09\nNaN\n\n\nsigma[3]\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n1.64\n11.56\nNaN",
    "crumbs": [
      "Models",
      "Latent Variable Models üöß"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#mathematical-details",
    "href": "18. Latent variable (wip).html#mathematical-details",
    "title": "Latent Variable Models (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details",
    "crumbs": [
      "Models",
      "Latent Variable Models üöß"
    ]
  },
  {
    "objectID": "11. Zero inflated.html",
    "href": "11. Zero inflated.html",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#general-principles",
    "href": "11. Zero inflated.html#general-principles",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "Zero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#example",
    "href": "11. Zero inflated.html#example",
    "title": "Zero-Inflated Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Zero-Inflated Poisson regression using the Bayesian Inference (BI) package. The data represent the production of books in a monastery (y), which is affected by the number of days that individuals work, as well as the number of days individuals drink. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi, jnp\n\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Simulated data------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n\n# Sample one year of production\nN = 365\ndrink = m.dist.binomial(1, prob_drink, shape = (N,), sample = True)\ny = (1 - drink) * m.dist.poisson(rate_work, shape = (N,), sample = True)\n\n# Setup device------------------------------------------------\nm.data_on_model = dict(\n    y=jnp.array(y)\n)\n\n# Define model ------------------------------------------------\ndef model(y):\n    al = m.dist.normal(1, 0.5, name='al')\n    ap = m.dist.normal(-1.5, 1, name='ap')\n    p = m.link.inv_logit(ap)\n    lambda_ = jnp.exp(al)\n    m.dist.zero_inflated_poisson(p, lambda_, obs=y)\n\n# Run MCMC ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:09,  1.33s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  10%|‚ñà         | 101/1000 [00:01&lt;00:09, 97.46it/s, 31 steps of size 1.02e+00. acc. prob=0.78]warmup:  20%|‚ñà‚ñâ        | 198/1000 [00:01&lt;00:03, 203.29it/s, 7 steps of size 6.86e-01. acc. prob=0.78]warmup:  29%|‚ñà‚ñà‚ñä       | 286/1000 [00:01&lt;00:02, 303.11it/s, 1 steps of size 1.55e+00. acc. prob=0.78]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 387/1000 [00:01&lt;00:01, 425.81it/s, 3 steps of size 3.84e-01. acc. prob=0.78]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/1000 [00:01&lt;00:00, 581.25it/s, 3 steps of size 6.62e-01. acc. prob=0.71]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/1000 [00:01&lt;00:00, 680.27it/s, 3 steps of size 6.62e-01. acc. prob=0.85]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/1000 [00:02&lt;00:00, 792.17it/s, 7 steps of size 6.62e-01. acc. prob=0.85]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/1000 [00:02&lt;00:00, 882.71it/s, 5 steps of size 6.62e-01. acc. prob=0.85]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/1000 [00:02&lt;00:00, 889.78it/s, 5 steps of size 6.62e-01. acc. prob=0.86]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 434.70it/s, 3 steps of size 6.62e-01. acc. prob=0.86]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nal\n0.01\n0.09\n-0.13\n0.15\n0.01\n0.00\n139.83\n196.67\nNaN\n\n\nap\n-1.28\n0.36\n-1.79\n-0.76\n0.03\n0.03\n163.47\n208.40\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Simulate data ------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n# sample one year of production\nN = as.integer(365)\ndrink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\ny = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\ndata = list()\ndata$y = y\nm$data_on_model = data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(y){\n  al = bi.dist.normal(1, 0.5, name='al', shape=c(1))\n  ap = bi.dist.normal(-1, 1, name='ap', shape=c(1))\n  p = m$link$inv_logit(ap)\n  lambda_ = jnp$exp(al)\n  bi.dist.zero_inflated_poisson(p, lambda_, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#references",
    "href": "11. Zero inflated.html#references",
    "title": "Zero-Inflated Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Zero-Inflated Models"
    ]
  },
  {
    "objectID": "start/Define model.html",
    "href": "start/Define model.html",
    "title": "Define model",
    "section": "",
    "text": "You can define your model by declaring a new function. The function should take the data as input and return the predicted values.\n\nPythonR\n\n\ndef model(Y, X):\n    # Define the model here\n    a = m.dist.normal(0, 1, name = 'a')\n    b = m.dist.normal(1, 1, name = 'b')\n    s = m.dist.exponential(1 name = 's')\n    \n    # Return the predicted values\n    m.dist.normal(a + b * X, obs = Y)\n\n\nmodel &lt;- function(Y, X){\n  # Define the model here\n    a = m.dist.normal(0, 1, name = 'a')\n    b = m.dist.normal(1, 1, name = 'b')\n    s = m.dist.exponential(1 name = 's')\n    m.dist.normal(a + b * X, obs = Y)\n}\n\n\n\nNote the additional obs argument when declaring the likelihood function. This argument is used to specify the observed data.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Define model"
    ]
  },
  {
    "objectID": "start/Import_data.html",
    "href": "start/Import_data.html",
    "title": "Import Data and handle it",
    "section": "",
    "text": "The BI class can import data from a csv file or from a dictionary for data that can‚Äôt be stored in a tabular format.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#import-tabular-data-from-a-csv-file",
    "href": "start/Import_data.html#import-tabular-data-from-a-csv-file",
    "title": "Import Data and handle it",
    "section": "Import tabular data from a csv file",
    "text": "Import tabular data from a csv file\n\nPythonR\n\n\nm.data(data_path, sep=';') \n\n\nm$data(data_path,  sep=';')",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#import-non-tabular-data",
    "href": "start/Import_data.html#import-non-tabular-data",
    "title": "Import Data and handle it",
    "section": "Import non tabular data",
    "text": "Import non tabular data\nFirst you need to create your own dictionary with the data.\n\nPythonR\n\n\nm.data_on_model = dict(\n    ID1 = Value1,\n    ID2 = Value2, \n)\n\n\nkeys &lt;- c(\"ID1\",\"ID2\")\nvalues &lt;- list(Value1,Value2)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Import_data.html#handle-data",
    "href": "start/Import_data.html#handle-data",
    "title": "Import Data and handle it",
    "section": "Handle data",
    "text": "Handle data\nFor tabular data, you can use some functions to manipulate the data:\n\nPerform one-hot encoding OHE: One-hot encoding is a technique that converts categorical variables into binary variables. This is useful when you have a large number of categories and want to use them as features in a model.\nCreate index encoding for categorical columns index: Index encoding is a technique that assigns a unique integer value to each category in a categorical variable. This is useful when you have a large number of categories and want to use them as features in a model.\nScale scale: Standardize the data by subtracting the mean and dividing the result by the standard deviation X_i = \\frac{X_i - \\mu}{\\sigma}.",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import Data and handle it"
    ]
  },
  {
    "objectID": "start/Fit model.html",
    "href": "start/Fit model.html",
    "title": "Fit model",
    "section": "",
    "text": "Once a function is defined declaring the model, you can fit the model to the data using the fit function.\n\nPythonR\n\n\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n\nm$fit(model) # Optimize model parameters through MCMC sampling",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Fit model"
    ]
  },
  {
    "objectID": "12. Survival analysis.html",
    "href": "12. Survival analysis.html",
    "title": "Survival Analysis üöß",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving until a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis üöß"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#general-principles",
    "href": "12. Survival analysis.html#general-principles",
    "title": "Survival Analysis üöß",
    "section": "",
    "text": "Survival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:\n\nHazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving until a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "Models",
      "Survival Analysis üöß"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#considerations",
    "href": "12. Survival analysis.html#considerations",
    "title": "Survival Analysis üöß",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nIn survival analysis:\n\nThe baseline hazard can follow distributions like Exponential, Weibull, or Gompertz, depending on the data.\nCensoring (when the event is not observed for some subjects) must be accounted for in the likelihood function. Proper handling is essential for unbiased results.\n\nBayesian survival models allow flexible handling of time-dependent covariates, random effects, and incorporate uncertainty more naturally than Frequentist methods.",
    "crumbs": [
      "Models",
      "Survival Analysis üöß"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#example",
    "href": "12. Survival analysis.html#example",
    "title": "Survival Analysis üöß",
    "section": "Example",
    "text": "Example\nHere‚Äôs an example of a Bayesian survival analysis using the Bayesian Inference (BI) package. The data come from a clinical trial of mastectomy for breast cancer. The goal is to estimate the effect of the metastasized covariate, coded as 0 (no metastasis) and 1 (metastasis), on the survival outcome event for each patient. Time is continuous and censoring is indicated by the event variable.\n\nPythonR\n\n\nfrom BI import bi\nimport numpy as np\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.mastectomy(only_path=True)\nm.data(data_path, sep=',') \n\nm.df.metastasized = (m.df.metastasized == \"yes\").astype(np.int64)\nm.df.event = jnp.array(m.df.event.values, dtype=jnp.int32)\n\n## Create survival object\nm.models.survival.surv_object(time='time', event='event', cov='metastasized', interval_length=3)\n\n# Plot censoring ------------------------------------------------\nm.models.survival.plot_censoring(cov='metastasized')\n\n# Model ------------------------------------------------\ndef model(intervals, death, metastasized, exposure):\n    # Parameter prior distributions-------------------------\n    ## Base hazard distribution\n    lambda0 = m.dist.gamma(0.01, 0.01, shape= intervals.shape, name = 'lambda0')\n    ## Covariate effect distribution\n    beta = m.dist.normal(0, 1000, shape = (1,),  name='beta')\n    ### Likelihood\n    #### Compute hazard rate based on covariate effect\n    lambda_ = m.models.survival.hazard_rate(cov = metastasized, beta = beta, lambda0 = lambda0)\n    #### Compute exposure rates\n    mu = exposure * lambda_\n\n    # Likelihood calculation\n    y = m.dist.poisson(mu + jnp.finfo(mu.dtype).tiny, obs = death)\n\n# Run mcmc ------------------------------------------------\nm.fit(model, num_samples=500) \n\n# Summary ------------------------------------------------\nprint(m.summary())\n\n# Plot hazards and survival function ------------------------------------------------\nm.models.survival.plot_surv()",
    "crumbs": [
      "Models",
      "Survival Analysis üöß"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#mathematical-details",
    "href": "12. Survival analysis.html#mathematical-details",
    "title": "Survival Analysis üöß",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n  ## Reference(s) https://en.wikipedia.org/wiki/Proportional_hazards_model https://www.mathworks.com/help/stats/cox-proportional-hazard-regression.html https://www.pymc.io/projects/examples/en/latest/survival_analysis/survival_analysis.html https://vflores-io.github.io/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/",
    "crumbs": [
      "Models",
      "Survival Analysis üöß"
    ]
  },
  {
    "objectID": "7. Poisson model.html",
    "href": "7. Poisson model.html",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable‚Äîe.g., counts of events occurring in a fixed interval of time or space‚Äîand one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials n is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#general-principles",
    "href": "7. Poisson model.html#general-principles",
    "title": "Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable‚Äîe.g., counts of events occurring in a fixed interval of time or space‚Äîand one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials n is unknown or uncountably large.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#considerations",
    "href": "7. Poisson model.html#considerations",
    "title": "Poisson Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nWe have the second link function üõà: log. The log link ensures that Œª is always positive.\nThe dependent variable in a Poisson regression must be a non-negative count.\nTo invert the log link function and linearly model the relationship between the predictor variables and the log of the mean rate parameter, we can apply the exponential function (see comment in code).\nA key assumption of the Poisson distribution is that the mean and variance of the count variable are equal. If the variance is greater than the mean, a condition known as overdispersion, a Gamma-Poisson model might be more appropriate.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#example",
    "href": "7. Poisson model.html#example",
    "title": "Poisson Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Poisson model using the Bayesian Inference (BI) package. Data consist of:\n\nA continuous dependent variable total_tools, which represents the number of tools produced by a civilization.\nA continuous independent variable population representing population size.\nA categorical independent variable cid representing different civilizations.\n\nThe goal is to estimate the production of tools based on population size, accounting for each civilization. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.kline(only_path = True)\nm.data(data_path, sep=';')\nm.scale(['population'])\nm.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n#m.data_to_model(['total_tools', 'population', 'cid'])\ndef model(cid, population, total_tools):\n    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n    l = jnp.exp(a[cid] + b[cid]*population)\n    m.dist.poisson(l, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;07:12,  2.31it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 388/1000 [00:00&lt;00:00, 962.45it/s, 15 steps of size 4.80e-01. acc. prob=0.79]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/1000 [00:00&lt;00:00, 1861.93it/s, 7 steps of size 5.19e-01. acc. prob=0.92]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1490.64it/s, 7 steps of size 5.19e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n3.22\n0.09\n3.07\n3.37\n0.01\n0.00\n289.74\n380.43\nNaN\n\n\na[1]\n3.63\n0.09\n3.50\n3.80\n0.00\n0.01\n444.71\n169.36\nNaN\n\n\nb[0]\n0.35\n0.05\n0.27\n0.42\n0.00\n0.00\n323.06\n445.13\nNaN\n\n\nb[1]\n0.04\n0.20\n-0.26\n0.36\n0.01\n0.01\n396.58\n324.48\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$kline(only_path = T), sep=';')\nm$scale(list('population'))# Scale\nm$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\nm$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(total_tools, population, cid){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n  l = jnp$exp(alpha[cid] + beta[cid]*population)\n  # Likelihood\n  m.dist.poisson(l, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#mathematical-details",
    "href": "7. Poisson model.html#mathematical-details",
    "title": "Poisson Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\nlog(\\lambda_i) = \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\log() is the log link function. This function links the log of the mean of the response variable, \\lambda_i, to the linear predictor, \\alpha + \\beta X_i. The logarithm is the canonical link function for the Poisson distribution. It ensures that the predicted mean, \\lambda_i = \\exp(\\alpha + \\beta X_i), will always be positive, as required for a Poisson rate parameter.\n\\alpha and \\beta are the intercept and regression coefficient, respectively, with their associated prior distributions.\nX_i is the value of the independent variable for observation i.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#notes",
    "href": "7. Poisson model.html#notes",
    "title": "Poisson Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\n\n\nReference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "7. Poisson model.html#references",
    "href": "7. Poisson model.html#references",
    "title": "Poisson Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html",
    "href": "8. Gamma-Poisson.html",
    "title": "Gamma-Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion üõà, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#general-principles",
    "href": "8. Gamma-Poisson.html#general-principles",
    "title": "Gamma-Poisson Model",
    "section": "",
    "text": "To model the relationship between a count outcome variable and one or more independent variables with overdispersion üõà, we can use the Negative Binomial model.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#considerations",
    "href": "8. Gamma-Poisson.html#considerations",
    "title": "Gamma-Poisson Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Poisson model.\nOverdispersion is handled because the Gamma-Poisson model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is log_days).",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#example",
    "href": "8. Gamma-Poisson.html#example",
    "title": "Gamma-Poisson Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package.\n\nPythonR\n\n\n\nfrom BI import bi, jnp\n\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path =  m.load.sim_gamma_poisson(only_path=True)\nm.data(data_path, sep=',') \nm.data_to_model(['log_days', 'monastery', 'y']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    phi = m.dist.exponential(1, name = 'phi', shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n    m.dist.poisson(rate = Lambda, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;09:22,  1.78it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   1%|          | 10/1000 [00:00&lt;00:53, 18.48it/s, 511 steps of size 9.70e-03. acc. prob=0.58]warmup:   2%|‚ñè         | 16/1000 [00:00&lt;00:36, 27.01it/s, 255 steps of size 1.31e-02. acc. prob=0.67]warmup:   2%|‚ñè         | 22/1000 [00:00&lt;00:28, 34.33it/s, 1023 steps of size 4.12e-03. acc. prob=0.68]warmup:   3%|‚ñé         | 28/1000 [00:01&lt;00:28, 34.61it/s, 255 steps of size 9.52e-03. acc. prob=0.71] warmup:   3%|‚ñé         | 33/1000 [00:01&lt;00:27, 35.78it/s, 511 steps of size 5.53e-03. acc. prob=0.71]warmup:   4%|‚ñç         | 40/1000 [00:01&lt;00:23, 41.06it/s, 511 steps of size 4.30e-03. acc. prob=0.72]warmup:   5%|‚ñç         | 47/1000 [00:01&lt;00:19, 47.88it/s, 41 steps of size 1.72e-03. acc. prob=0.72] warmup:   5%|‚ñå         | 53/1000 [00:01&lt;00:20, 46.74it/s, 511 steps of size 3.09e-03. acc. prob=0.73]warmup:   6%|‚ñå         | 59/1000 [00:01&lt;00:19, 48.20it/s, 511 steps of size 4.35e-03. acc. prob=0.74]warmup:   7%|‚ñã         | 68/1000 [00:01&lt;00:16, 57.29it/s, 255 steps of size 6.12e-03. acc. prob=0.75]warmup:   8%|‚ñä         | 75/1000 [00:01&lt;00:16, 57.52it/s, 511 steps of size 4.81e-03. acc. prob=0.75]warmup:   8%|‚ñä         | 82/1000 [00:02&lt;00:15, 59.87it/s, 127 steps of size 8.54e-03. acc. prob=0.76]warmup:   9%|‚ñâ         | 89/1000 [00:02&lt;00:15, 58.96it/s, 255 steps of size 2.71e-03. acc. prob=0.75]warmup:  10%|‚ñâ         | 96/1000 [00:02&lt;00:16, 54.85it/s, 255 steps of size 7.02e-03. acc. prob=0.76]warmup:  10%|‚ñà         | 102/1000 [00:02&lt;00:16, 53.45it/s, 63 steps of size 1.31e-01. acc. prob=0.76]warmup:  13%|‚ñà‚ñé        | 132/1000 [00:02&lt;00:07, 116.39it/s, 63 steps of size 1.39e-01. acc. prob=0.77]warmup:  16%|‚ñà‚ñã        | 165/1000 [00:02&lt;00:04, 168.90it/s, 127 steps of size 6.12e-02. acc. prob=0.77]warmup:  20%|‚ñà‚ñâ        | 196/1000 [00:02&lt;00:03, 206.33it/s, 31 steps of size 2.21e-01. acc. prob=0.78] warmup:  22%|‚ñà‚ñà‚ñè       | 224/1000 [00:02&lt;00:03, 225.30it/s, 31 steps of size 2.83e-01. acc. prob=0.78]warmup:  26%|‚ñà‚ñà‚ñå       | 259/1000 [00:02&lt;00:02, 260.20it/s, 15 steps of size 1.70e-01. acc. prob=0.78]warmup:  29%|‚ñà‚ñà‚ñâ       | 289/1000 [00:03&lt;00:02, 268.15it/s, 63 steps of size 1.10e-01. acc. prob=0.78]warmup:  32%|‚ñà‚ñà‚ñà‚ñé      | 325/1000 [00:03&lt;00:02, 292.59it/s, 31 steps of size 1.38e-01. acc. prob=0.78]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 359/1000 [00:03&lt;00:02, 305.44it/s, 31 steps of size 9.82e-02. acc. prob=0.78]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [00:03&lt;00:01, 310.77it/s, 31 steps of size 2.64e-01. acc. prob=0.78]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/1000 [00:03&lt;00:01, 321.13it/s, 31 steps of size 1.72e-01. acc. prob=0.78]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/1000 [00:03&lt;00:01, 318.83it/s, 31 steps of size 1.27e-01. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/1000 [00:03&lt;00:01, 306.48it/s, 31 steps of size 1.87e-01. acc. prob=0.78]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/1000 [00:03&lt;00:01, 268.35it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/1000 [00:03&lt;00:01, 265.32it/s, 31 steps of size 1.31e-01. acc. prob=0.90]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/1000 [00:04&lt;00:01, 259.08it/s, 31 steps of size 1.31e-01. acc. prob=0.88]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/1000 [00:04&lt;00:01, 247.37it/s, 31 steps of size 1.31e-01. acc. prob=0.88]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/1000 [00:04&lt;00:01, 243.73it/s, 31 steps of size 1.31e-01. acc. prob=0.88]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/1000 [00:04&lt;00:01, 258.17it/s, 31 steps of size 1.31e-01. acc. prob=0.88]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/1000 [00:04&lt;00:01, 276.84it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/1000 [00:04&lt;00:00, 299.13it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/1000 [00:04&lt;00:00, 312.32it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/1000 [00:04&lt;00:00, 326.80it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/1000 [00:04&lt;00:00, 335.72it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/1000 [00:04&lt;00:00, 336.00it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/1000 [00:05&lt;00:00, 344.09it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/1000 [00:05&lt;00:00, 345.12it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/1000 [00:05&lt;00:00, 342.81it/s, 31 steps of size 1.31e-01. acc. prob=0.89]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05&lt;00:00, 187.28it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nLambda[0]\n1.56\n0.38\n0.97\n2.11\n0.01\n0.02\n793.43\n367.44\nNaN\n\n\nLambda[1]\n1.51\n0.36\n1.02\n2.12\n0.01\n0.02\n877.99\n293.31\nNaN\n\n\nLambda[2]\n1.58\n0.38\n0.93\n2.12\n0.01\n0.02\n741.78\n372.31\nNaN\n\n\nLambda[3]\n1.40\n0.34\n0.93\n1.96\n0.01\n0.02\n937.17\n277.70\nNaN\n\n\nLambda[4]\n1.47\n0.37\n0.89\n2.00\n0.02\n0.02\n507.60\n428.36\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nLambda[3398]\n3.53\n0.77\n2.43\n4.67\n0.03\n0.04\n914.23\n399.13\nNaN\n\n\nLambda[3399]\n3.68\n0.79\n2.56\n5.06\n0.03\n0.06\n727.90\n235.65\nNaN\n\n\na[0]\n-0.42\n0.01\n-0.45\n-0.40\n0.00\n0.00\n40.06\n101.97\nNaN\n\n\nb[0]\n-2.75\n0.03\n-2.81\n-2.71\n0.00\n0.00\n57.05\n188.62\nNaN\n\n\nphi[0]\n17.63\n1.91\n14.32\n20.23\n0.56\n0.18\n12.25\n44.88\nNaN\n\n\n\n\n3403 rows √ó 9 columns\n\n\n\n\n\nlibrary(BayesianInference)\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$sim_gamma_poisson(only_path=T), sep = '')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  phi = bi.dist.exponential(1, name='phi', shape=c(1))\n  mu = jnp$exp(log_days + alpha + beta * monastery)\n  Lambda =  bi.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n  # Likelihood\n  bi.dist.poisson(rate=Lambda, obs=y)\n}\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#mathematical-details",
    "href": "8. Gamma-Poisson.html#mathematical-details",
    "title": "Gamma-Poisson Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\\lambda_i \\sim \\text{Gamma}(\\mu_i \\phi, \\phi)\n\n\n\\log(\\mu_i) = \\text{rates}_i + \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\n\n\\phi \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\lambda_i is the rate parameter of the Poisson distribution for observation i, assuming that each Poisson count observation has its own rate_i.\n\\mu_i is the mean rate parameter.\n\n\\phi controls the level of overdispersion in the rates.\n\\alpha is the intercept term.\n\\beta is the regression coefficient.\nX_i is the value of the predictor variable for observation i.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#notes",
    "href": "8. Gamma-Poisson.html#notes",
    "title": "Gamma-Poisson Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in chapter 2.\nWe can apply interaction terms similarly as in chapter 3.\nWe can apply categorical variables similarly as in chapter 4.",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#references",
    "href": "8. Gamma-Poisson.html#references",
    "title": "Gamma-Poisson Model",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Gamma-Poisson Model"
    ]
  },
  {
    "objectID": "api_dist.html",
    "href": "api_dist.html",
    "title": "Distributions",
    "section": "",
    "text": "Utils.np_dists.UnifiedDist is a class to unify various distribution methods and provide a consistent interface for sampling and inference.\n\nAsymmetric Laplace\nSamples from an Asymmetric Laplace distribution.\nThe Asymmetric Laplace distribution is a generalization of the Laplace distribution, where the two sides of the distribution are scaled differently. It is defined by a location parameter (loc), a scale parameter (scale), and an asymmetry parameter (asymmetry).\n\nf(x, \\kappa) = \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(-x\\kappa),\\quad x\\ge0\\\\\n= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(x/\\kappa),\\quad x&lt;0\\\\\n\nfor -\\infty &lt; x &lt; \\infty, \\kappa &gt; 0.\nlaplace_asymmetric takes ‚Äòkappa‚Äô as a shape parameter for \\kappa. For :\\kappa = 1, it is identical to a Laplace distribution\n\nArgs:\nbi.dist.asymmetric_laplace(\nloc=0.0,\nscale=1.0,\nasymmetry=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray or float): Location parameter of the distribution.\nscale (jnp.ndarray or float): Scale parameter of the distribution.\nasymmetry (jnp.ndarray or float): Asymmetry parameter of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\nvalidate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\n\n\n\nReturns:\n\nWhen sample=False: A BI AsymmetricLaplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the AsymmetricLaplace distribution (for direct sampling).\nWhen create_obj=True: The raw BI AsymmetricLaplace distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.asymmetric_laplace(loc=0.0, scale=1.0, asymmetry=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#asymmetriclaplace\n\n\nReferences\n[1] ‚ÄúAsymmetric Laplace distribution‚Äù, Wikipedia https://en.wikipedia.org/wiki/Asymmetric_Laplace_distribution\n[2] Kozubowski TJ and Podg√≥rski K. A Multivariate and Asymmetric Generalization of Laplace Distribution, Computational Statistics 15, 531‚Äì540 (2000). :doi:10.1007/PL00022717\n‚Äì\n\n\n\nAsymmetric Laplace Quantile\nSamples from an AsymmetricLaplaceQuantile distribution.\nThis distribution is an alternative parameterization of the AsymmetricLaplace distribution, commonly used in Bayesian quantile regression. It utilizes a quantile parameter to define the balance between the left- and right-hand sides of the distribution, representing the proportion of probability density that falls to the left-hand side.\n\nArgs:\nbi.dist.asymmetric_laplace_quantile(\nloc=0.0,\nscale=1.0,\nquantile=0.5,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (float): The location parameter of the distribution.\nsample (float): The scale parameter of the distribution.\nquantile (float): The quantile parameter, representing the proportion of probability density to the left of the median. Must be between 0 and 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI AsymmetricLaplaceQuantile distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the AsymmetricLaplaceQuantile distribution (for direct sampling).\nWhen create_obj=True: The raw BI AsymmetricLaplaceQuantile distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.asymmetric_laplace_quantile(loc=0.0, scale=1.0, quantile=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#asymmetriclaplacequantile\n\n\n\n\nBernoulli\nThe Bernoulli distribution models a single trial with two possible outcomes: success or failureIt models a single trial with exactly two possible outcomes:\n\nSuccess (often coded as 1) with probability (p),\nFailure (often coded as 0) with probability (1 - p).\n\nIt is a building block for many more complex discrete distributions (e.g.¬†binomial, geometric).\nWe write \nX \\sim {Bernoulli}(p)\n with 0 \\le p \\le 1.\n\n\nProbability Mass Function (PMF)\n\nP(X = x) =\n\\begin{cases}\np,  \\text{if } x = 1, \\\n1 - p,  \\text{if } x = 0, \\\n0,  \\text{otherwise.}\n\\end{cases}\n\nAlternatively, using an ‚Äúindicator‚Äù exponent form:\n\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in {0,1}.\n\n\n\nArgs:\nbi.dist.bernoulli(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray, optional): Probability of success for each Bernoulli trial. Must be between 0 and 1.\nlogits (jnp.ndarray, optional): Log-odds of success for each Bernoulli trial. probs = sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Bernoulli distribution object (for model building) when sample=False. JAX array of samples drawn from the Bernoulli distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli(probs=0.7, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#bernoulli\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Bernoulli_distribution ‚ÄúBernoulli distribution‚Äù\nhttps://stats.libretexts.org/Courses/Saint_Mary%27s_College_Notre_Dame/MATH_345__-Probability%28Kuter%29/3%3A_Discrete_Random_Variables/3.3%3A_Bernoulli_and_Binomial_Distributions ‚Äú3.3: Bernoulli and Binomial Distributions - Statistics LibreTexts‚Äù\n\n\n\n\n\nBernoulli Logits\nSamples from a Bernoulli distribution parameterized by logits.\nThe Bernoulli distribution models a single binary event (success or failure), parameterized by the log-odds ratio of success. The probability of success is given by the sigmoid function applied to the logit.\n\np = \\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\n\nwhere () is the log-odds (the logit).\n\nArgs:\nbi.dist.bernoulli_logits(\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlogits (jnp.ndarray, optional): Log-odds ratio of success. Must be real-valued.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building). Defaults to 0.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BernoulliLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli_logits(logits=jnp.array([0.2, 1, 2]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#bernoulli-logits\n\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Bernoulli_distribution ‚ÄúBernoulli distribution‚Äù\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Bernoulli ‚Äútfp. distributions.Bernoulli - Probability‚Äù\nhttps://mc-stan.org/docs/2_19/functions-reference/bernoulli-logit-distribution.html ‚Äú11.2 Bernoulli Distribution, Logit Parameterization - Stan‚Äù\n\n\n\n\n\nBernoulli Probs\nSamples from a Bernoulli distribution parameterized by probabilities.\nThe Bernoulli distribution models the probability of success in a single trial, where the outcome is binary (success or failure). It is characterized by a single parameter, probs, representing the probability of success.\n\nPr(X=1) = p = 1- Pr(X=0) = 1 - q, \\\\\nf(x)=p \\text{ if } k = 1 \\text{ else } q \\text{ if } k = 0\n\nwhere: p is the probability of success (0 &lt;= p &lt;= 1)\n\nArgs:\nbi.dist.bernoulli_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): The probability of success for each Bernoulli trial. Must be between 0 and 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BernoulliProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BernoulliProbs object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.bernoulli_probs(probs=jnp.array([0.2, 0.7, 0.5]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#bernoulliprobs\n\n\n\n\nBeta\nSamples from a Beta distribution, defined on the interval [0, 1]. The Beta distribution is a versatile distribution often used to model probabilities or proportions. It is parameterized by two positive shape parameters, usually denoted \\alpha and \\beta&gt;0, control the shape of the density (how much mass is pushed toward 0, 1, or intermediate).\n\nX \\sim Beta(\\alpha,\\beta), \\\\f(x)=\\frac{ x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}, \\\\\nB(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}, \\\\F(x)=I_{x}(\\alpha+\\beta)\n\nwhere B(\\alpha, \\beta) is the Beta function:\n\nB(\\alpha, \\beta) = \\int_0^1 x^{\\alpha - 1} (1 - x)^{\\beta - 1} , dx = \\frac{\\Gamma(\\alpha),\\Gamma (\\beta)}{\\Gamma(\\alpha + \\beta)}.\n\nwhere :num$` and \\beta are the concentration parameters, and B(x, y) is the Beta function.\n\nArgs:\nbi.dist.beta(\nconcentration1,\nconcentration0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration1 (jnp.ndarray): The first concentration parameter (shape parameter). Must be positive.\nconcentration0 (jnp.ndarray): The second concentration parameter (shape parameter). Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Beta distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Beta object (for advanced use cases).\n\n\n\n#### Example Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.beta(concentration1=1.0, concentration0=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#beta\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Beta_distribution ‚ÄúBeta distribution - Wikipedia‚Äù\nhttps://www.statlect.com/probability-distributions/beta-distribution ‚ÄúBeta distribution | Properties, proofs, exercises - StatLect‚Äù\nhttps://reference.wolfram.com/language/ref/BetaDistribution.html ‚ÄúBetaDistribution - Wolfram Language Documentation‚Äù\n\n\n\n\n\nBetaBinomial\nSamples from a BetaBinomial distribution, a compound distribution where the probability of success in a binomial experiment is drawn from a Beta distribution. This models situations where the underlying probability of success is not fixed but varies according to a prior belief represented by the Beta distribution. It is often used to model over-dispersion relative to the binomial distribution.\nFor X \\sim {BetaBinomial}(N,\\alpha,\\beta), for k = 0,1,\\dots,N:\n\n\\Pr(X = k) = \\binom{N}{k} ; \\frac{B(k + \\alpha,; N - k + \\beta)}{B(\\alpha,\\beta)},\n\nwhere (B(u,v)) is the Beta function (B(u,v) = \\frac{\\Gamma(u),\\Gamma(v)}{\\Gamma(u+v)}).\n\nArgs:\nbi.dist.beta_binomial(\nconcentration1,\nconcentration0,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration1 (jnp.ndarray): The first concentration parameter (alpha) of the Beta distribution.\nconcentration0 (jnp.ndarray): The second concentration parameter (beta) of the Beta distribution.\ntotal_count (jnp.ndarray): The number of Bernoulli trials in the Binomial part of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BetaBinomial distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaBinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI BetaBinomial distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.beta_binomial(concentration1=1.0, concentration0=1.0, total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#betabinomial\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Beta-binomial_distribution ‚ÄúBeta-binomial distribution‚Äù\nhttps://www.statisticshowto.com/beta-binomial-distribution/ ‚ÄúBeta-Binomial Distribution: Definition - Statistics How To‚Äù\nhttps://mc-stan.org/docs/2_19/functions-reference/beta-binomial-distribution.html ‚Äú12.3 Beta-Binomial Distribution | Stan Functions Reference‚Äù\n\n\n\n\n\nBeta Proportion\nThe Beta Proportion distribution is a reparameterization of the conventional Beta distribution in terms of a the variate mean and a precision parameter. It‚Äôs useful for modeling rates and proportions. It‚Äôs essentially the same family as the standard Beta((,)), but the mapping is:\n\n\\alpha = \\mu , \\kappa,\\quad \\beta = (1 - \\mu), \\kappa.\n\nThus the pdf is:\n\nf(\\theta \\mid \\mu, \\kappa) = \\frac{1}{B(\\mu\\kappa,,(1-\\mu)\\kappa)} ; \\theta^{\\mu\\kappa - 1} ; (1 - \\theta)^{(1 - \\mu)\\kappa         - 1}, \\\\\n\\theta \\in (0,1).\n\nThis parameterization is useful in hierarchical models, since one can put hyperpriors on () (the ‚Äúcenter‚Äù) and () (the ‚Äúspread‚Äù).\n\nArgs:\nbi.dist.beta_proportion(\nmean,\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nmean (jnp.ndarray): The mean of the BetaProportion distribution,must be between 0 and 1.\nconcentration (jnp.ndarray): The concentration parameter of the BetaProportion distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BetaProportion distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BetaProportion distribution (for direct sampling).\nWhen create_obj=True: The raw BI BetaProportion distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nsamples = m.dist.beta_proportion(mean=0.5, concentration=2.0, sample=True, shape=(1000,))\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#beta_proportion\n\n\nReferences:\nhttps://mc-stan.org/docs/2_19/functions-reference/beta-proportion-distribution.html ‚Äú19.2 Beta Proportion Distribution | Stan Functions Reference‚Äù\n\n\n\n\nBinomial\nThe Binomial distribution models the number of successes in a sequence of independent Bernoulli trials. It represents the probability of obtaining exactly k successes in n trials, where each trial has a probability p of success. It assumes:\n\nEach trial has exactly two possible outcomes (success or failure).\nThe probability of success $p is constant across trials.\nTrials are independent.\n\nWe denote: \nX \\sim {Binomial}(n, p).\n\n\nProbability Mass Function (PMF)\nFor k = 0,1,2,\\dots,n:\n\n\\Pr(X = k) = \\binom{n}{k} ; p^k ; (1 - p)^{,n - k},\n\nwhere \\binom{n}{k} = \\frac{n!}{k!(n-k)!}.\nThis formula counts the number of ways to choose which k out of n trials are the successes (the binomial coefficient) and then multiplies by the probability of each such configuration p^k (1-p)^{n-k}.\n\n\nArgs:\nbi.dist.binomial(\ntotal_count=1,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntotal_count (int): The number of trials n.\nprobs (jnp.ndarray, optional): The probability of success p for each trial. Must be between 0 and 1.\nlogits (jnp.ndarray, optional): The log-odds of success for each trial. probs = jax.nn.sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBinomial distribution object (for model building) when sample=False. JAX array of samples drawn from the Binomial distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial(total_count=10, probs=0.5, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#binomial\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Binomial_distribution ‚ÄúBinomial distribution‚Äù\nhttps://www.itl.nist.gov/div898/handbook/eda/section3/eda366i.htm ‚Äú1.3.6.6.18. Binomial Distribution - Information Technology Laboratory‚Äù\n\n\n\n\n\nBinomial Logits\nThe BinomialLogits distribution represents a binomial distribution parameterized by logits. It is useful when the probability of success is not directly known but is instead expressed as logits, which are the natural logarithm of the odds ratio.\n\np = {logit}^{-1}(\\alpha) = \\frac{1}{1 + e^{-\\alpha}}.\n\n\nArgs:\nbi.dist.binomial_logits(\nlogits,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlogits (jnp.ndarray): Log-odds of each success.\ntotal_count (int): Number of trials.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BinomialLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BinomialLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial_logits(logits=jnp.zeros(10), total_count=5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#binomialllogits\n\n\n\n\nBinomial Probs\nSamples from a Binomial distribution with specified probabilities for each trial.\nThe Binomial distribution models the number of successes in a sequence of independent Bernoulli trials, where each trial has the same probability of success.\nFor k = 0,1,2,\\dots,N:    \n\\Pr(X = k) = \\binom{N}{k} ; p^k ; (1 - p)^{,N - k}\n\nwhere \\binom{N}{k} = \\frac{N!}{k!(N-k)!}.\n\nArgs:\nbi.dist.binomial_probs(\nprobs,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): The probability of success for each trial. Must be between 0 and 1.\ntotal_count (int): The number of trials in each sequence.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI BinomialProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI BinomialLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.binomial_probs(probs=0.5, total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#binomialprobs\n\n\n\n\nConditional Autoregressive (CAR)\nThe CAR distribution models a vector of variables where each variable is a linear combination of its neighbors in a graph. The CAR model captures spatial dependence in areal data by modeling each observation as conditionally dependent on its neighbors.It specifies a joint distribution of a vector of random variables {y} = (y_1, y_2, \\dots, y_N) based on their conditional distributions, where each y_i is conditionally independent of all other variables given its neighbors.\n\nApplication: Widely used in disease mapping, environmental modeling, and spatial econometrics to account for spatial autocorrelation.\nConditional Distribution: Each y_i given its neighbors y_{-i} follows a normal distribution:\n\n\ny_i | y*{-i} \\sim N\\left( \\frac{\\sum*{j \\in N(i)} w_{ij} y_j}{\\sum_{j \\in N(i)} w_{ij}}, \\sigma^2 \\right)\n\nwhere:\n\n{N}(i) denotes the set of neighbors of unit i,\nw_{ij} are the weights representing the strength of the spatial relationship between units i and j,\n\\sigma^2 is the variance term. where \\mu_i is a function of the values of the neighbors of site i and \\Sigma_i is the variance of site i.\n\n\nArgs:\nbi.dist.car(\nloc,\ncorrelation,\nconditional_precision,\nadj_matrix,\nis_sparse=False,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (Union[float, Array]): Mean of the distribution.\ncorrelation (Union[float, Array]): Correlation between variables.\nconditional_precision (Union[float, Array]): Precision of the distribution.\nadj_matrix (Union[Array, scipy.sparse.spmatrix]): Adjacency matrix defining the graph. is_sparse (bool): Whether the adjacency matrix is sparse. Defaults to False.\n*validate_args/ (bool): Whether to validate arguments. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI CAR distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CAR object (for advanced use cases).\n\n\n\nReferences:\n\nhttps://www.pymc.io/projects/examples/en/latest/spatial/conditional_autoregressive_priors.html ‚ÄúConditional Autoregressive (CAR) Models for Spatial Data‚Äù\nhttps://mc-stan.org/learn-stan/case-studies/icar_stan.html ‚ÄúIntrinsic Auto-Regressive Models for Areal Data - Stan‚Äù\nhttps://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat08048 ‚ÄúConditional Autoregressive (CAR) Model - Schmidt‚Äù\n\n\n\n\n\nCategorical distribution.\nThe Categorical distribution, also known as the multinomial distribution, describes the probability of K different outcomes with probabilities {p_1, p_2, \\dots, p_K}.There‚Äôs no inherent ordering required among the categories.\n\nThe probabilities satisfy (p_i ) for all (i) and $ _{i=1}^K p_i = 1$ .\nIt‚Äôs essentially the distribution of a single draw from a discrete set of categories.\nIt generalises the Bernoulli distribution (which is the special case K=2).\n\nIt is commonly used to model discrete choices or classifications.\nIf X is a categorical random variable taking values in {1, 2, \\dots, K} with probabilities p_1, \\dots, p_K, then:\n\n\\Pr(X = i) = p_i, \\quad i = 1,\\dots, K.\n\nwhere p_k is the probability of outcome k, and the sum is over all possible outcomes.\n\nArgs:\nbi.dist.categorical(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): A 1D array of probabilities for each category. Must sum to 1.\nlogits (jnp.ndarray): A 1D array of unnormalized log probabilities for each category.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Categorical distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Categorical distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical(probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#categorical\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Categorical_distribution‚ÄùCategorical distribution‚Äù\nhttps://distribution-explorer.github.io/discrete/categorical.html‚ÄùCategorical distribution ‚Äî Probability Distribution Explorer documentation‚Äù\nhttps://static.hlt.bme.hu/semantics/external/pages/n-gram/en.wikipedia.org/wiki/Categorical_distribution.html‚ÄùCategorical distribution - Wikipedia‚Äù\n\n\n\n\n\nCategorical Logits\nSamples from a Categorical distribution with logits. This distribution represents a discrete probability distribution over a finite set of outcomes, where the probabilities are determined by the logits. The probability of each outcome is given by the softmax function applied to the logits.\nGiven logits $ = (_1, , _K) $, the probability of each category $ i $ is: \np_i = \\frac{e^{\\gamma_i}}{\\sum_{j=1}^{K} e^{\\gamma_j}}\n\nwhere $ p_i is the probability of category $ i , and the sum in the denominator ensures that the probabilities sum to 1.\n\nArgs:\nbi.dist.categorical_logits(\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlogits (jnp.ndarray): Log-odds of each category.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI CategoricalLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CategoricalLogits object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical_logits(logits=jnp.zeros(5), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#categoricallogits\n\n\n\n\nCategorical Probs distribution.\nSamples from a Categorical distribution.\nThe Categorical distribution is a discrete probability distribution that represents the probability of each outcome from a finite set of possibilities. It is often used to model the outcome of a random process with a fixed number of possible outcomes, such as the roll of a die or the selection of an item from a list.\n   \nP(x) = \\frac{probs_i}{\\sum_{k=1}^{K} probs_k}\n\n\nArgs:\nbi.dist.categorical_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): Probabilities for each category. Must sum to 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI CategoricalProbs distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI CategoricalProbs object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.categorical_probs(probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#categoricalprobs\n\n\n\n\nCauchy\nSamples from a Cauchy\nThe Cauchy distribution, also known as the Lorentz distribution, is a continuous probability distribution that arises frequently in various fields, including physics and statistics. It is characterized by its heavy tails, which extend indefinitely. This means it has a higher probability of extreme values compared to the normal distribution.\n\nParameters:\nLocation parameter (\\mu): Determines the peak of the distribution.\nScale parameter (\\gamma): Describes the half-width at half-maximum (HWHM) of the peak. \nf(x; \\mu, \\gamma) = \\frac{1}{\\pi \\gamma \\left[1 + \\left(\\frac{x - \\mu}{\\gamma}\\right)^2\\right]}\n\n\nwhere:\n\nx is the random variable,\n\\mu is the location parameter,\n\\gamma is the scale parameter.\n\n\nArgs:\nbi.dist.cauchy(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray or float, optional): Location parameter. Defaults to 0.0.\nsample (jnp.ndarray or float, optional): Scale parameter. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building). Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Cauchy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Cauchy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.cauchy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#cauchy\n\n\n\n\nChi-squared\nThe Chi-squared distribution is a continuous probability distribution that arises frequently in hypothesis testing, particularly in ANOVA and chi-squared tests. It is defined by a single positive parameter, degrees of freedom (df), the number of independent standard normal variables squared and summed, which determines the shape of the distribution.\n   \nf(x; k) = \\begin{cases}\n\\frac{x^{\\frac{k}{2} - 1} e^{-x/2}}{2^{\\frac{k}{2}} \\Gamma\\left(\\frac{k}{2}\\right)},  x &gt; 0 \\\n0,  \\text{otherwise}\n\\end{cases}\n\nwhere: * ( x ) is the random variable, * ( k ) is the degrees of freedom, * ( () ) is the gamma function.\n\nArgs:\nbi.dist.chi2(\ndf,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ndf (jnp.ndarray): Degrees of freedom. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Chi2 distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Chi2 object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu)\nm.dist.chi2(df=3.0, sample = True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#chi2\n\n\n\n\nCirculant Normal Multivariate normal\nCirculant Normal Multivariate normal distribution with covariance matrix ${C}that is positive-definite and circulant [1], i.e., has periodic boundary conditions. The density of a sample ${x}\\in{R}^n is the standard multivariate normal density\n\np\\left(x\\mid \\mu,{C}\\right) =\n\\frac{\\left({det}\\,{C}\\right)^{-1/2}}{\\left(2\\pi\\right)^{n / 2}}\n\\exp\\left(-\\frac{1}{2}\\left({x}- \\mu\\right)^\\intercal\n{C}^{-1}\\left({x}-\\mu\\right)\\right),\n\nwhere {det} denotes the determinant and ^\\intercal the transpose. Circulant matrices can be diagnolized efficiently using the discrete Fourier transform, allowing the log likelihood to be evaluated in n \\log n time for n observations.\n\nloc: Mean of the distribution \\mu.\ncovariance_row: First row of the circulant covariance matrix C. Because of periodic boundary conditions, the covariance matrix is fully determined by its first row (see jax.scipy.linalg.toeplitz for further details).\ncovariance_rfft: Real part of the real fast Fourier transform of :code:covariance_row, the first row of the circulant covariance matrix C.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\nReferences:\n\nWikipedia. (n.d.). Circulant matrix. Retrieved March 6, 2025, from https://en.wikipedia.org/wiki/Circulant_matrix\nWood, A. T. A., & Chan, G. (1994). Simulation of Stationary Gaussian Processes in \\left[0, 1\\right]^d. Journal of Computational and Graphical Statistics, 3(4), 409‚Äì432. https://doi.org/10.1080/10618600.1994.10474655\n\n\nArgs:\nbi.dist.circulant_normal(\nloc: jax.Array,\ncovariance_row: jax.Array = None,\ncovariance_rfft: jax.Array = None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc : jnp.ndarray Mean of the distribution \\mu.\ncovariance_row : jnp.ndarray, optional. First row of the circulant covariance matrix C. Defaults to None.\ncovariance_rfft : jnp.ndarray, optional Real part of the real fast Fourier transform of covariance_row. Defaults to None.\n\n\n\nReturns:\n\nWhen sample=False: A BI Circulant Normal Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Circulant Normal Distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#circulantnormal\n\n\nReferences:\nWikipedia. (n.d.). Circulant matrix. Retrieved March 6, 2025, from https://en.wikipedia.org/wiki/Circulant_matrix\nWood, A. T. A., & Chan, G. (1994). Simulation of Stationary Gaussian Processes in [0,1]^d. Journal of Computational and Graphical Statistics, 3(4), 409‚Äì432. https://doi.org/10.1080/10618600.1994.10474655\n\n\n\n\nDelta\nThe Delta distribution, also known as a point mass distribution, assigns probability 1 to a single point and 0 elsewhere. It‚Äôs useful for representing deterministic variables or as a building block for more complex distributions.\n   \nF(x) =\n\\begin{cases}\n0,  x &lt; a, \\\n1,  x \\ge a.\n\\end{cases}\n\n\nArgs:\nbi.dist.delta(\nv=0.0,\nlog_density=0.0,\nevent_dim=0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nv (jnp.ndarray): The location of the point mass.\nlog_density (float, optional): The log probability density of the point mass. This is primarily for creating distributions that are non-normalized or for specific advanced use cases. For a standard delta distribution, this should be 0. Defaults to 0.0.\nevent_dim (int, optional): The number of rightmost dimensions of v to interpret as event dimensions. Defaults to 0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Circulant Delta Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the BernoulliLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI Circulant Delta Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.delta(v=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#delta\n\n\n\n\nDirichlet\nSamples from a Dirichlet distribution.\nThe Dirichlet distribution is a multivariate generalization of the Beta distribution. It is a probability distribution over a simplex, which is a set of vectors where each element is non-negative and sums to one. It is often used as a prior distribution for categorical distributions.\nFor X = (X_1, \\dots, X_K \\sim {Dir}(\\alpha), the density is\n\nf( x; \\alpha) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n\nfor x in the simplex, and 0 otherwise. Here B(\\alpha) is the multivariate Beta function (the normalization constant):\n\nB(\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}.\n\n\nArgs:\nbi.dist.dirichlet(\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The concentration parameter(s) of the Dirichlet distribution. Must be a positive array.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Dirichlet Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Dirichlet distribution (for direct sampling).\nWhen create_obj=True: The raw BI Dirichlet Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.dirichlet(concentration=jnp.array([1.0, 1.0, 1.0]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#dirichlet\n\n\nReferences\nhttps://distribution-explorer.github.io/multivariate_continuous/dirichlet.html https://en.wikipedia.org/wiki/Dirichlet_distribution\n\n\n\n\nDirichlet-Multinomial\nCreates a Dirichlet-Multinomial compound distribution, which is a Multinomial distribution with a Dirichlet prior on its probabilities. It is often used in Bayesian statistics to model count data where the proportions of categories are uncertain.\nThe Dirichlet-Multinomial distribution describes a model in which you first draw a probability vector \n{p} = (p_1, \\dots, p_K) \\sim {Dirichlet}(\\alpha)\n and then you draw counts \n{x} = (x_1, \\dots, x_K) \\sim {Multinomial}(n, {p}).\n\nIntegrating out {p} gives you the marginal distribution of ${x}). * It generalizes the Beta-Binomial model to multiple categories (i.e.¬†(K &gt; 2)). * A key feature is that it introduces overdispersion relative to a standard multinomial: counts are more ‚Äúclumped‚Äù because the underlying probabilities vary according to the Dirichlet prior.\n\nArgs:\nbi.dist.dirichlet_multinomial(\nconcentration,\ntotal_count=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The concentration parameter (alpha) for the Dirichlet prior. Values must be positive. The last dimension is interpreted as the number of categories.\ntotal_count (int, jnp.ndarray, optional): The total number of trials (n). This must be a non-negative integer. Defaults to 1.\nvalidate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\n\nWhen sample=False: A BI dirichlet_multinomial Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the dirichlet_multinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI dirichlet_multinomial Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling\n# Sample a single vector of counts for 10 trials from 3 categories\ncounts = m.dist.dirichlet_multinomial(concentration=jnp.array([1.0, 1.0, 1.0]),total_count=10,sample=True)\n\n# Usage within a model\ndef my_model(obs_data=None):\n# Define a prior on the concentration parameter\nalpha = m.dist.half_cauchy(scale=jnp.ones(5), name='alpha', shape=(5,))\n\n# Model observed counts\nwith m.plate('data', len(obs_data)):\ny = m.dist.dirichlet_multinomial(\nconcentration=alpha,\ntotal_count=100,\nname='y',\nobs=obs_data\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#dirichletmultinomial\n\n\nReferences:\nhttps://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet_multinomial.html\n\n\n\n\nDiscrete Uniform\nSamples from a Discrete Uniform distribution.\nThe Discrete Uniform distribution defines a uniform distribution over a range of integers. It is characterized by a lower bound (low) and an upper bound (high), inclusive.\n   \nP(X = k) = \\frac{1}{high - low + 1}, \\quad k \\in \\{low, low+1, ..., high\\}\n\nOtherwise (if k is outside the range), $ P(X = k) = 0 $.\n\nArgs:\nbi.dist.discrete_uniform(\nlow=0,\nhigh=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlow (jnp.ndarray): The lower bound of the uniform range, inclusive.\nhigh (jnp.ndarray): The upper bound of the uniform range, inclusive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI DiscreteUniform Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the DiscreteUniform distribution (for direct sampling).\nWhen create_obj=True: The raw BI DiscreteUniform Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.discrete_uniform(low=0, high=5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#discreteuniform\n\n\nReferences:\n\n\n\n\nDoubly Truncated Power Law\nThis distribution represents a continuous power law with a finite support bounded between low and high, and with an exponent alpha. It is normalized over the interval [low, high] to ensure the area under the density function is 1.\nKey parameters:\n\n\\alpha &gt; 0: shape (exponent) of the power law\nx_{min}: lower cutoff (minimum possible value)\nx_{\\max}: upper cutoff (maximum possible value)\n\nThe probability density function (PDF) is defined as:\n\nf(x) = \\begin{cases}\n\\displaystyle \\frac{(1-\\alpha), x^{-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{min}^{1-\\alpha}},  \\alpha \\neq 1, ; x_{min} \\le x         \\le x_{\\max},\\\n\\displaystyle \\frac{1}{x, \\ln\\left(x_{\\max}/x_{min}\\right)},  \\alpha = 1, ; x_{min} \\le x \\le x_{\\max},\\\n0,  \\text{otherwise}.\n\\end{cases}\n\nwhere the normalization constant Z(\\alpha,a,b) is given by\n\nZ(\\alpha,a,b) =\n\\begin{cases}\n\\log(b) - \\log(a),  \\text{if } \\alpha = -1, \\\\\n\\dfrac{b^{1+\\alpha} - a^{1+\\alpha}}{1+\\alpha},  \\text{otherwise}.\n\\end{cases}\n\nThis distribution is useful for modeling data that follows a power-law behavior but is naturally bounded due to measurement or theoretical constraints (e.g., finite-size systems).\n\nArgs:\nbi.dist.doubly_truncated_power_law(\nalpha,\nlow,\nhigh,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nalpha (float or array-like): Power-law exponent.\nlow (float or array-like): Lower bound of the distribution (must be ‚â• 0).\nhigh (float or array-like): Upper bound of the distribution (must be &gt; 0).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\n*validate_args (bool, optional): Whether to validate the arguments. Defaults to True.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI doubly_truncated_power_law Distribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the doubly_truncated_power_law distribution (for direct sampling).\nWhen create_obj=True: The raw BI doubly_truncated_power_law Distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')        \nm.dist.doubly_truncated_power_law(low=0.1, high=10.0, alpha=2.0, sample=True)\n\n\nReferences:\nhttps://reference.wolframcloud.com/language/ref/TruncatedDistribution.html\n\n\n\n\nEuler‚ÄìMaruyama\nEuler‚ÄìMaruyama methode is a method for the approximate numerical solution of a stochastic differential equation (SDE). It simulates the solution to an SDE by iteratively applying the Euler method to each time step, incorporating a random perturbation to account for the diffusion term.\n\ndX_t = f(X_t, t) dt + g(X_t, t) dW_t\n\nwhere: - $X_tis the state of the system at time $t. - $f(X_t, t)is the drift coefficient. - $g(X_t, t) is the diffusion coefficient. - $dW_t` is a Wiener process (Brownian motion).\n\nArgs:\nbi.dist.euler_maruyama(\nt,\nsde_fn,\ninit_dist,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nt (jnp.ndarray): Discretized time steps.\nsde_fn (callable): A function that takes the current state and time as input and returns the drift and diffusion coefficients.\ninit_dist (Distribution): The initial distribution of the system.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\n*sample_shape (tuple, optional): The shape of the samples to draw. Defaults to None.\n*validate_args (bool, optional): Whether to validate the arguments. Defaults to True.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\njnp.ndarray: Samples drawn from the Euler‚ÄìMaruyama distribution.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.euler_maruyama(t=jnp.array([0.0, 0.1, 0.2]), sde_fn=lambda x, t: (x, 1.0), init_dist=m.dist.normal(0.0, 1.0, create_obj=True), sample = True)\n\n\n\n\nExponential\nThe Exponential distribution is a continuous probability distribution that models the time until an event occurs in a Poisson process, where events occur continuously and independently at a constant average rate. It is often used to model the duration of events, such as the time until a machine fails or the length of a phone call.\nFor $x $    \nF(x; \\lambda) = 1 - e^{-\\lambda x}\n\nZhere ( &gt; 0 ) is the rate parameter (the mean is ( 1/)).\n\nArgs:\nbi.dist.exponential(\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nrate (jnp.ndarray): The rate parameter, $`. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Exponential distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Exponential distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.exponential(rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#exponential\n\n\nReferences\nhttps://reference.wolfram.com/language/ref/ExponentialDistribution.html https://en.wikipedia.org/wiki/Exponential_distribution\n\n\n\n\nFolded\nSamples from a Folded distribution, which is the absolute value of a base univariate distribution. This distribution reflects the base distribution across the origin, effectively taking the absolute value of each sample.\n\np(x) = \\sum_{k=-\\infty}^{\\infty} p(x - 2k)\n\n\nArgs:\nbi.dist.folded_distribution(\nbase_dist,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (float, optional): Location parameter of the base distribution. Defaults to 0.0.\nsample (float, optional): Scale parameter of the base distribution. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI FoldedDistribution distribution object (for model building) when sample=False. JAX array of samples drawn from the FoldedDistribution distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.folded_distribution(m.dist.normal(loc=0.0, scale=1.0, create_obj = True), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#foldeddistribution\n\n\n\n\nGamma\nSamples from a Gamma distribution.\nThe Gamma distribution is a continuous probability distribution frequently used in Bayesian statistics, particularly as a prior distribution for variance parameters. It is defined by two positive shape parameters: concentration (k) and rate (\\lambda).\nThere are two common parameterizations:\n\nShape‚Äìscale: parameters (\\alpha, \\theta), with density \nf(x) = \\frac{1}{\\Gamma(\\alpha) \\theta^\\alpha} x^{\\alpha - 1} e^{-x/\\theta}, \\quad x \\ge 0\n\nShape‚Äìrate: parameters (\\alpha, \\lambda) where \\lambda = 1/\\theta, with density \nf(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\lambda x}, \\quad x \\ge 0\n\n\n\nArgs:\nbi.dist.gamma(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The shape parameter of the Gamma distribution (k &gt; 0).\nrate (jnp.ndarray): The rate parameter of the Gamma distribution (theta &gt; 0).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nGamma: A BI Gamma distribution object (for model building).\njnp.ndarray: A JAX array of samples drawn from the Gamma distribution (for direct sampling).\nGamma: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gamma(concentration=2.0, rate=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gamma\n\n\nReferences\nhttps://reference.wolfram.com/language/ref/GammaDistribution.html https://en.wikipedia.org/wiki/Gamma_distribution\n\n\n\n\nGamma Poisson\nThe Gamma-Poisson distribution, also known as the Negative Binomial distribution, models overdispersed count data. It arises from a hierarchical process where the rate parameter of a Poisson distribution is itself a random variable following a Gamma distribution. This structure allows the model to capture variability in count data that exceeds what is predicted by the Poisson distribution, making it suitable for applications like modeling RNA-sequencing data and microbial count.\n\nf(x) = \\frac{\\Gamma(x + \\beta) \\alpha^x}{\\Gamma(\\beta)(1 + \\alpha)^{\\beta + x} x!}, \\quad x = 0, 1, 2, \\ldots\n Where \\Gamma denotes the gamma function.\nThis CDF expresses the probability that the count variable ( X ) takes a value less than or equal to ( x ), incorporating the overdispersion introduced by the Gamma distribution.\n\nArgs:\nbi.dist.gamma_poisson(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): Shape parameter (alpha) of the Gamma distribution.\nrate (jnp.ndarray): Rate parameter (beta) for the Gamma distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI GammaPoisson distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the GammaPoisson distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gamma_poisson(concentration=1.0, rate=2.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gammapoisson\n\n\nReferences\n[1] https://www.statisticshowto.com/gamma-poisson-distribution https://en.wikipedia.org/wiki/Negative_binomial_distribution\n\n\n\n\nGaussian Copula\nA distribution that links the batch_shape[:-1] of a marginal distribution with a multivariate Gaussian copula, modelling the correlation between the axes. A copula is a multivariate distribution over the uniform distribution on [0, 1]. The Gaussian copula links the marginal distributions through a multivariate normal distribution.\n\nArgs:\nbi.dist.gaussian_copula(\nmarginal_dist,\ncorrelation_matrix=None,\ncorrelation_cholesky=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nmarginal_dist (Distribution): Distribution whose last batch axis is to be coupled.\ncorrelation_matrix (array_like, optional): Correlation matrix of the coupling multivariate normal distribution. Defaults to None.\ncorrelation_cholesky (array_like, optional): Correlation Cholesky factor of the coupling multivariate normal distribution. Defaults to None.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like Mi  xtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI GaussianCopula distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_copula(\nmarginal_dist = m.dist.beta(2.0, 5.0, create_obj = True), \ncorrelation_matrix = jnp.array([[1.0, 0.7],[0.7, 1.0]]), \nsample = True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussiancopula\n\n\n\n\nGaussian Copula Beta\nThis distribution combines a Gaussian copula with a Beta distribution. The Gaussian copula models the dependence structure between random variables, while the Beta distribution defines the marginal distributions of each variable.\n\nArgs:\nbi.dist.gaussian_copula_beta(\nconcentration1,\nconcentration0,\ncorrelation_matrix=None,\ncorrelation_cholesky=None,\nvalidate_args=False,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration1 (jnp.ndarray): The first shape parameter of the Beta distribution.\nconcentration0 (jnp.ndarray): The second shape parameter of the Beta distribution.\ncorrelation_matrix (array_like, optional): Correlation matrix of the coupling multivariate normal distribution. Defaults to None.\ncorrelation_cholesky (jnp.ndarray): The Cholesky decomposition of the correlation matrix.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nGaussianCopulaBeta: A BI GaussianCopulaBeta distribution object (for model building). jnp.ndarray: A JAX array of samples drawn from the GaussianCopulaBeta distribution (for direct sampling). Distribution: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_copula_beta(\nconcentration1 = jnp.array([2.0, 3.0]), \nconcentration0 = jnp.array([5.0, 3.0]),\ncorrelation_cholesky = jnp.linalg.cholesky(jnp.array([[1.0, 0.7],[0.7, 1.0]])), \nsample = True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussiancopulabetadistribution\n\n\n\n\nGaussian Random Walk\nCreates a distribution over a Gaussian random walk of a specified number of steps. This is a discrete-time stochastic process where the value at each step is the previous value plus a Gaussian-distributed increment. The distribution is over the entire path.\n\nArgs:\nbi.dist.gaussian_random_walk(\nscale=1.0,\nnum_steps=1,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (float, jnp.ndarray, optional): The standard deviation ($`) of the Gaussian increments. Must be positive. Defaults to 1.0.\n*num_steps (int, optional): The number of steps in the random walk, which determines the event shape of the distribution. Must be positive. Defaults to 1.\n*validate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\nBI.primitives.Messenger: A BI sample site object when used in a model context (sample=False). jnp.ndarray: A JAX array of samples drawn from the GaussianRandomWalk distribution (for direct sampling, sample=True). numpyro.distributions.Distribution: The raw BI distribution object (if create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling of a random walk with 100 steps\npath = m.dist.gaussian_random_walk(scale=0.5, num_steps=100, sample=True)\n\n# Usage within a model for a latent time series\ndef my_model(data=None):\n# Prior on the volatility of the random walk\nvolatility = m.dist.half_cauchy(scale=1.0, name='volatility')\n\n# The latent random walk\nlatent_process = m.dist.gaussian_random_walk(\nscale=volatility,\nnum_steps=len(data) if data is not None else 10,\nname='latent_process'\n)\n\n# Observation model\n# Assumes the observed data is the latent process plus some noise\nobs_noise = m.dist.half_cauchy(scale=1.0, name='obs_noise')\nwith m.plate('time', len(data) if data is not None else 10):\nreturn m.dist.normal(loc=latent_process, scale=obs_noise, obs=data, name='obs')\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussianrandomwalk\n\n\n\n\nGaussian State Space\nSamples from a Gaussian state space model.\n\nArgs:\nbi.dist.gaussian_state_space(\nnum_steps,\ntransition_matrix,\ncovariance_matrix=None,\nprecision_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nnum_steps (int): Number of steps.\ntransition_matrix (jnp.ndarray): State space transition matrix ${A}`.\ncovariance_matrix (jnp.ndarray, optional): Covariance of the innovation noise \\epsilon. Defaults to None.\n*precision_matrix (jnp.ndarray, optional): Precision matrix of the innovation noise \\epsilon. Defaults to None.\n*scale_tril (jnp.ndarray, optional): Scale matrix of the innovation noise {\\epsilon}. Defaults to None.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI GaussianStateSpace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the GaussianStateSpace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gaussian_state_space(num_steps=5, transition_matrix=jnp.array([[0.5]]), covariance_matrix =  jnp.array([[1.0, 0.6],[0.6, 1.0]]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gaussianstate\n\n\n\n\nGeometric distribution.\nThe Geometric distribution models the number of independent Bernoulli trials needed to obtain the first success (or equivalently the number of failures before the first success). Each trial has the same probability of success p, and trials are independent.\n\nX = number of trials until the first success (so support 1,2,3,\\dots).\n\n\nP(X = k) = (1 - p)^{,k-1} ; p,\\quad k = 1,2,\\dots\n\n\nArgs:\nbi.dist.geometric(\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray, optional): Probability of success on each trial. Must be between 0 and 1.\nlogits (jnp.ndarray, optional): Log-odds of success on each trial. probs = jax.nn.sigmoid(logits).\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Geometric distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Geometric distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric(probs=0.5, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#geometric\n\n\nReferences:\n\nhttps://www.britannica.com/topic/geometric-distribution ‚ÄúGeometric distribution | Definition, Formula, Examples, Illustration, & Applications | Britannica‚Äù\nhttps://en.wikipedia.org/wiki/Geometric_distribution ‚ÄúGeometric distribution‚Äù\nhttps://distribution-explorer.github.io/discrete/geometric.html ‚ÄúGeometric distribution ‚Äî Probability Distribution Explorer documentation‚Äù\n\n\n\n\n\nGeometricLogits\nSamples from a GeometricLogits distribution, which models the number of failures before the first success in a sequence of independent Bernoulli trials. It is parameterized by logits, which are transformed into probabilities using the sigmoid function.\n\nP(X = k) = (1 - p)^k , p,\n\nwhere:\n\nX is the number of failures before the first success.\nk is the number of failures.\np is the probability of success on each trial (derived from the logits).\n\n\nArgs:\nbi.dist.geometric_logits(\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlogits (jnp.ndarray): Log-odds parameterization of the probability of success.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI GeometricLogits distribution object (for model building) when sample=False. JAX array of samples drawn from the GeometricLogits distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric_logits(logits=jnp.zeros(10), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#geometriclogits\n\n\nReferences:\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Geometric\n\n\n\n\nGeometricProbs\nSamples from a Geometric\nThe Geometric distribution models the number of trials until the first success in a sequence of independent Bernoulli trials, where each trial has the same probability of success.\n\nP(X = k) = (1 - p)^k p\n\n\nArgs:\nbi.dist.geometric_probs(\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): Probability of success on each trial. Must be between 0 and 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI GeometricProbs distribution object (for model building). JAX array of samples drawn from the GeometricProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.geometric_probs(probs=0.5, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#geometricprobs\n\n\n\n\nGompertz\nThe Gompertz distribution is a distribution with support on the positive real line that is closely related to the Gumbel distribution. This implementation follows the notation used in the Wikipedia entry for the Gompertz distribution. See https://en.wikipedia.org/wiki/Gompertz_distribution.\nThe probability density function (PDF) is:\n\nF(x) = 1 - \\exp!\\bigl[-a (e^x - 1)\\bigr], \\quad x \\ge 0.\n\n\nArgs:\nbi.dist.gompertz(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The concentration parameter. Must be positive.\nrate (jnp.ndarray): The rate parameter. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Gompertz distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gompertz(concentration=1.0, rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gompertz #### References: https://en.wikipedia.org/wiki/Gompertz_distribution\n\n\n\n\nGumbel\nSamples from a Gumbel (or Extreme Value) distribution.\nThe Gumbel distribution is a continuous probability distribution named after German mathematician Carl Gumbel. It is often used to model the distribution of maximum values in a sequence of independent random variables.\n\nF(x;\\mu,\\beta) = \\exp\\bigl( -\\exp\\bigl( -\\tfrac{x - \\mu}{\\beta} \\bigr) \\bigr), \\quad x \\in {R}.\n\n\nArgs:\nbi.dist.gumbel(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray or float, optional): Location parameter. Defaults to 0.0.\nscale (jnp.ndarray or float, optional): Scale parameter. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building). Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. Defaults to None.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Gumbel distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Gumbel distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.gumbel(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#gumbel #### References: https://en.wikipedia.org/wiki/Gumbel_distribution\n\n\n\n\nHalfCauchy\nThe HalfCauchy distribution is a probability distribution that is half of the Cauchy distribution. It is defined on the positive real numbers and is often used in situations where only positive values are relevant.\n   \nf(x) = \\frac{1}{2} \\cdot \\frac{1}{\\pi \\cdot \\frac{1}{scale} \\cdot (x^2 + \\frac{1}{scale^2})}\n\n\nArgs:\nbi.dist.half_cauchy(\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (jnp.ndarray): The scale parameter of the Cauchy distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI HalfCauchy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the HalfCauchy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.half_cauchy(scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#halfcauchy\n\n\n\n\nHalfNormal\nSamples from a HalfNormal distribution.\nThe HalfNormal distribution is a distribution of the absolute value of a normal random variable. It is defined by a location parameter (implicitly 0) and a scale parameter.\n\nf(x; \\sigma) = \\frac{2}{\\pi,\\sigma} ; \\frac{1}{1 + \\bigl(x/\\sigma\\bigr)^2}, \\qquad x \\ge 0.\n\n\nArgs:\nbi.dist.half_normal(\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (float, array): The scale parameter of the distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI HalfNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the HalfNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.half_normal(scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#halfnormal\n\n\nReferences:\nhttps://distribution-explorer.github.io/continuous/halfcauchy.html\n\n\n\n\nImproper Uniform\nA helper distribution with zero :meth:log_prob over the support domain.\n\nArgs:\nbi.dist.improper_uniform(\nsupport,\nbatch_shape,\nevent_shape,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nsupport (numpyro.distributions.constraints.Constraint): The support of this distribution.\nbatch_shape (tuple): Batch shape of this distribution. It is usually safe to set batch_shape=().\nevent_shape (tuple): Event shape of this distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI ImproperUniform distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ImproperUniform distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import sample\nfrom numpyro.distributions import ImproperUniform, Normal, constraints\n\ndef model():\nx = sample('x', ImproperUniform(constraints.ordered_vector, (), event_shape=(10,)))\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#improperuniform\n\n\n\n\nInverse Gamma\nThe InverseGamma distribution is a two-parameter family of continuous probability distributions. It is defined by its shape \\alpha and rate \\beta parameters. It is often used as a prior distribution for precision parameters (inverse variance) in Bayesian statistics.\n\nf(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{-(\\alpha + 1)} \\exp\\left(-\\frac{\\beta}{x}\\right), \\quad x &gt; 0\n\nwhere: * (()) is the Gamma function.\n\nArgs:\nbi.dist.inverse_gamma(\nconcentration,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The concentration parameter \\alpha of the InverseGamma distribution. Must be positive.\nrate (jnp.ndarray): The rate parameter \\beta of the InverseGamma distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI InverseGamma distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the InverseGamma distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.inverse_gamma(concentration=2.0, rate=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#inversegamma\n\n\n\n\nKumaraswamy\nThe Kumaraswamy distribution is a continuous probability distribution defined on the interval [0, 1]. It is a flexible distribution that can take on various shapes depending on its parameters.\n\nf(x; a, b) = a b x^{a-1} (1 - x^a)^{b-1}, \\quad x \\in (0, 1)\n\nwhere: * ( a ) and ( b ) are shape parameters. * ( x ) is the random variable.\n\nArgs:\nbi.dist.kumaraswamy(\nconcentration1,\nconcentration0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration1 (jnp.ndarray): The first shape parameter. Must be positive.\nconcentration0 (jnp.ndarray): The second shape parameter. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Kumaraswamy distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Kumaraswamy distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.kumaraswamy(concentration1=2.0, concentration0=3.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#kumaraswamy #### Rerferences: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n\n\n\n\nLaplace\nSamples from a Laplace distribution, also known as the double exponential distribution. The Laplace distribution is defined by its location parameter (loc) and scale parameter (scale).\n\nf(x \\mid \\mu, b) = \\frac{1}{2b} \\exp\\left( -\\frac{|x - \\mu|}{b} \\right)\n\nWhere: * (\\mu) is the location parameter, indicating the peak of the distribution. * (b &gt; 0) is the scale parameter, controlling the spread of the distribution.\n\nArgs:\nbi.dist.laplace(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray): Location parameter of the Laplace distribution.\nsample (jnp.ndarray): Scale parameter of the Laplace distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Laplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Laplace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.laplace(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#laplace\n\n\n\n\nLeft Truncated\nSamples from a left-truncated distribution.\nA left-truncated distribution is a probability distribution obtained by restricting the support of another distribution to values greater than a specified lower bound. This is useful when dealing with data that is known to be greater than a certain value. All the ‚Äúmass‚Äù below (or equal to) (a) is excluded (not just unobserved, but removed from the sample/analysis).\n\n\nArgs:\nbi.dist.left_truncated_distribution(\nbase_dist,\nlow=0.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbase_dist: The base distribution to truncate. Must be univariate and have real support.\nlow: The lower truncation bound. Values less than this are excluded from the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI LeftTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LeftTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#lefttruncateddistribution\n\n\nReferences:\nhttps://en.wikipedia.org/wiki/Truncated_distribution https://encyclopediaofmath.org/wiki/Truncated_distribution\n\n\n\n\nLevy\nSamples from a Levy distribution. The L√©vy distribution is a continuous probability distribution on the positive real line (or shifted positive line) that is heavy-tailed, skewed, and arises naturally in connection with stable distributions (specifically the case with stability index \\alpha = \\tfrac12. It is often used in contexts such as hitting‚Äêtime problems for Brownian motion, physics (e.g., van der Waals line‚Äêshapes), and modelling very heavy‚Äêtailed phenomena. Let (X) be a L√©vy‚Äêdistributed random variable with location parameter \\mu and scale parameter (c &gt; 0). The support is x \\ge \\mu.\nPDF \nf(x; \\mu, c) = \\sqrt{\\frac{c}{2\\pi}} ; \\frac{1}{(x - \\mu)^{3/2}} ; \\exp!\\Bigl( -\\frac{c}{2 (x - \\mu)} \\Bigr),\n\\quad x \\ge \\mu.\n\n\nArgs:\nbi.dist.levy(\nloc,\nscale,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray): Location parameter.\nsample (jnp.ndarray): Scale parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Levy distribution object: When sample=False (for model building). JAX array: When sample=True (for direct sampling). BI distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.levy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#levy\n\n\n\n\nLewandowski Kurowicka Joe (LKJ)\nThe LKJ distribution is controlled by the concentration parameter \\eta to make the probability of the correlation matrix $M` proportional to \\det(M)^{\\eta - 1}. When \\eta = 1, the distribution is uniform over correlation matrices. When \\eta &gt; 1, the distribution favors samples with large determinants. When \\eta &lt; 1, the distribution favors samples with small determinants.\n\nP(M) \\propto |\\det(M)|^{\\eta - 1}\n\n\nArgs:\nbi.dist.lkj(\ndimension,\nconcentration=1.0,\nsample_method='onion',\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ndimension (int): The dimension of the correlation matrices.\nconcentration (ndarray): The concentration/shape parameter of the distribution (often referred to as eta). Must be positive.\nsample_method (str): Either ‚Äúcvine‚Äù or ‚Äúonion‚Äù. Both methods are proposed in [1] and offer the same distribution over correlation matrices. But they are different in how to generate samples. Defaults to ‚Äúonion‚Äù.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI LKJ distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LKJ distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.lkj(dimension=2, concentration=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lkj #### References: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n\n\n\n\nLKJ Cholesky\nThe LKJ (Leonard-Kj√¶rgaard-J√∏rgensen) Cholesky distribution is a family of distributions on symmetric matrices, often used as a prior for the Cholesky decomposition of a symmetric matrix. It is particularly useful in Bayesian inference for models with covariance structure. Given a lower triangular matrix (L) with unit diagonal entries, the PDF is: \np(L | \\eta) \\propto \\prod_{k=2}^d L_{kk}^{d-k+2\\eta-2}\n\nwhere: * L_{kk} is the diagonal element of (L). * \\eta is the shape parameter. * d is the dimension of the matrix.\n\n\\eta = 1: Uniform prior over correlation matrices.\n\\eta &gt; 1: Prior favors correlation matrices close to the identity matrix (i.e., weak correlations).\n\\eta &lt; 1: Prior allows stronger correlations.\n\n\nArgs:\nbi.dist.lkj_cholesky(\ndimension,\nconcentration=1.0,\nsample_method='onion',\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\n*dimension (int): The dimension of the correlation matrices.\n*concentration (float): A parameter controlling the concentration of the distribution around the identity matrix. Higher values indicate greater concentration. Must be greater than 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\nAttributes: concentration (float): The concentration parameter.\n\n\nReferences:\nhttps://mc-stan.org/docs/2_21/functions-reference/cholesky-lkj-correlation-distribution.html\n\n\n\n\nLog-Normal\nThe Log-Normal distribution is a probability distribution defined for positive real-valued random variables, parameterized by a location parameter (loc : \\mu) and a scale parameter (scale). It arises when the logarithm of a random variable is normally distributed. * A random variable X is log-normal if its natural logarithm \\ln X is (approximately) normally distributed. Equivalently, one can write: \nX = \\exp(Y), \\quad \\text{where } Y \\sim {N}(\\mu, \\sigma^2).\n\nBecause X is the exponential of a normal, it is always positive (support x&gt;0). It is useful in contexts where growth, multiplicative effects, or compounding factors dominate (e.g.¬†stock prices, income, sizes of biological organisms).\n\nParameters\n\n\\mu: the mean of \\ln X (i.e.¬†the location parameter in log-space)\n\\sigma &gt; 0: the standard deviation of \\ln X (i.e.¬†the ‚Äúscale‚Äù in log-space)\n\nSometimes another parameterization uses a threshold (shift) \\theta, i.e.¬†X = \\theta + \\exp(Y). For the two-parameter (no shift) case:\n\nPDF \nf(x; \\mu, \\sigma) = \\frac{1}{x,\\sigma\\sqrt{2\\pi}} ; \\exp!\\Bigl( -\\frac{(\\ln x - \\mu)^2}{2\\sigma^2} \\Bigr), \\quad x &gt; 0.\n\n\n\n\n\nArgs:\nbi.dist.log_normal(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (float): Location parameter.\nscale (float): Scale parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI LogNormal distribution object (for model building). JAX array of samples drawn from the LogNormal distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.log_normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#lognormal\n\n\nReferences:\nhttps://en.wikipedia.org/wiki/Log-normal_distribution\n\n\n\n\nLog-Uniform\nSamples from a Log Uniform distribution.\nThe Log Uniform distribution is defined over the positive real numbers and is the result of applying an exponential transformation to a uniform distribution over the interval [low, high]. It is often used when modeling parameters that must be positive.\nA random variable X is log-uniform on [a, b], with 0 &lt; a &lt; b, if \\ln X is uniformly distributed on [\\ln a, \\ln b]. Equivalently, the density of X is proportional to 1/x over that interval. This distribution is sometimes called the reciprocal distribution. It is useful in modeling scales spanning several orders of magnitude, where you want every decade (or log-interval) to have equal weight.\nFor x \\in [a, b]:\n\nPDF \nf(x) = \\frac{1}{x , [\\ln(b) - \\ln(a)]}, \\quad a \\le x \\le b.\n Outside [a, b], f(x) = 0.\nCDF \nF(x) = \\Pr(X \\le x) = \\frac{\\ln(x) - \\ln(a)}{\\ln(b) - \\ln(a)}, \\quad a \\le x \\le b.\n (For x &lt; a, F(x) = 0; for x &gt; b, F(x) = 1.\n\n\nArgs:\nbi.dist.log_uniform(\nlow,\nhigh,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlow (jnp.ndarray): The lower bound of the uniform distribution‚Äôs log-space. Must be positive.\nhigh (jnp.ndarray): The upper bound of the uniform distribution‚Äôs log-space. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI LogUniform distribution object (for model building) when sample=False.\nJAX array of samples drawn from the LogUniform distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.log_uniform(low=0.1, high=10.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#loguniform\n\n\nReferences:\nhttps://en.wikipedia.org/wiki/Reciprocal_distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.loguniform.html https://docs.scipy.org/doc/scipy/tutorial/stats/continuous_loguniform.html\n\n\n\n\nLogistic\nSamples from a Logistic distribution.\nThe Logistic distribution is a continuous probability distribution defined by two parameters: location and scale. It is often used to model growth processes and is closely related to the normal distribution.Its CDF is the logistic (sigmoid) function, which makes it appealing in modeling probabilities, logistic regression, and various growth models. It resembles the normal distribution in shape (bell‚Äêshaped, symmetric) but has heavier tails (i.e.¬†more probability in the extremes) and simpler closed‚Äêform expressions for the CDF.\n\nLocation parameter: \\mu \\in {R} ‚Äî the center or ‚Äúmean‚Äù of the distribution.\nScale parameter: s &gt; 0 ‚Äî controls the spread (similar role to standard deviation).\nSupport: $¬†x (-, +) $.\n\nLet X \\sim {Logistic}(\\mu, s). Then:\n\nF(x; \\mu, s) = \\frac{1}{1 + \\exp\\bigl(-\\frac{x - \\mu}{s}\\bigr)}.\n\nEquivalently: \nF(x) = \\frac12 + \\frac12 \\tanh!\\bigl(\\frac{x - \\mu}{2s}\\bigr).\n\n\nArgs:\nbi.dist.logistic(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray or float): The location parameter, specifying the median of the distribution. Defaults to 0.0.\nscale (jnp.ndarray or float): The scale parameter, which determines the spread of the distribution. Must be positive. Defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Logistic distribution object (for model building) when sample=False. JAX array of samples drawn from the Logistic distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.logistic(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#logistic\n\n\nReference:\nhttps://en.wikipedia.org/wiki/Logistic_distribution\n\n\n\n\nLow Rank Multivariate Normal\nThe Low-Rank Multivariate Normal (LRMVN) distribution is a parameterization of the multivariate normal distribution where the covariance matrix is expressed as a low-rank plus diagonal decomposition: \n\\Sigma = F F^\\top + D\n\nwhere F is a low-rank matrix (capturing correlations) and D is a diagonal matrix (capturing independent noise). This representation is often used in probabilistic modeling and variational inference to efficiently handle high-dimensional Gaussian distributions with structured covariance.\n\nParameters: - loc (jnp.ndarray): Mean vector.\ncov_factor (jnp.ndarray): Matrix used to construct the covariance matrix.\ncov_diag (jnp.ndarray): Diagonal elements of the covariance matrix.\n\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\nExample Usage:\nfrom BI import bi m = bi(‚Äòcpu‚Äô) event_size = 100 # Our distribution has 100 dimensions rank = 5\nm.dist.low_rank_multivariate_normal( - loc=m.dist.normal(0,1, shape = (event_size,), sample=True)2, cov_factor=m.dist.normal(0,1, shape = (event_size, rank), sample=True), cov_diag=jnp.exp(m.dist.normal(0,1, shape = (event_size,), sample=True))  0.1, sample=True )\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lowrankmultivariatenormal\n\n\nReference:\n\nMultivariate normal distribution ‚Äì Wikipedia\nTensorFlow Probability: LowRankMultivariateNormal\n\nbi.dist.low_rank_multivariate_normal(\nloc,\ncov_factor,\ncov_diag,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\n\n\n\nLower Truncated Power Law\nThe Lower-Truncated Power-Law distribution (also known as the Pareto Type I or power-law with a lower bound) models quantities that follow a heavy-tailed power-law behavior but are bounded below by a minimum value x_{min}. It is commonly used to describe phenomena such as wealth distributions, city sizes, and biological scaling laws\n\n\nArgs:\nbi.dist.lower_truncated_power_law(\nalpha,\nlow,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nalpha (jnp.ndarray): index of the power law distribution. Must be less than -1.\nlow (jnp.ndarray): lower bound of the distribution. Must be greater than 0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI LowerTruncatedPowerLaw distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the LowerTruncatedPowerLaw distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.lower_truncated_power_law(alpha=-2.0, low=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#lowertruncatedpowerlaw\n\n\nReference:\n\nPower-law distribution ‚Äì Wikipedia\nPareto distribution ‚Äì Wikipedia\nTruncated Pareto distribution\n\n\n\n\n\nMatrix Normal\nSamples from a Matrix Normal distribution, which is a multivariate normal distribution over matrices. The distribution is characterized by a location matrix and two lower triangular matrices that define the correlation structure. The distribution is related to the multivariate normal distribution in the following way. If X ~ MN(loc,U,V) then vec(X) ~ MVN(vec(loc), kron(V,U) ).\nLet (X) be (n p), with mean matrix M\\in R^{n\\times p}, row-covariance matrix U\\in R^{n\\times n} (positive-definite), and column-covariance matrix V\\in R^{p\\times p}(positive-definite). Then the PDF is:\n\nf(X;M,U,V) = \\frac{1}{(2\\pi)^{\\tfrac{np}{2}},|V|^{\\tfrac n2},|U|^{\\tfrac p2}},\n\\exp!\\Big[-\\tfrac12,{tr}!\\big( V^{-1}(X-M)^\\top,U^{-1}(X-M)\\big)\\Big].\n\nAlso the equivalence to vec-form is given: \n{vec}(X) \\sim  N_{np}\\big({vec}(M),,V\\otimes U\\big). ; ; \\text{([The Book of Statistical Proofs][3])}\n\n\nArgs:\nbi.dist.matrix_normal(\nloc,\nscale_tril_row,\nscale_tril_column,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (array_like): Location of the distribution.\nscale_tril_row (array_like): Lower cholesky of rows correlation matrix.\nscale_tril_column (array_like): Lower cholesky of columns correlation matrix.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI MatrixNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MatrixNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')            \nn_rows, n_cols = 3, 4\n\n- *loc* = jnp.zeros((n_rows, n_cols))\nU_row_cov = jnp.array([[1.0, 0.5, 0.2],\n[0.5, 1.0, 0.3],\n[0.2, 0.3, 1.0]])\nscale_tril_row = jnp.linalg.cholesky(U_row_cov)\n\nV_col_cov = jnp.array([[2.0, -0.8, 0.1, 0.4],\n[-0.8, 2.0, 0.2, -0.2],\n[0.1, 0.2, 2.0, 0.0],\n[0.4, -0.2, 0.0, 2.0]])\n\n# The argument passed to the distribution is its Cholesky factor\nscale_tril_column = jnp.linalg.cholesky(V_col_cov)\n\nm.dist.matrix_normal(\noc=loc, \nscale_tril_row=scale_tril_row, \nscale_tril_column=scale_tril_column, \nsample=True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#matrixnormal_lowercase\n\n\nReference:\n\n\nhttps://en.wikipedia.org/wiki/Matrix_normal_distribution ‚ÄúMatrix normal distribution‚Äù\n\n\nhttps://statproofbook.github.io/P/matn-pdf.html ‚ÄúProbability density function of the matrix-normal distribution | The Book of Statistical Proofs‚Äù\n\n\n\n\n\n\nA marginalized finite mixture of component distributions.\nA marginalised finite mixture of component distributions refers to a probability model in which you have a finite number K of component distributions (e.g., normals, Poissons, etc.), each weighted by a mixing probability from a Categorical distribution. We marginalise out the latent assignment variable (i.e., you don‚Äôt explicitly model which component each data point came from). The resulting distribution can be either a mixture_general (when component distributions are a list) or a mixture_same_Family (when component distributions are a single distribution).\nSuppose you have:\n\nMixing weights \\pi_1, \\dots, \\pi_K, with \\pi_j \\ge 0 and \\sum_{j=1}^K \\pi_j = 1.\nComponent distributions (densities or PMFs) f_j(x \\mid \\theta_j), for j = 1,\\dots,K.\n\nThen the marginalised mixture distribution for an observed X is:\n\np(X = x) = \\sum_{j=1}^K \\pi_j ; f_j(x \\mid \\theta_j).\n\nThis is exactly the mixture density/PMF you get when you marginalise the latent component‚Äëassignment variable Z \\in {1,\\dots,K} out of the joint distribution:\n\np(x) = \\sum_{z=1}^K p(z = j); p(x \\mid z = j) = \\sum_{j=1}^K \\pi_j ; f_j(x \\mid \\theta_j).\n\n\nArgs:\nbi.dist.mixture(\nmixing_distribution,\ncomponent_distributions,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=0,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nmixing_distribution (BI.distribution): The distribution used to determine the mixing weights.\ncomponent_distributions (list[BI.distribution]): The list of component distributions.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Mixture distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Mixture distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom jax import random\nimport BI as pyro\nm = pyro.distributions.Mixture(\npyro.distributions.Categorical(torch.ones(2)),\n[pyro.distributions.Normal(0, 1), pyro.distributions.Normal(2, 1)]\n)\nsamples = m.sample(sample_shape=(10,))\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixture\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Mixture_distribution ‚ÄúMixture distribution‚Äù\nhttps://pymc3-testing.readthedocs.io/en/rtd-docs/notebooks/marginalized_gaussian_mixture_model.html ‚ÄúMarginalized Gaussian Mixture Model ‚Äî PyMC3 3.1rc3 documentation‚Äù\n\n\n\n\n\nMixture General\nA mixture distribution is a probability distribution constructed by selecting one of several component distributions according to specified weights, and then drawing a sample from the chosen component. It allows modelling of heterogeneous populations and multimodal data.\nLet the mixture have (K) component distributions with densities f_i(x) for i = 1,\\dots,K. Let weights $w_i) satisfy \\sum_{i=1}^K w_i = 1. Then the PDF is \nf(x) = \\sum_{i=1}^K w_i ; f_i(x).\n\nParameters:\n\nmixing_distribution: A Categorical distribution representing the mixing weights.\ncomponent_distributions: A list of distributions representing the components of the mixture.\n**sample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n#### Returns:\n\nWhen sample=False: A BI MixtureGeneral distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MixtureGeneral distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n#### Example Usage:\nfrom BI import bi m = bi(‚Äòcpu‚Äô) m.dist.mixture_general( mixing_distribution=m.dist.categorical(probs=jnp.array([0.3, 0.7]), create_obj = True),\ncomponent_distributions=[m.dist.normal(loc=0.0, scale=1.0, create_obj=True),m.dist.normal (loc=0.0, scale=1.0, create_obj=True)], sample = True )\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixturegeneral\n\n\nReference:\n\n\n\nhttps://preliz.readthedocs.io/en/latest/distributions/gallery/mixture.html ‚ÄúMixture Distribution ‚Äî PreliZ 0.22.0 documentation‚Äù\n\n\nhttps://reliability.readthedocs.io/en/latest/Mixture%20models.html ‚ÄúMixture models ‚Äî reliability 0.9.0 documentation‚Äù\n\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.Mixture.html ‚ÄúMixture ‚Äî SciPy v1.16.2 Manual‚Äù\n\n\nbi.dist.mixture_general(\nmixing_distribution,\ncomponent_distributions,\nsupport=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\n\n\n\nFinite mixture of component distributions from the same family.\nA Mixture (Same-Family) distribution is a finite mixture in which all components come from the same parametric family (for example, all Normal distributions but with different parameters), and are combined via mixing weights. The class is typically denoted as:\nLet the mixture have (K) components. Let weights (w_i), (_{i=1}^K w_i = 1). Let the component family have density (f(x _i)) for each component (i). Then the mixture‚Äôs PDF is \nf_X(x) = \\sum_{i=1}^K w_i ; f(x \\mid \\theta_i).\n\nwhere each f(x \\mid \\theta_i) is from the same family with parameter \\theta_i.\n\nArgs:\nbi.dist.mixture_same_family(\nmixing_distribution,\ncomponent_distribution,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nDistribution Args:\nmixing_distribution: A Categorical distribution representing the mixing weights.\ncomponent_distributions: A list of distributions representing the components of the mixture.\nSampling / Modeling Args:\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\n**sample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI MixtureSameFamily distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MixtureSameFamily distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.mixture_same_family(\nmixing_distribution=m.dist.categorical(probs=jnp.array([0.3, 0.7]), create_obj = True), \ncomponent_distribution=m.dist.normal(loc=0.0, scale=1.0, shape = (2,), create_obj=True),\nsample = True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#mixture-, to_jax = True)same-family\n\n\nReference:\n\nMixture distributions\n\n\n\n\n\nMultinomial\nThe multinomial distribution is a discrete probability distribution that generalizes the Binomial distribution to the case of more than two possible outcomes per trial. Specifically:\n\nYou perform a fixed number n of independent trials.\nOn each trial there are k possible outcomes, labeled (1,2,,k).\nThe probability of outcome i on any trial is p_i, where p_1 + p_2 + \\cdots + p_k = 1.\nLet X_i be the number of trials (out of n) that produced outcome i. Then the vector $ X = (X_1, , X_k)$ follows a multinomial distribution.\n\nIt reduces to the binomial when (k=2).\nPMF (Probability Mass Function):\nFor $ x = (x_1,,x_k)$ with each x_i \\ge 0 integer and \\sum_{i=1}^k x_i = n, \n\\Pr(X_1 = x_1, \\dots, X_k = x_k)\n= \\frac{n!}{x_1!,x_2!\\cdots x_k!}\n; p_1^{,x_1}; p_2^{,x_2} \\cdots p_k^{,x_k}.\n\nIf \\sum_i x_i \\neq n, the probability is 0.\n\nArgs:\nbi.dist.multinomial(\ntotal_count=1,\nprobs=None,\nlogits=None,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntotal_count (int or jnp.ndarray): The number of trials.\nprobs (jnp.ndarray, optional): Event probabilities. Must sum to 1.\nlogits (jnp.ndarray, optional): Event log probabilities.\n*total_count_max (int, optional): An optional integer providing an upper bound on total_count. This is used for performance optimization with lax.scan when total_count is a dynamic JAX tracer, helping to avoid recompilation.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Multinomial distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Multinomial distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial(total_count=10, probs=jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#multinomial\n\n\nReferences:\n\nhttps://online.stat.psu.edu/stat504/book/export/html/667 ‚Äú2.3 - The Multinomial Distribution - STAT ONLINE‚Äù\nhttps://en.wikipedia.org/wiki/Multinomial_distribution ‚ÄúMultinomial distribution‚Äù\nhttps://faculty.washington.edu/yenchic/20A_stat512/Lec7_Multinomial.pdf ‚Äú[PDF] Lecture 7: Multinomial distribution‚Äù\n\n\n\n\n\nMultinomial Logits\nA multinomial logits distribution refers to a categorical (or more generally multinomial) distribution over (K) classes whose probabilities are given via the softmax of a vector of logits. That is, given a vector of real-valued logits \\ell = (\\ell_1, ‚Ä¶, \\ell_K), the class probabilities are: \np_k = \\frac{\\exp(\\ell_k)}{\\sum_{j=1}^K \\exp(\\ell_j)}.\n Then a single draw from the distribution yields one of the (K) classes (or for a multinomial count version, counts over the classes) with those probabilities.\nCategorical version (one draw): Let X \\in {1,2,\\dots,K} be the class. Then \n\\Pr(X=k \\mid \\ell) ;=; p_k = \\frac{\\exp(\\ell_k)}{\\sum_{j=1}^K \\exp(\\ell_j)}.\n This directly uses the logits \\ell.\nMultinomial version (n draws): If you draw (n) independent draws (or equivalently count vector) with probabilities p = (p_1,‚Ä¶,p_K), then for a count vector x = (x_1,‚Ä¶,x_K) with \\sum_k x_k = n, \n\\Pr(X = x \\mid n, p) = \\frac{n!}{x_1! \\cdots x_K!} ; \\prod_{k=1}^K p_k^{x_k}\n\nand p_k is given by the softmax of logits.\n\nArgs:\nbi.dist.multinomial_logits(\nlogits,\ntotal_count=1,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlogits (jnp.ndarray): Logits for each outcome. Must be at least one-dimensional.\ntotal_count (jnp.ndarray): The total number of trials.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultinomialLogits distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultinomialLogits distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial_logits(logits=jnp.array([1.0, 0.5], dtype=jnp.float32), total_count=jnp.array(5, dtype=jnp.int32), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multinomiallogits\n\n\nReference:\n\nhttps://en.wikipedia.org/wiki/Multinomial_distribution\n\n\n\n\n\nMultinomial Probs\nSamples from a Multinomial distribution.\nThe Multinomial distribution models the number of times each of several discrete outcomes occurs in a fixed number of trials. Each trial independently results in one of several outcomes, and each outcome has a probability of occurring.\n\nArgs:\nbi.dist.multinomial_probs(\nprobs,\ntotal_count=1,\ntotal_count_max=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nprobs (jnp.ndarray): Vector of probabilities for each outcome. Must sum to 1.\ntotal_count (jnp.ndarray): The number of trials.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI MultinomialProbs distribution object (for model building). JAX array of samples drawn from the MultinomialProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multinomial_probs(probs=jnp.array([0.2, 0.3, 0.5]), total_count=10, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multinomialprobs\n\n\n\n\nMultivariate Normal\nThe Multivariate Normal distribution, also known as the Gaussian distribution in multiple dimensions, is a probability distribution that arises frequently in statistics and machine learning. It is defined by its mean vector and covariance matrix, which describe the central tendency and spread of the distribution, respectively.\n\np(x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)\n\nwhere: - x is a n-dimensional vector of random variables. - \\mu is the mean vector. - \\Sigma is the covariance matrix.\n\nArgs:\nbi.dist.multivariate_normal(\nloc=0.0,\ncovariance_matrix=None,\nprecision_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (tuple): The mean vector of the distribution.\ncovariance_matrix (jnp.ndarray, optional): The covariance matrix of the distribution. Must be positive definite.\nprecision_matrix (jnp.ndarray, optional): The precision matrix (inverse of the covariance matrix) of the distribution. Must be positive definite.\nscale_tril (jnp.ndarray, optional): The lower triangular Cholesky decomposition of the covariance matrix.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultivariateNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultivariateNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.multivariate_normal(\n- *loc*=jnp.array([1.0, 0.0, -2.0]), \ncovariance_matrix=jnp.array([[ 2.0,  0.7, -0.3],\n[ 0.7,  1.0,  0.5],\n[-0.3,  0.5,  1.5]]), \nsample=True\n)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#multivariate-normal\n\n\nReference:\n\nMultivariate normal distribution ‚Äì Wikipedia\n\n\n\n\n\nMultivariate Student‚Äôs t\nThe Multivariate Student‚Äôs t distribution is a generalization of the Student‚Äôs t distribution to multiple dimensions. It is a heavy-tailed distribution that is often used to model data that is not normally distributed.\nThe PDF of the multivariate Student‚Äôs t-distribution for a random vector ( {x} ^d ) is given by:\n\nf({x}) = \\frac{\\Gamma\\left(\\frac{\\nu + d}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) \\nu^{d/2} \\pi^{d/2} | \\Sigma|^{1/2}} \\left(1 + \\frac{1}{\\nu} ({x} - \\mu)^T \\Sigma^{-1} ({x} - \\mu)\\right)^{-(\\nu + d)/2}\n\nwhere:\n\n$ () $ is the Gamma function.\n$ $ is the mean vector.\n$ $ is the scale (covariance) matrix.\n$ $ is the degrees of freedom.\n$ d $ is the dimensionality of $ {x} $.\n\n\nArgs:\nbi.dist.multivariate_student_t(\ndf,\nloc=0.0,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ndf (jnp.ndarray): Degrees of freedom, must be positive.\nloc (jnp.ndarray): Location vector, representing the mean of the distribution. scale_tril (jnp.ndarray): Lower triangular matrix defining the scale.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI MultivariateStudentT distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the MultivariateStudentT distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\nm.dist.multivariate_student_t(\ndf = 2,\n- *loc*=jnp.array([1.0, 0.0, -2.0]), \nscale_tril=jnp.linalg.cholesky(\njnp.array([[ 2.0,  0.7, -0.3],\n[ 0.7,  1.0,  0.5],\n[-0.3,  0.5,  1.5]])), \nsample=True\n)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#multivariatestudentt\n\n\nReference:\n\nMultivariate Student-t distribution ‚Äì Wikipedia\n\n\n\n\n\nNegative Binomial\nThe Negative Binomial distribution models the number of failures (or the total number of trials) in a sequence of independent Bernoulli trials with success probability (p), until a specified number total_count (r) of successes is achieved. It is often used as a count-data model when the variance exceeds the mean (‚Äúoverdispersion‚Äù) relative to a Poisson.\nPMF (Probability Mass Function): Given (X) = number of failures before the (r)th success (so (X = 0,1,2,)), with (r) successes fixed, success probability (p), failure probability (q = 1 - p):\n\n\\Pr(X = k) ;=; \\binom{k + r - 1}{k} ; p^r ; q^k, \\quad k = 0,1,2,\\dots\n\n\nArgs:\nbi.dist.negative_binomial(\ntotal_count,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntotal_count (jnp.ndarray): The total number of events.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI NegativeBinomial distribution object (for model building). JAX array of samples drawn from the NegativeBinomial distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial(total_count=5.0,probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#negativebinomial\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Negative_binomial_distribution\nhttps://www.statology.org/negative-binomial-distribution/\n\n\n\n\n\n\nSamples from a Negative Binomial\nThe NB2 parameterisation of the negative-binomial distribution is a count distribution used for modelling over-dispersed count data (variance &gt; mean). It is defined such that the variance grows quadratically in the mean:\n\nmathrm{Var}(Y) = \\mu + \\alpha,\\mu^2,\n where (= {E}[Y]) and (&gt;0) is the dispersion (heterogeneity) parameter. Because of this quadratic variance growth, it is called the NB2 family.\n\nP(k) = \\frac{\\Gamma(k + \\alpha)}{\\Gamma(k + 1) \\Gamma(\\alpha)} \\left(\\frac{\\beta}{\\alpha + \\beta}\\right)^k \\left(1 - \\frac{\\beta}{\\alpha + \\beta}\\right)^k\n\nOne commonly used form of the NB2 parameterisation is obtained via a Poisson‚ÄêGamma mixture: \nY \\mid \\lambda \\sim \\text{Poisson}(\\lambda), \\quad \\lambda \\sim \\text{Gamma}!\\Big(\\frac{1}{\\alpha},,\\frac{\\mu,\\alpha}{1}\\Big)\n\nwhich marginalises to:\n\n\\Pr(Y = y) = \\frac{\\Gamma(y + \\tfrac1\\alpha)}{\\Gamma(\\tfrac1\\alpha); y!}; \\bigg(\\frac{1}{1 + \\alpha,\\mu}\\bigg)^{1/\\alpha}; \\bigg(\\frac{\\alpha,\\mu}{1 + \\alpha,\\mu}\\bigg)^{y}, \\quad y = 0,1,2,\\dots\n This parameterisation matches the NB2 form with mean \\mu and dispersion \\alpha. (See Hilbe (2011) forderivation.)\n\nArgs:\nbi.dist.negative_binomial2(\nmean,\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nmean (jnp.ndarray or float): The mean of the distribution. This is equivalent to the mu parameter.\nconcentration (jnp.ndarray or float): The concentration parameter. This is equivalent to the alpha parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI NegativeBinomial distribution object: When sample=False (for model building). jnp.ndarray: A JAX array of samples drawn from the NegativeBinomial distribution (for direct sampling). BI NegativeBinomial distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial2(mean=2.0, concentration=3.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#negativebinomial\n\n\nReference:\n\n[Negative Binomial Regression]9https://www.cambridge.org/core/books/negative-binomial-regression/12D6281A46B9A980DC6021080C9419E7)\nNegative Binomial Regression: Second Edition\n\n\n\n\n\nNegative Binomial Logits\nSamples from a Negative Binomial Logits distribution.\nThe Negative Binomial Logits distribution is a generalization of the Negative Binomial distribution where the parameter ‚Äòr‚Äô (number of successes) is expressed as a function of a logit parameter. This allows for more flexible modeling of count data.\n\nArgs:\nbi.dist.negative_binomial_logits(\ntotal_count,\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntotal_count (jnp.ndarray): The parameter controlling the shape of the distribution. Represents the total number of trials.\nlogits (jnp.ndarray): The log-odds parameter. Related to the probability of success.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nNegative Binomial Logits: A BI Negative Binomial Logits distribution object (for model building).\njnp.ndarray: A JAX array of samples drawn from the Negative Binomial Logits distribution (for direct sampling).\nNegative Binomial Logits: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial_logits(total_count=5.0, logits=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#Negative Binomial Logits\n\n\n\n\nNegative Binomial with probabilities.\nThe Negative Binomial distribution models the number of failures before the first success in a sequence of independent Bernoulli trials. It is characterized by two parameters: ‚Äòconcentration‚Äô (r) and ‚Äòrate‚Äô (p). In this implementation, the ‚Äòconcentration‚Äô parameter is derived from ‚Äòtotal_count‚Äô and the ‚Äòrate‚Äô parameter is derived from ‚Äòprobs‚Äô.\n\nArgs:\nbi.dist.negative_binomial_probs(\ntotal_count,\nprobs,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntotal_count (jnp.ndarray): A numeric vector, matrix, or array representing the parameter.\nprobs (jnp.ndarray): A numeric vector representing event probabilities. Must sum to 1.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI NegativeBinomialProbs distribution object (for model building). JAX array of samples drawn from the NegativeBinomialProbs distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.negative_binomial_probs(total_count=10.0, probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#negativebinomialprobs\n\n\n\n\nNormal\nSamples from a Normal (Gaussian) distribution.\nThe Normal distribution is characterized by its mean (loc) and standard deviation (scale). It‚Äôs a continuous probability distribution that arises frequently in statistics and probability theory.\n   \nf(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\n\nArgs:\nbi.dist.normal(\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (jnp.ndarray): The mean of the distribution.\nscale (jnp.ndarray): The standard deviation of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Normal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Normal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#normal\n\n\nReferences:\n\nWikipedia: Normal distribution ‚Äì PDF\nWolfram MathWorld: Normal Distribution\n\n\n\n\n\nOrdered Logistic\nThe ordered logistic distribution is used for modeling ordinal outcome variables $ Y $ (i.e., categories with a natural order) via a latent continuous predictor $ $ and a set of increasing cut-points. When $ $ crosses thresholds, the observed Y\n\nThis formulation appears in e.g.¬†the CDF‚Äêlink setup of the proportional odds model.\n\nArgs:\nbi.dist.ordered_logistic(\npredictor,\ncutpoints,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\npredictor (jnp.ndarray): Prediction in real domain; typically this is output of a linear model.\ncutpoints (jnp.ndarray): Positions in real domain to separate categories.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI OrderedLogistic distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the OrderedLogistic distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.ordered_logistic(predictor=jnp.array([0.2, 0.5, 0.8]), cutpoints=jnp.array([-1.0, 0.0, 1.0]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#orderedlogistic\n\n\nReference:\n\nhttps://en.wikipedia.org/wiki/Ordered_logit\nhttps://mc-stan.org/docs/2_21/functions-reference/ordered-logistic-distribution.html\nhttps://labdisia.disia.unifi.it/grilli/files/Papers/Ordered_Logit.pdf‚Äù\n\n\n\n\n\nPareto\nSamples from a Pareto distribution.\nThe Pareto distribution, named after economist Vilfredo Pareto, is a power-law probability distribution used to describe phenomena with ‚Äúrich-get-richer‚Äù or ‚Äúheavy-tail‚Äù properties ‚Äî for example, income distribution, city sizes, or wealth concentration. It is characterized by:\n\na scale parameter $ x_m &gt; 0 $ (the minimum possible value), and\na shape parameter $ &gt; 0 $ (which controls the tail heaviness).\n\nA random variable $ X (, x_m) $ takes values $ x x_m $.\n\nf(x \\mid \\alpha, x_m) =\n\\begin{cases}\n\\dfrac{\\alpha x_m^\\alpha}{x^{\\alpha + 1}},  x \\ge x_m, \\\n0,  x &lt; x_m.\n\\end{cases}\n\n\nArgs:\nbi.dist.pareto(\nscale,\nalpha,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (jnp.ndarray or float): Scale parameter of the Pareto distribution. Must be positive.\nalpha (jnp.ndarray or float): Shape parameter of the Pareto distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Pareto distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Pareto distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.pareto(scale=2.0, alpha=3.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#pareto\n\n\nReference:\n\nWikipedia: Pareto distribution ‚Äì PDF\nArnold, B. C. (2015). Pareto Distributions, Second Edition. CRC Press.\n\n\n\n\n\nPoisson\nThe Poisson distribution models the probability of observing a given number of events k occurring in a fixed interval of time or space when these events happen independently and at a constant average rate $ &gt; 0 $. It is widely used for modeling count data, such as the number of emails received per hour or mutations in a DNA strand per unit length.\nFormally,\n\nK \\sim \\text{Poisson}(\\lambda)\n\nwhere \\lambda is both the mean and variance of the distribution.\nThe probability mass function (PMF) of the Poisson distribution is given by: \nP(K = k \\mid \\lambda) =\n\\frac{e^{-\\lambda} \\lambda^k}{k!},\n\\quad k = 0, 1, 2, \\dots\n\n\nArgs:\nbi.dist.poisson(\nrate,\nis_sparse=False,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nrate (jnp.ndarray): The rate parameter, representing the average number of events.\nis_sparse (bool, optional): Indicates whether the rate parameter is sparse. If True, a specialized sparse sampling implementation is used, which can be more efficient for models with many zero-rate components (e.g., zero-inflated models). Defaults to False.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Poisson distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Poisson distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.poisson(rate=2.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#poisson\n\n\nReferences:\n\nWikipedia: Poisson distribution ‚Äì PMF\nJohnson, N. L., Kotz, S., & Kemp, A. W. (1992). Univariate Discrete Distributions (2nd ed.). Wiley.\n\n\n\n\n\nProjected Normal\nThe projected normal distribution arises by taking a multivariate normal vector $ X N (, ) $ in $ R^n$ and projecting it to the unit sphere. This distribution is commonly used in directional statistics (data on circles or spheres) and supports asymmetric and even multimodal behaviours depending on parameters.\n\nArgs:\nbi.dist.projected_normal(\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): The concentration parameter, representing the direction towards which the samples are concentrated. Must be a JAX array with at least one dimension.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI ProjectedNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ProjectedNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.projected_normal(concentration=jnp.array([1.0, 3.0, 2.0]), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#projectednormal\n\n\n\n\nRelaxed Bernoulli\nThe Relaxed‚ÄØBernoulli is a continuous distribution on the interval (0,1) that smoothly approximates the discrete Bernoulli distribution (which has support {0,1}). It was introduced to allow for differentiable sampling of approximate binary random variables, which is useful in variational inference and other gradient‚Äêbased optimization settings.\nThe probability density function (PDF) is defined as:\n\nIt has a temperature parameter \\tau &gt; 0 controlling the smoothness: as \\tau \\to 0, the distribution concentrates near {0,‚ÄØ1} (thus approximating the discrete Bernoulli).\nFor larger \\tau, the sample becomes more ‚Äúsoft‚Äù and less binary, e.g., converging toward 0.5 for very high \\tau.\nIt is parameterised by a probability p (or equivalently logits \\ell = \\log\\frac{p}{1-p}).\n\nLet X \\in (0,1) follow a Relaxed‚ÄØBernoulli (Binary‚ÄØConcrete) distribution with location parameter \\alpha &gt; 0 (sometimes expressed via logits) and temperature parameter \\lambda &gt; 0. Then the PDF is:\n\np_{,\\alpha,;\\lambda}(x) =\n\\frac{;\\lambda,\\alpha,x^{-\\lambda - 1},(1 - x)^{-\\lambda - 1};}\n{\\Bigl(\\alpha,x^{-\\lambda} + (1 - x)^{-\\lambda}\\Bigr)^{2}},\n\\quad \\text{for } 0 &lt; x &lt; 1.\n\n\nArgs:\nbi.dist.relaxed_bernoulli(\ntemperature,\nprobs=None,\nlogits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntemperature (jnp.ndarray): The temperature parameter. Must be greater than 0.\nprobs (jnp.ndarray, optional): The probability of success. Must be in the interval [0, 1]. Only one of probs or logits can be specified.\nlogits (jnp.ndarray, optional): The log-odds of success. Must be in the interval [-inf, inf]. Only one of probs or logits can be specified.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI RelaxedBernoulli distribution object (for model building) when sample=False. A JAX array of samples drawn from the RelaxedBernoulli distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.relaxed_bernoulli(temperature=1.0, probs = jnp.array([0.2, 0.3, 0.5]), sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#relaxedbernoulli\n\n\nReferences\n\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/distributions/RelaxedBernoulli ‚Äútfp.distributions.RelaxedBernoulli | TensorFlow Probability‚Äù\nhttps://rstudio.github.io/tfprobability/reference/tfd_relaxed_bernoulli.html ‚ÄúRelaxedBernoulli distribution with temperature and logits ‚Ä¶‚Äù\n\n\n\n\n\nRelaxed Bernoulli Logits\nThe Relaxed Bernoulli (logits) is a continuous relaxation of the standard Bernoulli distribution, parameterised by logits (or probabilities) and a temperature parameter. Rather than producing strictly 0 or 1, it produces values in the continuous interval (0, 1). As the temperature ‚Üí 0 the distribution approximates a Bernoulli; as temperature ‚Üí \\infty the distribution approximates a uniform distribution.\nIt is used in variational inference and deep-learning contexts to allow gradient-based optimisation through otherwise discrete Bernoulli draws (see the ‚ÄúConcrete‚Äù or ‚ÄúGumbel-Softmax‚Äù literature: The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables and Categorical Reparameterization with Gumbel‚ÄëSoftmax ).\nBecause this is a continuous relaxation, the ‚Äúdensity‚Äù is defined over the unit interval (0,1) rather than a pmf at {0,1}. Using logistic or Gumbel‚Äêsoftmax style construction one can express it as:\nIf logits = ( ), and temperature = ( &gt; 0 ), then one generates \nu \\sim {Uniform}(0,1),\\quad\ng = -\\log(-\\log(u)),\\quad\nx = \\sigma\\big((\\ell + g)/\\tau\\big)\n where $ () $ is the logistic (sigmoid) function. Then x \\in (0,1) has the RelaxedBernoulli distribution.\nThe exact density formula can be derived via change of variables from logistic/Gumbel, but is somewhat involved (including Jacobian of sigmoid transform).\n\nArgs:\nbi.dist.relaxed_bernoulli_logits(\ntemperature,\nlogits,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ntemperature (jnp.ndarray): The temperature parameter, must be positive.\nlogits (jnp.ndarray): The logits parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nRelaxedBernoulliLogits: A BI RelaxedBernoulliLogits distribution object (for model building). jnp.ndarray: A JAX array of samples drawn from the RelaxedBernoulliLogits distribution (for direct sampling). RelaxedBernoulliLogits: The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.relaxed_bernoulli_logits(temperature=1.0, logits=0.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#relaxed-bernoulli-logits\n\n\nReference:\n\nhttps://arxiv.org/pdf/1611.00712\n\n\n\n\n\nRight Truncated\nA right-truncated distribution is a statistical distribution that arises when the possible values of a random variable are restricted to be below a certain specified value high. In essence, the right tail of the original distribution is ‚Äúcut off‚Äù at a particular point, and the remaining probability is redistributed among the allowable values. This type of distribution is common in various fields where there are inherent upper limits or observational constraints.\nThe probability density function (PDF) for a continuous right-truncated distribution or the probability mass function (PMF) for a discrete one is derived from the original distribution by normalizing it over the restricted range.\nFor a continuous random variable X:\nIf the original probability density function is f(x) and the cumulative distribution function is F(x), and the distribution is right-truncated at a value b (meaning x ‚â§ b), the new PDF, f_T(x), is: \nf_{trunc}(x \\mid X \\le b) =\n\\begin{cases}\n\\displaystyle \\frac{,f(x),}{,F(b),},  x \\le b, \\\n0,  x &gt; b.\n\\end{cases}\n\nFor a discrete random variable X:\nIf the original probability mass function is P(X = k) and the cumulative distribution function is F(k), and the distribution is right-truncated at a value b (meaning k ‚â§ b), the new PMF, P_T(X = k), is:\nP_T(X=k) = \\begin{cases} \\frac{P(X=k)}{F(b)}  \\text{if } k \\le b \\\\ 0  \\text{if } k &gt; b \\end{cases}\n\nArgs:\nbi.dist.right_truncated_distribution(\nbase_dist,\nhigh=0.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbase_dist: The base distribution to truncate. Must be a univariate distribution with real support.\nhigh (float, jnp.ndarray, optional): The upper truncation point. The support of the new distribution is -\\infty, \\text{high}. Defaults to 0.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI RightTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the RightTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.right_truncated_distribution(base_dist = m.dist.normal(0,1, create_obj = True), high=0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#righttruncateddistribution\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Truncated_normal_distribution\n\n\n\n\n\nSine Bivariate Von Mises\nIn probability theory and statistics, the bivariate von Mises distribution is a probability distribution describing values on a torus. It may be thought of as an analogue on the torus of the bivariate normal distribution. The distribution belongs to the field of directional statistics. The general bivariate von Mises distribution was first proposed by Kanti Mardia in 1975. One of its variants is today used in the field of bioinformatics to formulate a probabilistic model of protein structure in atomic detail, such as backbone-dependent rotamer libraries.\n\nArgs:\nbi.dist.sine_bivariate_vonmises(\nphi_loc,\npsi_loc,\nphi_concentration,\npsi_concentration,\ncorrelation=None,\nweighted_correlation=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\n)\n\nphi_loc (jnp.ndarray): The location parameter for the first angle (phi).\npsi_loc (jnp.ndarray): The location parameter for the second angle (psi).\nphi_concentration (jnp.ndarray): The concentration parameter for the first angle (phi). Must be positive.\npsi_concentration (jnp.ndarray): The concentration parameter for the second angle (psi). Must be positive.\ncorrelation (jnp.ndarray, optional): The correlation parameter between the two angles. One of correlation or weighted_correlation must be specified.\nweighted_correlation (jnp.ndarray, optional): An alternative correlation parameter. One of correlation or weighted_correlation must be specified.\nvalidate_args (bool, optional): Whether to enable validation of distribution parameters. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations. If provided, events with a True mask will be conditioned on obs, while the remaining events will be treated as latent variables. Defaults to None.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nshape (tuple, optional): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int, optional): The number of batch dimensions to reinterpret as event dimensions (used in model building).\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\n\n\n\nReturns:\nBI.primitives.Messenger: A BI sample site object when used in a model context (sample=False). jnp.ndarray: A JAX array of samples drawn from the SineBivariateVonMises distribution (for direct sampling, sample=True). numpyro.distributions.Distribution: The raw BI distribution object (if create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nimport jax.numpy as jnp\nm = bi('cpu')\n\n# Direct sampling\nsamples = m.dist.sine_bivariate_vonmises(\nphi_loc=0.0,\npsi_loc=jnp.pi,\nphi_concentration=1.0,\npsi_concentration=1.0,\ncorrelation=0.5,\nsample=True,\n- *shape*=(10,)\n)\n\n# Usage within a model\ndef my_model():\nangles = m.dist.sine_bivariate_vonmises(\nphi_loc=0.0,\npsi_loc=0.0,\nphi_concentration=2.0,\npsi_concentration=2.0,\nweighted_correlation=0.9,\nname='angles'\n)\n# ... rest of the model\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#sinebivariatevonmises\n\n\n\n\nSine-skewing\nThe sine-skewed von Mises distribution is an extension of the symmetric circular (or toroidal) von Mises (or bivariate von Mises) distribution to allow for skewness (asymmetry) via a sine-based skewing function. It is used to model directional data on the circle (or torus) that depart from symmetry.1\nParameters:\n\nbase_dist: Base density on a d-dimensional torus. Supported base distributions include: 1D :class:~numpyro.distributions.VonMises, :class:~numnumpyro.distributions.SineBivariateVonMises, 1D :class:~numpyro.distributions.ProjectedNormal, and :class:~numpyro.distributions.Uniform (-pi, pi).\nskewness: Skewness of the distribution.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\nPDF:\nThe probability density function (PDF) of the Sine Skewed X distribution is not explicitly defined here, but it is derived from the base distribution and the skewness parameter.\n\nExample Usage:\nfrom num.pyro import distributions as dist import num.pyro as pyro import num.numpy as np\nm = pyro.distributions.Normal(loc=0.0, scale=1.0) skewness = np.array([0.5, 0.5]) sine_skewed = dist.SineSkewed(base_dist=m, skewness=skewness) samples = sine_skewed.sample((1000,))\nbi.dist.sine_skewed(\nbase_dist: numpyro.distributions.distribution.Distribution,\nskewness,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\n\n\n\nSoftLaplace\nSmooth distribution with Laplace-like tail behavior.\nThis distribution corresponds to the log-convex density:\n\nz = (value - loc) / scale\nlog_prob = log(2 / pi) - log(scale) - logaddexp(z, -z)\n\nLike the Laplace density, this density has the heaviest possible tails (asymptotically) while still being log-convex. Unlike the Laplace distribution, this distribution is infinitely differentiable everywhere, and is thus suitable for HMC and Laplace approximation.\n\nArgs:\nbi.dist.soft_laplace(\nloc,\nscale,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc: Location parameter.\nscale: Scale parameter.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI SoftLaplace distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the SoftLaplace distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.soft_laplace(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#softlaplace\n\n\n\n\nStudent‚Äôs t\nThe Student‚Äôs t-distribution is a probability distribution that arises in hypothesis testing involving the mean of a normally distributed population when the population standard deviation is unknown. It is similar to the normal distribution, but has heavier tails, making it more robust to outliers. For large $ , it converges to the Normal distribution.$ X t_(, ) $$ where:\n\n$ $ is the location (mean) parameter\n$ &gt; 0 $ is the scale parameter\n$ &gt; 0 $ is the degrees of freedom controlling the tail heaviness\n\nThe probability density function (pdf) of the Student‚Äôs t distribution is: \nf(x \\mid \\nu, \\mu, \\sigma) = \\frac{\\Gamma!\\left(\\frac{\\nu+1}{2}\\right)}\n{\\Gamma!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}\\sigma}\n\\left[1 + \\frac{1}{\\nu}\n\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\n\\right]^{-\\frac{\\nu+1}{2}}\n\n\nThe heavier tails (for small ŒΩ) allow for larger outliers.\nFor $ $, this approaches $ {N}(, ^2) $.\n\n\nArgs:\nbi.dist.student_t(\ndf,\nloc=0.0,\nscale=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ndf (jnp.ndarray): Degrees of freedom, must be positive.\nloc (jnp.ndarray): Location parameter, defaults to 0.0.\nscale (jnp.ndarray): Scale parameter, defaults to 1.0.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with .expand(shape) to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI StudentT distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the StudentT distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.student_t(df = 2, loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#studentt\n\n\nReferences:\n\nWikipedia ‚Äì Student‚Äôs t distribution: PDF\n\n\n\n\n\nTruncated Cauchy\nThe Cauchy distribution, also known as the Lorentz distribution, is a continuous probability distribution that appears frequently in various areas of mathematics and physics. It is characterized by its heavy tails, which extend to infinity. The truncated version limits the support of the Cauchy distribution to a specified interval.\nLet the base Cauchy have parameters \\mu (location) and \\gamma &gt; 0 (scale). Let the truncation bounds be a (low) and b (high) (with a &lt; b). Then the PDF of the truncated Cauchy at x \\in [a,b] is: \nf_{T}(x) = \\frac{1}{\\pi,\\gamma};\\frac{1}{1 + \\bigl(\\tfrac{x - \\mu}{\\gamma}\\bigr)^2} ;\\Bigg/ ; \\left[F_{{Cauchy}}!\\left(\\tfrac{b - \\mu}{\\gamma}\\right) - F_{{Cauchy}}!\\left(\\tfrac{a - \\mu}{\\gamma}\\right)\\right],\n and f_{T}(x)=0 for x &lt; a or x &gt; b.\nHere F_{{Cauchy}} is the standard Cauchy CDF $with parameters \\mu,\\gamma).\n\nArgs:\nbi.dist.truncated_cauchy(\nloc=0.0,\nscale=1.0,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (float): Location parameter of the Cauchy distribution.\nscale (float): Scale parameter of the Cauchy distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI TruncatedCauchy distribution object (for model building) when sample=False.\nJAX array of samples drawn from the TruncatedCauchy distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_cauchy(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncatedcauchy\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Cauchy_distribution ‚ÄúCauchy distribution‚Äù\nhttps://www.tandfonline.com/doi/full/10.1080/00207390600595223 ‚ÄúFull article: A truncated Cauchy distribution‚Äù\nhttps://rstudio.github.io/tfprobability/reference/tfd_truncated_cauchy.html ‚ÄúThe Truncated Cauchy distribution. ‚Äî tfd_truncated_cauchy‚Äù\n\n\n\n\n\nTruncated\nSamples from a Truncated Distribution.\nA truncated distribution arises when you take a random variable X that originally has some distribution (with PDF f_X(x) and CDF F_X(x)) and you restrict attention only to those values of X that are above a given truncation point a. In other words you only observe X when X &gt; a. All the ‚Äúmass‚Äù below (or equal to) a is excluded (not just unobserved, but removed from the sample/analysis). This differs from censoring, where values below a threshold might be known (for example ‚Äú&lt; a‚Äù), but here they are entirely excluded from the domain. Left truncation is common in many applied fields ‚Äî for instance:\n\nIn survival analysis: subjects whose event time happens before the study start are not included.\nIn insurance or losses: only losses above a deductible (threshold) are recorded, so the loss distribution is left-truncated at that deductible.\nIn industrial/life-data: items used before data collection start, so only those with lifetime &gt; some lower bound are observed.\n\nLet (X) be a random variable with PDF. Choose truncation points a &lt; X \\le b. Then the truncated distribution (Y = X \\mid a &lt; X \\le b) has:\n\nf_Y(y) = \\frac{f_X(y)}{F_X(b) - F_X(a)}, \\quad \\text{for } a &lt; y \\le b\n and (0) outside that interval. * The lower bound (a) can be -\\infty (so only an upper truncation), and the upper bound (b) can be +\\infty (so only a lower truncation). * If both bounds are finite (i.e., doubly truncated), that is a valid and often‚Äêused scenario.\n\nArgs:\nbi.dist.truncated_distribution(\nbase_dist,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbase_dist: The base distribution to be truncated. This should be a univariate distribution. Currently, only the following distributions are supported: Cauchy, Laplace, Logistic, Normal, and StudentT.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI TruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the TruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_distribution(base_dist = m.dist.normal(0,1, create_obj = True), high=1, low = 0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncateddistribution\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Truncated_distribution ‚ÄúTruncated distribution‚Äù\nhttps://pages.stern.nyu.edu/~wgreene/Text/Edition8/PDF/M19_GREE1366_08_SE_C19.pdf ‚Äú19‚Äù\nhttps://search.r-project.org/CRAN/refmans/LaplacesDemon/html/dist.Truncated.html ‚ÄúR: Truncated Distributions‚Äù\nhttps://dspace.cuni.cz/bitstream/handle/20.500.11956/127921/130308341.pdf?isAllowed=y&sequence=1&utm_source.com ‚ÄúTruncated data thesis‚Äù\n\n\n\n\n\nTruncated Normal\nA truncated normal distribution is derived from a normal (Gaussian) random variable by restricting (truncating) its domain to an interval [a, b] (which could be one‚Äêsided, e.g., (a) only or (b) only). It is defined by its location (loc), scale (scale), lower bound a (low), and upper bound b (high). In effect: if X \\sim  N(\\mu, \\sigma^2), then the truncated version Y = X | (a \\le X \\le b) has the same ‚Äúshape‚Äù but only supports values in [a,b]. This is used when you know that values outside a range are impossible or not observed (e.g., measurement limits, natural bounds).\nLet X \\sim  N(\\mu, \\sigma^2). Define truncation bounds a &lt; b (could be a = -\\infty or b = +\\infty for one‚Äêsided truncation). Then for y \\in [a,b],\n\nf_Y(y) = \\frac{1}{\\sigma} ; \\frac{\\varphi!\\bigl(\\frac{y - \\mu}{\\sigma}\\bigr)}{\\Phi!\\bigl(\\frac{b - \\mu}{\\sigma}\\bigr) - \\Phi!\\bigl(\\frac{a - \\mu}{\\sigma}\\bigr)} ,\n\nwhere:\n\n\\varphi(z) is the standard normal PDF, \\varphi(z) = \\frac1{\\sqrt{2\\pi}} e^{-z^2/2}.\n\\Phi(z) is the standard normal CDF.\nThe denominator normalises the density so the total probability over ([a,b]) is 1.\n\nFor values y &lt; a or y &gt; b, f_Y(y) = 0.\n\nArgs:\nbi.dist.truncated_normal(\nloc=0.0,\nscale=1.0,\nlow=None,\nhigh=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nloc (float): The location parameter of the normal distribution.\nsample (float): The scale parameter of the normal distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI TruncatedNormal distribution object (for model building). JAX array of samples drawn from the TruncatedNormal distribution (for direct sampling). The raw BI distribution object (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_normal(loc=0.0, scale=1.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#truncatednormal_lowercase\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Truncated_normal_distribution ‚ÄúTruncated normal distribution‚Äù\nhttps://people.math.sc.edu/burkardt/m_src/truncated_normal/truncated_normal.html ‚ÄúTRUNCATED_NORMAL - The Truncated Normal Distribution‚Äù\nhttps://www.statisticshowto.com/truncated-normal-distribution/ ‚ÄúTruncated Distribution / Truncated Normal Distribution - Statistics ‚Ä¶‚Äù\nhttps://real-statistics.com/normal-distribution/truncated-normal-distribution/ ‚ÄúTruncated Normal Distribution | Real Statistics Using Excel‚Äù\nhttps://stats.stackexchange.com/questions/525894/understanding-the-pdf-of-a-truncated-normal-distribution ‚ÄúUnderstanding the pdf of a truncated normal distribution‚Äù\n\n\n\n\n\nTruncated PolyaGamma\nSamples from a Truncated PolyaGamma distribution.\nThis distribution is a truncated version of the PolyaGamma distribution, defined over the interval [0, truncation_point]. It is often used in Bayesian non-parametric models.\n\nArgs:\nbi.dist.truncated_polya_gamma(\nbatch_shape=(),\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbatch_shape (tuple): The shape of the batch dimension.\nevent (int): The number of batch dimensions to reinterpret as event dimensions.\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Truncated PolyaGamma distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Truncated PolyaGamma distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.truncated_polya_gamma(batch_shape=(), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#truncatedpolygammadistribution\n\n\n\n\nTwo Sided Truncated\nA ‚Äútwo-sided truncated distribution‚Äù is a general concept: you take a base continuous distribution and restrict it to an interval ([low, high]), discarding all mass outside, then renormalize so the inner portion integrates to 1. I‚Äôll spell out the general formulas, caveats, sampling strategies, and special cases (e.g.¬†truncated normal) to illustrate.\n\nf(x) =\n\\begin{cases}\n\\dfrac{p(x)}{P(\\text{low} \\le X \\le \\text{high})},  \\text{if } \\text{low} \\le x \\le \\text{high}, \\\\[6pt]\n0,  \\text{otherwise}.\n\\end{cases}\n\nwhere p(x) is the probability density function of the base distribution.\n\nArgs:\nbi.dist.two_sided_truncated_distribution(\nbase_dist,\nlow=0.0,\nhigh=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbase_dist: The base distribution to truncate.\nlow: The lower bound for truncation.\nhigh: The upper bound for truncation.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI TwoSidedTruncatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the TwoSidedTruncatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#twosidedtruncateddistribution\n\n\n\n\nUniform\nThe Uniform distribution is the simplest continuous distribution: every value in the interval ([a, b]) is equally likely. It is widely used for modeling complete randomness within a fixed range, random sampling, and as a building block for other distributions.\n   \nf(x) =\n\\begin{cases}\n\\dfrac{1}{b - a},  a \\le x \\le b, [2mm]\n0,  \\text{otherwise}.\n\\end{cases}\n\n\nArgs:\nbi.dist.uniform(\nlow=0.0,\nhigh=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlow (jnp.ndarray): The lower bound of the uniform interval.\nhigh (jnp.ndarray): The upper bound of the uniform interval.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily. Defaults to False.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Uniform distribution object (for model building) when sample=False.\nJAX array of samples drawn from the Uniform distribution (for direct sampling) when sample=True.\nThe raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.uniform(low=0.0, high=1.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#uniform\n\n\nReferences:\n\nWikipedia ‚Äì Uniform distribution (continuous)\n\n\n\n\n\nUnit\n\nThe Unit distribution is the simplest continuous probability distribution.\nEvery number in ([0,1]) is equally likely.\nOften used in simulation, Monte Carlo methods, and as a building block for generating other distributions via the inverse-CDF method. \nf(x) =\n\\begin{cases}\n1,  0 \\le x \\le 1, [1mm]\n0,  \\text{otherwise}.\n\\end{cases}\n\n\n\nArgs:\nbi.dist.unit(\nlog_factor,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nlog_factor (jnp.ndarray): Log factor for the unit distribution. This parameter determines the shape and batch size of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Unit distribution object: When sample=False (for model building). jnp.ndarray: A JAX array of samples drawn from the Unit distribution (for direct sampling). BI Unit distribution object: When create_obj=True (for advanced use cases).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.unit(log_factor=jnp.ones(5), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#unit\n\n\n\n\nWeibull\nSamples from a Weibull distribution.\nThe Weibull distribution is widely used for modeling lifetime or reliability data. Its shape parameter (k) controls the hazard function:\n\n(k &lt; 1): decreasing hazard (infant mortality)\n(k = 1): constant hazard ‚Üí reduces to Exponential distribution\n(k &gt; 1): increasing hazard (aging/failure over time)\n\n\nf(x \\mid \\lambda, k) =\n\\begin{cases}\n\\dfrac{k}{\\lambda} \\left(\\dfrac{x}{\\lambda}\\right)^{k-1} e^{-(x/\\lambda)^k},  x \\ge 0, [2mm]\n0,  x &lt; 0\n\\end{cases}\n\n\nArgs:\nbi.dist.weibull(\nscale,\nconcentration,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (jnp.ndarray): The scale parameter of the Weibull distribution. Must be positive.\nconcentration (jnp.ndarray): The concentration parameter of the Weibull distribution. Must be positive.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI Weibull distribution object (for model building) when sample=False. JAX array of samples drawn from the Weibull distribution (for direct sampling) when sample=True. The raw BI distribution object (for advanced use cases) when create_obj=True.\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.weibull(scale=1.0, concentration=2.0, sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#weibull\n\n\nReferences:\n\nWikipedia ‚Äì Weibull distribution\nWeibull, W. (1951). A Statistical Distribution Function of Wide Applicability. Journal of Applied Mechanics.\n\n\n\n\n\nWishart\nThe Wishart distribution is a multivariate distribution used to model positive definite matrices, often representing covariance matrices. It‚Äôs commonly used in Bayesian statistics and machine learning, particularly in models involving covariance estimation.\n\nX \\sim \\text{Wishart}(scale_matrix=V, concentration=n)\n\nwhere:\n\np = dimension of the square matrices\n{V} \\in {R}^{p \\times p} = positive definite scale matrix (scale_matrix)\nn \\ge p = degrees of freedom (concentration)\n\nThe Wishart distribution is the multivariate generalization of the chi-squared distribution. It is commonly used as the distribution of the sample covariance matrix for multivariate normal samples.\n\nf({X} \\mid {V}, n) =\n\\frac{ |{X}|^{(n-p-1)/2} \\exp\\big(-\\frac{1}{2} {tr}({V}^{-1} {X}) \\big) }\n{ 2^{np/2} |{V}|^{n/2} \\Gamma_p(n/2) }, \\quad {X} \\succ 0\n\nwhere:\n\n|{X}| = determinant of {X}\n{tr}(\\cdot) = trace\n\\Gamma_p(\\cdot) = multivariate Gamma function:\n\n\nArgs:\nbi.dist.wishart(\nconcentration,\nscale_matrix=None,\nrate_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nconcentration (jnp.ndarray): Positive concentration parameter analogous to the concentration of a Gamma distribution. The concentration must be larger than the dimensionality of the scale matrix.\nscale_matrix (jnp.ndarray, optional): Scale matrix analogous to the inverse rate of a Gamma distribution.\nrate_matrix (jnp.ndarray, optional): Rate matrix anaologous to the rate of a Gamma distribution.\nscale_tril (jnp.ndarray, optional): Cholesky decomposition of the :code:scale_matrix.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool, optional): Optional boolean array to mask observations.\ncreate_obj (bool, optional): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI Wishart distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the Wishart distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.wishart(concentration=5.0, scale_matrix=jnp.eye(2), sample=True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#wishart\n\n\nReferences:\n\nWikipedia ‚Äì Wishart distribution\n\n\n\n\n\nWishart Cholesky\n\nThe Wishart distribution is a distribution over positive definite matrices, often used as a prior for covariance or precision matrices in multivariate normal models.\nThe Cholesky parameterization of the Wishart (called ‚Äúwishart_cholesky‚Äù in Stan, for example) reparameterizes the Wishart over its lower (or upper) triangular Cholesky factor. This is useful for numerical stability and unconstrained parameterization in Bayesian sampling frameworks.\nIn this parameterization, one works with a lower‚Äêtriangular matrix L_W such that \n\\Sigma = L_W L_W^\\top\n and imposes a density over L_W corresponding to the induced Wishart density on ().\n\nThe probability density function (PDF) is given by:\nIf the dimension is K, degrees of freedom \\nu &gt; K - 1, and L_S is the lower‚Äêtriangular Cholesky factor of the scale matrix S, then the log density for L_W under the wishart_cholesky distribution is:\n\n\\log p(L_W \\mid \\nu, L_S)\n= \\log \\bigl[ \\text{Wishart}(L_W L_W^\\top \\mid \\nu, L_S L_S^\\top) \\bigr] + \\log \\bigl| J_{f^{-1}} \\bigr|\n\nwhere J_{f^{-1}} is the Jacobian of the transformation from L_W to \\Sigma = L_W L_W^\\top.\nThe Jacobian term (absolute log determinant) is:\n\n\\log |J_{f^{-1}}| = K \\log 2 ; \\sum_{k=1}^K (K - k + 1),\\log L_{W_{k,k}}.\n\nThus the density includes both the usual Wishart density on \\Sigma plus this extra Jacobian (which penalizes or weights the diagonal entries of L_W).\n\nParameters\n\nconcentration: (Tensor) Positive concentration parameter analogous to the concentration of a :class:Gamma distribution. The concentration must be larger than the dimensionality of the scale matrix.\nscale_matrix: (Tensor, optional) Scale matrix analogous to the inverse rate of a :class:Gamma distribution. If not provided, rate_matrix or scale_tril must be.\nrate_matrix: (Tensor, optional) Rate matrix anaologous to the rate of a :class:Gamma distribution. If not provided, scale_matrix or scale_tril must be.\nscale_tril: (Tensor, optional) Cholesky decomposition of the :code:scale_matrix. If not provided, scale_matrix or rate_matrix must be.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Wishart_distribution ‚ÄúWishart distribution‚Äù\nhttps://mc-stan.org/docs/2_32/functions-reference/wishart-cholesky-distribution.html‚Äù28.2 Wishart distribution, Cholesky Parameterization - Stan‚Äù\n\nbi.dist.wishart_cholesky(\nconcentration,\nscale_matrix=None,\nrate_matrix=None,\nscale_tril=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\n\n\n\nGeneric Zero Inflated\nA Zero-Inflated distribution combines a base distribution with a Bernoulli distribution to model data with an excess of zero values. It assumes that each observation is either drawn from the base distribution or is a zero with probability determined by the Bernoulli distribution (the ‚Äúgate‚Äù). A zero-inflated distribution arises when you take a random variable X that originally has some distribution (with PMF f_X(x) ) and you add extra mass at zero. In other words you only observe X when X \\ge 0 (if count) and you assume there are more zeros than what the original distribution predicts ‚Äî so you mix in a point-mass at zero. A zero-inflated model assumes two processes: a ‚Äústructural zero‚Äù process with probability \\pi, and another process (the base count/distribution) with probability 1 - \\pi, which itself may generate zeros or non-zeros.\n\nWhen / Why use it:\nData collection constraints: You can only observe values in a process when some condition is met; others are automatically zero (for example, when the event cannot happen for some units).\nMixtures of processes: Some observations are ‚Äústructural zeros‚Äù (no risk) and others follow a regular count process (with risk).\nOverdispersion & excess zeros: If you try a standard count distribution (Poisson, Negative Binomial) and you observe many more zeros than predicted (given the non-zero counts), a zero-inflated alternative may fit better.\nThe general PMF form is (for discrete count models): \nP(X=0) = \\pi + (1 - \\pi),P_{\\text{base}}(0),\n\\quad\nP(X=k) = (1 - \\pi),P_{\\text{base}}(k), ; k &gt; 0.\n\n\n\nArgs:\nbi.dist.zero_inflated_distribution(\nbase_dist,\ngate=None,\ngate_logits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nbase_dist (Distribution): The base distribution to be zero-inflated (e.g., Poisson, NegativeBinomial).\ngate (jnp.ndarray, optional): Probability of extra zeros (between 0 and 1).\ngate_logits (jnp.ndarray, optional): Log-odds of extra zeros.\nvalidate_args (bool, optional): Whether to validate parameter values. Defaults to None.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroInflatedDistribution distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroInflatedDistribution distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_distribution(base_dist=m.dist.poisson(rate=5, create_obj = True), gate = 0.3, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflateddistribution\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Zero-inflated_model ‚ÄúZero-inflated model‚Äù\nhttps://larmarange.github.io/guide-R/analyses_avancees/modeles-zero-inflated.html ‚Äú48 Mod√®les de comptage zero-inflated et hurdle ‚Äì guide-R‚Äù\n\n\n\n\n\nZero-Inflated Negative Binomial\nA Zero-Inflated Negative Binomial distribution is used for count data that exhibit both (a) over-dispersion relative to a Poisson (i.e., variance &gt; mean) and (b) an excess of zero counts beyond what a standard Negative Binomial would predict. It assumes two latent processes:\n\nWith probability $ $ (sometimes denoted \\psi or ‚Äúzero-inflation probability‚Äù) you are in a ‚Äústructural zero‚Äù state ‚Üí you observe a zero.\nWith probability 1 - \\pi, you come from a regular Negative Binomial distribution (with parameters e.g.¬†mean $ $ and dispersion parameter $ $ or size/r parameter) and then you might observe zero or a positive count.\n\nThus the model is a mixture of a point‚Äêmass at zero + a Negative Binomial for counts.\nThis distribution combines a Negative Binomial distribution with a binary gate variable. Observations are either drawn from the Negative Binomial distribution with probability (1 - gate) or are treated as zero with probability ‚Äògate‚Äô. This models data with excess zeros compared to what a standard Negative Binomial distribution would predict.\nLet X denote the count random variable, support $ {0,1,2,} $. Denote:\n\n\\pi = probability of being in the ‚Äúalways zero‚Äù (structural zero) process, with $ 0 $.\nThe count process is NB with parameters: mean \\mu&gt;0 and dispersion/shape parameter \\alpha&gt;0 (or equivalently size parameter (r)). Then:\n\n\nP(X = 0) = \\pi ;+; (1 - \\pi); P_{\\text{NB}}(0;\\mu,\\alpha),\n \nP(X = k) = (1 - \\pi); P_{\\text{NB}}(k;\\mu,\\alpha), \\quad k = 1,2,3,\\dots\n\nHere P_{\\text{NB}}(k;\\mu,\\alpha) is the PMF of the negative binomial distribution for count k.\n\nArgs:\nbi.dist.zero_inflated_negative_binomial2(\nmean,\nconcentration,\ngate=None,\ngate_logits=None,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nmean (jnp.ndarray or float): The mean of the Negative Binomial 2 distribution.\nconcentration (jnp.ndarray or float): The concentration parameter of the Negative Binomial 2 distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroInflatedNegativeBinomial2 distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroInflatedNegativeBinomial2 distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_negative_binomial2(mean=2.0, concentration=3.0, gate = 0.3, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflatednegativebinomial2\n\n\nReferences:\n\nhttps://www.ewadirect.com/proceedings/tns/article/view/27624 ‚ÄúSocial Networks Count Data: Negative Binomial Distributions and Extensions Versus Poisson and Bernoulli Models‚Äù\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC7880198/ ‚ÄúBayesian Zero-Inflated Negative Binomial Regression Based on P√≥lya-Gamma Mixtures - PMC‚Äù\nhttps://search.r-project.org/CRAN/refmans/emdbook/html/dzinbinom.html ‚ÄúR: Zero-inflated negative binomial distribution‚Äù\n\n\n\n\n\nA Zero Inflated Poisson\nThe Zero-Inflated Poisson distribution is a discrete count‚Äêdistribution designed for data with more zeros than would be expected under a standard Poisson. Essentially, it assumes two underlying processes:\n\nWith probability \\pi you are in a ‚Äústructural zero‚Äù state (i.e., you automatically get a zero count).\nWith probability 1 - \\pi you draw from a standard Poisson distribution with parameter \\lambda.\n\nThis results in a mixture distribution that places more mass at zero than a Poisson alone would. It‚Äôs widely used in, for instance, ecology (species counts with many zeros), insurance/claims problems, and any count‚Äêdata setting with excess zeros.\nThe probability density function (PDF) is given by:\nLet X be a random variable following a Zero-Inflated Poisson with parameters\n\n\\pi\\in[0,1]: zero-inflation probability (chance of being structural zero)\n\\lambda&gt;0: mean of the Poisson component\n\nThen: \nP(X = k) =\n\\begin{cases}\n\\pi ;+; (1 - \\pi),e^{-\\lambda},  \\text{if } k = 0, [6pt]\n(1 - \\pi),\\dfrac{\\lambda^k e^{-\\lambda}}{k!},  \\text{if } k = 1,2,3,\\dots\n\\end{cases}\n\n(Here the Poisson component contributes also a mass at zero of (1 - \\pi)e^{-\\lambda}, plus the extra \\pi mass.)\n\nArgs:\nbi.dist.zero_inflated_poisson(\ngate,\nrate=1.0,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\ngate (jnp.ndarray): The probability of observing a zero.\nrate (jnp.ndarray): The rate parameter of the underlying Poisson distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\nBI ZeroInflatedPoisson distribution object (when sample=False). JAX array of samples drawn from the ZeroInflatedPoisson distribution (when sample=True). The raw BI distribution object (when create_obj=True).\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_inflated_poisson(gate = 0.3, rate=2.0, sample=True)\n\n\nWrapper of: https://num.pyro.ai/en/stable/distributions.html#zeroinflatedpoisson\n\n\nReferences:\n\nhttps://en.wikipedia.org/wiki/Zero-inflated_model ‚ÄúZero-inflated model‚Äù\nhttps://www.statisticshowto.com/zero-inflated-poisson-distribution ‚ÄúZero-Inflated Poisson distribution - Statistics How To‚Äù\n\n\n\n\n\nZero Sum Normal\nA zero-sum normal is a variant of a multivariate normal in which one (or more) linear constraint(s) force certain components to sum to zero. In practice, it‚Äôs used to model vectors of random effects (e.g.¬†in hierarchical models) where the effects are constrained to sum to zero (to avoid overparameterization or enforce identifiability).\n\nZSN(\\sigma) = N(0, \\sigma^2 (I - \\tfrac{1}{n}J)) \\\\\n\\text{where} \\ ~ J_{ij} = 1 \\ ~ \\text{and} \\\\\nn = \\text{number of zero-sum axes}\n\nwhere J is the all-ones matrix and n is the number of constrained elements. So the covariance is \\sigma^2 (I - \\tfrac{1}{n} J), which ensures that the sum of the components is 0 (since $ J = _i x_i $).\n\nArgs:\nbi.dist.zero_sum_normal(\nscale,\nevent_shape,\nvalidate_args=None,\nname='x',\nobs=None,\nmask=None,\nsample=False,\nseed=None,\nshape=(),\nevent=0,\ncreate_obj=False,\nto_jax=True,\n)\n\nscale (array_like): Standard deviation of the underlying normal distribution before the zerosum constraint is enforced.\nevent_shape (tuple): Shape of the event dimension of the distribution.\nshape (tuple): A multi-purpose argument for shaping. When sample=False (model building), this is used with shape to set the distribution‚Äôs batch shape. When sample=True (direct sampling), this is used as sample_shape to draw a raw JAX array of the given shape.\nevent (int): The number of batch dimensions to reinterpret as event dimensions (used in model building).\nmask (jnp.ndarray, bool): Optional boolean array to mask observations.\ncreate_obj (bool): If True, returns the raw BI distribution object instead of creating a sample site. This is essential for building complex distributions like MixtureSameFamily.\nsample (bool, optional): A control-flow argument. If True, the function will directly sample a raw JAX array from the distribution, bypassing the BI model context. If False, it will create a BI.sample site within a model. Defaults to False.\nseed (int, optional): An integer used to generate a JAX PRNGKey for reproducible sampling when sample=True. This argument has no effect when sample=False, as randomness is handled by BI‚Äôs inference engine. Defaults to 0.\nobs (jnp.ndarray, optional): The observed value for this random variable. If provided, the sample site is conditioned on this value, and the function returns the observed value. If None, the site is treated as a latent (unobserved) random variable. Defaults to None.\nname (str, optional): The name of the sample site in a BI model. This is used to uniquely identify the random variable. Defaults to ‚Äòx‚Äô.\n\n\n\nReturns:\n\nWhen sample=False: A BI ZeroSumNormal distribution object (for model building).\nWhen sample=True: A JAX array of samples drawn from the ZeroSumNormal distribution (for direct sampling).\nWhen create_obj=True: The raw BI distribution object (for advanced use cases).\n\n\n\nExample Usage:\nfrom BI import bi\nm = bi('cpu')\nm.dist.zero_sum_normal(scale=1.0, event_shape = (2,), sample = True)\n\n\nWrapper of:\nhttps://num.pyro.ai/en/stable/distributions.html#zerosumnormal\n\n\nReferences:\nhttps://www.pymc.io/projects/docs/en/latest/api/distributions/generated/pymc.ZeroSumNormal.html",
    "crumbs": [
      "Get started",
      "Distributions"
    ]
  },
  {
    "objectID": "api_manip.html",
    "href": "api_manip.html",
    "title": "Handle data",
    "section": "",
    "text": "manip is a class to unify various diagnostics methods and provide a consistent interface for diagnostics.",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#perform-one-hot-encoding-on-specified-columns",
    "href": "api_manip.html#perform-one-hot-encoding-on-specified-columns",
    "title": "Handle data",
    "section": "Perform one-hot encoding on specified columns",
    "text": "Perform one-hot encoding on specified columns\n\nArgs:\n\ncols (str or list): Columns to encode. Use ‚Äòall‚Äô for all object-type columns\n\n\n\nReturns:\n\npd.DataFrame: DataFrame with encoded columns\n\nbi.dist.OHE(\nself,\ncols='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#load-data-from-csv-file",
    "href": "api_manip.html#load-data-from-csv-file",
    "title": "Handle data",
    "section": "Load data from CSV file",
    "text": "Load data from CSV file\n\nArgs:\n\npath (str): Path to the CSV file\n\n**kwargs*: Additional arguments for pd.read_csv\n\n\n\n\nReturns:\npd.DataFrame: Loaded dataframe\nbi.dist.data(\nself,\npath,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#prepare-data-for-model-input-in-jax-format",
    "href": "api_manip.html#prepare-data-for-model-input-in-jax-format",
    "title": "Handle data",
    "section": "Prepare data for model input in JAX format",
    "text": "Prepare data for model input in JAX format\n\nArgs:\n\ncols (list): List of columns to include in model data\n\n\n\nReturns:\n\ndict: JAX formatted dictionary\n\nbi.dist.data_to_model(\nself,\ncols,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#create-index-encoding-for-categorical-columns",
    "href": "api_manip.html#create-index-encoding-for-categorical-columns",
    "title": "Handle data",
    "section": "Create index encoding for categorical columns",
    "text": "Create index encoding for categorical columns\n\nArgs:\n\ncols (str or list): Columns to encode. Use ‚Äòall‚Äô for all object-type columns\n\n\n\nReturns:\n\npd.DataFrame: DataFrame with encoded columns\n\nbi.dist.index(\nself,\ncols='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-pandas-dataframe-to-jax-compatible-format-for-a-model",
    "href": "api_manip.html#convert-pandas-dataframe-to-jax-compatible-format-for-a-model",
    "title": "Handle data",
    "section": "Convert pandas dataframe to JAX compatible format for a model",
    "text": "Convert pandas dataframe to JAX compatible format for a model\n\nArgs:\n\nmodel: JAX model to prepare data for\nbit (str): Bit precision for numbers (default: 32)\n\n\n\nReturns:\n\ndict: JAX formatted dictionary\n\nbi.dist.pd_to_jax(\nself,\nmodel,\nbit=None,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#standardize-specified-columns",
    "href": "api_manip.html#standardize-specified-columns",
    "title": "Handle data",
    "section": "Standardize specified columns",
    "text": "Standardize specified columns\n\nArgs:\n\ndata (str or list): Columns to standardize. Use ‚Äòall‚Äô for all columns\n\n\n\nReturns:\n\npd.DataFrame: Standardized dataframe\n\nbi.dist.scale(\nself,\ndata='all',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#jax-jitted-function-to-scalestandardize-a-single-variable",
    "href": "api_manip.html#jax-jitted-function-to-scalestandardize-a-single-variable",
    "title": "Handle data",
    "section": "JAX-jitted function to scale/standardize a single variable",
    "text": "JAX-jitted function to scale/standardize a single variable\nbi.dist.scale_var(\nself,\nx,\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-specified-columns-to-float-type",
    "href": "api_manip.html#convert-specified-columns-to-float-type",
    "title": "Handle data",
    "section": "Convert specified columns to float type",
    "text": "Convert specified columns to float type\n\nArgs:\n\ncols (str or list): Columns to convert. Use ‚Äòall‚Äô for all columns\ntype (str): Float type to convert to (default: float32)\n\n\n\nReturns:\n\npd.DataFrame: Converted dataframe\n\nbi.dist.to_float(\nself,\ncols='all',\ntype='float32',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "api_manip.html#convert-specified-columns-to-integer-type",
    "href": "api_manip.html#convert-specified-columns-to-integer-type",
    "title": "Handle data",
    "section": "Convert specified columns to integer type",
    "text": "Convert specified columns to integer type\n\nArgs:\n\ncols (str or list): Columns to convert. Use ‚Äòall‚Äô for all columns\ntype (str): Integer type to convert to (default: int32)\n\n\n\nReturns:\n\npd.DataFrame: Converted dataframe\n\nbi.dist.to_int(\nself,\ncols='all',\ntype='int32',\n)",
    "crumbs": [
      "Get started",
      "Handle data"
    ]
  },
  {
    "objectID": "27. BNN.html",
    "href": "27. BNN.html",
    "title": "Bayesian Neural Networks",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of ‚Äúneurons.‚Äù Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our ‚Äúknobs.‚Äù\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\beta). In a neural network, which can have thousands or millions of weights, we don‚Äôt define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network‚Äôs predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\sigma that quantifies the data‚Äôs noise around the model‚Äôs predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#general-principles",
    "href": "27. BNN.html#general-principles",
    "title": "Bayesian Neural Networks",
    "section": "",
    "text": "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of ‚Äúneurons.‚Äù Each connection between neurons has a weight, and each neuron has a bias. These weights and biases are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its weights and biases. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n\nA Network Architecture, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our ‚Äúknobs.‚Äù\nPriors for Arrays of Weights and Biases. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope \\beta). In a neural network, which can have thousands or millions of weights, we don‚Äôt define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire array of parameters. For example, we might declare that all weights in a specific layer are drawn from the same Normal(0, 1) distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\nAn Output Distribution (Likelihood), which defines the probability of the data given the network‚Äôs predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term \\sigma that quantifies the data‚Äôs noise around the model‚Äôs predictions.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#considerations",
    "href": "27. BNN.html#considerations",
    "title": "Bayesian Neural Networks",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nLike all Bayesian models, BNNs consider model parameter uncertainty üõà. The parameters here are the network‚Äôs weights (W) and biases (b). We quantify our uncertainty about them through their posterior distribution üõà. Therefore, we must declare prior distributions üõà for all weights and biases, as well as for the output variance \\sigma.\nUnlike in a linear regression where the coefficient \\beta has a direct interpretation (e.g., the effect of weight on height), the individual weights and biases in a BNN are not directly interpretable. A single weight‚Äôs influence is entangled with thousands of other parameters through non-linear functions. Consequently, BNNs are best viewed as powerful predictive tools rather than explanatory ones. They excel at learning complex patterns and quantifying predictive uncertainty, but if the goal is to isolate and interpret the effect of a specific variable, a simpler model is often more appropriate.\nPrior distributions are built following these considerations:\n\nAs the data is typically scaled üõà (see introduction), we can use a standard Normal distribution (mean 0, standard deviation 1) as a weakly-informative prior for all weights and biases. This acts as a form of regularization.\nSince the output variance \\sigma must be positive, we can use a positively-defined distribution, such as the Exponential or Half-Normal.\n\nBNNs can be used for both regression and classification. The final layer‚Äôs activation and the chosen likelihood distribution depend on the task. For binary classification, a sigmoid activation is paired with a Bernoulli likelihood, which requires a link function üõà (logit) to connect the linear output of the network to the probability space [0, 1]. For regression, the identity activation is often used with a Gaussian likelihood.",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#example",
    "href": "27. BNN.html#example",
    "title": "Bayesian Neural Networks",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian Neural Network for regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to predict height from weight using a non-linear model.\n\nPythonR\n\n\n\nfrom BI import bi\nimport json\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n\nwith open('BNN.json', 'r', encoding='utf-8') as file:\n    # Load the JSON data into a Python dictionary\n    data = json.load(file)\n# X is already scaled\nX = jnp.array(data['X']) # Note X shape = (N,2) where first column is the intercept and second column is the predictor\nY = jnp.array(data['Y']) # Note Y shape = (N,1) where N is the number of observations\n\nm.data_on_model = dict(X = X, Y = Y)\n# Define model ------------------------------------------------\ndef model(X, Y,  D_H=5, D_Y=1):  \n    N, D_X = X.shape\n    \n    # First hidden layer: Transforms input to N √ó D_H (hidden units)\n    w1 = m.bnn.layer_linear(\n        X, \n        dist=m.dist.normal(\n            0, 1,  name='w1',shape=(D_X,D_H)\n            ),\n        activation='tanh'\n        )\n\n    # sample final layer of weights and neural network output\n    # Final layer (z3) computes linear combination of second hidden layer\n    w2 = m.bnn.layer_linear(\n        X=w1, \n        dist=m.dist.normal(0, 1,  name='w2',shape=(D_H,D_Y))\n        )\n\n    sigma = m.dist.exponential(1, name='sigma')\n\n    m.dist.normal(w2, sigma, obs=Y,name='Y')\n\n# Run mcmc ------------------------------------------------\nm.fit(model, num_samples=500, progress_bar=False)   # Approximate posterior distributions for weights, biases, and sigma\n\n# Predictions from the model ------------------------------------------------\npred = m.sample(samples = 500)['Y']\npred = pred[..., 0]\nmean_prediction = jnp.mean(pred, axis=0)\npercentiles = jnp.percentile(pred, jnp.array([5.0, 95.0]), axis=0)\n# make plots\nfig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n# plot training data\nax.plot(X[:, 1], Y[:, 0], \"kx\")\n# plot 90% confidence level of predictions\nax.fill_between(\n    X[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n)\n# plot mean prediction\nax.plot(X[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\nax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n\njax.local_device_count 16\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n\n\n[Text(0.5, 0, 'X'),\n Text(0, 0.5, 'Y'),\n Text(0.5, 1.0, 'Mean predictions with 90% CI')]\n\n\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\n# Import data ------------------------------------------------\npath = normalizePath(paste(system.file(package = \"BayesianInference\"),\"/data/BNN.json\", sep = ''))\ndata &lt;- fromJSON(path)\nm$data_on_model = list()\nm$data_on_model$X = jnp$array(data$X)\nm$data_on_model$Y = jnp$array(data$Y)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(X, Y, D_X = 2, D_H=5L, D_Y=1L){\n\n  w1 &lt;- m$bnn$layer_linear(X, dist=bi.dist.normal(0, 1,  name='w1',shape=c(D_X,D_H)), activation='tanh')\n  \n  w2 &lt;- m$bnn$layer_linear(\n    w1, \n    dist=bi.dist.normal(0, 1,  name='w2',shape=c(D_H,D_Y)),\n    activation='tanh'\n  )\n  \n  # Prior for the output standard deviation\n  s = bi.dist.exponential(1, name = 's')\n  \n  # Likelihood\n  bi.dist.normal(w2, s, obs = Y)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Approximate posterior distributions",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#mathematical-details",
    "href": "27. BNN.html#mathematical-details",
    "title": "Bayesian Neural Networks",
    "section": "Mathematical Details",
    "text": "Mathematical Details",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#notes",
    "href": "27. BNN.html#notes",
    "title": "Bayesian Neural Networks",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe primary difference between a Frequentist and Bayesian neural network lies in how parameters are treated. In the frequentist approach, weights and biases are point estimates found by minimizing a loss function (e.g., via gradient descent). Techniques like Dropout or L2 regularization are often used to prevent overfitting, which can be interpreted as approximations to a Bayesian treatment. In contrast, the Bayesian formulation does not seek a single best set of weights. Instead, it uses methods like MCMC or Variational Inference to approximate the entire posterior distribution for every weight and bias. This provides a principled and direct way to quantify model uncertainty.\nWhile present an example of non-linear regression, the Bayesian Neural Network can be used for linear regressions as well (keeping in mind that interpretation of the weights are impossible).",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "27. BNN.html#references",
    "href": "27. BNN.html#references",
    "title": "Bayesian Neural Networks",
    "section": "Reference(s)",
    "text": "Reference(s)\n(neal1995bayesian?)",
    "crumbs": [
      "Models",
      "Bayesian Neural Networks üöß"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "Scalable - Built on top of Numpyro, TensorFlow Probability and Jax, for cpu, gpu and tpu vectorization and parallelization.\nFlexibility trade-offs - low-level abstraction coding available but also pre-build function for high-level of abstraction.\nUnified - One framework for both Python and R.\nAccessibility - 30 documented models.\nIntuitive - model-building syntax.\n\n\n\n\n\n\n\n\nPipelineFull Model definitionPrebuilt functionModel to LatexDistributions visualization\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu') # cpu, gpu or : tuple\n\n# Import Data ------------------------------------------------\nm.data(data_path) \n\n# Define model ------------------------------------------------\ndef model(arg1,argb2):\n   pass\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\n# Diagnostics ------------------------------------------------\nm.diag()\n\n\n\ndef model(kcal_per_g, index_clade):\n    alpha = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    beta = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    sigma = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.normal(mu, s, obs=kcal_per_g)\n\n\ndef model(kcal_per_g, index_clade):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n      N_group = N_cafes,\n      group = cafe,\n      global_intercept= a,\n      global_slope= b,\n      group_name = 'cafe'\n    )\n\n\n#| label: model-to-latex\n#| results: hold\n#| echo: true\nfrom BI import bi\nm = bi(platform='cpu')\n\n# define model ------------------------------------------------\ndef model(weight, height):    \n    alpha = m.dist.normal( 178, 20, name = 'a')\n    beta = m.dist.log_normal( 0, 1, name = 'b')   \n    sigma = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(alpha + beta * weight , sigma, obs=height)\n\n# Run sampler ------------------------------------------------\nm.model = model\nm.latex()\n\n\\begin{aligned}\nheight &\\sim \\text{Normal}(\\alpha + \\beta * weight, \\sigma) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 50) \\\\\n\\beta &\\sim \\text{LogNormal}(0, 1) \\\\\n\\alpha &\\sim \\text{Normal}(178, 20)\n\\end{aligned}\n\n\n\n\nfrom BI import bi\nm = bi()\nm.dist.normal( 0, 1, name = 'a', shape=(100,2, 4), sample = True, to_jax = False).hist()\n\njax.local_device_count 16"
  },
  {
    "objectID": "index.html#an-open-source-library-for-python-and-r",
    "href": "index.html#an-open-source-library-for-python-and-r",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "Scalable - Built on top of Numpyro, TensorFlow Probability and Jax, for cpu, gpu and tpu vectorization and parallelization.\nFlexibility trade-offs - low-level abstraction coding available but also pre-build function for high-level of abstraction.\nUnified - One framework for both Python and R.\nAccessibility - 30 documented models.\nIntuitive - model-building syntax."
  },
  {
    "objectID": "index.html#a-unified-scalable-and-accessible-framework",
    "href": "index.html#a-unified-scalable-and-accessible-framework",
    "title": "Welcome to Bayesian Inference (BI)",
    "section": "",
    "text": "PipelineFull Model definitionPrebuilt functionModel to LatexDistributions visualization\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu') # cpu, gpu or : tuple\n\n# Import Data ------------------------------------------------\nm.data(data_path) \n\n# Define model ------------------------------------------------\ndef model(arg1,argb2):\n   pass\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\n# Diagnostics ------------------------------------------------\nm.diag()\n\n\n\ndef model(kcal_per_g, index_clade):\n    alpha = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    beta = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    sigma = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.normal(mu, s, obs=kcal_per_g)\n\n\ndef model(kcal_per_g, index_clade):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n      N_group = N_cafes,\n      group = cafe,\n      global_intercept= a,\n      global_slope= b,\n      group_name = 'cafe'\n    )\n\n\n#| label: model-to-latex\n#| results: hold\n#| echo: true\nfrom BI import bi\nm = bi(platform='cpu')\n\n# define model ------------------------------------------------\ndef model(weight, height):    \n    alpha = m.dist.normal( 178, 20, name = 'a')\n    beta = m.dist.log_normal( 0, 1, name = 'b')   \n    sigma = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(alpha + beta * weight , sigma, obs=height)\n\n# Run sampler ------------------------------------------------\nm.model = model\nm.latex()\n\n\\begin{aligned}\nheight &\\sim \\text{Normal}(\\alpha + \\beta * weight, \\sigma) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 50) \\\\\n\\beta &\\sim \\text{LogNormal}(0, 1) \\\\\n\\alpha &\\sim \\text{Normal}(178, 20)\n\\end{aligned}\n\n\n\n\nfrom BI import bi\nm = bi()\nm.dist.normal( 0, 1, name = 'a', shape=(100,2, 4), sample = True, to_jax = False).hist()\n\njax.local_device_count 16"
  },
  {
    "objectID": "10. Multinomial model.html",
    "href": "10. Multinomial model.html",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Multinomial model."
  },
  {
    "objectID": "10. Multinomial model.html#general-principles",
    "href": "10. Multinomial model.html#general-principles",
    "title": "Multinomial Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Multinomial model."
  },
  {
    "objectID": "10. Multinomial model.html#considerations",
    "href": "10. Multinomial model.html#considerations",
    "title": "Multinomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Categorical model."
  },
  {
    "objectID": "10. Multinomial model.html#example",
    "href": "10. Multinomial model.html#example",
    "title": "Multinomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = [0]\n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;07:36,  2.19it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  10%|‚ñâ         | 99/1000 [00:00&lt;00:03, 234.06it/s, 255 steps of size 1.69e-02. acc. prob=0.76]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 372/1000 [00:00&lt;00:00, 865.42it/s, 3 steps of size 6.81e-01. acc. prob=0.78] sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/1000 [00:00&lt;00:00, 1322.86it/s, 7 steps of size 7.96e-01. acc. prob=0.91]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/1000 [00:00&lt;00:00, 1714.98it/s, 7 steps of size 7.96e-01. acc. prob=0.90]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1122.35it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n0.00\n0.97\n-1.60\n1.51\n0.05\n0.04\n428.89\n395.09\nNaN\n\n\na[1]\n82.06\n1.02\n80.40\n83.58\n0.05\n0.05\n472.75\n340.24\nNaN\n\n\nb[0]\n40.96\n0.50\n40.12\n41.65\n0.02\n0.02\n616.30\n368.44\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys &lt;- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues &lt;- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n  \n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n  \n  # Likelihood\n  m$dist$multinomial(probs=p[career], obs=career)\n}\n\n\n# Run sampler ------------------------------------------------ \nm$fit(model)  \n\n# Summary ------------------------------------------------\nm$summary()"
  },
  {
    "objectID": "10. Multinomial model.html#mathematical-details",
    "href": "10. Multinomial model.html#mathematical-details",
    "title": "Multinomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable Y_i with K categories, the Dirichlet likelihood function is:\n\nY_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the outcome (i.e.¬†the vector of frequencies for each k categories) for observation i.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex üõà.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category."
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\beta_x for each continuous variable (e.g., \\beta_{weight} and \\beta_{age}).",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#general-principles",
    "href": "2. Multiple continuous Variables.html#general-principles",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "To study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\beta_x for each continuous variable (e.g., \\beta_{weight} and \\beta_{age}).",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "Multivariate Linear Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Regression for continuous variable.\nThe model interpretation of the regression coefficients \\beta_x is considered for fixed values of the other independent variable(s)‚Äô regression coefficients‚Äîi.e., for a given age, \\beta_{weight} represents the expected change in the dependent variable (height) for each one-unit increase in weight, holding all other variables (e.g., age) constant.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "Multivariate Linear Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (height, weight, age), and the goal is to estimate the effect of weight and age on height. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nfrom importlib.resources import files\n# Import\ndata_path = m.load.howell1(only_path = True)\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Subset data to adults\nm.scale(['weight', 'age']) # Normalize\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0, 50, name = 'sigma')\n    # Likelihood\n    m.dist.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;08:19,  2.00it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 244/1000 [00:00&lt;00:01, 541.14it/s, 7 steps of size 8.33e-01. acc. prob=0.78]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/1000 [00:00&lt;00:00, 1074.58it/s, 7 steps of size 6.01e-01. acc. prob=0.93]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/1000 [00:00&lt;00:00, 1448.11it/s, 7 steps of size 6.01e-01. acc. prob=0.92]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1122.45it/s, 3 steps of size 6.01e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha\n5.20\n0.49\n4.46\n6.06\n0.02\n0.02\n469.94\n349.45\nNaN\n\n\nbeta1\n0.20\n0.51\n-0.60\n1.03\n0.02\n0.03\n570.81\n264.55\nNaN\n\n\nbeta2\n-0.02\n0.49\n-0.89\n0.69\n0.02\n0.02\n576.00\n338.15\nNaN\n\n\nsigma\n49.98\n0.02\n49.96\n50.00\n0.00\n0.00\n579.62\n271.13\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(m$load$howell1(only_path = T), sep=';')# Import\nm$df = m$df[m$df$age &gt; 18,] # Subset data to adults\nm$scale(list('weight', 'age')) # Normalize\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight, age){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 0.5, name = 'a')\n  beta1 = bi.dist.normal(0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  bi.dist.normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor R users, if you create the regression coefficient in a single call:\nbetas = bi.dist.normal(0, 0.5, name = 'regression_coefficients', shape = (2,))\nyou need to index them starting by 0:\n m$normal(alpha + betas[0] * weight + betas[1] * age, sigma, obs=height)",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "Multivariate Linear Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variables (X_{1i}, X_{2i}, ..., X_{[K,i]}) and the dependent variable Y using the following equation:\n\nùëå_i = \\alpha +\\beta_1  ùëã_{[1,i]} + \\beta_2  ùëã_{[2,i]} + ... + \\beta_n  ùëã_{[K,i]} + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\nX_{[1,i]}, X_{[2,i]}, ‚Ä¶, X_{[K,i]} are the values of the independent variables for observation i.\n\\beta_1, \\beta_2, ‚Ä¶, \\beta_K are the regression coefficients.\n\\epsilon_i is the error term for observation i, and the vector of the error terms, \\epsilon, are assumed to be independent and identically distributed.\n\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian model as follows:\n\nùëå_i \\sim \\text{Normal}(\\alpha + \\sum_{k=1}^K  \\beta_k  X_{[K,i]}, œÉ)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_k \\sim \\text{Normal}(0,1)\n\n\nœÉ \\sim \\text{Uniform}(0, 50)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_k are slope coefficients for the K distinct independent variables, which also have unit-normal priors.\nX_{[1,i]}, X_{[2,i]}, ‚Ä¶, X_{[K,i]} are the values of the independent variables for observation i.\n\\sigma is a standard deviation parameter, which here has a Uniform prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "Multivariate Linear Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "16. Measuring error.html",
    "href": "16. Measuring error.html",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#general-principles",
    "href": "16. Measuring error.html#general-principles",
    "title": "Measurement Error Models",
    "section": "",
    "text": "Measurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#example",
    "href": "16. Measuring error.html#example",
    "title": "Measurement Error Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian measurement error model using the Bayesian Inference (BI) package. The data consist of three continuous variables (marriage rate, divorce rate, age), and the goal is to estimate the effect of age and marriage rate on the divorce rate while considering that the divorce rate has a measurement error. This example is based on McElreath (2018).\n\nPython\n\n\n\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\nm.data('WaffleDivorce.csv', sep=';') \nm.scale(['MedianAgeMarriage', 'Marriage']) # Scale\ndat = dict(\n    D_obs = m.z_score(m.df['Divorce'].values),   \n    D_sd = jnp.array(m.df['Divorce SE'].values / m.df['Divorce'].std()), \n    A = jnp.array(m.df['MedianAgeMarriage'].values), \n    M = jnp.array(m.df['Marriage'].values),\n    N = m.df.shape[0]   \n)\nm.data_on_model = dat # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(D_obs, D_sd, A, N, M):  \n    a = m.dist.normal(0, 0.2, name = 'a') \n    beta = m.dist.normal(0, 0.5, name = 'beta')\n    eta = m.dist.normal(0, 0.5, name = 'eta')  \n    s = m.dist.exponential(1, name = 's') \n    mu = a + beta * A + eta * M\n    D_true = m.dist.normal(mu, s, name = 'D_true') \n    m.dist.normal(D_true , D_sd, obs = D_obs) \n\n# Run MCMC ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;27:15,  1.64s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   6%|‚ñå         | 57/1000 [00:01&lt;00:20, 45.48it/s, 15 steps of size 8.86e-02. acc. prob=0.76]warmup:  12%|‚ñà‚ñè        | 115/1000 [00:01&lt;00:08, 100.43it/s, 15 steps of size 4.09e-01. acc. prob=0.78]warmup:  18%|‚ñà‚ñä        | 179/1000 [00:01&lt;00:04, 169.19it/s, 63 steps of size 1.55e-01. acc. prob=0.78]warmup:  25%|‚ñà‚ñà‚ñç       | 248/1000 [00:02&lt;00:03, 249.52it/s, 15 steps of size 5.82e-01. acc. prob=0.78]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 320/1000 [00:02&lt;00:02, 334.13it/s, 7 steps of size 2.15e-01. acc. prob=0.78] warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [00:02&lt;00:01, 413.01it/s, 15 steps of size 3.85e-01. acc. prob=0.79]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/1000 [00:02&lt;00:01, 484.31it/s, 7 steps of size 8.71e-01. acc. prob=0.79] sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [00:02&lt;00:00, 528.14it/s, 15 steps of size 2.98e-01. acc. prob=0.93]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/1000 [00:02&lt;00:00, 568.46it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/1000 [00:02&lt;00:00, 557.47it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/1000 [00:02&lt;00:00, 606.63it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/1000 [00:02&lt;00:00, 630.77it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/1000 [00:02&lt;00:00, 638.25it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/1000 [00:03&lt;00:00, 646.48it/s, 15 steps of size 2.98e-01. acc. prob=0.92]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03&lt;00:00, 316.90it/s, 15 steps of size 2.98e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nD_true[0]\n1.20\n0.35\n0.73\n1.83\n0.01\n0.02\n594.50\n311.15\nNaN\n\n\nD_true[1]\n0.72\n0.56\n-0.18\n1.54\n0.02\n0.03\n825.18\n357.15\nNaN\n\n\nD_true[2]\n0.46\n0.32\n-0.00\n0.99\n0.01\n0.02\n804.31\n242.18\nNaN\n\n\nD_true[3]\n1.44\n0.47\n0.65\n2.05\n0.02\n0.03\n761.95\n307.56\nNaN\n\n\nD_true[4]\n-0.91\n0.13\n-1.13\n-0.70\n0.00\n0.01\n988.33\n321.69\nNaN\n\n\nD_true[5]\n0.66\n0.41\n0.04\n1.36\n0.01\n0.02\n1246.87\n323.91\nNaN\n\n\nD_true[6]\n-1.39\n0.38\n-1.89\n-0.76\n0.01\n0.02\n1009.18\n298.94\nNaN\n\n\nD_true[7]\n-0.35\n0.52\n-1.17\n0.49\n0.02\n0.03\n1089.02\n423.66\nNaN\n\n\nD_true[8]\n-1.91\n0.62\n-2.89\n-0.98\n0.02\n0.03\n679.08\n393.51\nNaN\n\n\nD_true[9]\n-0.63\n0.17\n-0.87\n-0.36\n0.00\n0.01\n1349.49\n348.55\nNaN\n\n\nD_true[10]\n0.78\n0.27\n0.38\n1.22\n0.01\n0.01\n1051.89\n257.32\nNaN\n\n\nD_true[11]\n-0.55\n0.50\n-1.40\n0.22\n0.02\n0.02\n847.21\n393.66\nNaN\n\n\nD_true[12]\n0.14\n0.55\n-0.69\n0.99\n0.02\n0.03\n695.14\n205.29\nNaN\n\n\nD_true[13]\n-0.89\n0.24\n-1.25\n-0.51\n0.01\n0.01\n1197.99\n353.12\nNaN\n\n\nD_true[14]\n0.57\n0.28\n0.11\n1.00\n0.01\n0.01\n1263.50\n425.36\nNaN\n\n\nD_true[15]\n0.29\n0.34\n-0.20\n0.84\n0.01\n0.02\n902.36\n369.49\nNaN\n\n\nD_true[16]\n0.50\n0.42\n-0.09\n1.20\n0.01\n0.02\n918.81\n387.37\nNaN\n\n\nD_true[17]\n1.29\n0.37\n0.78\n1.93\n0.01\n0.02\n1072.81\n329.82\nNaN\n\n\nD_true[18]\n0.46\n0.39\n-0.13\n1.09\n0.01\n0.02\n858.17\n321.87\nNaN\n\n\nD_true[19]\n0.44\n0.56\n-0.49\n1.26\n0.03\n0.02\n483.84\n347.58\nNaN\n\n\nD_true[20]\n-0.57\n0.29\n-1.06\n-0.13\n0.01\n0.01\n840.30\n406.25\nNaN\n\n\nD_true[21]\n-1.11\n0.26\n-1.56\n-0.75\n0.01\n0.02\n1349.49\n173.98\nNaN\n\n\nD_true[22]\n-0.27\n0.25\n-0.63\n0.13\n0.01\n0.01\n1306.06\n365.28\nNaN\n\n\nD_true[23]\n-1.02\n0.29\n-1.48\n-0.57\n0.01\n0.02\n1007.31\n270.10\nNaN\n\n\nD_true[24]\n0.46\n0.40\n-0.28\n0.98\n0.01\n0.02\n1027.80\n353.03\nNaN\n\n\nD_true[25]\n-0.03\n0.28\n-0.52\n0.37\n0.01\n0.01\n1017.75\n393.51\nNaN\n\n\nD_true[26]\n-0.04\n0.53\n-0.88\n0.76\n0.02\n0.03\n1122.42\n305.21\nNaN\n\n\nD_true[27]\n-0.16\n0.39\n-0.71\n0.48\n0.01\n0.02\n1349.49\n351.46\nNaN\n\n\nD_true[28]\n-0.25\n0.52\n-1.03\n0.61\n0.02\n0.05\n966.76\n160.75\nNaN\n\n\nD_true[29]\n-1.83\n0.26\n-2.20\n-1.40\n0.01\n0.01\n1106.20\n352.03\nNaN\n\n\nD_true[30]\n0.19\n0.48\n-0.60\n0.93\n0.01\n0.03\n1349.49\n302.06\nNaN\n\n\nD_true[31]\n-1.67\n0.16\n-1.96\n-1.43\n0.00\n0.01\n1115.72\n284.32\nNaN\n\n\nD_true[32]\n0.12\n0.23\n-0.25\n0.48\n0.01\n0.01\n1187.57\n311.15\nNaN\n\n\nD_true[33]\n-0.08\n0.49\n-0.79\n0.76\n0.02\n0.03\n833.38\n302.17\nNaN\n\n\nD_true[34]\n-0.11\n0.25\n-0.48\n0.30\n0.01\n0.02\n1349.49\n219.22\nNaN\n\n\nD_true[35]\n1.31\n0.44\n0.62\n2.04\n0.01\n0.02\n1131.53\n353.12\nNaN\n\n\nD_true[36]\n0.24\n0.34\n-0.32\n0.74\n0.01\n0.02\n790.58\n383.87\nNaN\n\n\nD_true[37]\n-1.04\n0.21\n-1.38\n-0.73\n0.01\n0.01\n1349.49\n365.09\nNaN\n\n\nD_true[38]\n-0.91\n0.55\n-1.92\n-0.16\n0.02\n0.03\n767.57\n425.43\nNaN\n\n\nD_true[39]\n-0.71\n0.29\n-1.12\n-0.24\n0.01\n0.01\n945.80\n438.47\nNaN\n\n\nD_true[40]\n0.26\n0.53\n-0.64\n1.09\n0.02\n0.03\n1201.76\n255.08\nNaN\n\n\nD_true[41]\n0.76\n0.33\n0.14\n1.17\n0.01\n0.02\n763.53\n224.82\nNaN\n\n\nD_true[42]\n0.20\n0.17\n-0.11\n0.46\n0.00\n0.01\n1349.49\n461.64\nNaN\n\n\nD_true[43]\n0.80\n0.48\n0.07\n1.56\n0.02\n0.02\n931.02\n304.36\nNaN\n\n\nD_true[44]\n-0.40\n0.52\n-1.27\n0.35\n0.01\n0.03\n1349.49\n368.12\nNaN\n\n\nD_true[45]\n-0.39\n0.24\n-0.74\n-0.02\n0.01\n0.01\n933.88\n332.15\nNaN\n\n\nD_true[46]\n0.13\n0.31\n-0.30\n0.67\n0.01\n0.02\n1349.49\n294.58\nNaN\n\n\nD_true[47]\n0.60\n0.47\n-0.15\n1.36\n0.01\n0.03\n1337.89\n292.95\nNaN\n\n\nD_true[48]\n-0.64\n0.29\n-1.07\n-0.13\n0.01\n0.02\n1349.49\n337.37\nNaN\n\n\nD_true[49]\n0.88\n0.62\n-0.11\n1.85\n0.02\n0.03\n817.15\n370.41\nNaN\n\n\na\n-0.05\n0.10\n-0.21\n0.11\n0.00\n0.00\n700.95\n468.17\nNaN\n\n\nbeta\n-0.62\n0.16\n-0.88\n-0.37\n0.01\n0.01\n407.98\n254.65\nNaN\n\n\neta\n0.05\n0.17\n-0.18\n0.36\n0.01\n0.01\n370.43\n429.80\nNaN\n\n\ns\n0.60\n0.11\n0.44\n0.78\n0.01\n0.00\n325.49\n352.19\nNaN\n\n\n\n\n\n\n\nlibrary(BayesianInference)\njnp = reticulate::import('jax.numpy')\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/WaffleDivorce.csv\", sep = ''), sep=';')\n\nm$scale(list('MedianAgeMarriage', 'Marriage'))\n\nm$data_on_model$D_obs = m$z_score(jnp$array(m$df['Divorce']))\nm$data_on_model$D_sd = jnp$array(m$df['Divorce SE']) / sd(unlist(m$df['Divorce']))\nm$data_on_model$A = jnp$array(m$df['MedianAgeMarriage'])\nm$data_on_model$M = jnp$array(m$df['Marriage'])\nm$data_on_model$N = as.integer(nrow(m$df))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(D_obs, D_sd, A, N, M){\n  a = bi.dist.normal(0, 0.2, name = 'a') \n  beta = bi.dist.normal(0, 0.5, name = 'beta')\n  eta = bi.dist.normal(0, 0.5, name = 'eta') \n  s = bi.dist.exponential(1, name = 's') \n  mu = a + beta * A + eta * M\n  D_true = bi.dist.normal(mu, s, name = 'D_true') \n  bi.dist.normal(D_true , D_sd, obs = D_obs) \n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#mathematical-details",
    "href": "16. Measuring error.html#mathematical-details",
    "title": "Measurement Error Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian formulation\n\nD_i^* \\sim \\text{Normal}(D_i, \\varsigma_i)\n\n\nD_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\n\n\\mu_i = \\alpha + \\beta A_i + \\eta M_i\n\n\n\\sigma \\sim \\text{Normal}(1)\n\nwhere:\n\nD_i^* is the observed divorce rate.\nD_i is the true divorce rate.\n\\mu_i is the mean of the true divorce rate.\n\\sigma is the standard deviation of the true divorce rate.\n\\alpha is the intercept term.\n\\beta is the regression coefficient for age.\n\\eta is the regression coefficient for marriage rate.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#notes",
    "href": "16. Measuring error.html#notes",
    "title": "Measurement Error Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThis is an approach that can be extended to any kind of model previously described. For example, one could generate a Bernoulli measurement error model by generating a process for the probabilities of success and failure. We can even go further by potentially having an error rate that is present only in one of the two outcomes.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "16. Measuring error.html#references",
    "href": "16. Measuring error.html#references",
    "title": "Measurement Error Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Measurement Error Models"
    ]
  },
  {
    "objectID": "api_diag.html",
    "href": "api_diag.html",
    "title": "Diagnostics",
    "section": "",
    "text": "diag.diag is a class to unify various diagnostics methods and provide a consistent interface for diagnostics.\nCompute the widely applicable information criterion.\nEstimates the expected log pointwise predictive density (elpd) using WAIC. Also calculates the WAIC‚Äôs standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1004.2316\n\n\npointwise: bool If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for waic computation. scale: str Output scale for WAIC. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy. dask_kwargs : dict, optional Dask related kwargs passed to :func:~arviz.wrap_xarray_ufunc.\n\n\n\nELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_waic: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_waic: effective number parameters n_samples: number of samples n_data_points: number of data points warning: bool True if posterior variance of the log predictive densities exceeds 0.4 waic_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True scale: scale of the elpd\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.WAIC(\nself,\npointwise=None,\nvar_name=None,\nscale=None,\ndask_kwargs=None,\n)\n\nPlot autocorrelation of the MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_autocorr\nReturns: fig: Autocorrelation plot\nbi.dist.autocor(\nself,\n*args,\n**kwargs,\n)\n\nCompare models based on their expected log pointwise predictive density (ELPD).\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353\n\n\n\ncompare_dict: dict of {str: InferenceData or ELPDData} A dictionary of model names and :class:arviz.InferenceData or ELPDData. ic: str, optional Method to estimate the ELPD, available options are ‚Äúloo‚Äù or ‚Äúwaic‚Äù. Defaults to rcParams[\"stats.information_criterion\"]. method: str, optional Method used to estimate the weights for each model. Available options are:\n\n‚Äòstacking‚Äô : stacking of predictive distributions.\n‚ÄòBB-pseudo-BMA‚Äô : pseudo-Bayesian Model averaging using Akaike-type weighting. The weights are stabilized using the Bayesian bootstrap.\n‚Äòpseudo-BMA‚Äô: pseudo-Bayesian Model averaging using Akaike-type weighting, without Bootstrap stabilization (not recommended).\n\nFor more information read https://arxiv.org/abs/1704.02030 b_samples: int, optional default = 1000 Number of samples taken by the Bayesian bootstrap estimation. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. Defaults to rcParams[\"stats.ic_compare_method\"]. alpha: float, optional The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. When alpha=1 (default), the distribution is uniform on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1. seed: int or np.random.RandomState instance, optional If int or RandomState, use it for seeding Bayesian bootstrap. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. Default None the global :mod:numpy.random state is used. scale: str, optional Output scale for IC. Available options are:\n\nlog : (default) log-score (after Vehtari et al.¬†(2017))\nnegative_log : -1 * (log-score)\ndeviance : -2 * (log-score)\n\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy. var_name: str, optional If there is more than a single observed variable in the InferenceData, which should be used as the basis for comparison.\n\n\n\nA DataFrame, ordered from best to worst model (measured by the ELPD). The index reflects the key with which the models are passed to this function. The columns are: rank: The rank-order of the models. 0 is the best. elpd: ELPD estimated either using (PSIS-LOO-CV elpd_loo or WAIC elpd_waic). Higher ELPD indicates higher out-of-sample predictive fit (‚Äúbetter‚Äù model). If scale is deviance or negative_log smaller values indicates higher out-of-sample predictive fit (‚Äúbetter‚Äù model). pIC: Estimated effective number of parameters. elpd_diff: The difference in ELPD between two models. If more than two models are compared, the difference is computed relative to the top-ranked model, that always has a elpd_diff of 0. weight: Relative weight for each model. This can be loosely interpreted as the probability of each model (among the compared model) given the data. By default the uncertainty in the weights estimation is considered using Bayesian bootstrap. SE: Standard error of the ELPD estimate. If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap. dSE: Standard error of the difference in ELPD between each model and the top-ranked model. It‚Äôs always 0 for the top-ranked model. warning: A value of 1 indicates that the computation of the ELPD may not be reliable. This could be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details. scale: Scale used for the ELPD.\n\n\n\n.. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413‚Äì1432 (2017) see https://doi.org/10.1007/s11222-016-9696-4\nbi.dist.compare(\ncompare_dict,\nic=None,\nmethod='stacking',\nb_samples=1000,\nalpha=1,\nseed=None,\nscale=None,\nvar_name=None,\n)\n\nPlot density plots of the posterior distribution.\nArgs: var_names (list): Variables to include shade (float): Transparency of the filled area *args, **kwargs: Additional arguments for arviz.plot_density\nReturns: fig: Density plots\nbi.dist.density(\nself,\nvar_names=None,\nshade=0.2,\n*args,\n**kwargs,\n)\n\nCalculate effective sample size (ESS).\nArgs: *args, **kwargs: Additional arguments for arviz.ess\nReturns: ess: Effective sample sizes\nbi.dist.ess(\nself,\n*args,\n**kwargs,\n)\n\nCreate a forest plot of estimated values.\nArgs: list: Data to plot (default: self.trace) kind (str): Type of plot (default: ‚Äúridgeplot‚Äù) ess (bool): Include effective sample size var_names (list): Variables to include *args, **kwargs: Additional arguments for arviz.plot_forest\nReturns: fig: Forest plot\nbi.dist.forest(\nself,\nlist=None,\nkind='ridgeplot',\ness=True,\nvar_names=None,\n*args,\n**kwargs,\n)\n\nCompute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\nEstimates the expected log pointwise predictive density (elpd) using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO‚Äôs standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646\n\n\n\npointwise: bool, optional If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for loo computation. reff: float, optional Relative MCMC efficiency, ess / n i.e.¬†number of effective samples divided by the number of actual samples. Computed from trace by default. scale: str Output scale for loo. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy.\n\n\n\nELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_loo: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_loo: effective number of parameters n_samples: number of samples n_data_points: number of data points warning: bool True if the estimated shape parameter of Pareto distribution is greater than good_k. loo_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True pareto_k: array of Pareto shape values, only if pointwise True scale: scale of the elpd good_k: For a sample size S, the thresold is compute as min(1 - 1/log10(S), 0.7)\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.loo(\nself,\npointwise=None,\nvar_name=None,\nreff=None,\nscale=None,\n)\n\nPerform comprehensive model diagnostics.\nCreates various diagnostic plots including: - Posterior distributions - Autocorrelation plots - Trace plots - Rank plots - Forest plots\nStores plots in instance variables: self.plot_posterior, self.autocor, self.traces, self.rank, self.forest\nbi.dist.model_checks(\nself,\n)\n\nCreate pairwise plots of the posterior distribution.\nArgs: var_names (list): Variables to include kind (list): Type of plots (‚Äúscatter‚Äù and/or ‚Äúkde‚Äù) kde_kwargs (dict): Additional arguments for KDE plots marginals (bool): Include marginal distributions point_estimate (str): Point estimate to plot figsize (tuple): Size of the figure *args, **kwargs: Additional arguments for arviz.plot_pair\nReturns: fig: Pair plot\nbi.dist.pair(\nself,\nvar_names=None,\nkind=['scatter', 'kde'],\nkde_kwargs={'fill_last': False},\nmarginals=True,\npoint_estimate='median',\nfigsize=(11.5, 5),\n*args,\n**kwargs,\n)\n\nSummary plot for model comparison.\nModels are compared based on their expected log pointwise predictive density (ELPD). This plot is in the style of the one used in [2]_. Chapter 6 in the first edition or 7 in the second.\n\n\n\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend LOO in line with the work presented by [1]_.\n\n\n\ncomp_df : pandas.DataFrame Result of the :func:arviz.compare method. insample_dev : bool, default False Plot in-sample ELPD, that is the value of the information criteria without the penalization given by the effective number of parameters (p_loo or p_waic). plot_standard_error : bool, default True Plot the standard error of the ELPD. plot_ic_diff : bool, default False Plot standard error of the difference in ELPD between each model and the top-ranked model. order_by_rank : bool, default True If True ensure the best model is used as reference. legend : bool, default False Add legend to figure. figsize : (float, float), optional If None, size is (6, num of models) inches. title : bool, default True Show a tittle with a description of how to interpret the plot. textsize : float, optional Text size scaling factor for labels, titles and lines. If None it will be autoscaled based on figsize. labeller : Labeller, optional Class providing the method make_label_vert to generate the labels in the plot titles. Read the :ref:label_guide for more details and usage examples. plot_kwargs : dict, optional Optional arguments for plot elements. Currently accepts ‚Äòcolor_ic‚Äô, ‚Äòmarker_ic‚Äô, ‚Äòcolor_insample_dev‚Äô, ‚Äòmarker_insample_dev‚Äô, ‚Äòcolor_dse‚Äô, ‚Äòmarker_dse‚Äô, ‚Äòls_min_ic‚Äô ‚Äòcolor_ls_min_ic‚Äô, ‚Äòfontsize‚Äô ax : matplotlib_axes or bokeh_figure, optional Matplotlib axes or bokeh figure. backend : {‚Äúmatplotlib‚Äù, ‚Äúbokeh‚Äù}, default ‚Äúmatplotlib‚Äù Select plotting backend. backend_kwargs : bool, optional These are kwargs specific to the backend being used, passed to :func:matplotlib.pyplot.subplots or :class:bokeh.plotting.figure. For additional documentation check the plotting method of the backend. show : bool, optional Call backend show function.\n\n\n\naxes : matplotlib_axes or bokeh_figure\n\n\n\nplot_elpd : Plot pointwise elpd differences between two or more models. compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation. loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). waic : Compute the widely applicable information criterion.\n\n\n\n.. [1] Vehtari et al.¬†(2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC https://arxiv.org/abs/1507.04544\n.. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in R and Stan, Second edition, CRC Press.\nbi.dist.plot_compare(\ncomp_df,\ninsample_dev=False,\nplot_standard_error=True,\nplot_ic_diff=False,\norder_by_rank=True,\nlegend=False,\ntitle=True,\nfigsize=None,\ntextsize=None,\nlabeller=None,\nplot_kwargs=None,\nax=None,\nbackend=None,\nbackend_kwargs=None,\nshow=None,\n)\n\nPlot evolution of effective sample size across iterations.\nReturns: fig: ESS evolution plot\nbi.dist.plot_ess(\nself,\n)\n\nCreate a trace plot for visualizing MCMC diagnostics.\nArgs: var_names (list): List of variable names to include kind (str): Type of plot (default: ‚Äúrank_bars‚Äù) *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: plot: The trace plot object\nbi.dist.plot_trace(\nself,\nvar_names=None,\nkind='rank_bars',\n*args,\n**kwargs,\n)\n\nCreate posterior distribution plots.\nArgs: figsize (tuple): Size of the figure (width, height)\nReturns: fig: Matplotlib figure containing posterior plots\nbi.dist.posterior(\nself,\nfigsize=(8, 4),\n)\n\nVisualize prior distributions compared to log probability.\nArgs: N (int): Number of samples to draw from priors\nReturns: fig: Matplotlib figure containing the prior distribution plots\nbi.dist.prior_dist(\nself,\nN=100,\n)\n\nCreate rank plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_rank\nReturns: fig: Rank plots\nbi.dist.rank(\nself,\n*args,\n**kwargs,\n)\n\nCalculate R-hat statistics for convergence.\nArgs: *args, **kwargs: Additional arguments for arviz.rhat\nReturns: rhat: R-hat values\nbi.dist.rhat(\nself,\n*args,\n**kwargs,\n)\n\nCalculate summary statistics for the posterior distribution.\nArgs: round_to (int): Number of decimal places to round results kind (str): Type of statistics to compute (default: ‚Äústats‚Äù) hdi_prob (float): Probability for highest posterior density interval *args, **kwargs: Additional arguments for arviz.summary\nReturns: pd.DataFrame: Summary statistics of the posterior distribution\nbi.dist.summary(\nself,\nround_to=2,\nkind='stats',\nhdi_prob=0.89,\n*args,\n**kwargs,\n)\n\nConvert the sampler output to an arviz trace object.\nThis method prepares the trace for use with arviz diagnostic tools.\nReturns: self.trace: The arviz trace object containing the diagnostic data\nbi.dist.to_az(\nself,\nbackend='numpyro',\nsample_stats_name=['target_log_prob', 'log_accept_ratio', 'has_divergence', 'energy'],\n)\n\nCreate trace plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: fig: Trace plots\nbi.dist.traces(\nself,\n*args,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters",
    "href": "api_diag.html#parameters",
    "title": "Diagnostics",
    "section": "",
    "text": "pointwise: bool If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for waic computation. scale: str Output scale for WAIC. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy. dask_kwargs : dict, optional Dask related kwargs passed to :func:~arviz.wrap_xarray_ufunc.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns",
    "href": "api_diag.html#returns",
    "title": "Diagnostics",
    "section": "",
    "text": "ELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_waic: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_waic: effective number parameters n_samples: number of samples n_data_points: number of data points warning: bool True if posterior variance of the log predictive densities exceeds 0.4 waic_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True scale: scale of the elpd\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.WAIC(\nself,\npointwise=None,\nvar_name=None,\nscale=None,\ndask_kwargs=None,\n)\n\nPlot autocorrelation of the MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_autocorr\nReturns: fig: Autocorrelation plot\nbi.dist.autocor(\nself,\n*args,\n**kwargs,\n)\n\nCompare models based on their expected log pointwise predictive density (ELPD).\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-1",
    "href": "api_diag.html#parameters-1",
    "title": "Diagnostics",
    "section": "",
    "text": "compare_dict: dict of {str: InferenceData or ELPDData} A dictionary of model names and :class:arviz.InferenceData or ELPDData. ic: str, optional Method to estimate the ELPD, available options are ‚Äúloo‚Äù or ‚Äúwaic‚Äù. Defaults to rcParams[\"stats.information_criterion\"]. method: str, optional Method used to estimate the weights for each model. Available options are:\n\n‚Äòstacking‚Äô : stacking of predictive distributions.\n‚ÄòBB-pseudo-BMA‚Äô : pseudo-Bayesian Model averaging using Akaike-type weighting. The weights are stabilized using the Bayesian bootstrap.\n‚Äòpseudo-BMA‚Äô: pseudo-Bayesian Model averaging using Akaike-type weighting, without Bootstrap stabilization (not recommended).\n\nFor more information read https://arxiv.org/abs/1704.02030 b_samples: int, optional default = 1000 Number of samples taken by the Bayesian bootstrap estimation. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. Defaults to rcParams[\"stats.ic_compare_method\"]. alpha: float, optional The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. When alpha=1 (default), the distribution is uniform on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1. seed: int or np.random.RandomState instance, optional If int or RandomState, use it for seeding Bayesian bootstrap. Only useful when method = ‚ÄòBB-pseudo-BMA‚Äô. Default None the global :mod:numpy.random state is used. scale: str, optional Output scale for IC. Available options are:\n\nlog : (default) log-score (after Vehtari et al.¬†(2017))\nnegative_log : -1 * (log-score)\ndeviance : -2 * (log-score)\n\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy. var_name: str, optional If there is more than a single observed variable in the InferenceData, which should be used as the basis for comparison.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-1",
    "href": "api_diag.html#returns-1",
    "title": "Diagnostics",
    "section": "",
    "text": "A DataFrame, ordered from best to worst model (measured by the ELPD). The index reflects the key with which the models are passed to this function. The columns are: rank: The rank-order of the models. 0 is the best. elpd: ELPD estimated either using (PSIS-LOO-CV elpd_loo or WAIC elpd_waic). Higher ELPD indicates higher out-of-sample predictive fit (‚Äúbetter‚Äù model). If scale is deviance or negative_log smaller values indicates higher out-of-sample predictive fit (‚Äúbetter‚Äù model). pIC: Estimated effective number of parameters. elpd_diff: The difference in ELPD between two models. If more than two models are compared, the difference is computed relative to the top-ranked model, that always has a elpd_diff of 0. weight: Relative weight for each model. This can be loosely interpreted as the probability of each model (among the compared model) given the data. By default the uncertainty in the weights estimation is considered using Bayesian bootstrap. SE: Standard error of the ELPD estimate. If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap. dSE: Standard error of the difference in ELPD between each model and the top-ranked model. It‚Äôs always 0 for the top-ranked model. warning: A value of 1 indicates that the computation of the ELPD may not be reliable. This could be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details. scale: Scale used for the ELPD.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#references",
    "href": "api_diag.html#references",
    "title": "Diagnostics",
    "section": "",
    "text": ".. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413‚Äì1432 (2017) see https://doi.org/10.1007/s11222-016-9696-4\nbi.dist.compare(\ncompare_dict,\nic=None,\nmethod='stacking',\nb_samples=1000,\nalpha=1,\nseed=None,\nscale=None,\nvar_name=None,\n)\n\nPlot density plots of the posterior distribution.\nArgs: var_names (list): Variables to include shade (float): Transparency of the filled area *args, **kwargs: Additional arguments for arviz.plot_density\nReturns: fig: Density plots\nbi.dist.density(\nself,\nvar_names=None,\nshade=0.2,\n*args,\n**kwargs,\n)\n\nCalculate effective sample size (ESS).\nArgs: *args, **kwargs: Additional arguments for arviz.ess\nReturns: ess: Effective sample sizes\nbi.dist.ess(\nself,\n*args,\n**kwargs,\n)\n\nCreate a forest plot of estimated values.\nArgs: list: Data to plot (default: self.trace) kind (str): Type of plot (default: ‚Äúridgeplot‚Äù) ess (bool): Include effective sample size var_names (list): Variables to include *args, **kwargs: Additional arguments for arviz.plot_forest\nReturns: fig: Forest plot\nbi.dist.forest(\nself,\nlist=None,\nkind='ridgeplot',\ness=True,\nvar_names=None,\n*args,\n**kwargs,\n)\n\nCompute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\nEstimates the expected log pointwise predictive density (elpd) using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO‚Äôs standard error and the effective number of parameters. Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-2",
    "href": "api_diag.html#parameters-2",
    "title": "Diagnostics",
    "section": "",
    "text": "pointwise: bool, optional If True the pointwise predictive accuracy will be returned. Defaults to stats.ic_pointwise rcParam. var_name : str, optional The name of the variable in log_likelihood groups storing the pointwise log likelihood data to use for loo computation. reff: float, optional Relative MCMC efficiency, ess / n i.e.¬†number of effective samples divided by the number of actual samples. Computed from trace by default. scale: str Output scale for loo. Available options are:\n\nlog : (default) log-score\nnegative_log : -1 * log-score\ndeviance : -2 * log-score\n\nA higher log-score (or a lower deviance or negative log_score) indicates a model with better predictive accuracy.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-2",
    "href": "api_diag.html#returns-2",
    "title": "Diagnostics",
    "section": "",
    "text": "ELPDData object (inherits from :class:pandas.Series) with the following row/attributes: elpd_loo: approximated expected log pointwise predictive density (elpd) se: standard error of the elpd p_loo: effective number of parameters n_samples: number of samples n_data_points: number of data points warning: bool True if the estimated shape parameter of Pareto distribution is greater than good_k. loo_i: :class:~xarray.DataArray with the pointwise predictive accuracy, only if pointwise=True pareto_k: array of Pareto shape values, only if pointwise True scale: scale of the elpd good_k: For a sample size S, the thresold is compute as min(1 - 1/log10(S), 0.7)\nThe returned object has a custom print method that overrides pd.Series method.\nbi.dist.loo(\nself,\npointwise=None,\nvar_name=None,\nreff=None,\nscale=None,\n)\n\nPerform comprehensive model diagnostics.\nCreates various diagnostic plots including: - Posterior distributions - Autocorrelation plots - Trace plots - Rank plots - Forest plots\nStores plots in instance variables: self.plot_posterior, self.autocor, self.traces, self.rank, self.forest\nbi.dist.model_checks(\nself,\n)\n\nCreate pairwise plots of the posterior distribution.\nArgs: var_names (list): Variables to include kind (list): Type of plots (‚Äúscatter‚Äù and/or ‚Äúkde‚Äù) kde_kwargs (dict): Additional arguments for KDE plots marginals (bool): Include marginal distributions point_estimate (str): Point estimate to plot figsize (tuple): Size of the figure *args, **kwargs: Additional arguments for arviz.plot_pair\nReturns: fig: Pair plot\nbi.dist.pair(\nself,\nvar_names=None,\nkind=['scatter', 'kde'],\nkde_kwargs={'fill_last': False},\nmarginals=True,\npoint_estimate='median',\nfigsize=(11.5, 5),\n*args,\n**kwargs,\n)\n\nSummary plot for model comparison.\nModels are compared based on their expected log pointwise predictive density (ELPD). This plot is in the style of the one used in [2]_. Chapter 6 in the first edition or 7 in the second.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#notes",
    "href": "api_diag.html#notes",
    "title": "Diagnostics",
    "section": "",
    "text": "The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend LOO in line with the work presented by [1]_.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#parameters-3",
    "href": "api_diag.html#parameters-3",
    "title": "Diagnostics",
    "section": "",
    "text": "comp_df : pandas.DataFrame Result of the :func:arviz.compare method. insample_dev : bool, default False Plot in-sample ELPD, that is the value of the information criteria without the penalization given by the effective number of parameters (p_loo or p_waic). plot_standard_error : bool, default True Plot the standard error of the ELPD. plot_ic_diff : bool, default False Plot standard error of the difference in ELPD between each model and the top-ranked model. order_by_rank : bool, default True If True ensure the best model is used as reference. legend : bool, default False Add legend to figure. figsize : (float, float), optional If None, size is (6, num of models) inches. title : bool, default True Show a tittle with a description of how to interpret the plot. textsize : float, optional Text size scaling factor for labels, titles and lines. If None it will be autoscaled based on figsize. labeller : Labeller, optional Class providing the method make_label_vert to generate the labels in the plot titles. Read the :ref:label_guide for more details and usage examples. plot_kwargs : dict, optional Optional arguments for plot elements. Currently accepts ‚Äòcolor_ic‚Äô, ‚Äòmarker_ic‚Äô, ‚Äòcolor_insample_dev‚Äô, ‚Äòmarker_insample_dev‚Äô, ‚Äòcolor_dse‚Äô, ‚Äòmarker_dse‚Äô, ‚Äòls_min_ic‚Äô ‚Äòcolor_ls_min_ic‚Äô, ‚Äòfontsize‚Äô ax : matplotlib_axes or bokeh_figure, optional Matplotlib axes or bokeh figure. backend : {‚Äúmatplotlib‚Äù, ‚Äúbokeh‚Äù}, default ‚Äúmatplotlib‚Äù Select plotting backend. backend_kwargs : bool, optional These are kwargs specific to the backend being used, passed to :func:matplotlib.pyplot.subplots or :class:bokeh.plotting.figure. For additional documentation check the plotting method of the backend. show : bool, optional Call backend show function.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#returns-3",
    "href": "api_diag.html#returns-3",
    "title": "Diagnostics",
    "section": "",
    "text": "axes : matplotlib_axes or bokeh_figure",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#see-also",
    "href": "api_diag.html#see-also",
    "title": "Diagnostics",
    "section": "",
    "text": "plot_elpd : Plot pointwise elpd differences between two or more models. compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation. loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). waic : Compute the widely applicable information criterion.",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "api_diag.html#references-1",
    "href": "api_diag.html#references-1",
    "title": "Diagnostics",
    "section": "",
    "text": ".. [1] Vehtari et al.¬†(2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC https://arxiv.org/abs/1507.04544\n.. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in R and Stan, Second edition, CRC Press.\nbi.dist.plot_compare(\ncomp_df,\ninsample_dev=False,\nplot_standard_error=True,\nplot_ic_diff=False,\norder_by_rank=True,\nlegend=False,\ntitle=True,\nfigsize=None,\ntextsize=None,\nlabeller=None,\nplot_kwargs=None,\nax=None,\nbackend=None,\nbackend_kwargs=None,\nshow=None,\n)\n\nPlot evolution of effective sample size across iterations.\nReturns: fig: ESS evolution plot\nbi.dist.plot_ess(\nself,\n)\n\nCreate a trace plot for visualizing MCMC diagnostics.\nArgs: var_names (list): List of variable names to include kind (str): Type of plot (default: ‚Äúrank_bars‚Äù) *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: plot: The trace plot object\nbi.dist.plot_trace(\nself,\nvar_names=None,\nkind='rank_bars',\n*args,\n**kwargs,\n)\n\nCreate posterior distribution plots.\nArgs: figsize (tuple): Size of the figure (width, height)\nReturns: fig: Matplotlib figure containing posterior plots\nbi.dist.posterior(\nself,\nfigsize=(8, 4),\n)\n\nVisualize prior distributions compared to log probability.\nArgs: N (int): Number of samples to draw from priors\nReturns: fig: Matplotlib figure containing the prior distribution plots\nbi.dist.prior_dist(\nself,\nN=100,\n)\n\nCreate rank plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_rank\nReturns: fig: Rank plots\nbi.dist.rank(\nself,\n*args,\n**kwargs,\n)\n\nCalculate R-hat statistics for convergence.\nArgs: *args, **kwargs: Additional arguments for arviz.rhat\nReturns: rhat: R-hat values\nbi.dist.rhat(\nself,\n*args,\n**kwargs,\n)\n\nCalculate summary statistics for the posterior distribution.\nArgs: round_to (int): Number of decimal places to round results kind (str): Type of statistics to compute (default: ‚Äústats‚Äù) hdi_prob (float): Probability for highest posterior density interval *args, **kwargs: Additional arguments for arviz.summary\nReturns: pd.DataFrame: Summary statistics of the posterior distribution\nbi.dist.summary(\nself,\nround_to=2,\nkind='stats',\nhdi_prob=0.89,\n*args,\n**kwargs,\n)\n\nConvert the sampler output to an arviz trace object.\nThis method prepares the trace for use with arviz diagnostic tools.\nReturns: self.trace: The arviz trace object containing the diagnostic data\nbi.dist.to_az(\nself,\nbackend='numpyro',\nsample_stats_name=['target_log_prob', 'log_accept_ratio', 'has_divergence', 'energy'],\n)\n\nCreate trace plots for MCMC chains.\nArgs: *args, **kwargs: Additional arguments for arviz.plot_trace\nReturns: fig: Trace plots\nbi.dist.traces(\nself,\n*args,\n**kwargs,\n)",
    "crumbs": [
      "Get started",
      "Diagnostics"
    ]
  },
  {
    "objectID": "5. Binomial model.html",
    "href": "5. Binomial model.html",
    "title": "Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary dependent variable‚Äîe.g., counts of successes/failures, yes/no, or 1/0‚Äîand one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#general-principles",
    "href": "5. Binomial model.html#general-principles",
    "title": "Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary dependent variable‚Äîe.g., counts of successes/failures, yes/no, or 1/0‚Äîand one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#considerations",
    "href": "5. Binomial model.html#considerations",
    "title": "Binomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nThis is the first model for which we need a link function: e.g., the logit function. The logit link function converts the linear combination of predictor variables into probabilities, making it suitable for modeling the probability of binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring that model predictions fall within the bounds of the binomial distribution‚Äôs success parameter \\in(0,1).",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#example",
    "href": "5. Binomial model.html#example",
    "title": "Binomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which lever each chimpanzee pulled in an experimental setup. The goal is to evaluate the probability of pulling the left side. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.chimpanzees(only_path = True)\nm.data(data_path, sep=';')\nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.dist.binomial(total_count = 1, logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;08:52,  1.88it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 243/1000 [00:00&lt;00:01, 512.85it/s, 3 steps of size 1.04e+00. acc. prob=0.78]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/1000 [00:00&lt;00:00, 877.23it/s, 1 steps of size 1.99e+00. acc. prob=0.79]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/1000 [00:00&lt;00:00, 1138.47it/s, 3 steps of size 1.10e+00. acc. prob=0.93]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/1000 [00:00&lt;00:00, 1355.23it/s, 7 steps of size 1.10e+00. acc. prob=0.93]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01&lt;00:00, 973.96it/s, 3 steps of size 1.10e+00. acc. prob=0.93]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n0.33\n0.09\n0.2\n0.47\n0.01\n0.0\n167.61\n173.42\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$chimpanzees(only_path = T), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  bi.dist.binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#mathematical-details",
    "href": "5. Binomial model.html#mathematical-details",
    "title": "Binomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian formulation\nWe can express the Bayesian Binomial regression model including prior distributions as follows:\n\nY_i \\sim \\text{Binomial}(N_i, p_i)\n\n\nlogit(p_i) = \\alpha + \\beta X_i\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\nWhere:\n\nY_i is the count of successes for observation i (often a binary 0 or 1).\nN_i is the count of trials for observation i (1 in the case of binary outcomes, as in the example for total_count above).\np_i is the probability of success (0 &lt; p_i &lt; 1) for observation i, the probability of a success.\nlogit(p_i) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.\n\\beta and \\alpha are the regression coefficient and intercept, respectively.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#notes",
    "href": "5. Binomial model.html#notes",
    "title": "Binomial Model",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\nBelow is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled, and three independent variables (actor, side, cond). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as for the different conditions.\n\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';')\nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.dist.binomial(total_count = 1, logits=p, obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n# Diagnostic ------------------------------------------------\nm.summary()\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  a = bi.dist.normal( 0, 1.5, name = 'a')\n  b = bi.dist.normal( 0, 0.5, name = 'b')\n  p = a[actor] + b[treatment]\n  # Likelihood\n  m$binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "5. Binomial model.html#references",
    "href": "5. Binomial model.html#references",
    "title": "Binomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Binomial Model"
    ]
  },
  {
    "objectID": "Advance/Build in models.html",
    "href": "Advance/Build in models.html",
    "title": "Build in models",
    "section": "",
    "text": "from BI import bi, jnp\n\nm=bi()\nm.data('iris.csv', sep=',') # Data is already scaled\nm.data_on_model = dict(\n    X=jnp.array(m.df.iloc[:,0:-2].values)\n)\nm.fit(m.models.pca(type=\"classic\"), progress_bar=False) # or robust, sparse, classic, sparse_robust_ard\n\nm.models.pca.plot(\n    X=m.df.iloc[:,0:-2].values,\n    y=m.df.iloc[:,-2].values, \n    feature_names=m.df.columns[0:-2], \n    target_names=m.df.iloc[:,-1].unique(),\n    color_var=m.df.iloc[:,0].values,\n    shape_var=m.df.iloc[:,-2].values\n)\n\njax.local_device_count 16",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in models"
    ]
  },
  {
    "objectID": "Advance/Build in models.html#python",
    "href": "Advance/Build in models.html#python",
    "title": "Build in models",
    "section": "Python",
    "text": "Python\n\nfrom BI import bi\nfrom sklearn.datasets import make_blobs\nm = bi()\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\nm.data_on_model = dict(data=data,T=10)\nm.fit(m.models.dpmm)\nm.plot(data,m.sampler) # ‚ö†Ô∏è Experimental feature\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:02&lt;42:59,  2.58s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|‚ñè         | 15/1000 [00:02&lt;02:14,  7.30it/s, 511 steps of size 9.88e-03. acc. prob=0.65]warmup:   2%|‚ñè         | 19/1000 [00:02&lt;01:49,  8.92it/s, 511 steps of size 5.50e-03. acc. prob=0.67]warmup:   2%|‚ñè         | 23/1000 [00:03&lt;01:41,  9.59it/s, 319 steps of size 2.97e-02. acc. prob=0.71]warmup:   3%|‚ñé         | 27/1000 [00:03&lt;01:21, 11.99it/s, 255 steps of size 1.66e-02. acc. prob=0.72]warmup:   3%|‚ñé         | 31/1000 [00:03&lt;01:06, 14.52it/s, 255 steps of size 1.59e-02. acc. prob=0.72]warmup:   4%|‚ñé         | 37/1000 [00:03&lt;00:55, 17.44it/s, 511 steps of size 2.80e-02. acc. prob=0.74]warmup:   4%|‚ñç         | 40/1000 [00:04&lt;00:56, 16.92it/s, 31 steps of size 3.96e-02. acc. prob=0.75] warmup:   4%|‚ñç         | 45/1000 [00:04&lt;00:44, 21.64it/s, 159 steps of size 8.74e-02. acc. prob=0.76]warmup:   5%|‚ñç         | 49/1000 [00:04&lt;00:48, 19.80it/s, 127 steps of size 5.86e-02. acc. prob=0.76]warmup:   6%|‚ñå         | 55/1000 [00:04&lt;00:38, 24.73it/s, 255 steps of size 3.37e-02. acc. prob=0.75]warmup:   6%|‚ñå         | 60/1000 [00:04&lt;00:32, 28.96it/s, 63 steps of size 3.53e-02. acc. prob=0.76] warmup:   7%|‚ñã         | 67/1000 [00:04&lt;00:26, 35.61it/s, 127 steps of size 5.84e-02. acc. prob=0.76]warmup:   7%|‚ñã         | 74/1000 [00:04&lt;00:22, 41.49it/s, 63 steps of size 4.28e-02. acc. prob=0.76] warmup:   8%|‚ñä         | 79/1000 [00:04&lt;00:23, 39.56it/s, 127 steps of size 4.89e-02. acc. prob=0.77]warmup:   8%|‚ñä         | 85/1000 [00:05&lt;00:20, 44.17it/s, 15 steps of size 4.18e-02. acc. prob=0.77] warmup:   9%|‚ñâ         | 93/1000 [00:05&lt;00:17, 51.58it/s, 63 steps of size 1.00e-01. acc. prob=0.77]warmup:  10%|‚ñà         | 100/1000 [00:05&lt;00:17, 52.76it/s, 95 steps of size 7.61e-02. acc. prob=0.77]warmup:  11%|‚ñà         | 106/1000 [00:05&lt;00:19, 46.70it/s, 127 steps of size 6.15e-02. acc. prob=0.77]warmup:  11%|‚ñà         | 111/1000 [00:05&lt;00:20, 43.18it/s, 63 steps of size 1.32e-01. acc. prob=0.77] warmup:  12%|‚ñà‚ñè        | 116/1000 [00:05&lt;00:24, 36.30it/s, 63 steps of size 1.35e-01. acc. prob=0.77]warmup:  12%|‚ñà‚ñè        | 120/1000 [00:05&lt;00:25, 34.91it/s, 63 steps of size 7.45e-02. acc. prob=0.77]warmup:  12%|‚ñà‚ñè        | 124/1000 [00:06&lt;00:25, 35.03it/s, 255 steps of size 3.41e-02. acc. prob=0.77]warmup:  13%|‚ñà‚ñé        | 128/1000 [00:06&lt;00:25, 34.00it/s, 31 steps of size 2.62e-02. acc. prob=0.77] warmup:  13%|‚ñà‚ñé        | 132/1000 [00:06&lt;00:25, 33.39it/s, 63 steps of size 1.02e-01. acc. prob=0.77]warmup:  14%|‚ñà‚ñé        | 137/1000 [00:06&lt;00:24, 35.72it/s, 255 steps of size 3.71e-02. acc. prob=0.77]warmup:  14%|‚ñà‚ñç        | 142/1000 [00:06&lt;00:24, 35.12it/s, 255 steps of size 4.23e-02. acc. prob=0.77]warmup:  15%|‚ñà‚ñç        | 147/1000 [00:06&lt;00:22, 38.57it/s, 63 steps of size 8.59e-02. acc. prob=0.78] warmup:  15%|‚ñà‚ñå        | 154/1000 [00:06&lt;00:18, 45.22it/s, 127 steps of size 5.55e-02. acc. prob=0.77]warmup:  16%|‚ñà‚ñå        | 161/1000 [00:06&lt;00:16, 50.37it/s, 63 steps of size 1.56e-01. acc. prob=0.77] warmup:  17%|‚ñà‚ñã        | 168/1000 [00:07&lt;00:15, 55.04it/s, 31 steps of size 2.59e-02. acc. prob=0.77]warmup:  17%|‚ñà‚ñã        | 174/1000 [00:07&lt;00:20, 40.71it/s, 127 steps of size 7.51e-02. acc. prob=0.77]warmup:  18%|‚ñà‚ñä        | 180/1000 [00:07&lt;00:18, 44.95it/s, 63 steps of size 1.57e-01. acc. prob=0.78] warmup:  19%|‚ñà‚ñâ        | 188/1000 [00:07&lt;00:15, 52.09it/s, 63 steps of size 1.08e-01. acc. prob=0.78]warmup:  20%|‚ñà‚ñâ        | 198/1000 [00:07&lt;00:12, 62.70it/s, 31 steps of size 1.99e-01. acc. prob=0.78]warmup:  21%|‚ñà‚ñà        | 210/1000 [00:07&lt;00:10, 75.95it/s, 31 steps of size 1.63e-01. acc. prob=0.78]warmup:  22%|‚ñà‚ñà‚ñè       | 219/1000 [00:07&lt;00:09, 79.68it/s, 15 steps of size 1.09e-01. acc. prob=0.78]warmup:  23%|‚ñà‚ñà‚ñé       | 229/1000 [00:07&lt;00:09, 84.83it/s, 63 steps of size 8.33e-02. acc. prob=0.78]warmup:  24%|‚ñà‚ñà‚ñç       | 238/1000 [00:08&lt;00:09, 83.31it/s, 31 steps of size 1.42e-01. acc. prob=0.78]warmup:  25%|‚ñà‚ñà‚ñç       | 247/1000 [00:08&lt;00:08, 85.03it/s, 31 steps of size 1.92e-01. acc. prob=0.78]warmup:  26%|‚ñà‚ñà‚ñå       | 256/1000 [00:08&lt;00:08, 85.46it/s, 31 steps of size 1.82e-02. acc. prob=0.78]warmup:  26%|‚ñà‚ñà‚ñã       | 265/1000 [00:08&lt;00:16, 44.02it/s, 63 steps of size 1.82e-01. acc. prob=0.78]warmup:  27%|‚ñà‚ñà‚ñã       | 272/1000 [00:09&lt;00:23, 31.10it/s, 63 steps of size 1.29e-01. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 278/1000 [00:09&lt;00:22, 32.63it/s, 31 steps of size 1.06e-01. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 283/1000 [00:09&lt;00:21, 33.11it/s, 31 steps of size 1.78e-01. acc. prob=0.78]warmup:  29%|‚ñà‚ñà‚ñâ       | 288/1000 [00:09&lt;00:21, 32.83it/s, 63 steps of size 1.34e-02. acc. prob=0.78]warmup:  29%|‚ñà‚ñà‚ñâ       | 293/1000 [00:09&lt;00:24, 29.02it/s, 31 steps of size 1.70e-01. acc. prob=0.78]warmup:  30%|‚ñà‚ñà‚ñà       | 300/1000 [00:09&lt;00:19, 35.61it/s, 31 steps of size 9.51e-02. acc. prob=0.78]warmup:  30%|‚ñà‚ñà‚ñà       | 305/1000 [00:10&lt;00:19, 34.95it/s, 127 steps of size 3.13e-02. acc. prob=0.78]warmup:  31%|‚ñà‚ñà‚ñà       | 310/1000 [00:10&lt;00:27, 24.76it/s, 255 steps of size 2.30e-02. acc. prob=0.78]warmup:  31%|‚ñà‚ñà‚ñà‚ñè      | 314/1000 [00:10&lt;00:27, 25.04it/s, 63 steps of size 1.04e-01. acc. prob=0.78] warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 324/1000 [00:10&lt;00:18, 37.55it/s, 31 steps of size 1.46e-01. acc. prob=0.78]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 331/1000 [00:10&lt;00:15, 42.90it/s, 31 steps of size 4.24e-02. acc. prob=0.78]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 337/1000 [00:10&lt;00:14, 44.43it/s, 63 steps of size 6.09e-02. acc. prob=0.78]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 343/1000 [00:11&lt;00:16, 39.50it/s, 127 steps of size 4.55e-02. acc. prob=0.78]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 350/1000 [00:11&lt;00:15, 42.69it/s, 127 steps of size 4.96e-02. acc. prob=0.78]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 355/1000 [00:11&lt;00:15, 42.34it/s, 63 steps of size 4.70e-02. acc. prob=0.78] warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 361/1000 [00:11&lt;00:13, 45.80it/s, 63 steps of size 1.12e-01. acc. prob=0.78]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 368/1000 [00:11&lt;00:12, 51.70it/s, 31 steps of size 3.62e-02. acc. prob=0.78]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 374/1000 [00:11&lt;00:13, 46.85it/s, 31 steps of size 5.59e-02. acc. prob=0.78]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 380/1000 [00:11&lt;00:14, 44.19it/s, 63 steps of size 1.07e-01. acc. prob=0.78]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 386/1000 [00:11&lt;00:12, 47.69it/s, 63 steps of size 6.86e-02. acc. prob=0.78]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [00:12&lt;00:13, 45.50it/s, 63 steps of size 8.69e-02. acc. prob=0.78]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 397/1000 [00:12&lt;00:13, 43.12it/s, 127 steps of size 3.79e-02. acc. prob=0.78]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 402/1000 [00:12&lt;00:15, 39.13it/s, 63 steps of size 6.12e-02. acc. prob=0.78] warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 407/1000 [00:12&lt;00:17, 32.97it/s, 255 steps of size 2.23e-02. acc. prob=0.78]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 411/1000 [00:12&lt;00:22, 25.93it/s, 127 steps of size 5.93e-02. acc. prob=0.78]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/1000 [00:12&lt;00:18, 31.79it/s, 31 steps of size 5.71e-02. acc. prob=0.78] warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/1000 [00:13&lt;00:14, 38.86it/s, 31 steps of size 9.55e-02. acc. prob=0.78]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/1000 [00:13&lt;00:11, 47.74it/s, 31 steps of size 4.30e-02. acc. prob=0.78]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/1000 [00:13&lt;00:12, 46.18it/s, 63 steps of size 6.16e-02. acc. prob=0.78]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/1000 [00:13&lt;00:12, 45.80it/s, 63 steps of size 8.61e-02. acc. prob=0.79]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/1000 [00:13&lt;00:13, 41.04it/s, 127 steps of size 2.98e-02. acc. prob=0.78]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/1000 [00:13&lt;00:16, 32.53it/s, 255 steps of size 1.38e-02. acc. prob=0.78]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/1000 [00:15&lt;00:58,  9.27it/s, 63 steps of size 5.24e-02. acc. prob=0.78] warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/1000 [00:15&lt;00:50, 10.73it/s, 31 steps of size 7.84e-02. acc. prob=0.78]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/1000 [00:15&lt;00:47, 11.30it/s, 255 steps of size 2.61e-02. acc. prob=0.78]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/1000 [00:16&lt;00:49, 10.83it/s, 63 steps of size 9.44e-02. acc. prob=0.78] warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/1000 [00:16&lt;00:46, 11.34it/s, 255 steps of size 2.90e-02. acc. prob=0.78]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/1000 [00:16&lt;00:44, 11.79it/s, 63 steps of size 9.14e-02. acc. prob=0.78] warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/1000 [00:16&lt;00:46, 11.38it/s, 255 steps of size 2.98e-02. acc. prob=0.78]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/1000 [00:16&lt;00:44, 11.84it/s, 63 steps of size 5.97e-03. acc. prob=0.78] warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/1000 [00:17&lt;01:01,  8.51it/s, 511 steps of size 1.74e-02. acc. prob=0.78]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/1000 [00:17&lt;00:55,  9.39it/s, 127 steps of size 4.40e-02. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/1000 [00:17&lt;00:38, 13.49it/s, 31 steps of size 3.30e-02. acc. prob=0.78] warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/1000 [00:17&lt;00:35, 14.49it/s, 63 steps of size 8.34e-02. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/1000 [00:17&lt;00:23, 21.24it/s, 31 steps of size 2.90e-02. acc. prob=0.78]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/1000 [00:17&lt;00:23, 21.52it/s, 255 steps of size 2.77e-02. acc. prob=0.78]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/1000 [00:18&lt;00:21, 23.04it/s, 63 steps of size 4.79e-02. acc. prob=0.78] sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/1000 [00:18&lt;00:18, 26.82it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/1000 [00:18&lt;00:17, 27.86it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 512/1000 [00:18&lt;00:16, 30.05it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/1000 [00:18&lt;00:15, 31.41it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/1000 [00:18&lt;00:13, 35.32it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/1000 [00:18&lt;00:14, 32.77it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/1000 [00:18&lt;00:14, 31.55it/s, 63 steps of size 4.79e-02. acc. prob=0.97]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [00:18&lt;00:14, 32.22it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 537/1000 [00:19&lt;00:16, 28.17it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/1000 [00:19&lt;00:15, 29.83it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/1000 [00:19&lt;00:14, 30.98it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/1000 [00:19&lt;00:16, 26.90it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/1000 [00:19&lt;00:17, 26.07it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/1000 [00:19&lt;00:19, 23.25it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/1000 [00:20&lt;00:16, 26.94it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/1000 [00:20&lt;00:13, 32.31it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/1000 [00:20&lt;00:13, 31.67it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/1000 [00:20&lt;00:11, 37.65it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/1000 [00:20&lt;00:12, 34.83it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/1000 [00:20&lt;00:12, 33.89it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/1000 [00:20&lt;00:12, 33.21it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/1000 [00:20&lt;00:12, 33.89it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/1000 [00:20&lt;00:10, 36.88it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/1000 [00:21&lt;00:09, 40.28it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/1000 [00:21&lt;00:10, 36.40it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/1000 [00:21&lt;00:10, 36.81it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/1000 [00:21&lt;00:11, 35.00it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/1000 [00:21&lt;00:10, 38.02it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/1000 [00:21&lt;00:08, 41.91it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/1000 [00:21&lt;00:08, 42.87it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/1000 [00:21&lt;00:08, 43.47it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [00:22&lt;00:08, 43.69it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/1000 [00:22&lt;00:08, 42.34it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/1000 [00:22&lt;00:08, 41.46it/s, 127 steps of size 4.79e-02. acc. prob=0.96]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/1000 [00:22&lt;00:08, 41.67it/s, 63 steps of size 4.79e-02. acc. prob=0.96] sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/1000 [00:22&lt;00:09, 35.14it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/1000 [00:22&lt;00:10, 31.13it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/1000 [00:22&lt;00:10, 31.06it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/1000 [00:23&lt;00:12, 26.29it/s, 63 steps of size 4.79e-02. acc. prob=0.96]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/1000 [00:23&lt;00:11, 27.24it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/1000 [00:23&lt;00:13, 23.50it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/1000 [00:23&lt;00:13, 24.24it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/1000 [00:23&lt;00:11, 27.01it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/1000 [00:23&lt;00:10, 29.72it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/1000 [00:23&lt;00:10, 28.00it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [00:24&lt;00:09, 32.14it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/1000 [00:24&lt;00:08, 33.67it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/1000 [00:24&lt;00:08, 34.94it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 712/1000 [00:24&lt;00:08, 34.08it/s, 255 steps of size 4.79e-02. acc. prob=0.95]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/1000 [00:24&lt;00:07, 36.32it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/1000 [00:24&lt;00:07, 36.05it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/1000 [00:24&lt;00:07, 37.80it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/1000 [00:24&lt;00:06, 39.88it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/1000 [00:25&lt;00:06, 39.14it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/1000 [00:25&lt;00:07, 36.46it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/1000 [00:25&lt;00:07, 35.16it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/1000 [00:25&lt;00:06, 36.18it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/1000 [00:25&lt;00:07, 34.32it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/1000 [00:25&lt;00:06, 38.16it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/1000 [00:25&lt;00:07, 33.27it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/1000 [00:25&lt;00:06, 34.89it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/1000 [00:25&lt;00:06, 35.36it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/1000 [00:26&lt;00:05, 38.41it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/1000 [00:26&lt;00:05, 41.22it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/1000 [00:26&lt;00:05, 39.70it/s, 191 steps of size 4.79e-02. acc. prob=0.95]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/1000 [00:26&lt;00:05, 36.89it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/1000 [00:26&lt;00:05, 40.75it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/1000 [00:26&lt;00:05, 39.22it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/1000 [00:26&lt;00:04, 39.95it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/1000 [00:27&lt;00:04, 38.32it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/1000 [00:27&lt;00:04, 37.75it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/1000 [00:27&lt;00:05, 35.25it/s, 191 steps of size 4.79e-02. acc. prob=0.95]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/1000 [00:27&lt;00:04, 37.20it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/1000 [00:27&lt;00:04, 36.70it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/1000 [00:27&lt;00:04, 36.44it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/1000 [00:27&lt;00:04, 39.69it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/1000 [00:27&lt;00:03, 41.28it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/1000 [00:27&lt;00:03, 40.76it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/1000 [00:28&lt;00:04, 34.37it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/1000 [00:28&lt;00:04, 34.09it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [00:28&lt;00:03, 36.86it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/1000 [00:28&lt;00:03, 39.04it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/1000 [00:28&lt;00:03, 41.52it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/1000 [00:28&lt;00:02, 42.03it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/1000 [00:28&lt;00:02, 42.35it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/1000 [00:28&lt;00:02, 41.97it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/1000 [00:29&lt;00:03, 35.85it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/1000 [00:29&lt;00:02, 35.65it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/1000 [00:29&lt;00:02, 39.08it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/1000 [00:29&lt;00:02, 37.08it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/1000 [00:29&lt;00:02, 36.77it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/1000 [00:29&lt;00:02, 36.71it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/1000 [00:29&lt;00:02, 36.23it/s, 63 steps of size 4.79e-02. acc. prob=0.95] sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/1000 [00:29&lt;00:02, 37.73it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/1000 [00:30&lt;00:01, 39.04it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/1000 [00:30&lt;00:01, 41.11it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 937/1000 [00:30&lt;00:01, 41.48it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/1000 [00:30&lt;00:01, 41.30it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/1000 [00:30&lt;00:01, 41.80it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/1000 [00:30&lt;00:01, 45.21it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/1000 [00:30&lt;00:01, 41.65it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/1000 [00:30&lt;00:00, 42.29it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/1000 [00:31&lt;00:00, 43.26it/s, 127 steps of size 4.79e-02. acc. prob=0.95]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/1000 [00:31&lt;00:00, 45.71it/s, 63 steps of size 4.79e-02. acc. prob=0.94] sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/1000 [00:31&lt;00:00, 47.30it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/1000 [00:31&lt;00:00, 45.90it/s, 63 steps of size 4.79e-02. acc. prob=0.94]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/1000 [00:31&lt;00:00, 46.32it/s, 63 steps of size 4.79e-02. acc. prob=0.95]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/1000 [00:31&lt;00:00, 42.20it/s, 127 steps of size 4.79e-02. acc. prob=0.94]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:31&lt;00:00, 31.50it/s, 63 steps of size 4.79e-02. acc. prob=0.94]",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in models"
    ]
  },
  {
    "objectID": "Advance/build in functions.html",
    "href": "Advance/build in functions.html",
    "title": "Build in functions",
    "section": "",
    "text": "Varying effects\n\nVarying interceptsVarying slopesVarying intercepts and slopes\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    alpha = m.effects.varying_intercept(group=tank,group_name = 'tank')\n    m.dist.binomial(total_count = density, logits = alpha, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\n\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n        N_group = N_cafes,\n        group = cafe,\n        global_intercept= a,\n        global_slope= b,\n        group_name = 'cafe')\n    \n\n    mu = varying_intercept + varying_slope* afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n\n\n\n\nGaussian processes\n\nSquared Exponential KernelPeriodic KernelLocally Periodic Kernel\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_sq_exp\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_periodic\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.gaussian.kernel_periodic_local\n\n\n\n\n\nNetworks effects\n\nSender receiverDyadicBlock model\n\n\nsr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n\nm.net.dyadic_effect(dyadic_predictors)\n\n\nm.net.block_model(Merica: vector[integer],3)\n\n\n\n\n\nNetwork metrics\n\nDegree (undirected, out-degree, in-degree)Strength (undirected, out-strength , in-strength)EigenvectorClustering coefficientDensityGeodesic_distanceDiameter\n\n\nm.net.degree(adj_matrix_jax)\nm.net.indegree(adj_matrix_jax)\nm.net.outdegree(adj_matrix_jax)\n\n\nm.net.strength(adj_matrix_jax)\nm.net.instrength(adj_matrix_jax)\nm.net.outstrength(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)\n\n\nm.net.eigen(adj_matrix_jax)",
    "crumbs": [
      "Get started",
      "Advance",
      "Build in functions"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "If you have a case where you believe the effect of one independent variable depends on the value of another independent variable, you can use regression analysis with interaction terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two independent variables (see note on how this multiplication arises).",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#general-principles",
    "href": "3. Interaction between continuous variables.html#general-principles",
    "title": "Interaction Terms in Regression",
    "section": "",
    "text": "If you have a case where you believe the effect of one independent variable depends on the value of another independent variable, you can use regression analysis with interaction terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two independent variables (see note on how this multiplication arises).",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "Interaction Terms in Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same assumptions as for Regression for continuous variable.\nWe wish to model the relationship between a dependent variable, Y, and an independent variable, X_1, whose effect varies as a function of a second independent variable X_2. To do this, we explicitly model the hypothesis that the slope between Y and X_1 depends on (i.e., is conditional on) X_2.\nFor continuous interactions with normalized data, the intercept becomes the grand mean üõà of the outcome variable.\nThe interpretation of slopes estimates is more complex. The coefficient for a non-interaction term reflects the expected change in Y when X_1 increases by one unit, holding X_2 constant at its average value. The coefficient for the interaction term represents how the effect of X_1 on Y changes depending on the value of X_2, and vice versa, showing how the relationship between the two variables influences the outcome Y.\nTriptych üõà plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "Interaction Terms in Regression",
    "section": "Example",
    "text": "Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. The data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.tulips(only_path = True)\nm.data(data_path, sep=';')\nm.scale(['blooms', 'water', 'shade']) # Normalize\n\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.dist.normal(mu, sigma, obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;08:22,  1.99it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 374/1000 [00:00&lt;00:00, 827.08it/s, 3 steps of size 1.58e+00. acc. prob=0.79]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/1000 [00:00&lt;00:00, 1557.77it/s, 7 steps of size 6.86e-01. acc. prob=0.89]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1308.72it/s, 7 steps of size 6.86e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n0.09\n0.10\n-0.05\n0.25\n0.0\n0.01\n560.88\n176.01\nNaN\n\n\nbs[0]\n-0.31\n0.11\n-0.49\n-0.13\n0.0\n0.01\n634.25\n265.47\nNaN\n\n\nbw[0]\n0.56\n0.10\n0.39\n0.72\n0.0\n0.00\n510.54\n317.55\nNaN\n\n\nbws[0]\n-0.32\n0.11\n-0.53\n-0.17\n0.0\n0.01\n504.61\n382.08\nNaN\n\n\nsigma[0]\n0.57\n0.09\n0.42\n0.70\n0.0\n0.00\n484.71\n399.50\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(m$load$tulips(only_path = T), sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Normalize\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(blooms, water,shade){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')\n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint')\n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "Interaction Terms in Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#frequentist-formulation",
    "href": "3. Interaction between continuous variables.html#frequentist-formulation",
    "title": "Interaction Terms in Regression",
    "section": "Frequentist formulation",
    "text": "Frequentist formulation\nWe model the relationship between the input features (X_1 and X_2) and the target variable (Y) using the following equation: \nùëå_i = \\alpha + \\beta_1 ùëã_{[1,i]} + \\beta_2 ùëã_{[2,i]} + \\beta_3 ùëã_{[1,i]} ùëã_{[2,i]} + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\nX_{[1,i]} and X_{[2,i]} are the values of the two independent variables for observation i.\n\\beta_1 and \\beta_2 are the coefficients for X_{1} and X_{2}, respectively, when the other variable has value 0.\n\\beta_3 is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\n\\epsilon_i is the error term, assumed to be independent and normally distributed.\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model as follows:\n\nY_i \\sim \\text{Normal}(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_1 \\sim \\text{Normal}(0,1)\n\n\n\\beta_2 \\sim \\text{Normal}(0,1)\n\n\n\\beta_{3} \\sim \\text{Normal}(0,1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_1 and \\beta_2 are the coefficients for X_{1} and X_{2}, respectively, when the other variable has value 0.\n\\beta_3 is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\nX_{[1,i]} and X_{[2,i]} are the two values of the independent continuous variables for observation i.\n\\sigma is a standard deviation parameter, which here has an Exponential prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#Notes",
    "href": "3. Interaction between continuous variables.html#Notes",
    "title": "Interaction Terms in Regression",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe interaction term equation: \nY_i \\sim Normal(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n\ncan be re-written as: \nY_i \\sim Normal(\\alpha + (\\beta_1 + \\beta_{3} X_{[2,i]}) X_{[1,i]} + \\beta_2 X_{[2,i]}, \\sigma)\n\nsimply by factoring the terms with X_{[1,i]} in them. The result is that the coefficient on X_{[1,i]} is written specifically as a linear regression model of X_{[2,i]}.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "Interaction Terms in Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Interaction Terms in Regression"
    ]
  },
  {
    "objectID": "21. DPMM.html",
    "href": "21. DPMM.html",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a unsupervised clustering method Gershman and Blei (2012). Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its location and its spread.\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#general-principles",
    "href": "21. DPMM.html#general-principles",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "",
    "text": "To discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Mixture Model (DPMM). This is a unsupervised clustering method Gershman and Blei (2012). Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\nHow many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its location and its spread.\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#considerations",
    "href": "21. DPMM.html#considerations",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA DPMM is a Bayesian model üõà that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation where we cap the maximum number of clusters at K and use the Stick-Breaking Process üõà.\nThe key parameters and their priors are:\n\nConcentration \\alpha: This single parameter controls the tendency to create new clusters. A low Œ± favors fewer, larger clusters, while a high Œ± allows for many smaller clusters. We typically place a Gamma prior on \\alpha to learn its value from the data.\n\n\nCluster Weights w: Generated via the Stick-Breaking process from \\alpha. These are the probabilities of drawing a data point from any given cluster.\nCluster Parameters (\\mu, \\Sigma): Each potential cluster has a mean \\mu and a covariance matrix \\Sigma. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, 14). However, if the data is one-dimensional, we use a univariate normal distribution.\n\nThe model is often implemented in its marginalized form üõà. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#example",
    "href": "21. DPMM.html#example",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "Example",
    "text": "Example\nBelow is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\nPythonR\n\n\n\nfrom BI import bi, jnp\nfrom BI.Models.DPMM import mix_weights\nfrom sklearn.datasets import make_blobs\nimport numpyro\n\nm = bi()\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n#  The model\ndef dpmm(data, T=10):\n    N, D = data.shape  # Number of features\n    data_mean = jnp.mean(data, axis=0)\n    data_std = jnp.std(data, axis=0)*2\n\n    # 1) stick-breaking weights\n    alpha = m.dist.gamma(1.0, 10.0,name='alpha')\n\n    with m.dist.plate(\"beta_plate\", T - 1):\n        beta = m.dist.beta(1, alpha)\n\n    w = numpyro.deterministic(\"w\",mix_weights(beta))\n\n    # 2) component parameters\n    with m.dist.plate(\"components\", T):\n        mu = m.dist.multivariate_normal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n        sigma = m.dist.log_normal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n\n        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n\n    # 3) Latent cluster assignments for each data point\n    with m.dist.plate(\"data\", N):\n        # Sample the assignment for each data point\n        z = m.dist.categorical(w, name = 'z') # shape (N,)  \n\n        # Sample the data point from the assigned component\n        m.dist.multivariate_normal(loc=mu[z], scale_tril=scale_tril[z],\n            obs=data, name = 'Y'\n        )  \n\nm.data_on_model = dict(data=data)\nm.fit(dpmm)  # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n\n/home/sosa/work/BI/BI/Main/main.py:244: FutureWarning:\n\nSome algorithms will automatically enumerate the discrete latent site z of your model. In the future, enumerated sites need to be marked with `infer={'enumerate': 'parallel'}`.\n\n\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;18:09,  1.09s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|‚ñè         | 18/1000 [00:01&lt;00:48, 20.44it/s, 127 steps of size 5.28e-02. acc. prob=0.70]warmup:   3%|‚ñé         | 28/1000 [00:01&lt;00:47, 20.62it/s, 319 steps of size 4.90e-02. acc. prob=0.73]warmup:   4%|‚ñé         | 35/1000 [00:01&lt;00:41, 23.52it/s, 31 steps of size 1.05e-01. acc. prob=0.75] warmup:   4%|‚ñç         | 41/1000 [00:01&lt;00:34, 27.41it/s, 511 steps of size 1.18e-02. acc. prob=0.73]warmup:   5%|‚ñç         | 47/1000 [00:02&lt;00:30, 31.72it/s, 31 steps of size 2.64e-02. acc. prob=0.75] warmup:   5%|‚ñå         | 53/1000 [00:02&lt;00:27, 34.83it/s, 255 steps of size 3.67e-02. acc. prob=0.75]warmup:   6%|‚ñå         | 58/1000 [00:02&lt;00:27, 34.45it/s, 511 steps of size 3.43e-02. acc. prob=0.76]warmup:   6%|‚ñã         | 63/1000 [00:02&lt;00:28, 33.44it/s, 63 steps of size 4.17e-02. acc. prob=0.76] warmup:   7%|‚ñã         | 68/1000 [00:02&lt;00:25, 36.25it/s, 79 steps of size 4.77e-02. acc. prob=0.76]warmup:  10%|‚ñâ         | 99/1000 [00:02&lt;00:09, 96.27it/s, 31 steps of size 6.63e-02. acc. prob=0.77]warmup:  12%|‚ñà‚ñè        | 120/1000 [00:02&lt;00:07, 123.02it/s, 31 steps of size 4.17e-01. acc. prob=0.78]warmup:  14%|‚ñà‚ñé        | 135/1000 [00:03&lt;00:07, 118.20it/s, 63 steps of size 1.53e-01. acc. prob=0.78]warmup:  16%|‚ñà‚ñå        | 155/1000 [00:03&lt;00:06, 137.49it/s, 63 steps of size 1.18e-01. acc. prob=0.77]warmup:  18%|‚ñà‚ñä        | 176/1000 [00:03&lt;00:05, 155.42it/s, 31 steps of size 2.21e-01. acc. prob=0.78]warmup:  20%|‚ñà‚ñà        | 205/1000 [00:03&lt;00:04, 190.39it/s, 63 steps of size 1.17e-01. acc. prob=0.78]warmup:  24%|‚ñà‚ñà‚ñé       | 236/1000 [00:03&lt;00:03, 223.02it/s, 7 steps of size 5.48e-01. acc. prob=0.78] warmup:  26%|‚ñà‚ñà‚ñå       | 260/1000 [00:03&lt;00:03, 219.30it/s, 63 steps of size 7.61e-03. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 283/1000 [00:03&lt;00:05, 128.33it/s, 63 steps of size 9.48e-02. acc. prob=0.78]warmup:  30%|‚ñà‚ñà‚ñà       | 301/1000 [00:04&lt;00:05, 124.93it/s, 31 steps of size 1.94e-01. acc. prob=0.78]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 319/1000 [00:04&lt;00:05, 134.85it/s, 31 steps of size 1.47e-01. acc. prob=0.78]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 336/1000 [00:04&lt;00:04, 142.47it/s, 63 steps of size 6.35e-02. acc. prob=0.78]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 353/1000 [00:04&lt;00:05, 123.40it/s, 63 steps of size 9.99e-02. acc. prob=0.78]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 373/1000 [00:04&lt;00:04, 139.94it/s, 31 steps of size 1.44e-01. acc. prob=0.78]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [00:04&lt;00:04, 142.81it/s, 255 steps of size 3.08e-02. acc. prob=0.78]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 408/1000 [00:04&lt;00:04, 130.34it/s, 127 steps of size 3.87e-02. acc. prob=0.78]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/1000 [00:04&lt;00:04, 133.80it/s, 31 steps of size 1.58e-01. acc. prob=0.79] warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/1000 [00:05&lt;00:04, 137.58it/s, 31 steps of size 1.39e-01. acc. prob=0.79]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/1000 [00:05&lt;00:04, 120.08it/s, 63 steps of size 1.02e-01. acc. prob=0.78]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/1000 [00:05&lt;00:06, 88.50it/s, 127 steps of size 1.45e-02. acc. prob=0.78]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/1000 [00:05&lt;00:06, 78.88it/s, 255 steps of size 8.36e-02. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/1000 [00:05&lt;00:06, 76.59it/s, 127 steps of size 6.08e-02. acc. prob=0.78]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/1000 [00:05&lt;00:06, 77.20it/s, 63 steps of size 4.29e-02. acc. prob=0.78] sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/1000 [00:06&lt;00:06, 76.52it/s, 63 steps of size 5.44e-02. acc. prob=0.91]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/1000 [00:06&lt;00:05, 83.53it/s, 63 steps of size 5.44e-02. acc. prob=0.92]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/1000 [00:06&lt;00:05, 91.76it/s, 63 steps of size 5.44e-02. acc. prob=0.94]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/1000 [00:06&lt;00:04, 93.35it/s, 63 steps of size 5.44e-02. acc. prob=0.94]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/1000 [00:06&lt;00:05, 85.63it/s, 63 steps of size 5.44e-02. acc. prob=0.92]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/1000 [00:06&lt;00:05, 81.56it/s, 127 steps of size 5.44e-02. acc. prob=0.91]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/1000 [00:06&lt;00:05, 77.12it/s, 255 steps of size 5.44e-02. acc. prob=0.91]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/1000 [00:06&lt;00:05, 81.90it/s, 127 steps of size 5.44e-02. acc. prob=0.90]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/1000 [00:06&lt;00:05, 75.94it/s, 63 steps of size 5.44e-02. acc. prob=0.86] sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/1000 [00:07&lt;00:05, 76.56it/s, 127 steps of size 5.44e-02. acc. prob=0.83]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/1000 [00:07&lt;00:05, 73.55it/s, 63 steps of size 5.44e-02. acc. prob=0.81] sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/1000 [00:07&lt;00:05, 67.26it/s, 575 steps of size 5.44e-02. acc. prob=0.79]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/1000 [00:07&lt;00:05, 71.70it/s, 63 steps of size 5.44e-02. acc. prob=0.78] sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/1000 [00:07&lt;00:04, 80.56it/s, 63 steps of size 5.44e-02. acc. prob=0.80]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/1000 [00:07&lt;00:03, 90.45it/s, 63 steps of size 5.44e-02. acc. prob=0.80]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/1000 [00:07&lt;00:03, 93.58it/s, 127 steps of size 5.44e-02. acc. prob=0.80]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/1000 [00:07&lt;00:03, 94.20it/s, 63 steps of size 5.44e-02. acc. prob=0.81] sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/1000 [00:07&lt;00:03, 93.19it/s, 63 steps of size 5.44e-02. acc. prob=0.82]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/1000 [00:08&lt;00:03, 96.02it/s, 63 steps of size 5.44e-02. acc. prob=0.82]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/1000 [00:08&lt;00:02, 101.04it/s, 63 steps of size 5.44e-02. acc. prob=0.83]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/1000 [00:08&lt;00:02, 103.95it/s, 63 steps of size 5.44e-02. acc. prob=0.84]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/1000 [00:08&lt;00:02, 106.75it/s, 63 steps of size 5.44e-02. acc. prob=0.84]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/1000 [00:08&lt;00:02, 108.90it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/1000 [00:08&lt;00:02, 108.60it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/1000 [00:08&lt;00:02, 108.52it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/1000 [00:08&lt;00:02, 103.52it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/1000 [00:08&lt;00:02, 108.01it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/1000 [00:09&lt;00:02, 99.32it/s, 63 steps of size 5.44e-02. acc. prob=0.85] sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/1000 [00:09&lt;00:01, 104.47it/s, 63 steps of size 5.44e-02. acc. prob=0.85]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/1000 [00:09&lt;00:01, 106.43it/s, 127 steps of size 5.44e-02. acc. prob=0.86]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/1000 [00:09&lt;00:01, 105.15it/s, 63 steps of size 5.44e-02. acc. prob=0.86] sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/1000 [00:09&lt;00:01, 104.55it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/1000 [00:09&lt;00:01, 106.94it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [00:09&lt;00:01, 109.39it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/1000 [00:09&lt;00:01, 109.52it/s, 63 steps of size 5.44e-02. acc. prob=0.87]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/1000 [00:09&lt;00:01, 106.65it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/1000 [00:10&lt;00:01, 103.36it/s, 127 steps of size 5.44e-02. acc. prob=0.86]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/1000 [00:10&lt;00:00, 104.89it/s, 63 steps of size 5.44e-02. acc. prob=0.86] sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/1000 [00:10&lt;00:00, 104.26it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/1000 [00:10&lt;00:00, 106.99it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/1000 [00:10&lt;00:00, 107.15it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/1000 [00:10&lt;00:00, 105.51it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/1000 [00:10&lt;00:00, 105.85it/s, 63 steps of size 5.44e-02. acc. prob=0.86]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/1000 [00:10&lt;00:00, 107.30it/s, 63 steps of size 5.44e-02. acc. prob=0.87]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/1000 [00:10&lt;00:00, 108.44it/s, 63 steps of size 5.44e-02. acc. prob=0.87]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/1000 [00:11&lt;00:00, 101.60it/s, 127 steps of size 5.44e-02. acc. prob=0.87]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:11&lt;00:00, 90.29it/s, 127 steps of size 5.44e-02. acc. prob=0.87]\n\n\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\nModel found 8 clusters.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#mathematical-details",
    "href": "21. DPMM.html#mathematical-details",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThe process involves two keys submodels. The first, aims to identify the location and scale of K potential clusters. The second, aims to identify which cluster is most likely to have generated a given data point.\n\n\\begin{aligned}\n\\begin{pmatrix}\nY_{i,1} \\\\\n\\vdots \\\\\nY_{i,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\n\\mu_{z_i,1} \\\\\n\\vdots \\\\\n\\mu_{z_i,D}\n\\end{pmatrix},\n\\,\n\\Sigma_{z_i}\n\\right) \\\\\n\\\\\n\\begin{pmatrix}\n\\mu_{k,1} \\\\\n\\vdots \\\\\n\\mu_{k,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\nA_{1} \\\\\n\\vdots \\\\\nA_{D}\n\\end{pmatrix},\n\\, B\n\\right) \\\\\n\\\\\n\\Sigma_k &= \\sigma_k \\Omega_k \\sigma_k \\\\\n\\\\\n\\sigma_{k} &\\sim \\text{HalfCauchy}(1) \\\\\n\\\\\n\\Omega_k &\\sim \\text{LKJ}(2) \\\\\n\\\\\nz_{i} &\\sim \\text{Categorical}(\\pi) \\\\\n\\\\\n\\pi_{i}(\\beta_{1:K})  &=  \\beta_i \\prod_{j&lt;K} (1-\\beta_j) \\\\\n\\\\\n\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n\\\\\n\\alpha &\\sim \\text{Gamma}(1, 10) \\\\\n\\end{aligned}\n\nWhere :\n\n\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix} is the i-th observation of a D-dimensional data array.\n\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix} is the k-th parameter vector of dimension D.\n\\begin{pmatrix} A_{1} \\\\ \\vdots \\\\ A_{D} \\end{pmatrix} is a prior for the mean vector as derived from mean of the raw data.\nB is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\\Sigma_k is the DxD covariance matrix of the k-th cluster (it is composed from \\sigma_k and \\Omega_k).\n\\sigma_k is a diagonal matrix of standard deviations for the k-th cluster.\n\\Omega_k is a correlation matrix for the k-th cluster.\nz_i is a latent variable that maps observation i to cluster k.\n\\pi is a vector of K cluster weights, some of which may be close to zero if the predicted number of clusters is less than the maximum number of clusters.\n\\beta_k: The set of K Beta-distributed random variables used in the stick-breaking process to construct the mixture weights.\n\\alpha: The concentration parameter, controlling the effective number of clusters.",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#notes",
    "href": "21. DPMM.html#notes",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe primary advantage of the DPMM is the automatic inference of the number of clusters. The posterior distribution of the weights w reveals which components are ‚Äúactive‚Äù, giving a probabilistic estimate of K.\nPrior \\alpha strongly influence the predicted number of clusters. Below are examples of this relationship:\n\n\nImpact of Gamma Prior Hyperparameters on Cluster Counts\n\n\n\n\n\n\n\n\n\nShape\nRate\nE[\\alpha]\nE[K] (approx)\nBehavior\n\n\n\n\n1\n15\n0.067\n0.35\nForces very few clusters\n\n\n5\n1\n5\n26\nEncourages many small clusters\n\n\n10\n2\n5\n26\nSame mean, less variance\n\n\n2\n0.5\n4\n21\nModerately many clusters\n\n\n15\n1\n15\n78\nExplosive prior cluster count",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "21. DPMM.html#references",
    "href": "21. DPMM.html#references",
    "title": "Dirichlet Process Mixture Models üöß",
    "section": "Reference(s)",
    "text": "Reference(s)\nhttps://en.wikipedia.org/wiki/Dirichlet_process https://pyro.ai/examples/dirichlet_process_mixture.html",
    "crumbs": [
      "Models",
      "Dirichlet Process Mixture Models üöß"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "Univariate Linear Regression",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\alpha, which represents the origin of the line,i.e., the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\beta, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA standard deviation term \\sigma, which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#general-principles",
    "href": "1. Linear Regression for continuous variable.html#general-principles",
    "title": "Univariate Linear Regression",
    "section": "",
    "text": "To study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\nAn intercept \\alpha, which represents the origin of the line,i.e., the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\beta, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA standard deviation term \\sigma, which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#considerations",
    "href": "1. Linear Regression for continuous variable.html#considerations",
    "title": "Univariate Linear Regression",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nBayesian models allow us to update our understanding of parameters conditional on an observed data set. This allows us to consider model parameter uncertainty üõà, which quantifies our confidence or uncertainty in the parameters in the form of a posterior distribution üõà. Therefore, we need to declare prior distributions üõà for each model parameter, in this case for: \\alpha, \\beta, and \\sigma.\nPrior distributions are built following these considerations:\n\nAs the data are normalizedüõà (see introduction), we can use a Normal distribution for \\alpha and \\beta, with a mean of 0 and a standard deviation of 1. This tends to be a weakly regularizing prior, and weaker priors like a Normal(0,10) are also possible.\nSince \\sigma must be strictly positive, we must use a distribution with support on the positive reals, such as the Exponential or Folded-Normal distribution.\n\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without depending on a non linear link function üõà (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "Univariate Linear Regression",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.howell1(only_path = True)\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Subset data to adults\nm.scale(['weight']) # Normalize\n\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.log_normal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.dist.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;08:00,  2.08it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 264/1000 [00:00&lt;00:01, 603.23it/s, 7 steps of size 4.99e-01. acc. prob=0.78]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/1000 [00:00&lt;00:00, 1193.04it/s, 7 steps of size 7.35e-01. acc. prob=0.93]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/1000 [00:00&lt;00:00, 1659.12it/s, 7 steps of size 7.35e-01. acc. prob=0.93]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1208.35it/s, 7 steps of size 7.35e-01. acc. prob=0.93]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na\n154.64\n0.26\n154.27\n155.14\n0.01\n0.01\n422.23\n407.54\nNaN\n\n\nb\n5.82\n0.29\n5.40\n6.31\n0.02\n0.01\n376.62\n356.33\nNaN\n\n\ns\n5.15\n0.20\n4.81\n5.47\n0.01\n0.01\n446.91\n333.97\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\nm &lt;- importBI(platform = \"cpu\")\n\n# Load csv file\nm$data(m$load$howell1(only_path = T), sep = \";\")\n\n# Filter data frame\nm$df &lt;- m$df[m$df$age &gt; 18, ] # Subset data to adults\n\n# Scale\nm$scale(list(\"weight\")) # Normalize\n\n# Convert data to JAX arrays\nm$data_to_model(list(\"weight\", \"height\"))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight) {\n    # Parameter prior distributions\n    s &lt;- bi.dist.uniform(0, 50, name = \"s\")\n    a &lt;- bi.dist.normal(178, 20, name = \"a\")\n    b &lt;- bi.dist.normal(0, 1, name = \"b\")\n\n    # Likelihood\n    bi.dist.normal(a + b * weight, s, obs = height)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "Univariate Linear Regression",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist Formulation\nThe following equation describe the frequentist formulation of linear regression:\n\nY_i = \\alpha + \\beta  X_i + \\epsilon_i\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\n\\beta is the regression coefficient.\nX_i is the input variable for observation i.\n\\epsilon_i is the error term for observation i, and the vector of the error terms, \\epsilon, are assumed to be independent and identically distributed.\n\n\n\nBayesian Formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this regression model using the following model:\n\nY_i \\sim \\text{Normal}(\\alpha + \\beta   X_i, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Uniform}(0, 50)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha and \\beta are the intercept and regression coefficient, respectively.\nX_i is the independent variable for observation i.\n\\sigma is the standard deviation of the Normal distribution, which describes the variance in the relationship between the dependent variable Y and the independent variable X.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#notes",
    "href": "1. Linear Regression for continuous variable.html#notes",
    "title": "Univariate Linear Regression",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nWe observe a difference between the Frequentist and the Bayesian formulation regarding the error term. Indeed, in the Frequentist formulation, the error term \\epsilon represents residual fluctuations around the predicted values. This assumption leads to point estimates for \\alpha and \\beta. In contrast, the Bayesian formulation treats \\sigma as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "Univariate Linear Regression",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Univariate Linear Regression"
    ]
  },
  {
    "objectID": "10. Dirichlet model.html",
    "href": "10. Dirichlet model.html",
    "title": "Dirichlet Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Dirichlet model."
  },
  {
    "objectID": "10. Dirichlet model.html#general-principles",
    "href": "10. Dirichlet model.html#general-principles",
    "title": "Dirichlet Model",
    "section": "",
    "text": "To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a Dirichlet model."
  },
  {
    "objectID": "10. Dirichlet model.html#considerations",
    "href": "10. Dirichlet model.html#considerations",
    "title": "Dirichlet Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for the Multinomial model."
  },
  {
    "objectID": "10. Dirichlet model.html#example",
    "href": "10. Dirichlet model.html#example",
    "title": "Dirichlet Model",
    "section": "Example",
    "text": "Example\n\nPythonR"
  },
  {
    "objectID": "10. Dirichlet model.html#mathematical-details",
    "href": "10. Dirichlet model.html#mathematical-details",
    "title": "Dirichlet Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable Y_i with ùêæ categories, the Dirichlet likelihood function is:\n\nY_i \\sim \\text{Dirichlet}(\\theta_i  \\kappa) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\kappa \\sim \\text{Exponential}(1) \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n\nWhere:\n\nY_i is the outcome simplex üõà for observation i.\n\\kappa is the concentration parameter, it controls the prior weight on each category.\n\\theta_i is a vector unique to each observation, i, which gives the probability of observing i in category k.\n\\phi_i give the linear model for each of the k categories. Note that we use the softmax function to ensure that that the probabilities \\theta_i form a simplex üõà.\nEach element of \\phi_i is obtained by applying a linear regression model with its own respective intercept \\alpha_k and slope coefficient \\beta_k. To ensure the model is identifiable, one category, K, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category."
  },
  {
    "objectID": "10. Dirichlet model.html#references",
    "href": "10. Dirichlet model.html#references",
    "title": "Dirichlet Model",
    "section": "Reference(s)",
    "text": "Reference(s)"
  },
  {
    "objectID": "15. Gaussian processes.html",
    "href": "15. Gaussian processes.html",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#general-principles",
    "href": "15. Gaussian processes.html#general-principles",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Through varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#considerations",
    "href": "15. Gaussian processes.html#considerations",
    "title": "Gaussian Processes",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nTo capture complex, non-linear relationships in data where the underlying function is smooth but has an unknown functional form, GPs use a kernel üõà.\nThe choice of kernel hyperparameters can significantly impact results; thus, GPs require choosing an appropriate kernel function that captures the expected behavior of your data.\nThrough kernel definition, we can incorporate domain knowledge.\nThey scale poorly with dataset size (O(n¬≥) complexity) due to matrix operations; thus, memory requirements can be substantial for large datasets, which has led to neural networks being used instead to resolve large non-linear problems.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#example",
    "href": "15. Gaussian processes.html#example",
    "title": "Gaussian Processes",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Gaussian Process regression using the Bayesian Inference (BI) package. Data consist of a continuous dependent variable (total_tools), representing the number of tools invented in the islands, and a continuous independent variable (population), representing the population of the islands. The goal is to estimate the effect of population on the total tools. We use the distance matrix of the islands for the kernel function in order to capture the spatial dependence of the relationship. This example is based on McElreath (2018).\n\nPythonPython (Build in function)R\n\n\n\nfrom BI import bi, jnp\nimport pandas as pd\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.kline2(only_path=True)\nm.data(data_path, sep=';') \n\n\ndata_path2 = files('BI.Resources') / 'islandsDistMatrix.csv'\nislandsDistMatrix = pd.read_csv(data_path2, index_col=0)\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    # non-centered Gaussian Process prior\n    etasq = m.dist.exponential(2, name = 'etasq')\n    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n    SIGMA = etasq * jnp.exp(-rhosq * jnp.square(Dmat))\n    SIGMA = SIGMA.at[jnp.diag_indices(Dmat.shape[0])].add(etasq)\n    k = m.dist.multivariate_normal(0, SIGMA, name = 'k')\n\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;11:41,  1.43it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   4%|‚ñç         | 43/1000 [00:00&lt;00:13, 71.39it/s, 383 steps of size 1.21e-02. acc. prob=0.74]warmup:   8%|‚ñä         | 77/1000 [00:00&lt;00:07, 123.09it/s, 127 steps of size 1.45e-02. acc. prob=0.76]warmup:  11%|‚ñà         | 112/1000 [00:01&lt;00:05, 171.41it/s, 255 steps of size 6.22e-02. acc. prob=0.77]warmup:  14%|‚ñà‚ñç        | 145/1000 [00:01&lt;00:04, 207.19it/s, 127 steps of size 9.25e-02. acc. prob=0.77]warmup:  18%|‚ñà‚ñä        | 177/1000 [00:01&lt;00:03, 222.27it/s, 6 steps of size 8.92e-03. acc. prob=0.77]  warmup:  21%|‚ñà‚ñà        | 207/1000 [00:01&lt;00:03, 220.45it/s, 255 steps of size 3.96e-02. acc. prob=0.78]warmup:  24%|‚ñà‚ñà‚ñç       | 241/1000 [00:01&lt;00:03, 248.30it/s, 191 steps of size 6.20e-02. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 277/1000 [00:01&lt;00:02, 275.94it/s, 511 steps of size 2.24e-02. acc. prob=0.78]warmup:  31%|‚ñà‚ñà‚ñà       | 312/1000 [00:01&lt;00:02, 294.31it/s, 511 steps of size 1.65e-02. acc. prob=0.78]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 345/1000 [00:01&lt;00:02, 303.67it/s, 255 steps of size 2.41e-02. acc. prob=0.78]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 385/1000 [00:01&lt;00:01, 330.66it/s, 127 steps of size 2.83e-02. acc. prob=0.78]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/1000 [00:01&lt;00:01, 333.90it/s, 63 steps of size 1.98e-02. acc. prob=0.78] warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/1000 [00:02&lt;00:01, 350.73it/s, 255 steps of size 2.05e-02. acc. prob=0.78]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/1000 [00:02&lt;00:01, 338.54it/s, 9 steps of size 1.89e-02. acc. prob=0.78]  sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/1000 [00:02&lt;00:01, 333.04it/s, 127 steps of size 2.85e-02. acc. prob=0.93]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/1000 [00:02&lt;00:01, 339.29it/s, 127 steps of size 2.85e-02. acc. prob=0.92]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/1000 [00:02&lt;00:01, 333.38it/s, 127 steps of size 2.85e-02. acc. prob=0.92]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/1000 [00:02&lt;00:01, 320.39it/s, 63 steps of size 2.85e-02. acc. prob=0.93] sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/1000 [00:02&lt;00:01, 325.67it/s, 127 steps of size 2.85e-02. acc. prob=0.93]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/1000 [00:02&lt;00:00, 318.72it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/1000 [00:02&lt;00:00, 298.84it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/1000 [00:03&lt;00:00, 311.64it/s, 63 steps of size 2.85e-02. acc. prob=0.94] sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/1000 [00:03&lt;00:00, 313.58it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/1000 [00:03&lt;00:00, 314.26it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/1000 [00:03&lt;00:00, 311.80it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/1000 [00:03&lt;00:00, 321.80it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/1000 [00:03&lt;00:00, 342.14it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/1000 [00:03&lt;00:00, 334.86it/s, 127 steps of size 2.85e-02. acc. prob=0.94]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03&lt;00:00, 265.13it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na\n1.39\n1.01\n0.17\n2.81\n0.07\n0.05\n133.98\n214.31\nNaN\n\n\nb\n0.29\n0.08\n0.17\n0.41\n0.01\n0.00\n184.17\n119.24\nNaN\n\n\netasq\n0.08\n0.07\n0.01\n0.15\n0.01\n0.01\n136.57\n149.24\nNaN\n\n\ng\n0.64\n0.59\n0.02\n1.31\n0.03\n0.05\n194.77\n235.45\nNaN\n\n\nk[0]\n-0.21\n0.30\n-0.64\n0.24\n0.02\n0.03\n170.75\n120.39\nNaN\n\n\nk[1]\n0.03\n0.28\n-0.34\n0.48\n0.02\n0.02\n166.01\n144.33\nNaN\n\n\nk[2]\n-0.07\n0.27\n-0.40\n0.39\n0.03\n0.03\n115.89\n108.80\nNaN\n\n\nk[3]\n0.33\n0.23\n-0.01\n0.74\n0.02\n0.02\n136.59\n96.46\nNaN\n\n\nk[4]\n0.02\n0.24\n-0.34\n0.33\n0.02\n0.03\n118.65\n96.09\nNaN\n\n\nk[5]\n-0.41\n0.28\n-0.78\n-0.00\n0.03\n0.03\n120.78\n94.91\nNaN\n\n\nk[6]\n0.11\n0.23\n-0.23\n0.42\n0.02\n0.03\n105.71\n78.35\nNaN\n\n\nk[7]\n-0.24\n0.25\n-0.64\n0.11\n0.03\n0.03\n109.64\n71.99\nNaN\n\n\nk[8]\n0.24\n0.24\n-0.07\n0.61\n0.03\n0.03\n97.87\n68.36\nNaN\n\n\nk[9]\n-0.23\n0.34\n-0.71\n0.26\n0.04\n0.04\n103.11\n73.45\nNaN\n\n\nrhosq\n1.69\n1.65\n0.01\n3.62\n0.08\n0.11\n246.84\n201.20\nNaN\n\n\n\n\n\n\n\n\n\n\nfrom BI import bi, jnp\nimport pandas as pd\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.kline2(only_path=True)\nm.data(data_path, sep=';') \n\nislandsDistMatrix = m.load.islands_dist_matrix(frame = False)['data']\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    k = m.gaussian.gaussian_process(Dmat)\n\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;09:37,  1.73it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   5%|‚ñå         | 53/1000 [00:00&lt;00:09, 104.41it/s, 255 steps of size 1.01e-02. acc. prob=0.74]warmup:   9%|‚ñâ         | 88/1000 [00:00&lt;00:05, 157.55it/s, 127 steps of size 2.12e-02. acc. prob=0.76]warmup:  12%|‚ñà‚ñè        | 122/1000 [00:00&lt;00:04, 200.45it/s, 63 steps of size 9.73e-02. acc. prob=0.77]warmup:  16%|‚ñà‚ñå        | 157/1000 [00:00&lt;00:03, 237.27it/s, 127 steps of size 4.46e-02. acc. prob=0.77]warmup:  19%|‚ñà‚ñâ        | 191/1000 [00:01&lt;00:03, 252.53it/s, 63 steps of size 8.04e-02. acc. prob=0.78] warmup:  23%|‚ñà‚ñà‚ñé       | 229/1000 [00:01&lt;00:02, 285.17it/s, 127 steps of size 2.93e-02. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 276/1000 [00:01&lt;00:02, 333.55it/s, 255 steps of size 4.56e-02. acc. prob=0.78]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 319/1000 [00:01&lt;00:01, 360.21it/s, 63 steps of size 6.18e-02. acc. prob=0.78] warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 359/1000 [00:01&lt;00:01, 370.31it/s, 191 steps of size 2.64e-02. acc. prob=0.78]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [00:01&lt;00:01, 372.82it/s, 23 steps of size 1.74e-02. acc. prob=0.78] warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/1000 [00:01&lt;00:01, 325.72it/s, 127 steps of size 3.82e-02. acc. prob=0.78]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/1000 [00:01&lt;00:01, 296.91it/s, 127 steps of size 3.69e-02. acc. prob=0.78]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/1000 [00:02&lt;00:01, 306.27it/s, 127 steps of size 3.82e-02. acc. prob=0.94]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/1000 [00:02&lt;00:01, 341.14it/s, 63 steps of size 3.82e-02. acc. prob=0.91] sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/1000 [00:02&lt;00:01, 364.29it/s, 127 steps of size 3.82e-02. acc. prob=0.91]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/1000 [00:02&lt;00:01, 337.58it/s, 63 steps of size 3.82e-02. acc. prob=0.92] sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/1000 [00:02&lt;00:01, 328.53it/s, 63 steps of size 3.82e-02. acc. prob=0.92]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/1000 [00:02&lt;00:00, 346.50it/s, 63 steps of size 3.82e-02. acc. prob=0.92]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/1000 [00:02&lt;00:00, 370.93it/s, 255 steps of size 3.82e-02. acc. prob=0.92]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/1000 [00:02&lt;00:00, 393.18it/s, 63 steps of size 3.82e-02. acc. prob=0.91] sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/1000 [00:02&lt;00:00, 389.39it/s, 127 steps of size 3.82e-02. acc. prob=0.91]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/1000 [00:02&lt;00:00, 398.15it/s, 95 steps of size 3.82e-02. acc. prob=0.91] sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/1000 [00:03&lt;00:00, 412.32it/s, 63 steps of size 3.82e-02. acc. prob=0.91]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/1000 [00:03&lt;00:00, 436.67it/s, 63 steps of size 3.82e-02. acc. prob=0.92]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03&lt;00:00, 309.22it/s, 63 steps of size 3.82e-02. acc. prob=0.91]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na\n1.42\n1.11\n0.07\n2.72\n0.07\n0.09\n162.72\n159.21\nNaN\n\n\nb\n0.28\n0.08\n0.17\n0.44\n0.01\n0.00\n80.44\n174.63\nNaN\n\n\netasq\n0.20\n0.18\n0.01\n0.39\n0.01\n0.02\n145.06\n171.68\nNaN\n\n\ng\n0.65\n0.66\n0.02\n1.35\n0.07\n0.06\n70.85\n107.84\nNaN\n\n\nkernel[0]\n-0.17\n0.33\n-0.74\n0.27\n0.04\n0.03\n54.29\n92.78\nNaN\n\n\nkernel[1]\n-0.02\n0.32\n-0.54\n0.48\n0.05\n0.03\n48.22\n79.88\nNaN\n\n\nkernel[2]\n-0.07\n0.32\n-0.48\n0.50\n0.05\n0.04\n51.01\n55.60\nNaN\n\n\nkernel[3]\n0.35\n0.28\n-0.10\n0.78\n0.04\n0.03\n51.18\n83.79\nNaN\n\n\nkernel[4]\n0.07\n0.29\n-0.35\n0.50\n0.04\n0.03\n53.67\n57.59\nNaN\n\n\nkernel[5]\n-0.40\n0.29\n-0.80\n0.05\n0.04\n0.04\n59.51\n68.94\nNaN\n\n\nkernel[6]\n0.12\n0.29\n-0.21\n0.60\n0.04\n0.04\n51.47\n63.64\nNaN\n\n\nkernel[7]\n-0.22\n0.29\n-0.59\n0.27\n0.04\n0.04\n55.68\n76.35\nNaN\n\n\nkernel[8]\n0.26\n0.28\n-0.17\n0.67\n0.04\n0.03\n57.48\n50.38\nNaN\n\n\nkernel[9]\n-0.19\n0.38\n-0.76\n0.39\n0.04\n0.04\n81.79\n95.49\nNaN\n\n\nrhosq\n1.38\n1.73\n0.03\n3.37\n0.12\n0.13\n134.65\n216.24\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\njnp = reticulate::import('jax.numpy')\npd = reticulate::import('pandas')\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$kline2(only_path=T), sep=';')\nislandsDistMatrix = m$load$islands_dist_matrix(frame = FALSE)$data\nm$data_to_model(list('total_tools', 'population'))\nm$data_on_model$society = jnp$arange(0,10, dtype='int64')\nm$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(Dmat, population, society, total_tools){\n  a = bi.dist.exponential(1, name = 'a')\n  b = bi.dist.exponential(1, name = 'b')\n  g = bi.dist.exponential(1, name = 'g')\n  \n  # non-centered Gaussian Process prior\n  etasq = bi.dist.exponential(2, name = 'etasq')\n  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n  k = m$gaussian$gaussian_process(Dmat, etasq, rhosq, 0.01)\n  \n  lambda_ = a * population**b / g * jnp$exp(k[society])\n  m$dist$poisson(lambda_, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#mathematical-details",
    "href": "15. Gaussian processes.html#mathematical-details",
    "title": "Gaussian Processes",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormula\nThe following equation allows us to evaluate the relationship between the dependent variable Y distributed normal, and the independent variable X while incorporating a GP for the effect of variable Q:\n\nY_{[i]} \\sim \\text{Normal}( \\alpha + \\beta  X_{[i]} + \\gamma_{[Q_{[i]}]}, \\sigma)\n\nwhere:\n\nY_{[i]} is the i-th value for the dependent variable Y.\n\\alpha is the intercept term.\n\\beta is the regression coefficient term.\nX_{[i]} is the i-th value for the independent variable X.\nQ_{[i]} is an integer-valued independent variable (e.g., year-of-birth, age, year) for observation i.\n\\gamma is a vector output from a Gaussian process:\n\n\n\\gamma\n\\sim \\text{MVNormal} \\left(\nZ,\n\\varsigma\\Omega\\varsigma\n\\right)\n\nwhere:\n\nZ represents the mean vector of the multivariate normal distribution and set to zero üõà.\n\\varsigma is a diagonal matrix of standard deviations.\n\\Omega is a correlation matrix.\nMultiple kernel functions for \\Omega exist and will be discussed in the Note(s) section. But the most common one is the quadratic kernel:\n\n\n\\Omega_{[i,j]} = \\eta \\exp(-\\phi^2 D_{[i,j]}^2)\n\nWhere:\n\n\\eta is the maximal correlation.\n\\phi determines the rate of decline.\nD_{[i,j]} is the distance between the i-th and j-th categories.\n\n\n\nBayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this GP using the following model:\n\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\n\n\\gamma \\sim \\text{MVNormal} \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\n\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\eta^2 \\sim \\text{HalfCauchy}(0,1)\n\n\np^2 \\sim \\text{HalfCauchy}(0,1)\n\nwhere:\n\nY_i is the i-th value for the dependent variable Y.\n\\alpha is the intercept term with a prior of \\text{Normal}(0,1).\n\\beta is the regression coefficient term with a prior of \\text{Normal}(0,1).\nX_i is the i-th value for the independent variable X.\n\\gamma_{Z_i} is the Gaussian process i-th value for the independent variable Z.\n\\gamma is the latent function modeled by the GP.\nK_{ij} is the kernel function evaluated at the corresponding points, K_{ij} = k(Z_i, Z_j), with priors of HalfCauchy(0,1) for \\eta^2 and p^2 to ensure positive values.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#notes",
    "href": "15. Gaussian processes.html#notes",
    "title": "Gaussian Processes",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nCommon kernel functions include:\n\nRadial Basis Function (RBF) or Squared Exponential Kernel: k(x,x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\nRational Quadratic Kernel, this kernel is equivalent to adding together many RBF kernels with different length scales: k(x,x') = \\sigma^2 \\left(1 + \\frac{||x-x'||^2}{2l^2}\\right)^{-\\alpha}\nPeriodic kernel allows for modeling functions that repeat themselves exactly: k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)\nLocally Periodic Kernel:\n\nk(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right) \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\n\nAny slope or intercept in your model can be defined using a Gaussian Process.",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#references",
    "href": "15. Gaussian processes.html#references",
    "title": "Gaussian Processes",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.\n\n\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/",
    "crumbs": [
      "Models",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html",
    "href": "26. Network Based Diffusion analysis (wip).html",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links Hasenjager, Leadbeater, and Hoppitt (2021). The basic model underlying NBDA states that at time t an individual, i, learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time t-1).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "href": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "title": "Network-Based Diffusion Analysis",
    "section": "",
    "text": "The principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links Hasenjager, Leadbeater, and Hoppitt (2021). The basic model underlying NBDA states that at time t an individual, i, learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:\n\nWhere the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time t-1).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#considerations",
    "href": "26. Network Based Diffusion analysis (wip).html#considerations",
    "title": "Network-Based Diffusion Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThere are two main NBDA variants: order-of-acquisition diffusion analysis (OADA), which takes as data the order in which individuals acquired the target behaviour, and time-of-acquisition diffusion analysis (TADA), which uses the times of acquisition of the target behaviour.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#example",
    "href": "26. Network Based Diffusion analysis (wip).html#example",
    "title": "Network-Based Diffusion Analysis",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Multiplex network model using the Bayesian Inference (BI) package Nightingale et al. (2015):\n\nPythonR",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "href": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "title": "Network-Based Diffusion Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFormulation\nThere are two parameters of interest in the basic time of acquisition diffusion analysis model: the rate of social transmission be-tween individuals per unit of network connection,s, and the baseline rate of trait performance in the absence of social transmission, Œª_0.\n\n\\lambda_i(t) = \\lambda_0(t) (1- z_i(t))  \\left[ s \\sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \\right]\n\nWhere:\n\n\\lambda_i(t) is the rate at which individuals i acquire the task solution at time t.\n\\lambda_0(t) is a baseline acquisition function determining the distribution of latencies to acquisition in the absence of social transmission (that is, through asocial learning). It can be specify by an exponential or Weibull distrbution.\nz_i(t) gives the status (1 = informed, 0 = na√Øve) of individual i at time t.\ns is the regression coefficients capturing the effect of x on the hazard have an assigned a normal prior.\n(1- z_i(t)) and z_j (-1) terms ensure that the task solution is only transmitted from informed to uninformed individuals:\n\n\nz_j(t) =  Y_i \\sim \\begin{cases}\n0, & \\text{if j is naive} \\\\\n1, & \\text{if j is informed}\n\\end{cases}",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#notes",
    "href": "26. Network Based Diffusion analysis (wip).html#notes",
    "title": "Network-Based Diffusion Analysis",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#references",
    "href": "26. Network Based Diffusion analysis (wip).html#references",
    "title": "Network-Based Diffusion Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nHasenjager, Matthew J., Ellouise Leadbeater, and William Hoppitt. 2021. ‚ÄúDetecting and Quantifying Social Transmission Using Network-Based Diffusion Analysis.‚Äù Journal of Animal Ecology 90 (1): 8‚Äì26. https://doi.org/https://doi.org/10.1111/1365-2656.13307.\n\n\nNightingale, Glenna, Neeltje J Boogert, Kevin N Laland, and Will Hoppitt. 2015. ‚ÄúQuantifying Diffusion in Social Networks: A Bayesian Approach.‚Äù Animal Social Networks, 38‚Äì52.",
    "crumbs": [
      "Models",
      "Network-Based Diffusion Analysis"
    ]
  },
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "Bayesian analysis with BI",
    "section": "",
    "text": "This document is a guide to Bayesian analysis and the implementation of Bayesian Inference (BI) package. It is intended for users ranging from those with little or no experience to advanced practitioners. In this introduction, we outline the main steps of Bayesian analysis. Each of the subsequent chapters present increasingly complex models. Each chapter will have the same structure in order to allow users to easily find the information they are looking for. The structure is as follows:\nWe recommend reading the introduction first since some key concepts here will not be revisited in later chapters.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#modeling-likelihood",
    "href": "0. Introduction.html#modeling-likelihood",
    "title": "Bayesian analysis with BI",
    "section": "Modeling Likelihood",
    "text": "Modeling Likelihood\nOnce the likelihood is defined, we can now define the mathematical equations that describe our parameters (\\mu and \\sigma) and their relationship with the dependent variable Y. We can express this relationship in the form of a linear function:\n\n\\mu = \\alpha + \\beta X\n\nWhere \\alpha is the intercept üõà and \\beta is the slope üõà of the regression line. These parameters are the unknowns that we want to estimate to evaluate the strength and direction of the relationship between the independent variable X and the dependent variable Y.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions",
    "href": "0. Introduction.html#link-functions",
    "title": "Bayesian analysis with BI",
    "section": "Link functions",
    "text": "Link functions\nDepending on the type of problem you are trying to solve (classification, regression, etc.) and the type of data you are working with (continuous, discrete, binomial, etc.), you will need to choose the appropriate distribution to describe the relationships in the data. For each different outcome distribution, you will need to use an appropriate link function üõà.\nFor the moment, we just need to know that these different distributions require a link function (for each specific family we will discuss the corresponding link function in their respective chapters); however, below is a table summarizing some of the most common link functions, the mathematical form of each, their typical applications, and how to interpret them. Link functions in BI can be accessed through the class bi.link.XXX where XXX is the name of the link function. Let \\mu be a real number output from a linear model, and g(\\mu) be the corresponding link function. Then the table shows the most common link functions and their interpretations.\n\n\n\n\n\n\n\n\n\nLink Function\nMathematical Form of Inverse Link\nTypical Use / Model\nInterpretation & Range\n\n\n\n\nIdentity\ng(\\mu) = \\mu\nLinear regression (Normal)\nDirectly models \\mu; the outputs space is also the set of real numbers.\n\n\nLogit\ng(\\mu) = \\frac{1}{1+\\exp(-\\mu)}\nLogistic regression (Binomial)\nThe output space is the unit interval [0, 1] ; coefficients reflect log-odds.\n\n\nProbit\ng(\\mu) = \\Phi(\\mu)\nProbit regression (Binomial)\nSimilar to logit; uses the standard normal CDF, \\Phi.\n\n\nLog\ng(\\mu) = \\exp(\\mu)\nPoisson, Gamma regression (Count data)\nThe output space is the set of positive reals.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#the-prior-distributions",
    "href": "0. Introduction.html#the-prior-distributions",
    "title": "Bayesian analysis with BI",
    "section": "The Prior Distributions",
    "text": "The Prior Distributions\nFor each parameter in our model, we need to define a prior distribution üõà that encodes our initial beliefs about the parameter. In the case of the linear regression model, we need to specify prior distributions for the intercept, \\alpha, the slope, \\beta, and the standard deviation, \\sigma.\n\n\\alpha \\sim \\text{Normal}(0, 1)\n\n\n\\beta \\sim \\text{Normal}(0, 1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nAnd with this, we can write our entire model as:\n\nY \\sim \\text{Normal}(\\mu, \\sigma)\n \n\\mu = \\alpha + \\beta X\n \n\\alpha \\sim \\text{Normal}(0, 1)\n \n\\beta \\sim \\text{Normal}(0, 1)\n \n\\sigma \\sim \\text{Exponential}(1)\n\nIn BI, you code all this statements within a single function. In that function you can use any probability distribution, link function, and mathematical operations required for your model (if they are supported by JAX). BI has been designed to allow you to declare your model as close as possible to the mathematical notation. For example, the model above can be written in BI as:\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\nbeta = 2.5\nalpha = 0.5\nsigma = 1.0\nX = m.dist.normal(0, 1, sample = True)\nY = m.dist.normal(alpha + beta * X, sigma, sample = True)\n\nm.data_on_model = dict(X=X, Y=Y)\n\ndef model(X, Y):    \n    alpha = m.dist.normal( 0, 1, name = 'alpha', shape= (1,))\n    beta = m.dist.normal( 0, 1, name = 'beta', shape= (1,))   \n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    m.normal(alpha + beta * X, sigma, obs=Y)\nThe code snippet provides several key features of the BI package:\n\nFirst, you need to initialize a BI object.\nThen, you can store data as a JAX array dictionary using the m.data_on_model function. If all the data can be stored in a data frame (e.g., Linear Regression for continuous variable), you do not need to use m.data_on_model, as the BI object automatically detects the data provided in the model arguments. However, sometimes you may need different data structures such as vectors and 2D arrays (e.g., Network model).\nRegarding distribution parameters, note the difference depending on whether you are generating data outside a function (e.g., for simulation purposes) or specifying priors inside a model function. In the former case, the argument sample should be set to True. However, if you are specifying priors within a model function, this argument is False by default.\nFinally, note that each parameter declared in the model must have a unique name as well as a shape. The shape refers to the number of parameters you want to estimate. For example, if you want to estimate a different \\beta for each independent variable, you would declare \\beta with a shape equal to the number of independent variables. By default, the shape is one, so technically you don‚Äôt need to specify it. In this example, we highlight this feature explicitly.\n\n\nWhich prior distribution range to use?\nThe choice of prior ranges can significantly affect Bayesian analysis results. There are several approaches to selecting them:\n\nExpert Knowledge: The prior distributions can be based on expert knowledge or historical data. This approach is useful when there is a lot of information available about the parameters.\nNoninformative Priors: When there is little or no information about the parameters, noninformative priors can be used. These priors are designed to have minimal influence on the posterior distribution, allowing the data to dominate the inference process.\nScaled data: If the data are scaled üõà, the prior distributions can be chosen to reflect this. For example, if the data are scaled, the prior distributions for the intercept and slope can be centered around 0 and 1, respectively. By scaling the independent variable, we obtain a unit of change based on variance; that is, the effect represents a one‚Äìstandard‚Äìdeviation change in X on Y. Scaling the data improves both numerical stability. When all data are scaled to the same range, it leads to more stable numerical behavior during estimation. Additionally, it facilitates setting priors that are both meaningful and relatively uninformative. By aligning the scale of the data with the scale assumed in the priors, we ensure that the posterior distributions exhibit reasonable spread and that our uncertainty quantification is consistent with the data‚Äôs scale. For the remainder of the document, we will assume that the data are scaled. However, users should be aware that it is often necessary to rescale parameters estimates by the standard deviation of the data in order to get parameters that are interpretable on the natural scale.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fit-and-posterior-distribution",
    "href": "0. Introduction.html#model-fit-and-posterior-distribution",
    "title": "Bayesian analysis with BI",
    "section": "Model fit and posterior distribution",
    "text": "Model fit and posterior distribution\nOnce data are observed, Bayes‚Äô Theorem üõà is used to evaluate how well a given set of parameter values fits the data:\n\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\nWhere:\n\n\\theta represents the unknown parameters we are interested in.\nP(\\theta) is the prior distribution, representing our beliefs about \\theta before seeing the data.\nP(\\text{data} \\mid \\theta) is the likelihood, representing the model of how the data are generated given \\theta.\nP(\\theta \\mid \\text{data}) is the posterior distribution, representing our updated beliefs after observing the data. It tells us not only the most likely value of \\theta (e.g., \\alpha, \\beta, and \\sigma in our case) but also quantifies the uncertainty in these estimates.\n\nVarious techniques can be used to approximate the mathematical definition of Bayes‚Äô theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Descriptions of these algorithms are out of the scope of this document. For more information, please refer to Wikipedia. In BI, we use MCMC and it can be called as m.fit(model) where model is the function that describes the model.",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "Bayesian analysis with BI",
    "section": "Model diagnostic",
    "text": "Model diagnostic\nOnce a Bayesian model has been fit, it is crucial to evaluate how well it captures the observed data and to assess whether the Markov chain Monte Carlo (MCMC) sampling has converged. Bayesian model diagnostics help us answer questions like: ‚ÄúAre our uncertainty estimates reliable?‚Äù, ‚ÄúDoes the model generate data similar to what we observed?‚Äù, and ‚ÄúHave the chains mixed well?‚Äù Multiple diagnostics approaches can be used to assess the model‚Äôs performance. Below are some key diagnostic tools and techniques available in BI within the class BI.diag.XXX where XXX is the name of the diagnostic tool.\n\n\n\n\n\n\n\n\n\nDiagnostic Tool\nPurpose\nKey Indicator\nInterpretation\n\n\n\n\nposterior predictive checks (PPCs) üõà\nAssess if the model can reproduce observed data\nGraphs, p-values, summary stats\nGood fit if simulated data resemble observed data\n\n\nCredible Interval (CI)\nQuantify uncertainty in parameter estimates\n95% CI or other percentage\n95% probability the parameter lies within the interval\n\n\nhighest posterior density intervals (HPDI) üõà\nIdentify the narrowest interval containing a given probability mass density\n95% HPDI\nSmallest interval capturing 95% of the posterior density\n\n\neffective sample size (ESS) üõà\nMeasure independent information in the chain\nESS value (ideally high)\nLow ESS indicates high autocorrelation (poor mixing) ot to few samples correlations\n\n\npotential scale reduction factor (Rhat) üõà\nCheck convergence across multiple chains\nRhat ‚âà 1 (typically &lt;1.01)\nValues near 1 indicate convergence; &gt;1 suggests non-convergence\n\n\nTrace plots üõà\nVisualize the sampling path to check convergence and mixing\nPlot showing parameter values over iterations\nStationary, ‚Äòhairy caterpillar‚Äô pattern suggests convergence\n\n\nautocorrelation plots üõà\nAssess dependency between samples over lags\nAutocorrelation values across lags\nRapid decay to zero suggests good mixing; slow decay indicates poor mixing\n\n\ndensity plots üõà\nVisualize the posterior distribution of a parameter\nSmoothness and shape of the curve\nUnimodal and smooth suggests convergence; multimodal or irregular may suggest poor mixing",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-comparison",
    "href": "0. Introduction.html#model-comparison",
    "title": "Bayesian analysis with BI",
    "section": "Model comparison",
    "text": "Model comparison\nModel comparison is performed by evaluating how well different models explain the observed data while accounting for model complexity. Multiple criteria can be used to compare models, and are summarized in the table below. In BI, we can compare models using Watanabe-Akaike Information Criterion (WAIC) with the function m.diag.waic(model1, model2).\n\n\n\n\n\n\n\n\n\n\nCriterion\nPurpose\nInterpretation\nStrengths\nWeaknesses\n\n\n\n\nDIC (Deviance Information Criterion)\nMeasures model fit while penalizing complexity\nLower values indicate better model fit\nSimple to compute, useful for hierarchical models\nSensitive to the number of parameters, not always reliable in complex models\n\n\nWAIC (Watanabe-Akaike Information Criterion)\nEstimates out-of-sample predictive accuracy while penalizing complexity\nLower values indicate better models\nMore robust than DIC, accounts for overfitting\nComputationally intensive for large models\n\n\nBF (Bayes Factor)\nQuantifies relative support for two models based on marginal likelihoods\nBF &gt; 1 favors the numerator model, BF &lt; 1 favors the denominator\nProvides direct evidence comparison, works with different model types\nSensitive to prior choices, requires good model specification",
    "crumbs": [
      "Models",
      "Bayesian analysis with BI"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html",
    "href": "6. Beta binomial model.html",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#general-principles",
    "href": "6. Beta binomial model.html#general-principles",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "To model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#considerations",
    "href": "6. Beta binomial model.html#considerations",
    "title": "Beta-Binomial Model",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Binomial regression.\nA Beta-Binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.\nA Beta distribution is a continuous probability distribution defined on the interval. It is characterized by two positive shape parameters, commonly denoted as Œ± and Œ≤, which control the shape of the distribution. In the context of the provided equations, \\gamma and \\eta serve as these shape parameters. These parameters determine the shape of the distribution, allowing it to model a wide variety of random variables representing proportions or probabilities. How the \\gamma and \\eta parameters influence the distribution‚Äôs shape can be summarized as follows:\nWhen \\gamma &gt; 1 and \\eta &gt; 1, the distribution is unimodal (bell-shaped), with the peak becoming sharper as the values of \\gamma and \\eta increase. If \\gamma and \\eta are equal and greater than 1, the distribution is symmetrical and centered around 0.5.\nIf \\gamma &lt; 1 and \\eta &lt; 1, the distribution is U-shaped, with peaks at both 0 and 1.\nThe skewness of the distribution is determined by the relative values of \\gamma and \\eta. If \\gamma &gt; \\eta, the distribution is skewed toward 1 (left-skewed), meaning more of the probability mass is concentrated on higher values. Conversely, if \\eta &gt; \\gamma, the distribution is skewed toward 0 (right-skewed), with more probability mass on lower values. The mean of the distribution is given by \\gamma / (\\gamma + \\eta).\n\nTherefore, by adjusting the shape parameters \\gamma and \\eta, the Beta distribution offers significant flexibility in modeling various types of prior beliefs about probabilities.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#example",
    "href": "6. Beta binomial model.html#example",
    "title": "Beta-Binomial Model",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression using the Bayesian Inference (BI) package. The data consist of:\n\nOne binary dependent variable (admit), which represents candidates‚Äô admission status.\nOne independent categorical variable representing individuals‚Äô gender (gid).\nAdditionally, we have the number of applications (applications) per gender, which will be used to account for independent rates.\n\nThe goal is to evaluate whether the probability of admission is different between genders, while accounting for differences in the number of applications between genders. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.ucbadmit(only_path = True)\nm.data(data_path, sep=';') \nm.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n\n# Define model ------------------------------------------------\ndef model(gid, applications, admit):\n    # Prior for overall concentration scaling (positive, via exponential)\n    phi = m.dist.exponential(1, name='phi')\n    \n    # Priors for group-level intercepts (two groups, normal-distributed)\n    alpha = m.dist.normal(0., 1.5, shape=(2,), name='alpha')\n    \n    # Shifted concentration scale (avoids too small values)\n    theta = phi + 2\n    \n    # Group-specific mean success probability (mapped to [0,1] with sigmoid)\n    pbar = m.link.inv_logit(alpha[gid])\n    \n    # Beta distribution parameter for \"successes\"\n    concentration1 = pbar * theta\n    \n    # Beta distribution parameter for \"failures\"\n    concentration0 = (1 - pbar) * theta\n    \n    # Likelihood: admissions modeled with Beta-Binomial\n    m.dist.beta_binomial(\n        total_count=applications,\n        concentration1=concentration1,\n        concentration0=concentration0,\n        obs=admit\n    )\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;13:02,  1.28it/s, 3 steps of size 2.34e+00. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 332/1000 [00:00&lt;00:01, 511.52it/s, 15 steps of size 6.65e-01. acc. prob=0.79]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/1000 [00:00&lt;00:00, 1112.17it/s, 7 steps of size 5.62e-01. acc. prob=0.88]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01&lt;00:00, 949.75it/s, 7 steps of size 5.62e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha[0]\n-0.42\n0.39\n-1.00\n0.19\n0.02\n0.03\n470.21\n230.54\nNaN\n\n\nalpha[1]\n-0.30\n0.42\n-0.96\n0.34\n0.02\n0.02\n431.00\n297.75\nNaN\n\n\nphi\n1.00\n0.78\n0.00\n2.04\n0.04\n0.04\n284.63\n189.52\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\njnp = reticulate::import(\"jax.numpy\")\n# import data ------------------------------------------------\nm$data(m$load$ucbadmit(only_path = T), sep=';')\nm$data_on_model$gid = jnp$array(as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)))\nm$data_on_model$applications = jnp$array(as.integer(unlist(m$df[\"applications\"])))\nm$data_on_model$admit = jnp$array(as.integer(unlist(m$df[\"admit\"])))\n# Define model ------------------------------------------------\nmodel &lt;- function(gid, applications, admit){\n  # Parameter prior distributions\n  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n  t = phi + 2\n  pbar = m$link$inv_logit(alpha[gid])\n  gamma = pbar * t\n  eta = (1 - pbar) * t\n  # Likelihood\n\n  m$dist$beta_binomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n\nm$data_on_model",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#mathematical-details",
    "href": "6. Beta binomial model.html#mathematical-details",
    "title": "Beta-Binomial Model",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nBayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY_i \\sim \\text{BetaBinomial}(N_i, \\gamma_i, \\eta_i)\n\n\n\\gamma_i = p_i   \\tau\n\n\n\\eta_i = (1 - p_i ) \\tau\n\n\np_i = \\text{logit}^{-1}(\\alpha + \\beta * X_i)\n\n\n\\tau = \\phi + 2\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\n\n\\phi \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the count of successes for the i-th observation, which follows a Beta-binomial distribution with N_i trials.\n\\gamma_i represents the concentration parameter for the number of successes, derived from the probability of success, p_i, and scaled by \\tau.\n\\eta_i represents the concentration parameter for failures, derived from the probability of failure (1 - p_i) and also scaled by \\tau.\np_i is the probability of success for the i-th observation. The logit function transforms the linear predictor (which can take any real value) into a probability value between 0 and 1.\n\\tau is derived from ùúô and is used as a scaling factor for the shape parameters ùõæ and ùúÇ.\n\\beta and \\alpha are the regression coefficient and intercept, respectively.\nœï is a random variable following an Exponential distribution with a rate of 1.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#references",
    "href": "6. Beta binomial model.html#references",
    "title": "Beta-Binomial Model",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Beta-Binomial Model"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html",
    "href": "8. Poisson mode with offset.html",
    "title": "Poisson Model with an Offset",
    "section": "",
    "text": "When we want to model count data, where the counts are observed over different periods or areas of exposure, we use a Poisson model with an offset. This is a type of generalized linear model used for modeling count data and contingency tables.\nAn offset is a predictor variable with a coefficient that is fixed at 1. It is used to account for the ‚Äúexposure‚Äù variable, which represents the opportunity for an event to occur. For instance, if we are counting the number of sick individuals in different cities, the population of each city would be the exposure variable. A city with a larger population is expected to have more sick individuals. The offset accounts for this by essentially modeling the rate of events per unit of exposure.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#general-principles",
    "href": "8. Poisson mode with offset.html#general-principles",
    "title": "Poisson Model with an Offset",
    "section": "",
    "text": "When we want to model count data, where the counts are observed over different periods or areas of exposure, we use a Poisson model with an offset. This is a type of generalized linear model used for modeling count data and contingency tables.\nAn offset is a predictor variable with a coefficient that is fixed at 1. It is used to account for the ‚Äúexposure‚Äù variable, which represents the opportunity for an event to occur. For instance, if we are counting the number of sick individuals in different cities, the population of each city would be the exposure variable. A city with a larger population is expected to have more sick individuals. The offset accounts for this by essentially modeling the rate of events per unit of exposure.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#considerations",
    "href": "8. Poisson mode with offset.html#considerations",
    "title": "Poisson Model with an Offset",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nThe dependent variable in a Poisson regression must be a non-negative count.\nThe exposure variable used as an offset cannot contain zeros.\nA key assumption of the Poisson distribution is that the mean and variance of the count variable are equal. If the variance is greater than the mean, a condition known as overdispersion, a Negative Binomial regression might be more appropriate.\nThe logarithm of the exposure variable is typically used as the offset. This is because Poisson regression models the logarithm of the expected count. By including the log of the exposure as an offset, we are effectively modeling the rate.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#example",
    "href": "8. Poisson mode with offset.html#example",
    "title": "Poisson Model with an Offset",
    "section": "Example",
    "text": "Example\nBelow is an example of code that demonstrates a Bayesian Poisson regression with an offset. The data consists of the number of elephant aggressions (agressions), the age of the elephants (age), and the number of years they have been observed (years_obs). The goal is to model the rate of aggressions per year, accounting for the age of the elephants.\n\nPythonR",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#mathematical-details",
    "href": "8. Poisson mode with offset.html#mathematical-details",
    "title": "Poisson Model with an Offset",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the independent variables (X) and the expected count (Œª) using the following equation:\n\n\\log(\\lambda_i) = \\alpha + \\beta X_i + \\log(\\text{exposure}_i)\n\nWhere:\n\n\\lambda_i is the expected count for observation i.\n\\alpha is the intercept term.\n\\beta is the regression coefficient for the independent variable.\nX_i is the value of the independent variable for observation i.\n\\log(\\text{exposure}_i) is the offset, which is the natural logarithm of the exposure for observation i.\n\nThe number of observed counts Y_i is assumed to follow a Poisson distribution with mean \\lambda_i:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\nBayesian formulation\nIn the Bayesian framework, we assign prior distributions to the model parameters. The model can be expressed as:\n\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\n\n\\log(\\lambda_i) = \\alpha + \\beta X_i + \\log(\\text{exposure}_i)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta \\sim \\text{Normal}(0,1)\n\nWhere:\n\nY_i is the observed count for observation i.\n\\lambda_i is the expected count.\n\\alpha is the intercept with a unit-normal prior.\n\\beta is the slope coefficient with a unit-normal prior.\nX_i is the independent variable.\n\\log(\\text{exposure}_i) is the offset.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#notes",
    "href": "8. Poisson mode with offset.html#notes",
    "title": "Poisson Model with an Offset",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nThe use of an offset is crucial when the goal is to compare rates of events rather than absolute counts.\nIt is a common practice to use the natural logarithm of the exposure variable as the offset.",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "8. Poisson mode with offset.html#references",
    "href": "8. Poisson mode with offset.html#references",
    "title": "Poisson Model with an Offset",
    "section": "Reference(s)",
    "text": "Reference(s)",
    "crumbs": [
      "Models",
      "Poisson Model with an Offset"
    ]
  },
  {
    "objectID": "20. GMM.html",
    "href": "20. GMM.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model‚Äôs goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\mu) and its shape/spread (covariance \\Sigma).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#general-principles",
    "href": "20. GMM.html#general-principles",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "To discover group structures or clusters in data, we can use a Gaussian Mixture Model (GMM). This is a parametric clustering method. A GMM assumes that the data is generated from a mixture of a pre-specified number (K) of different Gaussian distributions. The model‚Äôs goal is to figure out:\n\nThe properties of each of the K clusters: For each of the K clusters, it estimates its center (mean \\mu) and its shape/spread (covariance \\Sigma).\nThe mixture weights: It estimates the proportion of the data that belongs to each cluster.\nThe assignment of each data point: It determines the probability of each data point belonging to each of the K clusters.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#considerations",
    "href": "20. GMM.html#considerations",
    "title": "Gaussian Mixture Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA GMM is a Bayesian model üõà that considers uncertainty in all its parameters, except for the number of clusters, K, which must be fixed in advance.\nThe key parameters and their priors are:\n\nNumber of Clusters K: This is a fixed hyperparameter that you must choose before running the model. Choosing the right K often involves running the model multiple times and using model comparison criteria (like cross-validation, AIC, or BIC).\nCluster Weights w: These are the probabilities of drawing a data point from any given cluster. Since there are a fixed number K of them and they must sum to 1, they are typically given a Dirichlet prior. A symmetric Dirichlet prior (e.g., Dirichlet(1, 1, ..., 1)) represents an initial belief that all clusters are equally likely.\n**Cluster Parameters (\\mu, \\Sigma): Each of the K clusters has a mean \\mu and a covariance matrix \\Sigma. We place priors on these to define our beliefs about their plausible values.\n\nLike the DPMM, the model is often implemented in its marginalized form . Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\nTo increase accuracy we run a k-means algorithm to initialize the cluster mean priors.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#example",
    "href": "20. GMM.html#example",
    "title": "Gaussian Mixture Models",
    "section": "Example",
    "text": "Example\nBelow is an example of a GMM implemented in BI. The goal is to cluster a synthetic dataset into a pre-specified K=4 groups.\n\nPythonR\n\n\n\nfrom BI import bi, jnp\nfrom sklearn.datasets import make_blobs\n\nm = bi()\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n\n#  The model\ndef gmm(data, K, initial_means): # Here K is the *exact* number of clusters\n    D = data.shape[1]  # Number of features\n    alpha_prior = 0.5 * jnp.ones(K)\n    w = m.dist.dirichlet(concentration=alpha_prior, name='weights') \n\n    with m.dist.plate(\"components\", K): # Use fixed K\n        mu = m.dist.multivariate_normal(loc=initial_means, covariance_matrix=0.1*jnp.eye(D), name='mu')        \n        sigma = m.dist.half_cauchy(1, shape=(D,), event=1, name='sigma')\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name='Lcorr')\n\n        scale_tril = sigma[..., None] * Lcorr\n\n    m.dist.mixture_same_family(\n        mixing_distribution=m.dist.categorical(probs=w, create_obj=True),\n        component_distribution=m.dist.multivariate_normal(loc=mu, scale_tril=scale_tril, create_obj=True),\n        name=\"obs\",\n        obs=data\n    )\n\nm.data_on_model = {\"data\": data,\"K\": 8 }\nm.fit(gmm) # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:03&lt;1:04:15,  3.86s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|‚ñè         | 23/1000 [00:03&lt;02:00,  8.10it/s, 63 steps of size 2.05e-02. acc. prob=0.71]warmup:   4%|‚ñé         | 36/1000 [00:04&lt;01:10, 13.75it/s, 31 steps of size 4.17e-02. acc. prob=0.74]warmup:   5%|‚ñç         | 49/1000 [00:04&lt;00:45, 21.12it/s, 7 steps of size 5.89e-02. acc. prob=0.76] warmup:   6%|‚ñã         | 64/1000 [00:04&lt;00:29, 31.79it/s, 3 steps of size 2.17e-02. acc. prob=0.76]warmup:   8%|‚ñä         | 82/1000 [00:04&lt;00:19, 47.36it/s, 15 steps of size 5.50e-02. acc. prob=0.77]warmup:  10%|‚ñà         | 100/1000 [00:04&lt;00:13, 64.62it/s, 7 steps of size 3.20e-02. acc. prob=0.77]warmup:  12%|‚ñà‚ñè        | 116/1000 [00:04&lt;00:12, 73.21it/s, 15 steps of size 4.80e-01. acc. prob=0.77]warmup:  13%|‚ñà‚ñé        | 133/1000 [00:04&lt;00:09, 88.47it/s, 15 steps of size 4.34e-01. acc. prob=0.78]warmup:  15%|‚ñà‚ñç        | 149/1000 [00:04&lt;00:08, 102.06it/s, 15 steps of size 6.05e-01. acc. prob=0.78]warmup:  16%|‚ñà‚ñã        | 165/1000 [00:05&lt;00:07, 107.99it/s, 63 steps of size 1.81e-01. acc. prob=0.78]warmup:  18%|‚ñà‚ñä        | 180/1000 [00:05&lt;00:07, 116.42it/s, 7 steps of size 2.76e-01. acc. prob=0.78] warmup:  20%|‚ñà‚ñâ        | 197/1000 [00:05&lt;00:06, 127.43it/s, 15 steps of size 7.08e-01. acc. prob=0.78]warmup:  22%|‚ñà‚ñà‚ñè       | 215/1000 [00:05&lt;00:05, 140.43it/s, 15 steps of size 4.57e-01. acc. prob=0.78]warmup:  23%|‚ñà‚ñà‚ñé       | 232/1000 [00:05&lt;00:05, 147.81it/s, 7 steps of size 2.27e-01. acc. prob=0.78] warmup:  25%|‚ñà‚ñà‚ñç       | 249/1000 [00:05&lt;00:04, 152.38it/s, 7 steps of size 7.49e-01. acc. prob=0.78]warmup:  27%|‚ñà‚ñà‚ñã       | 266/1000 [00:05&lt;00:04, 153.18it/s, 7 steps of size 7.23e-02. acc. prob=0.78]warmup:  28%|‚ñà‚ñà‚ñä       | 282/1000 [00:05&lt;00:05, 140.10it/s, 15 steps of size 4.06e-01. acc. prob=0.78]warmup:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [00:05&lt;00:04, 147.28it/s, 3 steps of size 1.46e-01. acc. prob=0.78] warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 315/1000 [00:05&lt;00:04, 145.94it/s, 15 steps of size 5.39e-01. acc. prob=0.78]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 332/1000 [00:06&lt;00:04, 152.36it/s, 7 steps of size 5.26e-01. acc. prob=0.78] warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 350/1000 [00:06&lt;00:04, 157.67it/s, 7 steps of size 3.69e-01. acc. prob=0.78]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 367/1000 [00:06&lt;00:03, 160.18it/s, 15 steps of size 3.91e-01. acc. prob=0.78]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 384/1000 [00:06&lt;00:03, 154.49it/s, 15 steps of size 4.02e-01. acc. prob=0.78]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 405/1000 [00:06&lt;00:03, 169.14it/s, 7 steps of size 7.39e-01. acc. prob=0.79] warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/1000 [00:06&lt;00:03, 182.18it/s, 7 steps of size 3.54e-01. acc. prob=0.79]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/1000 [00:06&lt;00:03, 183.26it/s, 7 steps of size 5.37e-01. acc. prob=0.79]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/1000 [00:06&lt;00:02, 194.34it/s, 31 steps of size 1.98e-01. acc. prob=0.78]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/1000 [00:06&lt;00:02, 189.67it/s, 7 steps of size 1.28e-01. acc. prob=0.78] sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/1000 [00:07&lt;00:02, 187.10it/s, 15 steps of size 4.07e-01. acc. prob=0.91]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/1000 [00:07&lt;00:02, 186.70it/s, 15 steps of size 4.07e-01. acc. prob=0.89]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/1000 [00:07&lt;00:02, 182.24it/s, 15 steps of size 4.07e-01. acc. prob=0.89]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/1000 [00:07&lt;00:02, 189.22it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/1000 [00:07&lt;00:02, 191.37it/s, 7 steps of size 4.07e-01. acc. prob=0.89]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/1000 [00:07&lt;00:02, 166.36it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/1000 [00:07&lt;00:02, 165.88it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/1000 [00:07&lt;00:02, 165.45it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/1000 [00:07&lt;00:01, 174.66it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 687/1000 [00:08&lt;00:01, 184.20it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/1000 [00:08&lt;00:01, 183.43it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/1000 [00:08&lt;00:01, 184.36it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/1000 [00:08&lt;00:01, 188.44it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/1000 [00:08&lt;00:01, 184.77it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/1000 [00:08&lt;00:01, 182.49it/s, 7 steps of size 4.07e-01. acc. prob=0.90]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/1000 [00:08&lt;00:01, 177.82it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/1000 [00:08&lt;00:00, 181.03it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/1000 [00:08&lt;00:00, 182.79it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [00:09&lt;00:00, 177.22it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/1000 [00:09&lt;00:00, 171.58it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/1000 [00:09&lt;00:00, 177.27it/s, 15 steps of size 4.07e-01. acc. prob=0.90]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/1000 [00:09&lt;00:00, 182.54it/s, 23 steps of size 4.07e-01. acc. prob=0.89]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/1000 [00:09&lt;00:00, 195.87it/s, 7 steps of size 4.07e-01. acc. prob=0.90] sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 962/1000 [00:09&lt;00:00, 184.01it/s, 7 steps of size 4.07e-01. acc. prob=0.89]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/1000 [00:09&lt;00:00, 159.78it/s, 7 steps of size 4.07e-01. acc. prob=0.90]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/1000 [00:09&lt;00:00, 158.65it/s, 7 steps of size 4.07e-01. acc. prob=0.89]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:09&lt;00:00, 101.66it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\n\n\nThis function is experimental. Use it with caution.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#mathematical-details",
    "href": "20. GMM.html#mathematical-details",
    "title": "Gaussian Mixture Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nThis section describes the generative process for a GMM.\n\n\\begin{pmatrix}\nY_{i,1} \\\\\n\\vdots \\\\\nY_{i,D}\n\\end{pmatrix}\n\\sim\n\\text{MVN}\\left(\n\\begin{pmatrix}\n\\mu_{z_i,1} \\\\\n\\vdots \\\\\n\\mu_{z_i,D}\n\\end{pmatrix},\n\\Sigma_{z_i}\n\\right)\n\n\n\\begin{pmatrix}\n\\mu_{k,1} \\\\\n\\vdots \\\\\n\\mu_{k,D}\n\\end{pmatrix}\n\\sim\n\\text{MVN}\\left(\n\\begin{pmatrix}\nA_{k,1} \\\\\n\\vdots \\\\\nA_{k,D}\n\\end{pmatrix},\nB\n\\right)\n\n\n\\Sigma_k = \\sigma_k \\Omega_k \\sigma_k\n\n\n\\sigma_k \\sim \\text{HalfCauchy}(1)\n\n\n\\Omega_k \\sim \\text{LKJ}(2)\n\n\nz_{i} \\sim \\text{Categorical}(\\pi)\n\n\n\\pi \\sim \\text{Dirichlet}(0.5, \\dots, 0.5)\n\nWhere :\n\n\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix} is the i-th observation of a D-dimensional data array.\n\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix} is the k-th parameter vector of dimension D.\n\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix} is a prior for the k-th mean vector as derived by a KMEANS clustering algorithm.\nB is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\\Sigma_k is the DxD covariance matrix of the k-th cluster (it is composed from \\sigma_k and \\Omega_k).\n\\sigma_k is a diagonal matrix of standard deviations for the k-th cluster.\n\\Omega_k is a correlation matrix for the k-th cluster.\nz_i is a latent variable that maps observation i to cluster k.\n\\pi is a vector of K cluster weights.\n\nWhere :\n\n\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix} is the i-th observation of a D-dimensional data array.\n\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix} is the k-th parameter vector of dimension D.\n\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix} is a prior for the k-th mean vector as derived by a KMEANS clustering algorithm.\nB is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\\Sigma_k is the DxD covariance matrix of the k-th cluster (it is composed from \\sigma_k and \\Omega_k).\n\\sigma_k is a diagonal matrix of standard deviations for the k-th cluster.\n\\Omega_k is a correlation matrix for the k-th cluster.\nz_i is a latent variable that maps observation i to cluster k.\n\\pi is a vector of K cluster weights.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#notes",
    "href": "20. GMM.html#notes",
    "title": "Gaussian Mixture Models",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary challenge of the GMM compared to the DPMM is the need to manually specify the number of clusters K. If the chosen K is too small, the model may merge distinct clusters. If K is too large, it may split natural clusters into meaningless sub-groups. Therefore, applying a GMM often involves an outer loop of model selection where one fits the model for a range of K values and uses a scoring metric to select the best one.",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "20. GMM.html#references",
    "href": "20. GMM.html#references",
    "title": "Gaussian Mixture Models",
    "section": "Reference(s)",
    "text": "Reference(s)\nC. M. Bishop (2006). Pattern Recognition and Machine Learning. Springer. (Chapter 9)",
    "crumbs": [
      "Models",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html",
    "href": "24. Network control for data collection biases (wip).html",
    "title": "Controlling for Network Biases",
    "section": "",
    "text": "Data collection biases are a persistent issue in studies of social networks. Two main types of biases can be considered: exposure biases üõà and censoring biases üõà.\nTo account for exposure biases, we can switch the network link probability model from a Poisson distribution to a Binomial distribution, as the binomial distribution allows us to account for the number of trials for each data estimation.\nTo address censoring biases, we need to add an additional equation to account for the probability of missing an interaction during observation when modeling the interaction between individuals i and j.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#considerations",
    "href": "24. Network control for data collection biases (wip).html#considerations",
    "title": "Controlling for Network Biases",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-1",
    "href": "24. Network control for data collection biases (wip).html#example-1",
    "title": "Controlling for Network Biases",
    "section": "Example 1",
    "text": "Example 1\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases. This example is based on Sosa et al. (n.d.).\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure_mat,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.binomial(total_count = m.net.mat_to_edgl(exposure_mat), logits = jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.fit(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-2",
    "href": "24. Network control for data collection biases (wip).html#example-2",
    "title": "Controlling for Network Biases",
    "section": "Example 2",
    "text": "Example 2\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases and censoring biases:",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#mathematical-details",
    "href": "24. Network control for data collection biases (wip).html#mathematical-details",
    "title": "Controlling for Network Biases",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\n\nY_{[i,j]} \\sim \\text{Binomial}\\Big(E_{[i,j]}, Q_{[i,j]}  \\Big)\n\n\nQ_{[i,j]} = \\phi_{[i,j]}\\eta_{[i]}\\eta_{[j]}\n\nWhere:\n\nE_{[i,j]} is the number of trials for each observation (i.e., the sampling effort).\nQ_{[i,j]} is the indicator of a true tie between i and j, defined as: \nQ_{[i,j]} \\sim \\begin{cases}\n0 & \\text{if no interaction occurs or if } i \\text{ or } j \\text{ is not detectable} \\\\\n1 & \\text{if } i \\text{ and } j \\text{ are both detectable}\n\\end{cases}\n\n\\phi_{[i,j]} is the probability of a true tie between i and j.\n\\eta_{[i]} is the probability of individual i being detectable.\n\\eta_{[j]} is the probability of individual j being detectable.\n\n\n\nDefining formula sub-equations and prior distributions\nWe can let \\eta_{[i]} depend on individual-specific covariates. To model the probability of censoring, we can model 1-\\eta_{[i]}: \n\\text{logit}(1-\\eta_{[i]}) = \\mu_\\psi + \\hat\\psi_{[i]}  \\sigma_\\psi + \\dots\n\nWhere:\n\n\\mu_\\psi is the intercept term.\n\\sigma_\\psi is a scalar for the variance of random effects.\n\\hat\\psi_{[i]}\\sim \\text{Normal}(0,1), and the ellipsis signifies any linear model of coefficients and individual-level covariates. For example, if C is an animal-specific measure, like a binary variable for cryptic coloration, then the ellipsis may be replaced with \\kappa_{[5]}C_{[i]} to give the effects of coloration on censoring probability.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#notes",
    "href": "24. Network control for data collection biases (wip).html#notes",
    "title": "Controlling for Network Biases",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nOne major limitation of this model is the necessity of having an estimation of the censoring bias for each individual.",
    "crumbs": [
      "Models",
      "Controlling for Network Biases"
    ]
  },
  {
    "objectID": "start/Import_class.html",
    "href": "start/Import_class.html",
    "title": "Import BI class",
    "section": "",
    "text": "Before anything, you need to import the BI class. This will allow you to create a BI object that will be used to : 1) import data, 2) define the model, 3) fit the model, 4) summarize the results, and 5) plot the results.\n\nPythonR\n\n\nfrom BI import bi\nm = bi()\n\n\nlibrary(BayesInference)\nm=importBI(platform='cpu')\n\n\n\n\nArguments:\n\nplatform: (str, optional). The hardware platform to use for computation. Options include:\n\n‚Äòcpu‚Äô: Use CPU(s) for computation\n‚Äògpu‚Äô: Use GPU(s) for computation\n‚Äòtpu‚Äô: Use TPU(s) for computation\n\nDefaults to ‚Äòcpu‚Äô.\ncores: (int, optional). Number of CPU cores to allocate for computation. If None, all available CPU cores will be used. Only applicable when platform is ‚Äòcpu‚Äô.\ndeallocate: (bool, optional). Whether to deallocate any existing device before setting up a new configuration. Defaults to False.\n\n\n\nExamples\nsetup_device(platform='cpu')\n\nsetup_device(platform='gpu') # Only for BayesInference[gpu]\n\nsetup_device(platform='cpu', cores=4) # Specifying CPU cores",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Import BI class"
    ]
  },
  {
    "objectID": "start/Diagnostics.html",
    "href": "start/Diagnostics.html",
    "title": "Model Diagnostic",
    "section": "",
    "text": "The BI class can compute model diagnostics for a given model.\nLets consider the following model for a linear regression:\nY_i \\sim \\text{Normal}(\\alpha + \\beta   X_i, \\sigma)\n\\alpha \\sim \\text{Normal}(0, 1)\n\\beta \\sim \\text{Normal}(0, 1)\n\\sigma \\sim \\text{Uniform}(0, 50)\nfrom BI import bi\nimport jax.numpy as jnp\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\nm.data('Howell1.csv', sep=';') \nm.df = m.df[m.df.age &gt; 18]\nm.scale(data=['weight'])\n\n\n # define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal( 178, 20, name = 'a')\n    b = m.dist.log_normal(  0, 1, name = 'b')   \n    s = m.dist.uniform( 0, 50, name = 's')\n    m.dist.normal(a + b * weight , s, obs=height, shape=(weight.shape[0],))\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500,num_chains=4) \nm.summary()\n\njax.local_device_count 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na\n154.64\n0.29\n154.16\n155.07\n0.01\n0.01\n1711.30\n1283.42\n1.0\n\n\nb\n5.82\n0.29\n5.33\n6.23\n0.01\n0.01\n1999.81\n1298.15\n1.0\n\n\ns\n5.14\n0.20\n4.82\n5.45\n0.00\n0.00\n2088.50\n1526.51\n1.0",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#predictions-from-model-based-on-specific-data-value",
    "href": "start/Diagnostics.html#predictions-from-model-based-on-specific-data-value",
    "title": "Model Diagnostic",
    "section": "Predictions from model based on specific data value",
    "text": "Predictions from model based on specific data value\n\nm.sample() # Predictions from model base on data in data_on_model\nm.sample(data=dict(weight=jnp.array([0.4])), remove_obs=False)# Predictions from a given value\n\n/home/sosa/work/BI/BI/Main/main.py:417: UserWarning:\n\nSample's batch dimension size 2000 is different from the provided 1 num_samples argument. Defaulting to 2000.\n\n\n\n{'x': Array([[159.34970727],\n        [163.37257718],\n        [158.07636766],\n        ...,\n        [162.42332523],\n        [147.89303169],\n        [161.40022364]], dtype=float64)}",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#forest-plot-of-estimated-values",
    "href": "start/Diagnostics.html#forest-plot-of-estimated-values",
    "title": "Model Diagnostic",
    "section": "Forest plot of estimated values",
    "text": "Forest plot of estimated values\n\nm.diag.forest()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#density-plots-of-the-posterior-distribution",
    "href": "start/Diagnostics.html#density-plots-of-the-posterior-distribution",
    "title": "Model Diagnostic",
    "section": "Density plots of the posterior distribution",
    "text": "Density plots of the posterior distribution\n\nm.diag.density()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#posterior-distribution-plots",
    "href": "start/Diagnostics.html#posterior-distribution-plots",
    "title": "Model Diagnostic",
    "section": "Posterior distribution plots",
    "text": "Posterior distribution plots\n\nm.diag.posterior()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#trace-plots-for-mcmc-chains",
    "href": "start/Diagnostics.html#trace-plots-for-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Trace plots for MCMC chains",
    "text": "Trace plots for MCMC chains\n\nm.diag.plot_trace()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#pairwise-plots-of-the-posterior-distribution",
    "href": "start/Diagnostics.html#pairwise-plots-of-the-posterior-distribution",
    "title": "Model Diagnostic",
    "section": "Pairwise plots of the posterior distribution",
    "text": "Pairwise plots of the posterior distribution\n\nm.diag.pair()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#plot-autocorrelation-of-mcmc-chains",
    "href": "start/Diagnostics.html#plot-autocorrelation-of-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Plot autocorrelation of MCMC chains",
    "text": "Plot autocorrelation of MCMC chains\n\nm.diag.autocor()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#create-rank-plots-for-mcmc-chains",
    "href": "start/Diagnostics.html#create-rank-plots-for-mcmc-chains",
    "title": "Model Diagnostic",
    "section": "Create rank plots for MCMC chains",
    "text": "Create rank plots for MCMC chains\n\nm.diag.rank()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#evolution-of-effective-sample-size-across-iterations",
    "href": "start/Diagnostics.html#evolution-of-effective-sample-size-across-iterations",
    "title": "Model Diagnostic",
    "section": "Evolution of effective sample size across iterations",
    "text": "Evolution of effective sample size across iterations\n\nm.diag.plot_ess()",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#pareto-smoothed",
    "href": "start/Diagnostics.html#pareto-smoothed",
    "title": "Model Diagnostic",
    "section": "Pareto-smoothed",
    "text": "Pareto-smoothed\n\nm.diag.loo()\n\nComputed from 2000 posterior samples and 346 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1058.55    14.72\np_loo        3.26        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      346  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Diagnostics.html#widely-applicable-information-criterion",
    "href": "start/Diagnostics.html#widely-applicable-information-criterion",
    "title": "Model Diagnostic",
    "section": "Widely applicable information criterion",
    "text": "Widely applicable information criterion\n\nm.diag.WAIC()\n\nComputed from 2000 posterior samples and 346 observations log-likelihood matrix.\n\n          Estimate       SE\nelpd_waic -1058.54    14.72\np_waic        3.26        -",
    "crumbs": [
      "Get started",
      "Pipeline",
      "Model Diagnostic"
    ]
  },
  {
    "objectID": "start/Installation.html",
    "href": "start/Installation.html",
    "title": "Installation",
    "section": "",
    "text": "You can run BI on python or R. For R users you need to have installed python and the R reticulate.\n\nPythonR\n\n\n#| code-fold: false\npip install BayesInference # For CPU support\npip install BayesInference[gpu] # For GPU support \n\n\n#| code-fold: false\npackage.install(BayesianInference)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAs reticulate uses its own python environment, you need to install the python version of BI through reticulate. You can do this by running the following command in R:\nreticulate::py_install(\"BayesianInference\", pip = TRUE)\nGPU support is only available for linux and WSL2 systems.",
    "crumbs": [
      "Get started",
      "Installation",
      "Installation"
    ]
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "Regression with a Categorical Independent Variable",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#general-principles",
    "href": "4. Categorical variable.html#general-principles",
    "title": "Regression with a Categorical Independent Variable",
    "section": "",
    "text": "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "Regression with a Categorical Independent Variable",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nWe have the same considerations as for Regression for a Continuous Variable.\nAs we generate regression coefficients for each k category, we need to specify a prior with a shape equal to the number of categories k in the code (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare confidence intervals or p-values directly.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "Regression with a Categorical Independent Variable",
    "section": "Example",
    "text": "Example\nBelow is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (kcal_per_g), representing the caloric value of milk per gram, a categorical independent variable (index_clade), representing species clade membership, and a continuous independent variable (mass), representing the mass of individuals in the clade. The goal is to estimate the differences in milk calories between clades. This example is based on McElreath (2018).\n\nPythonR\n\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.milk(only_path = True)\nm.data(data_path, sep=';') \nm.index([\"clade\"]) # Convert clade names into index\nm.scale(['kcal_per_g']) # Scale\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade, mass):\n    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    b = m.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    s = m.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.dist.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\njax.local_device_count 32\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:00&lt;07:46,  2.14it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   8%|‚ñä         | 82/1000 [00:00&lt;00:04, 190.46it/s, 511 steps of size 5.98e-03. acc. prob=0.75]warmup:  28%|‚ñà‚ñà‚ñä       | 279/1000 [00:00&lt;00:01, 634.51it/s, 7 steps of size 1.74e-01. acc. prob=0.78] sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [00:00&lt;00:00, 1135.10it/s, 15 steps of size 3.48e-01. acc. prob=0.90]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/1000 [00:00&lt;00:00, 1456.79it/s, 15 steps of size 3.48e-01. acc. prob=0.93]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00&lt;00:00, 1032.95it/s, 15 steps of size 3.48e-01. acc. prob=0.93]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\na[0]\n-0.32\n0.35\n-0.79\n0.34\n0.02\n0.02\n430.93\n365.09\nNaN\n\n\na[1]\n0.59\n0.30\n0.10\n1.06\n0.01\n0.01\n412.79\n283.95\nNaN\n\n\na[2]\n0.31\n0.37\n-0.28\n0.87\n0.02\n0.02\n404.55\n368.44\nNaN\n\n\na[3]\n-0.17\n0.49\n-0.92\n0.64\n0.03\n0.02\n376.94\n232.82\nNaN\n\n\nb[0]\n-0.00\n0.01\n-0.02\n0.01\n0.00\n0.00\n456.57\n350.35\nNaN\n\n\nb[1]\n-0.17\n0.12\n-0.36\n0.04\n0.01\n0.01\n356.03\n342.61\nNaN\n\n\nb[2]\n0.08\n0.07\n-0.02\n0.19\n0.00\n0.00\n371.61\n388.81\nNaN\n\n\nb[3]\n-0.27\n0.27\n-0.72\n0.13\n0.01\n0.01\n352.82\n217.58\nNaN\n\n\ns\n0.80\n0.12\n0.59\n0.97\n0.01\n0.00\n386.61\n438.95\nNaN\n\n\n\n\n\n\n\n\n\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(m$load$milk(only_path = T), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(kcal_per_g, index_clade){\n  # Parameter prior distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma = bi.dist.exponential(1, name = 's')\n  # Likelihood\n  bi.dist.normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor R users, when working with indices you have to ensure 1) that indices are intergers (i.e.¬†as.integer(index_clade)) and, 2) that indices start at 0 (i.e.¬†as.integer(index_clade)-1).",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "Regression with a Categorical Independent Variable",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\nY_i = \\alpha + \\beta_k X_i + \\sigma\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term.\n\\beta_k are the regression coefficients for each k category.\nX_i is the encoded categorical input variable for observation i.\n\\sigma is the error term.\n\nWe can interpret \\beta_i as the effect of each category on Y relative to the baseline (usually one of the categories or the intercept).\n\n\nBayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\nY \\sim \\text{Normal}(\\alpha +  \\beta_K X, \\sigma)\n\n\n\\alpha \\sim \\text{Normal}(0,1)\n\n\n\\beta_K \\sim \\text{Normal}(0,1)\n\n\n\\sigma \\sim \\text{Exponential}(1)\n\nWhere:\n\nY_i is the dependent variable for observation i.\n\\alpha is the intercept term, which in this case has a unit-normal prior.\n\\beta_K are slope coefficients for the K distinct independent variables categories, which also have unit-normal priors.\nX_i is the encoded categorical input variable for observation i.\n\\sigma is a standard deviation parameter, which here has a Exponential prior that constrains it to be positive.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "Regression with a Categorical Independent Variable",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms similarly to Chapter 3: Interaction between Continuous Variables.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "Regression with a Categorical Independent Variable",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Models",
      "Regression with a Categorical Independent Variable"
    ]
  },
  {
    "objectID": "get started.html",
    "href": "get started.html",
    "title": "Installation",
    "section": "",
    "text": "You can run BI on python or R. For R users you need to have installed python and the R reticulate package.\n\nPythonR\n\n\npip install BayesInference\n\n\npackage.install(BayesInference)"
  },
  {
    "objectID": "get started.html#import-tabular-data-from-a-csv-file",
    "href": "get started.html#import-tabular-data-from-a-csv-file",
    "title": "Installation",
    "section": "Import tabular data from a csv file",
    "text": "Import tabular data from a csv file\n\nPythonR\n\n\nm.data(data_path, sep=';') \n\n\nm$data(data_path,  sep=';')"
  },
  {
    "objectID": "get started.html#import-non-tabular-data",
    "href": "get started.html#import-non-tabular-data",
    "title": "Installation",
    "section": "Import non tabular data",
    "text": "Import non tabular data\nFirst you need to create our own dictionary with the data.\n\nPythonR\n\n\nm.data_on_model = dict(\n    ID1 = Value1,\n    ID2 = Value2, \n)\n\n\nkeys &lt;- c(\"ID1\",\"ID2\")\nvalues &lt;- list(Value1,Value2)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data"
  },
  {
    "objectID": "17. Missing data (wip).html#general-principles",
    "href": "17. Missing data (wip).html#general-principles",
    "title": "Handling Missing Data (WIP)",
    "section": "General Principles",
    "text": "General Principles",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#considerations",
    "href": "17. Missing data (wip).html#considerations",
    "title": "Handling Missing Data (WIP)",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#example",
    "href": "17. Missing data (wip).html#example",
    "title": "Handling Missing Data (WIP)",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating Bayesian Missing data model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#mathematical-details",
    "href": "17. Missing data (wip).html#mathematical-details",
    "title": "Handling Missing Data (WIP)",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nFrequentist formulation\n\n\nBayesian formulation",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#notes",
    "href": "17. Missing data (wip).html#notes",
    "title": "Handling Missing Data (WIP)",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#references",
    "href": "17. Missing data (wip).html#references",
    "title": "Handling Missing Data (WIP)",
    "section": "Reference(s)",
    "text": "Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "Models",
      "Handling Missing Data (WIP)"
    ]
  },
  {
    "objectID": "19. PCA.html",
    "href": "19. PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) (Tipping and Bishop 1999) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\nPCA is employed for dimensionality reduction, particularly in scenarios involving high-dimensional datasets, as it effectively reduces complexity while explicitly accounting for uncertainty in the underlying latent structure. This approach also plays a crucial role in data visualization, enabling the projection of intricate, high-dimensional data into more interpretable 2D or 3D representations. Additionally, PCA excels in feature extraction, where the latent variables it identifies can be repurposed as informative features for subsequent tasks, such as classification or clustering. By modeling latent variables, it further enhances the interpretability and utility of the data for a variety of analytical applications.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#general-principles",
    "href": "19. PCA.html#general-principles",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) (Tipping and Bishop 1999) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\nPCA is employed for dimensionality reduction, particularly in scenarios involving high-dimensional datasets, as it effectively reduces complexity while explicitly accounting for uncertainty in the underlying latent structure. This approach also plays a crucial role in data visualization, enabling the projection of intricate, high-dimensional data into more interpretable 2D or 3D representations. Additionally, PCA excels in feature extraction, where the latent variables it identifies can be repurposed as informative features for subsequent tasks, such as classification or clustering. By modeling latent variables, it further enhances the interpretability and utility of the data for a variety of analytical applications.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#considerations",
    "href": "19. PCA.html#considerations",
    "title": "Principal Component Analysis",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nNote\n\n\n\n\nIn Bayesian PCA, we assume prior distributions for the latent variables Z and the principal component loadings W. We place Gaussian priors on both Z and W and learn their posterior distributions using the observed data X.\nRobustness to Outliers: Standard PCA are sensitive to outliers due to the assumption of Gaussian noise. Robust variants of Robust Bayesian PCA address this by employing heavy-tailed distributions for the noise model, such as the Student‚Äôs t-distribution, which reduces the influence of outliers (Archambeau, Delannay, and Verleysen 2006; Bouwmans and Zahzah 2014).\nAutomatic Dimensionality Selection: Through techniques like Automatic Relevance Determination (ARD), Bayesian PCA can automatically determine the effective dimensionality of the latent space. Priors are placed on the relevance of each principal component, and components that are not supported by the data are effectively ‚Äúswitched off‚Äô (C. Bishop 1998; C. M. Bishop and Nasrabadi 2006).\nSparsity for High-Dimensional Data: In high-dimensional settings, it is often desirable for the principal components to be influenced by only a subset of the original features, leading to more interpretable results. Sparse Bayesian PCA achieves this by placing sparsity-inducing priors (e.g., Laplacian or spike-and-slab priors) on the loading (Sigg and Buhmann 2008; Zou, Hastie, and Tibshirani 2006).",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#example",
    "href": "19. PCA.html#example",
    "title": "Principal Component Analysis",
    "section": "Example",
    "text": "Example\nHere is an example code snippet demonstrating Bayesian PCA using BI:\n\nBuild in functionsStandard\n\n\n\nfrom BI import bi,jnp\nm=bi()\nm.data('iris.csv', sep=',') # Data is already scaled\nm.data_on_model = dict(\n    X=jnp.array(m.df.iloc[:,0:-2].values)\n)\nm.fit(m.models.pca(type=\"classic\"), progress_bar=False) # or robust, sparse, classic, sparse_robust_ard\nm.models.pca.plot(\n    X=m.df.iloc[:,0:-2].values,\n    y=m.df.iloc[:,-2].values, \n    feature_names=m.df.columns[0:-2], \n    target_names=m.df.iloc[:,-1].unique(),\n    color_var=m.df.iloc[:,0].values,\n    shape_var=m.df.iloc[:,-2].values\n)\n\njax.local_device_count 16\n\n\n\n\n\n\n\n\n\n\n\ndef model(x_train, data_dim, latent_dim, num_datapoints): \n    # Gaussian prior for the principal component 'W'.\n    w = m.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')\n\n    # Gaussian prior on the latent variables 'Z'\n    z = m.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')\n\n    # Exponential prior on the noise variance 'epsilon'\n    epsilon = m.dist.exponential(1, name='epsilon')\n\n    # Likelihood\n    m.dist.normal(w @ z , epsilon, obs = x_train)  \n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model)",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#mathematical-details",
    "href": "19. PCA.html#mathematical-details",
    "title": "Principal Component Analysis",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nWe assume the observed data matrix X is centered and arranged with features as rows and samples as columns, X \\in \\mathbb{R}^{N \\times V} where N is the number of observations and V the number of variables. The generative model projects the data into a lower-dimensional space with K latent variables, K \\leq V, using the following equation: \nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n\nwhere :\n\nX is the observed data matrix.\nZ \\in \\mathbb{R}^{N \\times K} is the latent variable matrix (latent features) with K \\ll D. Z is defined by a Normal distribution with mean 0 and variance 1.\nW \\in \\mathbb{R}^{K \\times V} is the matrix of principal components (projection matrix). W is defined by a Normal distribution with mean 0 and variance 1.\n\\sigma is the standard deviation of the normal distribution.\n\nThe likelihood and priors are defined element-wise for v=1...V, n=1...N, and k=1...K. for the following models:\n\nStandard PCA\n\nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n \nZ \\sim \\text{Normal}(0, 1)\n \nW \\sim \\text{Normal}(0, 1)\n \n\\sigma \\sim \\text{Exponential}(1)",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#note",
    "href": "19. PCA.html#note",
    "title": "Principal Component Analysis",
    "section": "Note",
    "text": "Note\n\nTo account for sign ambiguity üõà in PCA, we can set the number of latent dimensions K to be equal to the number of variables V. Then, we can calculate the dot product between the estimated parameters and the data. If it is negative, we multiply the estimated parameters by -1 to align them with the data. Below, a code snippet highlights how to do this:\n\n\ntrue_params = jnp.array(real_data)      \nestimated_params = jnp.array(m.posteriors) \n\n# Compute dot product\ndot_product = jnp.dot(true_params, estimated_params)\n\n# Align signs if necessary\nif dot_product &lt; 0:\n    estimated_params = -estimated_params\n\n# Plot the aligned parameters\nplt.scatter(true_params, estimated_params, alpha=0.7)\nplt.plot([min(true_params), max(true_params)], [min(true_params), max(true_params)], 'r--')\nplt.xlabel('True Parameters')\nplt.ylabel('Estimated Parameters')\nplt.title('True vs. Estimated Parameters After Sign Alignment')\nplt.show()",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "19. PCA.html#references",
    "href": "19. PCA.html#references",
    "title": "Principal Component Analysis",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nArchambeau, C√©dric, Nicolas Delannay, and Michel Verleysen. 2006. ‚ÄúRobust Probabilistic Projections.‚Äù In Proceedings of the 23rd International Conference on Machine Learning, 33‚Äì40.\n\n\nBishop, Christopher. 1998. ‚ÄúBayesian Pca.‚Äù Advances in Neural Information Processing Systems 11.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer. https://doi.org/https://link.springer.com/book/9780387310732.\n\n\nBouwmans, Thierry, and El Hadi Zahzah. 2014. ‚ÄúRobust PCA via Principal Component Pursuit: A Review for a Comparative Evaluation in Video Surveillance.‚Äù Computer Vision and Image Understanding 122: 22‚Äì34. https://doi.org/https://doi.org/10.1016/j.cviu.2013.11.009.\n\n\nSigg, Christian D, and Joachim M Buhmann. 2008. ‚ÄúExpectation-Maximization for Sparse and Non-Negative PCA.‚Äù In Proceedings of the 25th International Conference on Machine Learning, 960‚Äì67.\n\n\nTipping, Michael E, and Christopher M Bishop. 1999. ‚ÄúProbabilistic Principal Component Analysis.‚Äù Journal of the Royal Statistical Society Series B: Statistical Methodology 61 (3): 611‚Äì22.\n\n\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. ‚ÄúSparse Principal Component Analysis.‚Äù Journal of Computational and Graphical Statistics 15 (2): 265‚Äì86.",
    "crumbs": [
      "Models",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "23. Network with block model.html",
    "href": "23. Network with block model.html",
    "title": "Stochastic Block Models",
    "section": "",
    "text": "Within networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#considerations",
    "href": "23. Network with block model.html#considerations",
    "title": "Stochastic Block Models",
    "section": "Considerations",
    "text": "Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\nIn addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (Any in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#example",
    "href": "23. Network with block model.html#example",
    "title": "Stochastic Block Models",
    "section": "Example",
    "text": "Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the Network model example, with the addition of covariates Any, Merica, and Quantum, representing the block membership of each node. This example is based on Ross, McElreath, and Redhead (2024).\n\n# Setup device------------------------------------------------\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = True)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = True)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\ncategory = m.dist.categorical(jnp.array([.25,.25,.25,.25]), sample = True, shape  = (N,))\nN_grp, N_by_grp = jnp.unique(category, return_counts=True)\nN_grp = N_grp.shape[0]\n\ndef sim_network(kinship, individual_predictor,category):\n  # Intercept\n  B_intercept = m.net.block_model(jnp.full((N,),0), 1, N, sample = True)\n  B_category = m.net.block_model(category, N_grp, N_by_grp, sample = True)\n\n  # SR\n  sr = m.net.sender_receiver(\n    individual_predictor, \n    individual_predictor, \n    s_mu = 0.4, r_mu = -0.4, sample = True)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = True)\n\n\n\n  return m.dist.bernoulli(\n    logits = B_intercept + B_category + sr + DR, \n    sample = True\n    )\n\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor, category)\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = dict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor, \n    category = category\n)\n\n\ndef model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors,category):\n    N_id = focal_individual_predictors.shape[0]\n\n    # Block ---------------------------------------\n    B_intercept = m.net.block_model(jnp.full((N_id,),0), 1, N_id, sample = True)\n    B_category = m.net.block_model(category, N_grp, N_by_grp, sample = True)\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors, \n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n    m.dist.bernoulli(logits = B_intercept + B_category + sr + dr, obs=network)\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1)\n\njax.local_device_count 16\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:07&lt;2:00:32,  7.24s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   0%|          | 5/1000 [00:07&lt;18:27,  1.11s/it, 1023 steps of size 6.62e-05. acc. prob=0.00]warmup:   1%|          | 7/1000 [00:07&lt;12:15,  1.35it/s, 1023 steps of size 2.69e-07. acc. prob=0.00]warmup:   1%|          | 9/1000 [00:07&lt;08:42,  1.90it/s, 1023 steps of size 1.34e-09. acc. prob=0.00]warmup:   1%|          | 10/1000 [00:08&lt;07:23,  2.23it/s, 1023 steps of size 1.03e-10. acc. prob=0.00]warmup:   1%|          | 11/1000 [00:08&lt;06:13,  2.65it/s, 1023 steps of size 8.48e-12. acc. prob=0.00]warmup:   1%|          | 12/1000 [00:08&lt;05:14,  3.14it/s, 1023 steps of size 7.42e-13. acc. prob=0.00]warmup:   1%|‚ñè         | 13/1000 [00:08&lt;04:26,  3.71it/s, 1023 steps of size 6.90e-14. acc. prob=0.00]warmup:   1%|‚ñè         | 14/1000 [00:08&lt;03:49,  4.30it/s, 1023 steps of size 6.82e-15. acc. prob=0.00]warmup:   2%|‚ñè         | 15/1000 [00:08&lt;03:21,  4.89it/s, 1023 steps of size 7.12e-16. acc. prob=0.00]warmup:   2%|‚ñè         | 16/1000 [00:08&lt;02:59,  5.48it/s, 1023 steps of size 7.86e-17. acc. prob=0.00]warmup:   2%|‚ñè         | 17/1000 [00:08&lt;02:44,  5.97it/s, 1023 steps of size 9.14e-18. acc. prob=0.00]warmup:   2%|‚ñè         | 18/1000 [00:09&lt;02:35,  6.32it/s, 1023 steps of size 1.12e-18. acc. prob=0.00]warmup:   2%|‚ñè         | 19/1000 [00:09&lt;02:27,  6.66it/s, 1023 steps of size 1.43e-19. acc. prob=0.00]warmup:   2%|‚ñè         | 20/1000 [00:09&lt;02:21,  6.91it/s, 1023 steps of size 1.92e-20. acc. prob=0.00]warmup:   2%|‚ñè         | 21/1000 [00:09&lt;02:18,  7.08it/s, 1023 steps of size 2.68e-21. acc. prob=0.00]warmup:   2%|‚ñè         | 22/1000 [00:09&lt;02:14,  7.29it/s, 1023 steps of size 3.92e-22. acc. prob=0.00]warmup:   2%|‚ñè         | 23/1000 [00:09&lt;02:12,  7.39it/s, 1023 steps of size 5.94e-23. acc. prob=0.00]warmup:   2%|‚ñè         | 24/1000 [00:09&lt;02:10,  7.47it/s, 1023 steps of size 9.35e-24. acc. prob=0.00]warmup:   2%|‚ñé         | 25/1000 [00:10&lt;02:10,  7.49it/s, 1023 steps of size 1.52e-24. acc. prob=0.00]warmup:   3%|‚ñé         | 26/1000 [00:10&lt;02:08,  7.58it/s, 1023 steps of size 2.57e-25. acc. prob=0.00]warmup:   3%|‚ñé         | 27/1000 [00:10&lt;02:08,  7.57it/s, 1023 steps of size 4.49e-26. acc. prob=0.00]warmup:   3%|‚ñé         | 28/1000 [00:10&lt;02:08,  7.58it/s, 1023 steps of size 8.07e-27. acc. prob=0.00]warmup:   3%|‚ñé         | 29/1000 [00:10&lt;02:09,  7.53it/s, 1023 steps of size 1.50e-27. acc. prob=0.00]warmup:   3%|‚ñé         | 30/1000 [00:10&lt;02:07,  7.60it/s, 1023 steps of size 2.85e-28. acc. prob=0.00]warmup:   3%|‚ñé         | 31/1000 [00:10&lt;02:07,  7.58it/s, 1023 steps of size 5.59e-29. acc. prob=0.00]warmup:   3%|‚ñé         | 32/1000 [00:10&lt;02:06,  7.63it/s, 1023 steps of size 1.13e-29. acc. prob=0.00]warmup:   3%|‚ñé         | 33/1000 [00:11&lt;02:05,  7.69it/s, 1023 steps of size 2.32e-30. acc. prob=0.00]warmup:   3%|‚ñé         | 34/1000 [00:11&lt;02:06,  7.66it/s, 1023 steps of size 4.91e-31. acc. prob=0.00]warmup:   4%|‚ñé         | 35/1000 [00:11&lt;02:06,  7.62it/s, 1023 steps of size 1.06e-31. acc. prob=0.00]warmup:   4%|‚ñé         | 36/1000 [00:11&lt;02:05,  7.68it/s, 1023 steps of size 2.35e-32. acc. prob=0.00]warmup:   4%|‚ñé         | 37/1000 [00:11&lt;02:05,  7.70it/s, 1023 steps of size 5.32e-33. acc. prob=0.00]warmup:   4%|‚ñç         | 38/1000 [00:11&lt;02:05,  7.66it/s, 1023 steps of size 1.23e-33. acc. prob=0.00]warmup:   4%|‚ñç         | 39/1000 [00:11&lt;02:05,  7.66it/s, 1023 steps of size 2.89e-34. acc. prob=0.00]warmup:   4%|‚ñç         | 40/1000 [00:11&lt;02:06,  7.61it/s, 1023 steps of size 6.95e-35. acc. prob=0.00]warmup:   4%|‚ñç         | 41/1000 [00:12&lt;02:05,  7.65it/s, 1023 steps of size 1.70e-35. acc. prob=0.00]warmup:   4%|‚ñç         | 42/1000 [00:12&lt;02:04,  7.67it/s, 1023 steps of size 4.24e-36. acc. prob=0.00]warmup:   4%|‚ñç         | 43/1000 [00:12&lt;02:08,  7.42it/s, 1023 steps of size 1.08e-36. acc. prob=0.00]warmup:   4%|‚ñç         | 44/1000 [00:12&lt;02:09,  7.38it/s, 1023 steps of size 2.77e-37. acc. prob=0.00]warmup:   4%|‚ñç         | 45/1000 [00:12&lt;02:08,  7.43it/s, 1023 steps of size 7.27e-38. acc. prob=0.00]warmup:   5%|‚ñç         | 46/1000 [00:12&lt;02:06,  7.53it/s, 1023 steps of size 1.94e-38. acc. prob=0.00]warmup:   5%|‚ñç         | 47/1000 [00:12&lt;02:05,  7.58it/s, 1023 steps of size 5.24e-39. acc. prob=0.00]warmup:   5%|‚ñç         | 48/1000 [00:13&lt;02:04,  7.64it/s, 1023 steps of size 1.44e-39. acc. prob=0.00]warmup:   5%|‚ñç         | 49/1000 [00:13&lt;02:04,  7.63it/s, 1023 steps of size 4.01e-40. acc. prob=0.00]warmup:   5%|‚ñå         | 50/1000 [00:13&lt;02:04,  7.64it/s, 1023 steps of size 1.13e-40. acc. prob=0.00]warmup:   5%|‚ñå         | 51/1000 [00:13&lt;02:04,  7.60it/s, 1023 steps of size 3.25e-41. acc. prob=0.00]warmup:   5%|‚ñå         | 52/1000 [00:13&lt;02:05,  7.53it/s, 1023 steps of size 9.42e-42. acc. prob=0.00]warmup:   5%|‚ñå         | 53/1000 [00:13&lt;02:05,  7.56it/s, 1023 steps of size 2.77e-42. acc. prob=0.00]warmup:   5%|‚ñå         | 54/1000 [00:13&lt;02:04,  7.62it/s, 1023 steps of size 8.24e-43. acc. prob=0.00]warmup:   6%|‚ñå         | 55/1000 [00:13&lt;02:03,  7.68it/s, 1023 steps of size 2.48e-43. acc. prob=0.00]warmup:   6%|‚ñå         | 56/1000 [00:14&lt;02:03,  7.65it/s, 1023 steps of size 7.57e-44. acc. prob=0.00]warmup:   6%|‚ñå         | 57/1000 [00:14&lt;02:03,  7.63it/s, 1023 steps of size 2.34e-44. acc. prob=0.00]warmup:   6%|‚ñå         | 58/1000 [00:14&lt;02:02,  7.71it/s, 1023 steps of size 7.29e-45. acc. prob=0.00]warmup:   6%|‚ñå         | 59/1000 [00:14&lt;02:01,  7.72it/s, 1023 steps of size 2.30e-45. acc. prob=0.00]warmup:   6%|‚ñå         | 60/1000 [00:14&lt;02:02,  7.68it/s, 1023 steps of size 7.32e-46. acc. prob=0.00]warmup:   6%|‚ñå         | 61/1000 [00:14&lt;02:03,  7.62it/s, 1023 steps of size 2.36e-46. acc. prob=0.00]warmup:   6%|‚ñå         | 62/1000 [00:14&lt;02:02,  7.63it/s, 1023 steps of size 7.67e-47. acc. prob=0.00]warmup:   6%|‚ñã         | 63/1000 [00:15&lt;02:02,  7.66it/s, 1023 steps of size 2.52e-47. acc. prob=0.00]warmup:   6%|‚ñã         | 64/1000 [00:15&lt;02:01,  7.70it/s, 1023 steps of size 8.36e-48. acc. prob=0.00]warmup:   6%|‚ñã         | 65/1000 [00:15&lt;02:02,  7.65it/s, 1023 steps of size 2.80e-48. acc. prob=0.00]warmup:   7%|‚ñã         | 66/1000 [00:15&lt;02:01,  7.72it/s, 1023 steps of size 9.47e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 67/1000 [00:15&lt;02:01,  7.70it/s, 1023 steps of size 3.23e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 68/1000 [00:15&lt;02:00,  7.73it/s, 1023 steps of size 1.11e-49. acc. prob=0.00]warmup:   7%|‚ñã         | 69/1000 [00:15&lt;02:01,  7.67it/s, 1023 steps of size 3.86e-50. acc. prob=0.00]warmup:   7%|‚ñã         | 70/1000 [00:15&lt;02:00,  7.69it/s, 1023 steps of size 1.35e-50. acc. prob=0.00]warmup:   7%|‚ñã         | 71/1000 [00:16&lt;02:00,  7.70it/s, 1023 steps of size 4.76e-51. acc. prob=0.00]warmup:   7%|‚ñã         | 72/1000 [00:16&lt;02:00,  7.69it/s, 1023 steps of size 1.69e-51. acc. prob=0.00]warmup:   7%|‚ñã         | 73/1000 [00:16&lt;02:00,  7.72it/s, 1023 steps of size 6.07e-52. acc. prob=0.00]warmup:   7%|‚ñã         | 74/1000 [00:16&lt;01:59,  7.73it/s, 1023 steps of size 2.19e-52. acc. prob=0.00]warmup:   8%|‚ñä         | 75/1000 [00:16&lt;01:59,  7.77it/s, 1023 steps of size 7.98e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 76/1000 [00:16&lt;01:58,  7.78it/s, 1023 steps of size 2.93e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 77/1000 [00:16&lt;01:59,  7.74it/s, 1023 steps of size 1.08e-53. acc. prob=0.00]warmup:   8%|‚ñä         | 78/1000 [00:16&lt;01:59,  7.73it/s, 1023 steps of size 4.02e-54. acc. prob=0.00]warmup:   8%|‚ñä         | 79/1000 [00:17&lt;01:59,  7.74it/s, 1023 steps of size 1.51e-54. acc. prob=0.00]warmup:   8%|‚ñä         | 80/1000 [00:17&lt;01:58,  7.75it/s, 1023 steps of size 5.68e-55. acc. prob=0.00]warmup:   8%|‚ñä         | 81/1000 [00:17&lt;01:58,  7.73it/s, 1023 steps of size 2.16e-55. acc. prob=0.00]warmup:   8%|‚ñä         | 82/1000 [00:17&lt;01:59,  7.70it/s, 1023 steps of size 8.25e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 83/1000 [00:17&lt;01:58,  7.74it/s, 1023 steps of size 3.17e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 84/1000 [00:17&lt;01:58,  7.75it/s, 1023 steps of size 1.23e-56. acc. prob=0.00]warmup:   8%|‚ñä         | 85/1000 [00:17&lt;01:58,  7.73it/s, 1023 steps of size 4.78e-57. acc. prob=0.00]warmup:   9%|‚ñä         | 86/1000 [00:17&lt;01:58,  7.69it/s, 1023 steps of size 1.87e-57. acc. prob=0.00]warmup:   9%|‚ñä         | 87/1000 [00:18&lt;01:58,  7.71it/s, 1023 steps of size 7.39e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 88/1000 [00:18&lt;01:58,  7.71it/s, 1023 steps of size 2.93e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 89/1000 [00:18&lt;01:58,  7.71it/s, 1023 steps of size 1.17e-58. acc. prob=0.00]warmup:   9%|‚ñâ         | 90/1000 [00:18&lt;02:03,  7.35it/s, 1023 steps of size 4.69e-59. acc. prob=0.00]warmup:   9%|‚ñâ         | 91/1000 [00:18&lt;02:01,  7.48it/s, 1023 steps of size 1.89e-59. acc. prob=0.00]warmup:   9%|‚ñâ         | 92/1000 [00:18&lt;02:00,  7.53it/s, 1023 steps of size 7.67e-60. acc. prob=0.00]warmup:   9%|‚ñâ         | 93/1000 [00:18&lt;01:59,  7.56it/s, 1023 steps of size 3.13e-60. acc. prob=0.00]warmup:   9%|‚ñâ         | 94/1000 [00:19&lt;01:59,  7.56it/s, 1023 steps of size 1.28e-60. acc. prob=0.00]warmup:  10%|‚ñâ         | 95/1000 [00:19&lt;01:58,  7.62it/s, 1023 steps of size 5.28e-61. acc. prob=0.00]warmup:  10%|‚ñâ         | 96/1000 [00:19&lt;01:58,  7.61it/s, 1023 steps of size 2.19e-61. acc. prob=0.00]warmup:  10%|‚ñâ         | 97/1000 [00:19&lt;01:57,  7.66it/s, 1023 steps of size 9.10e-62. acc. prob=0.00]warmup:  10%|‚ñâ         | 98/1000 [00:19&lt;01:59,  7.57it/s, 1023 steps of size 3.81e-62. acc. prob=0.00]warmup:  10%|‚ñâ         | 99/1000 [00:19&lt;01:57,  7.65it/s, 1023 steps of size 1.60e-62. acc. prob=0.00]warmup:  10%|‚ñà         | 100/1000 [00:19&lt;01:56,  7.71it/s, 1023 steps of size 6.76e-63. acc. prob=0.00]warmup:  10%|‚ñà         | 101/1000 [00:19&lt;01:56,  7.72it/s, 1023 steps of size 1.58e-62. acc. prob=0.00]warmup:  10%|‚ñà         | 102/1000 [00:20&lt;01:56,  7.70it/s, 1023 steps of size 1.56e-63. acc. prob=0.00]warmup:  10%|‚ñà         | 103/1000 [00:20&lt;01:56,  7.71it/s, 1023 steps of size 1.13e-64. acc. prob=0.00]warmup:  10%|‚ñà         | 104/1000 [00:20&lt;01:57,  7.66it/s, 1023 steps of size 7.23e-66. acc. prob=0.00]warmup:  10%|‚ñà         | 105/1000 [00:20&lt;01:56,  7.66it/s, 1023 steps of size 4.47e-67. acc. prob=0.00]warmup:  11%|‚ñà         | 106/1000 [00:20&lt;01:56,  7.69it/s, 1023 steps of size 2.80e-68. acc. prob=0.00]warmup:  11%|‚ñà         | 107/1000 [00:20&lt;01:56,  7.64it/s, 1023 steps of size 1.82e-69. acc. prob=0.00]warmup:  11%|‚ñà         | 108/1000 [00:20&lt;01:56,  7.66it/s, 1023 steps of size 1.24e-70. acc. prob=0.00]warmup:  11%|‚ñà         | 109/1000 [00:21&lt;01:55,  7.69it/s, 1023 steps of size 9.02e-72. acc. prob=0.00]warmup:  11%|‚ñà         | 110/1000 [00:21&lt;01:55,  7.71it/s, 1023 steps of size 6.97e-73. acc. prob=0.00]warmup:  11%|‚ñà         | 111/1000 [00:21&lt;01:57,  7.58it/s, 1023 steps of size 5.73e-74. acc. prob=0.00]warmup:  11%|‚ñà         | 112/1000 [00:21&lt;01:56,  7.60it/s, 1023 steps of size 5.01e-75. acc. prob=0.00]warmup:  11%|‚ñà‚ñè        | 113/1000 [00:21&lt;01:55,  7.66it/s, 1023 steps of size 4.67e-76. acc. prob=0.00]warmup:  11%|‚ñà‚ñè        | 114/1000 [00:21&lt;01:55,  7.66it/s, 1023 steps of size 4.61e-77. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 115/1000 [00:21&lt;01:54,  7.70it/s, 1023 steps of size 4.81e-78. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 116/1000 [00:21&lt;01:54,  7.74it/s, 1023 steps of size 5.31e-79. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 117/1000 [00:22&lt;01:54,  7.75it/s, 1023 steps of size 6.18e-80. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 118/1000 [00:22&lt;01:54,  7.72it/s, 1023 steps of size 7.55e-81. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 119/1000 [00:22&lt;01:55,  7.62it/s, 1023 steps of size 9.67e-82. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 120/1000 [00:22&lt;01:55,  7.63it/s, 1023 steps of size 1.30e-82. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 121/1000 [00:22&lt;01:57,  7.51it/s, 1023 steps of size 1.81e-83. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 122/1000 [00:22&lt;01:56,  7.57it/s, 1023 steps of size 2.65e-84. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 123/1000 [00:22&lt;01:55,  7.59it/s, 1023 steps of size 4.01e-85. acc. prob=0.00]warmup:  12%|‚ñà‚ñè        | 124/1000 [00:22&lt;01:54,  7.63it/s, 1023 steps of size 6.32e-86. acc. prob=0.00]warmup:  12%|‚ñà‚ñé        | 125/1000 [00:23&lt;01:54,  7.66it/s, 1023 steps of size 1.03e-86. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 126/1000 [00:23&lt;01:53,  7.71it/s, 1023 steps of size 1.74e-87. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 127/1000 [00:23&lt;01:54,  7.62it/s, 1023 steps of size 3.03e-88. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 128/1000 [00:23&lt;01:54,  7.61it/s, 1023 steps of size 5.46e-89. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 129/1000 [00:23&lt;01:53,  7.65it/s, 1023 steps of size 1.01e-89. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 130/1000 [00:23&lt;01:53,  7.68it/s, 1023 steps of size 1.93e-90. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 131/1000 [00:23&lt;01:52,  7.73it/s, 1023 steps of size 3.78e-91. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 132/1000 [00:24&lt;01:52,  7.72it/s, 1023 steps of size 7.60e-92. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 133/1000 [00:24&lt;01:52,  7.69it/s, 1023 steps of size 1.57e-92. acc. prob=0.00]warmup:  13%|‚ñà‚ñé        | 134/1000 [00:24&lt;01:54,  7.58it/s, 1023 steps of size 3.32e-93. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 135/1000 [00:24&lt;01:56,  7.44it/s, 1023 steps of size 7.18e-94. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 136/1000 [00:24&lt;01:55,  7.50it/s, 1023 steps of size 1.59e-94. acc. prob=0.00]warmup:  14%|‚ñà‚ñé        | 137/1000 [00:24&lt;01:55,  7.50it/s, 1023 steps of size 3.59e-95. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 138/1000 [00:24&lt;01:54,  7.52it/s, 1023 steps of size 8.30e-96. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 139/1000 [00:24&lt;01:57,  7.35it/s, 1023 steps of size 1.96e-96. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 140/1000 [00:25&lt;01:58,  7.27it/s, 1023 steps of size 4.70e-97. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 141/1000 [00:25&lt;01:56,  7.35it/s, 1023 steps of size 1.15e-97. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 142/1000 [00:25&lt;01:58,  7.26it/s, 1023 steps of size 2.87e-98. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 143/1000 [00:25&lt;01:57,  7.29it/s, 1023 steps of size 7.27e-99. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 144/1000 [00:25&lt;01:59,  7.19it/s, 1023 steps of size 1.88e-99. acc. prob=0.00]warmup:  14%|‚ñà‚ñç        | 145/1000 [00:25&lt;01:57,  7.27it/s, 1023 steps of size 4.92e-100. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 146/1000 [00:25&lt;01:58,  7.21it/s, 1023 steps of size 1.31e-100. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 147/1000 [00:26&lt;01:57,  7.29it/s, 1023 steps of size 3.54e-101. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 148/1000 [00:26&lt;01:59,  7.13it/s, 1023 steps of size 9.73e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñç        | 149/1000 [00:26&lt;01:56,  7.30it/s, 1023 steps of size 2.71e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 150/1000 [00:26&lt;01:54,  7.43it/s, 1023 steps of size 7.66e-103. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 151/1000 [00:26&lt;01:54,  7.43it/s, 1023 steps of size 1.79e-102. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 152/1000 [00:26&lt;01:52,  7.52it/s, 1023 steps of size 1.76e-103. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 153/1000 [00:26&lt;01:52,  7.56it/s, 1023 steps of size 1.28e-104. acc. prob=0.00]warmup:  15%|‚ñà‚ñå        | 154/1000 [00:26&lt;01:50,  7.62it/s, 1023 steps of size 8.19e-106. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 155/1000 [00:27&lt;01:49,  7.69it/s, 1023 steps of size 5.07e-107. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 156/1000 [00:27&lt;01:49,  7.67it/s, 1023 steps of size 3.17e-108. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 157/1000 [00:27&lt;01:50,  7.61it/s, 1023 steps of size 2.06e-109. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 158/1000 [00:27&lt;01:51,  7.58it/s, 1023 steps of size 1.41e-110. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 159/1000 [00:27&lt;01:55,  7.29it/s, 1023 steps of size 1.02e-111. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 160/1000 [00:27&lt;01:55,  7.25it/s, 1023 steps of size 7.89e-113. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 161/1000 [00:27&lt;01:59,  7.02it/s, 1023 steps of size 6.49e-114. acc. prob=0.00]warmup:  16%|‚ñà‚ñå        | 162/1000 [00:28&lt;01:58,  7.05it/s, 1023 steps of size 5.68e-115. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 163/1000 [00:28&lt;01:56,  7.20it/s, 1023 steps of size 5.29e-116. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 164/1000 [00:28&lt;01:53,  7.34it/s, 1023 steps of size 5.22e-117. acc. prob=0.00]warmup:  16%|‚ñà‚ñã        | 165/1000 [00:28&lt;01:51,  7.47it/s, 1023 steps of size 5.46e-118. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 166/1000 [00:28&lt;01:50,  7.54it/s, 1023 steps of size 6.02e-119. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 167/1000 [00:28&lt;02:14,  6.19it/s, 1023 steps of size 7.00e-120. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 168/1000 [00:28&lt;02:08,  6.47it/s, 1023 steps of size 8.55e-121. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 169/1000 [00:29&lt;02:03,  6.73it/s, 1023 steps of size 1.10e-121. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 170/1000 [00:29&lt;01:59,  6.92it/s, 1023 steps of size 1.47e-122. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 171/1000 [00:29&lt;01:58,  7.00it/s, 1023 steps of size 2.06e-123. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 172/1000 [00:29&lt;01:56,  7.08it/s, 1023 steps of size 3.00e-124. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 173/1000 [00:29&lt;01:55,  7.17it/s, 1023 steps of size 4.55e-125. acc. prob=0.00]warmup:  17%|‚ñà‚ñã        | 174/1000 [00:29&lt;01:53,  7.26it/s, 1023 steps of size 7.16e-126. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 175/1000 [00:29&lt;01:52,  7.37it/s, 1023 steps of size 1.17e-126. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 176/1000 [00:30&lt;01:52,  7.31it/s, 1023 steps of size 1.97e-127. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 177/1000 [00:30&lt;01:51,  7.40it/s, 1023 steps of size 3.44e-128. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 178/1000 [00:30&lt;01:53,  7.23it/s, 1023 steps of size 6.18e-129. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 179/1000 [00:30&lt;01:53,  7.24it/s, 1023 steps of size 1.15e-129. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 180/1000 [00:30&lt;01:54,  7.15it/s, 1023 steps of size 2.19e-130. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 181/1000 [00:30&lt;01:53,  7.21it/s, 1023 steps of size 4.28e-131. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 182/1000 [00:30&lt;01:54,  7.14it/s, 1023 steps of size 8.62e-132. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 183/1000 [00:31&lt;01:54,  7.14it/s, 1023 steps of size 1.78e-132. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 184/1000 [00:31&lt;01:52,  7.26it/s, 1023 steps of size 3.76e-133. acc. prob=0.00]warmup:  18%|‚ñà‚ñä        | 185/1000 [00:31&lt;01:50,  7.40it/s, 1023 steps of size 8.14e-134. acc. prob=0.00]warmup:  19%|‚ñà‚ñä        | 186/1000 [00:31&lt;01:48,  7.51it/s, 1023 steps of size 1.80e-134. acc. prob=0.00]warmup:  19%|‚ñà‚ñä        | 187/1000 [00:31&lt;01:47,  7.57it/s, 1023 steps of size 4.07e-135. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 188/1000 [00:31&lt;01:46,  7.61it/s, 1023 steps of size 9.41e-136. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 189/1000 [00:31&lt;01:46,  7.65it/s, 1023 steps of size 2.22e-136. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 190/1000 [00:31&lt;01:45,  7.67it/s, 1023 steps of size 5.32e-137. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 191/1000 [00:32&lt;01:45,  7.66it/s, 1023 steps of size 1.30e-137. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 192/1000 [00:32&lt;01:50,  7.34it/s, 1023 steps of size 3.25e-138. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 193/1000 [00:32&lt;01:51,  7.24it/s, 1023 steps of size 8.24e-139. acc. prob=0.00]warmup:  19%|‚ñà‚ñâ        | 194/1000 [00:32&lt;01:51,  7.24it/s, 1023 steps of size 2.12e-139. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 195/1000 [00:32&lt;01:50,  7.30it/s, 1023 steps of size 5.57e-140. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 196/1000 [00:32&lt;01:52,  7.14it/s, 1023 steps of size 1.48e-140. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 197/1000 [00:32&lt;01:50,  7.26it/s, 1023 steps of size 4.02e-141. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 198/1000 [00:33&lt;01:50,  7.28it/s, 1023 steps of size 1.10e-141. acc. prob=0.00]warmup:  20%|‚ñà‚ñâ        | 199/1000 [00:33&lt;01:49,  7.29it/s, 1023 steps of size 3.07e-142. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 200/1000 [00:33&lt;01:48,  7.40it/s, 1023 steps of size 8.68e-143. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 201/1000 [00:33&lt;01:48,  7.36it/s, 1023 steps of size 2.49e-143. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 202/1000 [00:33&lt;01:48,  7.37it/s, 1023 steps of size 7.22e-144. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 203/1000 [00:33&lt;01:46,  7.48it/s, 1023 steps of size 2.12e-144. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 204/1000 [00:33&lt;01:45,  7.56it/s, 1023 steps of size 6.31e-145. acc. prob=0.00]warmup:  20%|‚ñà‚ñà        | 205/1000 [00:33&lt;01:45,  7.54it/s, 1023 steps of size 1.90e-145. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 206/1000 [00:34&lt;01:44,  7.56it/s, 1023 steps of size 5.80e-146. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 207/1000 [00:34&lt;01:45,  7.55it/s, 1023 steps of size 1.79e-146. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 208/1000 [00:34&lt;01:44,  7.56it/s, 1023 steps of size 5.58e-147. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 209/1000 [00:34&lt;01:44,  7.54it/s, 1023 steps of size 1.76e-147. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 210/1000 [00:34&lt;01:44,  7.54it/s, 1023 steps of size 5.61e-148. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 211/1000 [00:34&lt;01:43,  7.62it/s, 1023 steps of size 1.81e-148. acc. prob=0.00]warmup:  21%|‚ñà‚ñà        | 212/1000 [00:34&lt;01:42,  7.68it/s, 1023 steps of size 5.88e-149. acc. prob=0.00]warmup:  21%|‚ñà‚ñà‚ñè       | 213/1000 [00:35&lt;01:42,  7.70it/s, 1023 steps of size 1.93e-149. acc. prob=0.00]warmup:  21%|‚ñà‚ñà‚ñè       | 214/1000 [00:35&lt;01:41,  7.72it/s, 1023 steps of size 6.41e-150. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 215/1000 [00:35&lt;01:41,  7.76it/s, 1023 steps of size 2.15e-150. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 216/1000 [00:35&lt;01:41,  7.76it/s, 1023 steps of size 7.25e-151. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 217/1000 [00:35&lt;01:42,  7.66it/s, 1023 steps of size 2.47e-151. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 218/1000 [00:35&lt;01:43,  7.56it/s, 1023 steps of size 8.51e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 219/1000 [00:35&lt;01:49,  7.15it/s, 1023 steps of size 2.95e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 220/1000 [00:35&lt;01:48,  7.18it/s, 1023 steps of size 1.03e-152. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 221/1000 [00:36&lt;01:46,  7.30it/s, 1023 steps of size 3.65e-153. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 222/1000 [00:36&lt;01:44,  7.42it/s, 1023 steps of size 1.30e-153. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 223/1000 [00:36&lt;01:44,  7.45it/s, 1023 steps of size 4.65e-154. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñè       | 224/1000 [00:36&lt;01:44,  7.44it/s, 1023 steps of size 1.68e-154. acc. prob=0.00]warmup:  22%|‚ñà‚ñà‚ñé       | 225/1000 [00:36&lt;01:44,  7.40it/s, 1023 steps of size 6.11e-155. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 226/1000 [00:36&lt;01:44,  7.40it/s, 1023 steps of size 2.24e-155. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 227/1000 [00:36&lt;01:43,  7.49it/s, 1023 steps of size 8.28e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 228/1000 [00:37&lt;01:42,  7.54it/s, 1023 steps of size 3.08e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 229/1000 [00:37&lt;01:41,  7.59it/s, 1023 steps of size 1.15e-156. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 230/1000 [00:37&lt;01:42,  7.52it/s, 1023 steps of size 4.35e-157. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 231/1000 [00:37&lt;01:41,  7.55it/s, 1023 steps of size 1.65e-157. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 232/1000 [00:37&lt;01:41,  7.56it/s, 1023 steps of size 6.32e-158. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 233/1000 [00:37&lt;01:42,  7.48it/s, 1023 steps of size 2.43e-158. acc. prob=0.00]warmup:  23%|‚ñà‚ñà‚ñé       | 234/1000 [00:37&lt;01:42,  7.44it/s, 1023 steps of size 9.40e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 235/1000 [00:37&lt;01:43,  7.42it/s, 1023 steps of size 3.66e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 236/1000 [00:41&lt;12:54,  1.01s/it, 1023 steps of size 1.44e-159. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñé       | 237/1000 [00:41&lt;09:32,  1.33it/s, 1023 steps of size 5.66e-160. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 238/1000 [00:41&lt;07:11,  1.77it/s, 1023 steps of size 2.24e-160. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 239/1000 [00:41&lt;05:30,  2.30it/s, 1023 steps of size 8.95e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 240/1000 [00:41&lt;04:21,  2.91it/s, 1023 steps of size 3.59e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 241/1000 [00:41&lt;03:32,  3.57it/s, 1023 steps of size 1.45e-161. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 242/1000 [00:41&lt;02:59,  4.23it/s, 1023 steps of size 5.87e-162. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 243/1000 [00:41&lt;02:35,  4.87it/s, 1023 steps of size 2.39e-162. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 244/1000 [00:42&lt;02:18,  5.47it/s, 1023 steps of size 9.81e-163. acc. prob=0.00]warmup:  24%|‚ñà‚ñà‚ñç       | 245/1000 [00:42&lt;02:06,  5.98it/s, 1023 steps of size 4.04e-163. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 246/1000 [00:42&lt;01:58,  6.38it/s, 1023 steps of size 1.67e-163. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 247/1000 [00:42&lt;01:51,  6.73it/s, 1023 steps of size 6.97e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 248/1000 [00:42&lt;01:47,  6.99it/s, 1023 steps of size 2.92e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñç       | 249/1000 [00:42&lt;01:46,  7.07it/s, 1023 steps of size 1.23e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 250/1000 [00:42&lt;01:43,  7.22it/s, 1023 steps of size 5.18e-165. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 251/1000 [00:43&lt;01:41,  7.35it/s, 1023 steps of size 1.21e-164. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 252/1000 [00:43&lt;01:40,  7.44it/s, 1023 steps of size 1.19e-165. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 253/1000 [00:43&lt;01:40,  7.42it/s, 1023 steps of size 8.64e-167. acc. prob=0.00]warmup:  25%|‚ñà‚ñà‚ñå       | 254/1000 [00:43&lt;01:40,  7.40it/s, 1023 steps of size 5.54e-168. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 255/1000 [00:43&lt;01:39,  7.46it/s, 1023 steps of size 3.43e-169. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 256/1000 [00:43&lt;01:38,  7.54it/s, 1023 steps of size 2.14e-170. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 257/1000 [00:43&lt;01:37,  7.62it/s, 1023 steps of size 1.39e-171. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 258/1000 [00:43&lt;01:38,  7.55it/s, 1023 steps of size 9.53e-173. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 259/1000 [00:44&lt;01:37,  7.57it/s, 1023 steps of size 6.91e-174. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 260/1000 [00:44&lt;01:36,  7.65it/s, 1023 steps of size 5.34e-175. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 261/1000 [00:44&lt;01:36,  7.70it/s, 1023 steps of size 4.39e-176. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñå       | 262/1000 [00:44&lt;01:36,  7.67it/s, 1023 steps of size 3.84e-177. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 263/1000 [00:44&lt;01:35,  7.73it/s, 1023 steps of size 3.57e-178. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 264/1000 [00:44&lt;01:34,  7.76it/s, 1023 steps of size 3.53e-179. acc. prob=0.00]warmup:  26%|‚ñà‚ñà‚ñã       | 265/1000 [00:44&lt;01:35,  7.71it/s, 1023 steps of size 3.69e-180. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 266/1000 [00:44&lt;01:34,  7.74it/s, 1023 steps of size 4.07e-181. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 267/1000 [00:45&lt;01:34,  7.72it/s, 1023 steps of size 4.73e-182. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 268/1000 [00:45&lt;01:36,  7.57it/s, 1023 steps of size 5.78e-183. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 269/1000 [00:45&lt;01:36,  7.57it/s, 1023 steps of size 7.41e-184. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 270/1000 [00:45&lt;01:36,  7.55it/s, 1023 steps of size 9.93e-185. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 271/1000 [00:45&lt;01:36,  7.54it/s, 1023 steps of size 1.39e-185. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 272/1000 [00:45&lt;01:35,  7.60it/s, 1023 steps of size 2.03e-186. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 273/1000 [00:45&lt;01:36,  7.57it/s, 1023 steps of size 3.07e-187. acc. prob=0.00]warmup:  27%|‚ñà‚ñà‚ñã       | 274/1000 [00:46&lt;01:35,  7.58it/s, 1023 steps of size 4.84e-188. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 275/1000 [00:46&lt;01:36,  7.54it/s, 1023 steps of size 7.89e-189. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 276/1000 [00:46&lt;01:37,  7.43it/s, 1023 steps of size 1.33e-189. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 277/1000 [00:46&lt;01:37,  7.40it/s, 1023 steps of size 2.32e-190. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 278/1000 [00:46&lt;01:36,  7.48it/s, 1023 steps of size 4.18e-191. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 279/1000 [00:46&lt;01:35,  7.57it/s, 1023 steps of size 7.74e-192. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 280/1000 [00:46&lt;01:34,  7.59it/s, 1023 steps of size 1.48e-192. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 281/1000 [00:46&lt;01:34,  7.64it/s, 1023 steps of size 2.89e-193. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 282/1000 [00:47&lt;01:34,  7.61it/s, 1023 steps of size 5.82e-194. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 283/1000 [00:47&lt;01:33,  7.64it/s, 1023 steps of size 1.20e-194. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 284/1000 [00:47&lt;01:34,  7.61it/s, 1023 steps of size 2.54e-195. acc. prob=0.00]warmup:  28%|‚ñà‚ñà‚ñä       | 285/1000 [00:47&lt;01:33,  7.61it/s, 1023 steps of size 5.50e-196. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñä       | 286/1000 [00:47&lt;01:33,  7.67it/s, 1023 steps of size 1.22e-196. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñä       | 287/1000 [00:47&lt;01:32,  7.68it/s, 1023 steps of size 2.75e-197. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 288/1000 [00:47&lt;01:32,  7.69it/s, 1023 steps of size 6.36e-198. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 289/1000 [00:48&lt;01:32,  7.67it/s, 1023 steps of size 1.50e-198. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 290/1000 [00:48&lt;01:32,  7.66it/s, 1023 steps of size 3.60e-199. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 291/1000 [00:48&lt;01:32,  7.70it/s, 1023 steps of size 8.81e-200. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 292/1000 [00:48&lt;01:31,  7.71it/s, 1023 steps of size 2.20e-200. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 293/1000 [00:48&lt;01:31,  7.70it/s, 1023 steps of size 5.57e-201. acc. prob=0.00]warmup:  29%|‚ñà‚ñà‚ñâ       | 294/1000 [00:48&lt;01:31,  7.70it/s, 1023 steps of size 1.44e-201. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 295/1000 [00:48&lt;01:31,  7.67it/s, 1023 steps of size 3.77e-202. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 296/1000 [00:48&lt;01:32,  7.64it/s, 1023 steps of size 1.00e-202. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 297/1000 [00:49&lt;01:32,  7.60it/s, 1023 steps of size 2.71e-203. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 298/1000 [00:49&lt;01:32,  7.59it/s, 1023 steps of size 7.45e-204. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [00:49&lt;01:33,  7.51it/s, 1023 steps of size 2.08e-204. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 300/1000 [00:49&lt;01:32,  7.57it/s, 1023 steps of size 5.87e-205. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 301/1000 [00:49&lt;01:32,  7.56it/s, 1023 steps of size 1.68e-205. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 302/1000 [00:49&lt;01:31,  7.63it/s, 1023 steps of size 4.88e-206. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 303/1000 [00:49&lt;01:31,  7.65it/s, 1023 steps of size 1.43e-206. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 304/1000 [00:49&lt;01:30,  7.66it/s, 1023 steps of size 4.27e-207. acc. prob=0.00]warmup:  30%|‚ñà‚ñà‚ñà       | 305/1000 [00:50&lt;01:30,  7.68it/s, 1023 steps of size 1.29e-207. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 306/1000 [00:50&lt;01:30,  7.65it/s, 1023 steps of size 3.92e-208. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 307/1000 [00:50&lt;01:30,  7.68it/s, 1023 steps of size 1.21e-208. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 308/1000 [00:50&lt;01:30,  7.69it/s, 1023 steps of size 3.77e-209. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 309/1000 [00:50&lt;01:30,  7.67it/s, 1023 steps of size 1.19e-209. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 310/1000 [00:50&lt;01:29,  7.71it/s, 1023 steps of size 3.79e-210. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 311/1000 [00:50&lt;01:29,  7.71it/s, 1023 steps of size 1.22e-210. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà       | 312/1000 [00:51&lt;01:29,  7.69it/s, 1023 steps of size 3.97e-211. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà‚ñè      | 313/1000 [00:51&lt;01:30,  7.61it/s, 1023 steps of size 1.31e-211. acc. prob=0.00]warmup:  31%|‚ñà‚ñà‚ñà‚ñè      | 314/1000 [00:51&lt;01:30,  7.59it/s, 1023 steps of size 4.33e-212. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 315/1000 [00:51&lt;01:29,  7.67it/s, 1023 steps of size 1.45e-212. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 316/1000 [00:51&lt;01:29,  7.68it/s, 1023 steps of size 4.90e-213. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 317/1000 [00:51&lt;01:29,  7.65it/s, 1023 steps of size 1.67e-213. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 318/1000 [00:51&lt;01:30,  7.51it/s, 1023 steps of size 5.75e-214. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 319/1000 [00:51&lt;01:30,  7.52it/s, 1023 steps of size 2.00e-214. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 320/1000 [00:52&lt;01:29,  7.58it/s, 1023 steps of size 6.98e-215. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 321/1000 [00:52&lt;01:29,  7.62it/s, 1023 steps of size 2.46e-215. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 322/1000 [00:52&lt;01:28,  7.68it/s, 1023 steps of size 8.77e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 323/1000 [00:52&lt;01:28,  7.67it/s, 1023 steps of size 3.14e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñè      | 324/1000 [00:52&lt;01:27,  7.69it/s, 1023 steps of size 1.14e-216. acc. prob=0.00]warmup:  32%|‚ñà‚ñà‚ñà‚ñé      | 325/1000 [00:52&lt;01:28,  7.67it/s, 1023 steps of size 4.13e-217. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 326/1000 [00:52&lt;01:27,  7.71it/s, 1023 steps of size 1.52e-217. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 327/1000 [00:53&lt;01:27,  7.70it/s, 1023 steps of size 5.60e-218. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 328/1000 [00:53&lt;01:30,  7.39it/s, 1023 steps of size 2.08e-218. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 329/1000 [00:53&lt;01:30,  7.43it/s, 1023 steps of size 7.80e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 330/1000 [00:53&lt;01:31,  7.33it/s, 1023 steps of size 2.94e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 331/1000 [00:53&lt;01:31,  7.28it/s, 1023 steps of size 1.12e-219. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 332/1000 [00:53&lt;01:30,  7.35it/s, 1023 steps of size 4.27e-220. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 333/1000 [00:53&lt;01:29,  7.44it/s, 1023 steps of size 1.64e-220. acc. prob=0.00]warmup:  33%|‚ñà‚ñà‚ñà‚ñé      | 334/1000 [00:53&lt;01:28,  7.49it/s, 1023 steps of size 6.36e-221. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 335/1000 [00:54&lt;01:27,  7.58it/s, 1023 steps of size 2.48e-221. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 336/1000 [00:54&lt;01:27,  7.61it/s, 1023 steps of size 9.70e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñé      | 337/1000 [00:54&lt;01:27,  7.58it/s, 1023 steps of size 3.82e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 338/1000 [00:54&lt;01:28,  7.50it/s, 1023 steps of size 1.52e-222. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 339/1000 [00:54&lt;01:28,  7.48it/s, 1023 steps of size 6.05e-223. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 340/1000 [00:54&lt;01:27,  7.57it/s, 1023 steps of size 2.43e-223. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 341/1000 [00:54&lt;01:27,  7.57it/s, 1023 steps of size 9.79e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 342/1000 [00:55&lt;01:26,  7.58it/s, 1023 steps of size 3.97e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 343/1000 [00:55&lt;01:28,  7.45it/s, 1023 steps of size 1.62e-224. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 344/1000 [00:55&lt;01:29,  7.36it/s, 1023 steps of size 6.63e-225. acc. prob=0.00]warmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 345/1000 [00:55&lt;01:28,  7.37it/s, 1023 steps of size 2.73e-225. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 346/1000 [00:55&lt;01:29,  7.34it/s, 1023 steps of size 1.13e-225. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 347/1000 [00:55&lt;01:29,  7.32it/s, 1023 steps of size 4.71e-226. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 348/1000 [00:55&lt;01:28,  7.33it/s, 1023 steps of size 1.97e-226. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñç      | 349/1000 [00:55&lt;01:28,  7.37it/s, 1023 steps of size 8.29e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 350/1000 [00:56&lt;01:27,  7.47it/s, 1023 steps of size 3.50e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 351/1000 [00:56&lt;01:25,  7.56it/s, 1023 steps of size 1.49e-227. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 352/1000 [00:56&lt;01:25,  7.60it/s, 1023 steps of size 6.33e-228. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 353/1000 [00:56&lt;01:25,  7.61it/s, 1023 steps of size 2.71e-228. acc. prob=0.00]warmup:  35%|‚ñà‚ñà‚ñà‚ñå      | 354/1000 [00:56&lt;01:25,  7.53it/s, 1023 steps of size 1.17e-228. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 355/1000 [00:56&lt;01:27,  7.41it/s, 1023 steps of size 5.04e-229. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 356/1000 [00:56&lt;01:26,  7.42it/s, 1023 steps of size 2.19e-229. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 357/1000 [00:57&lt;01:25,  7.48it/s, 1023 steps of size 9.54e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 358/1000 [00:57&lt;01:24,  7.57it/s, 1023 steps of size 4.18e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 359/1000 [00:57&lt;01:24,  7.56it/s, 1023 steps of size 1.84e-230. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 360/1000 [00:57&lt;01:25,  7.49it/s, 1023 steps of size 8.10e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 361/1000 [00:57&lt;01:24,  7.55it/s, 1023 steps of size 3.59e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñå      | 362/1000 [00:57&lt;01:23,  7.61it/s, 1023 steps of size 1.60e-231. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 363/1000 [00:57&lt;01:23,  7.66it/s, 1023 steps of size 7.14e-232. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 364/1000 [00:57&lt;01:22,  7.68it/s, 1023 steps of size 3.20e-232. acc. prob=0.00]warmup:  36%|‚ñà‚ñà‚ñà‚ñã      | 365/1000 [00:58&lt;01:22,  7.70it/s, 1023 steps of size 1.44e-232. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 366/1000 [00:58&lt;01:23,  7.57it/s, 1023 steps of size 6.51e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 367/1000 [00:58&lt;01:24,  7.49it/s, 1023 steps of size 2.95e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 368/1000 [00:58&lt;01:25,  7.39it/s, 1023 steps of size 1.35e-233. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 369/1000 [00:58&lt;01:24,  7.46it/s, 1023 steps of size 6.15e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 370/1000 [00:58&lt;01:24,  7.49it/s, 1023 steps of size 2.82e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 371/1000 [00:58&lt;01:23,  7.51it/s, 1023 steps of size 1.30e-234. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 372/1000 [00:59&lt;01:23,  7.54it/s, 1023 steps of size 5.99e-235. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 373/1000 [00:59&lt;01:24,  7.46it/s, 1023 steps of size 2.78e-235. acc. prob=0.00]warmup:  37%|‚ñà‚ñà‚ñà‚ñã      | 374/1000 [00:59&lt;01:23,  7.54it/s, 1023 steps of size 1.29e-235. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 375/1000 [00:59&lt;01:23,  7.46it/s, 1023 steps of size 6.02e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 376/1000 [00:59&lt;01:24,  7.40it/s, 1023 steps of size 2.82e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 377/1000 [00:59&lt;01:24,  7.37it/s, 1023 steps of size 1.32e-236. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 378/1000 [00:59&lt;01:23,  7.46it/s, 1023 steps of size 6.24e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 379/1000 [00:59&lt;01:22,  7.56it/s, 1023 steps of size 2.95e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 380/1000 [01:00&lt;01:24,  7.30it/s, 1023 steps of size 1.40e-237. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 381/1000 [01:00&lt;01:24,  7.29it/s, 1023 steps of size 6.65e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 382/1000 [01:00&lt;01:23,  7.41it/s, 1023 steps of size 3.17e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 383/1000 [01:00&lt;01:22,  7.45it/s, 1023 steps of size 1.52e-238. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 384/1000 [01:00&lt;01:23,  7.39it/s, 1023 steps of size 7.29e-239. acc. prob=0.00]warmup:  38%|‚ñà‚ñà‚ñà‚ñä      | 385/1000 [01:00&lt;01:22,  7.43it/s, 1023 steps of size 3.51e-239. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 386/1000 [01:00&lt;01:22,  7.47it/s, 1023 steps of size 1.70e-239. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñä      | 387/1000 [01:01&lt;01:20,  7.57it/s, 1023 steps of size 8.21e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 388/1000 [01:01&lt;01:21,  7.54it/s, 1023 steps of size 3.99e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 389/1000 [01:01&lt;01:21,  7.48it/s, 1023 steps of size 1.94e-240. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 390/1000 [01:01&lt;01:21,  7.52it/s, 1023 steps of size 9.49e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 391/1000 [01:01&lt;01:20,  7.53it/s, 1023 steps of size 4.65e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 392/1000 [01:01&lt;01:21,  7.47it/s, 1023 steps of size 2.28e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 393/1000 [01:01&lt;01:22,  7.38it/s, 1023 steps of size 1.12e-241. acc. prob=0.00]warmup:  39%|‚ñà‚ñà‚ñà‚ñâ      | 394/1000 [01:01&lt;01:22,  7.37it/s, 1023 steps of size 5.55e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 395/1000 [01:02&lt;01:20,  7.48it/s, 1023 steps of size 2.75e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 396/1000 [01:02&lt;01:20,  7.54it/s, 1023 steps of size 1.36e-242. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 397/1000 [01:02&lt;01:19,  7.55it/s, 1023 steps of size 6.79e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 398/1000 [01:02&lt;01:18,  7.63it/s, 1023 steps of size 3.39e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [01:02&lt;01:20,  7.44it/s, 1023 steps of size 1.69e-243. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 400/1000 [01:02&lt;01:20,  7.48it/s, 1023 steps of size 8.49e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 401/1000 [01:02&lt;01:19,  7.51it/s, 1023 steps of size 4.27e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 402/1000 [01:03&lt;01:19,  7.55it/s, 1023 steps of size 2.15e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 403/1000 [01:03&lt;01:18,  7.58it/s, 1023 steps of size 1.09e-244. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 404/1000 [01:03&lt;01:18,  7.63it/s, 1023 steps of size 5.51e-245. acc. prob=0.00]warmup:  40%|‚ñà‚ñà‚ñà‚ñà      | 405/1000 [01:03&lt;01:18,  7.60it/s, 1023 steps of size 2.80e-245. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 406/1000 [01:03&lt;01:18,  7.61it/s, 1023 steps of size 1.42e-245. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 407/1000 [01:03&lt;01:18,  7.59it/s, 1023 steps of size 7.25e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 408/1000 [01:03&lt;01:19,  7.47it/s, 1023 steps of size 3.71e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 409/1000 [01:03&lt;01:20,  7.36it/s, 1023 steps of size 1.90e-246. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 410/1000 [01:04&lt;01:19,  7.38it/s, 1023 steps of size 9.76e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 411/1000 [01:04&lt;01:19,  7.43it/s, 1023 steps of size 5.02e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà      | 412/1000 [01:04&lt;01:18,  7.47it/s, 1023 steps of size 2.59e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 413/1000 [01:04&lt;01:18,  7.49it/s, 1023 steps of size 1.34e-247. acc. prob=0.00]warmup:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 414/1000 [01:04&lt;01:17,  7.53it/s, 1023 steps of size 6.94e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 415/1000 [01:04&lt;01:18,  7.43it/s, 1023 steps of size 3.60e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/1000 [01:04&lt;01:20,  7.26it/s, 1023 steps of size 1.88e-248. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/1000 [01:05&lt;01:19,  7.35it/s, 1023 steps of size 9.78e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 418/1000 [01:05&lt;01:20,  7.22it/s, 1023 steps of size 5.11e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 419/1000 [01:05&lt;01:20,  7.24it/s, 1023 steps of size 2.68e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/1000 [01:05&lt;01:21,  7.09it/s, 1023 steps of size 1.40e-249. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 421/1000 [01:05&lt;01:21,  7.13it/s, 1023 steps of size 7.38e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 422/1000 [01:05&lt;01:21,  7.13it/s, 1023 steps of size 3.89e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/1000 [01:05&lt;01:23,  6.94it/s, 1023 steps of size 2.05e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/1000 [01:06&lt;01:21,  7.07it/s, 1023 steps of size 1.09e-250. acc. prob=0.00]warmup:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/1000 [01:06&lt;01:24,  6.79it/s, 1023 steps of size 5.76e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 426/1000 [01:06&lt;01:24,  6.82it/s, 1023 steps of size 3.06e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/1000 [01:06&lt;01:21,  7.00it/s, 1023 steps of size 1.63e-251. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 428/1000 [01:06&lt;01:20,  7.12it/s, 1023 steps of size 8.67e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/1000 [01:06&lt;01:18,  7.32it/s, 1023 steps of size 4.63e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 430/1000 [01:06&lt;01:16,  7.42it/s, 1023 steps of size 2.48e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 431/1000 [01:07&lt;01:16,  7.46it/s, 1023 steps of size 1.33e-252. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/1000 [01:07&lt;01:15,  7.56it/s, 1023 steps of size 7.13e-253. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433/1000 [01:07&lt;01:14,  7.59it/s, 1023 steps of size 3.84e-253. acc. prob=0.00]warmup:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 434/1000 [01:07&lt;01:16,  7.42it/s, 1023 steps of size 2.07e-253. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 435/1000 [01:07&lt;01:17,  7.33it/s, 1023 steps of size 1.12e-253. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 436/1000 [01:07&lt;01:19,  7.13it/s, 1023 steps of size 6.05e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/1000 [01:07&lt;01:19,  7.11it/s, 1023 steps of size 3.28e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/1000 [01:07&lt;01:17,  7.29it/s, 1023 steps of size 1.78e-254. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 439/1000 [01:08&lt;01:15,  7.42it/s, 1023 steps of size 9.67e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/1000 [01:08&lt;01:15,  7.41it/s, 1023 steps of size 5.27e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 441/1000 [01:08&lt;01:14,  7.48it/s, 1023 steps of size 2.88e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 442/1000 [01:08&lt;01:14,  7.53it/s, 1023 steps of size 1.57e-255. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 443/1000 [01:08&lt;01:13,  7.55it/s, 1023 steps of size 8.61e-256. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/1000 [01:08&lt;01:13,  7.55it/s, 1023 steps of size 4.72e-256. acc. prob=0.00]warmup:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445/1000 [01:08&lt;01:13,  7.59it/s, 1023 steps of size 2.59e-256. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/1000 [01:09&lt;01:12,  7.66it/s, 1023 steps of size 1.43e-256. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 447/1000 [01:09&lt;01:12,  7.67it/s, 1023 steps of size 7.87e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/1000 [01:09&lt;01:11,  7.70it/s, 1023 steps of size 4.34e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/1000 [01:09&lt;01:11,  7.73it/s, 1023 steps of size 2.40e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 450/1000 [01:09&lt;01:11,  7.70it/s, 1023 steps of size 1.33e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/1000 [01:09&lt;01:11,  7.67it/s, 1023 steps of size 3.11e-257. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/1000 [01:09&lt;01:11,  7.72it/s, 1023 steps of size 3.06e-258. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/1000 [01:09&lt;01:11,  7.70it/s, 1023 steps of size 2.22e-259. acc. prob=0.00]warmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/1000 [01:10&lt;01:10,  7.70it/s, 1023 steps of size 1.42e-260. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/1000 [01:10&lt;01:11,  7.65it/s, 1023 steps of size 8.80e-262. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 456/1000 [01:10&lt;01:11,  7.59it/s, 1023 steps of size 5.51e-263. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 457/1000 [01:10&lt;01:12,  7.54it/s, 1023 steps of size 3.58e-264. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/1000 [01:10&lt;01:11,  7.60it/s, 1023 steps of size 2.45e-265. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 459/1000 [01:10&lt;01:12,  7.50it/s, 1023 steps of size 1.78e-266. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/1000 [01:10&lt;01:12,  7.48it/s, 1023 steps of size 1.37e-267. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/1000 [01:10&lt;01:12,  7.46it/s, 1023 steps of size 1.13e-268. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 462/1000 [01:11&lt;01:11,  7.55it/s, 1023 steps of size 9.87e-270. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 463/1000 [01:11&lt;01:10,  7.60it/s, 1023 steps of size 9.18e-271. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/1000 [01:11&lt;01:10,  7.57it/s, 1023 steps of size 9.07e-272. acc. prob=0.00]warmup:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/1000 [01:11&lt;01:11,  7.52it/s, 1023 steps of size 9.48e-273. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/1000 [01:11&lt;01:11,  7.43it/s, 1023 steps of size 1.05e-273. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467/1000 [01:11&lt;01:12,  7.39it/s, 1023 steps of size 1.22e-274. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/1000 [01:11&lt;01:10,  7.51it/s, 1023 steps of size 1.49e-275. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/1000 [01:12&lt;01:10,  7.57it/s, 1023 steps of size 1.90e-276. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/1000 [01:12&lt;01:09,  7.61it/s, 1023 steps of size 2.55e-277. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/1000 [01:12&lt;01:09,  7.60it/s, 1023 steps of size 3.57e-278. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/1000 [01:12&lt;01:09,  7.64it/s, 1023 steps of size 5.21e-279. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/1000 [01:12&lt;01:09,  7.56it/s, 1023 steps of size 7.90e-280. acc. prob=0.00]warmup:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 474/1000 [01:12&lt;01:09,  7.60it/s, 1023 steps of size 1.24e-280. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475/1000 [01:12&lt;01:08,  7.63it/s, 1023 steps of size 2.03e-281. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/1000 [01:12&lt;01:08,  7.65it/s, 1023 steps of size 3.42e-282. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/1000 [01:13&lt;01:08,  7.67it/s, 1023 steps of size 5.97e-283. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/1000 [01:13&lt;01:08,  7.67it/s, 1023 steps of size 1.07e-283. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 479/1000 [01:16&lt;08:47,  1.01s/it, 1023 steps of size 1.99e-284. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/1000 [01:16&lt;06:28,  1.34it/s, 1023 steps of size 3.80e-285. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481/1000 [01:16&lt;04:51,  1.78it/s, 1023 steps of size 7.44e-286. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/1000 [01:16&lt;03:44,  2.31it/s, 1023 steps of size 1.50e-286. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 483/1000 [01:16&lt;02:57,  2.91it/s, 1023 steps of size 3.09e-287. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 484/1000 [01:16&lt;02:24,  3.57it/s, 1023 steps of size 6.53e-288. acc. prob=0.00]warmup:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 485/1000 [01:17&lt;02:00,  4.27it/s, 1023 steps of size 1.41e-288. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/1000 [01:17&lt;01:44,  4.94it/s, 1023 steps of size 3.13e-289. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/1000 [01:17&lt;01:32,  5.53it/s, 1023 steps of size 7.07e-290. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/1000 [01:17&lt;01:25,  6.02it/s, 1023 steps of size 1.63e-290. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 489/1000 [01:17&lt;01:19,  6.47it/s, 1023 steps of size 3.85e-291. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/1000 [01:17&lt;01:15,  6.79it/s, 1023 steps of size 9.25e-292. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491/1000 [01:17&lt;01:12,  6.98it/s, 1023 steps of size 2.26e-292. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 492/1000 [01:18&lt;01:11,  7.14it/s, 1023 steps of size 5.64e-293. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/1000 [01:18&lt;01:10,  7.22it/s, 1023 steps of size 1.43e-293. acc. prob=0.00]warmup:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 494/1000 [01:18&lt;01:09,  7.27it/s, 1023 steps of size 3.69e-294. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 495/1000 [01:18&lt;01:08,  7.32it/s, 1023 steps of size 9.68e-295. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/1000 [01:18&lt;01:07,  7.46it/s, 1023 steps of size 2.58e-295. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/1000 [01:18&lt;01:07,  7.48it/s, 1023 steps of size 6.98e-296. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 498/1000 [01:18&lt;01:06,  7.50it/s, 1023 steps of size 1.92e-296. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [01:18&lt;01:06,  7.53it/s, 1023 steps of size 5.34e-297. acc. prob=0.00]warmup:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/1000 [01:19&lt;01:06,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/1000 [01:19&lt;01:05,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 502/1000 [01:19&lt;01:04,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/1000 [01:19&lt;01:04,  7.71it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/1000 [01:19&lt;01:04,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/1000 [01:19&lt;01:04,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 506/1000 [01:19&lt;01:04,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/1000 [01:19&lt;01:04,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/1000 [01:20&lt;01:03,  7.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 509/1000 [01:20&lt;01:04,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/1000 [01:20&lt;01:04,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 511/1000 [01:20&lt;01:03,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 512/1000 [01:20&lt;01:03,  7.69it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 513/1000 [01:20&lt;01:03,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 514/1000 [01:20&lt;01:03,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 515/1000 [01:21&lt;01:04,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/1000 [01:21&lt;01:04,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/1000 [01:21&lt;01:05,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 518/1000 [01:21&lt;01:06,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 519/1000 [01:21&lt;01:05,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/1000 [01:21&lt;01:04,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/1000 [01:21&lt;01:03,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 522/1000 [01:21&lt;01:04,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 523/1000 [01:22&lt;01:03,  7.48it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/1000 [01:22&lt;01:03,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/1000 [01:22&lt;01:03,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 526/1000 [01:22&lt;01:02,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 527/1000 [01:22&lt;01:02,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/1000 [01:22&lt;01:01,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/1000 [01:22&lt;01:01,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 530/1000 [01:23&lt;01:01,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/1000 [01:23&lt;01:04,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532/1000 [01:23&lt;01:03,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/1000 [01:23&lt;01:02,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 534/1000 [01:23&lt;01:01,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 535/1000 [01:23&lt;01:01,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 536/1000 [01:23&lt;01:01,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 537/1000 [01:23&lt;01:00,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/1000 [01:24&lt;01:00,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 539/1000 [01:24&lt;00:59,  7.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/1000 [01:24&lt;00:59,  7.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/1000 [01:24&lt;00:59,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 542/1000 [01:24&lt;01:00,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 543/1000 [01:24&lt;01:01,  7.48it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/1000 [01:24&lt;01:00,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/1000 [01:25&lt;01:00,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 546/1000 [01:25&lt;01:01,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/1000 [01:25&lt;01:01,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/1000 [01:25&lt;01:01,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/1000 [01:25&lt;01:01,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 550/1000 [01:25&lt;01:01,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/1000 [01:25&lt;01:00,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/1000 [01:25&lt;01:01,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 553/1000 [01:26&lt;01:00,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 554/1000 [01:26&lt;01:01,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/1000 [01:26&lt;01:01,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/1000 [01:26&lt;01:00,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/1000 [01:26&lt;01:00,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 558/1000 [01:26&lt;00:59,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/1000 [01:26&lt;00:58,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/1000 [01:27&lt;00:58,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/1000 [01:27&lt;00:59,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 562/1000 [01:27&lt;00:59,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563/1000 [01:27&lt;01:00,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/1000 [01:27&lt;00:59,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 565/1000 [01:27&lt;01:00,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/1000 [01:27&lt;01:00,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/1000 [01:28&lt;01:00,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/1000 [01:28&lt;00:59,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/1000 [01:28&lt;00:59,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 570/1000 [01:28&lt;00:58,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 571/1000 [01:28&lt;00:58,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 572/1000 [01:28&lt;00:58,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 573/1000 [01:28&lt;00:58,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/1000 [01:28&lt;00:58,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 575/1000 [01:29&lt;00:58,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/1000 [01:29&lt;00:57,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 577/1000 [01:29&lt;00:57,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/1000 [01:29&lt;00:57,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579/1000 [01:29&lt;00:57,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/1000 [01:29&lt;00:57,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 581/1000 [01:29&lt;00:57,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/1000 [01:30&lt;00:56,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583/1000 [01:30&lt;00:56,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 584/1000 [01:30&lt;00:55,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/1000 [01:30&lt;00:54,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/1000 [01:30&lt;00:54,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 587/1000 [01:30&lt;00:54,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/1000 [01:30&lt;00:54,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/1000 [01:30&lt;00:53,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/1000 [01:31&lt;00:53,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/1000 [01:31&lt;00:57,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/1000 [01:31&lt;00:58,  6.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 593/1000 [01:31&lt;00:58,  6.95it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/1000 [01:31&lt;00:57,  7.03it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/1000 [01:31&lt;00:56,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 596/1000 [01:31&lt;00:54,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 597/1000 [01:32&lt;00:54,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/1000 [01:32&lt;00:53,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [01:32&lt;00:52,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/1000 [01:32&lt;00:53,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/1000 [01:32&lt;00:52,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/1000 [01:32&lt;00:53,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 603/1000 [01:32&lt;00:52,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/1000 [01:33&lt;00:52,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/1000 [01:33&lt;00:52,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/1000 [01:33&lt;00:53,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/1000 [01:33&lt;01:01,  6.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/1000 [01:33&lt;01:13,  5.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/1000 [01:33&lt;01:18,  4.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/1000 [01:34&lt;01:20,  4.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/1000 [01:34&lt;01:19,  4.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 612/1000 [01:34&lt;01:20,  4.82it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 613/1000 [01:34&lt;01:23,  4.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/1000 [01:35&lt;01:22,  4.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 615/1000 [01:35&lt;01:20,  4.76it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 616/1000 [01:35&lt;01:12,  5.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 617/1000 [01:35&lt;01:07,  5.70it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 618/1000 [01:35&lt;01:01,  6.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/1000 [01:35&lt;00:58,  6.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/1000 [01:35&lt;00:56,  6.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 621/1000 [01:36&lt;00:55,  6.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 622/1000 [01:36&lt;00:53,  7.06it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 623/1000 [01:36&lt;00:53,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/1000 [01:36&lt;00:51,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/1000 [01:36&lt;00:51,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/1000 [01:36&lt;00:50,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/1000 [01:36&lt;00:50,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/1000 [01:37&lt;00:50,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 629/1000 [01:37&lt;00:50,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/1000 [01:37&lt;00:50,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/1000 [01:37&lt;00:50,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/1000 [01:37&lt;00:50,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/1000 [01:37&lt;00:49,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 634/1000 [01:37&lt;00:49,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/1000 [01:37&lt;00:50,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/1000 [01:38&lt;00:50,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 637/1000 [01:38&lt;00:50,  7.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 638/1000 [01:38&lt;00:51,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 639/1000 [01:38&lt;00:51,  6.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [01:38&lt;00:51,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 641/1000 [01:38&lt;00:52,  6.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 642/1000 [01:38&lt;00:50,  7.05it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/1000 [01:39&lt;00:50,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 644/1000 [01:39&lt;00:49,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/1000 [01:39&lt;00:48,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 646/1000 [01:39&lt;00:48,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 647/1000 [01:39&lt;00:48,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 648/1000 [01:39&lt;00:48,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 649/1000 [01:39&lt;00:48,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/1000 [01:40&lt;00:49,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/1000 [01:40&lt;00:50,  6.98it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 652/1000 [01:40&lt;00:50,  6.84it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 653/1000 [01:40&lt;00:50,  6.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/1000 [01:40&lt;00:50,  6.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/1000 [01:40&lt;00:51,  6.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/1000 [01:40&lt;00:50,  6.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 657/1000 [01:41&lt;00:49,  6.87it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 658/1000 [01:41&lt;00:49,  6.91it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 659/1000 [01:41&lt;00:48,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/1000 [01:41&lt;00:47,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 661/1000 [01:41&lt;00:47,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 662/1000 [01:41&lt;00:47,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/1000 [01:41&lt;00:46,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/1000 [01:42&lt;00:45,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 665/1000 [01:42&lt;00:45,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/1000 [01:42&lt;00:44,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/1000 [01:42&lt;00:45,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/1000 [01:42&lt;00:44,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 669/1000 [01:42&lt;00:44,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/1000 [01:42&lt;00:44,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 671/1000 [01:43&lt;00:43,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/1000 [01:43&lt;00:43,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 673/1000 [01:43&lt;00:42,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/1000 [01:43&lt;00:42,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/1000 [01:43&lt;00:42,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/1000 [01:43&lt;00:42,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 677/1000 [01:43&lt;00:42,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 678/1000 [01:43&lt;00:42,  7.60it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/1000 [01:44&lt;00:42,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/1000 [01:44&lt;00:42,  7.54it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 681/1000 [01:44&lt;00:42,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/1000 [01:44&lt;00:44,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 683/1000 [01:44&lt;00:45,  6.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 684/1000 [01:44&lt;00:44,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/1000 [01:44&lt;00:44,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/1000 [01:45&lt;00:43,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 687/1000 [01:45&lt;00:42,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/1000 [01:45&lt;00:42,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 689/1000 [01:45&lt;00:41,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/1000 [01:45&lt;00:41,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691/1000 [01:45&lt;00:40,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 692/1000 [01:45&lt;00:41,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 693/1000 [01:45&lt;00:40,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/1000 [01:46&lt;00:41,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 695/1000 [01:46&lt;00:42,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/1000 [01:46&lt;00:41,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/1000 [01:46&lt;00:41,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 698/1000 [01:46&lt;00:40,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [01:46&lt;00:40,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/1000 [01:46&lt;00:40,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/1000 [01:47&lt;00:40,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702/1000 [01:47&lt;00:39,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/1000 [01:47&lt;00:39,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/1000 [01:47&lt;00:39,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 705/1000 [01:47&lt;00:39,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/1000 [01:47&lt;00:39,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/1000 [01:47&lt;00:39,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 708/1000 [01:48&lt;00:39,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/1000 [01:48&lt;00:39,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/1000 [01:48&lt;00:39,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 711/1000 [01:48&lt;00:39,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 712/1000 [01:51&lt;04:52,  1.02s/it, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 713/1000 [01:51&lt;03:35,  1.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 714/1000 [01:51&lt;02:43,  1.75it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 715/1000 [01:51&lt;02:06,  2.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/1000 [01:52&lt;01:40,  2.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/1000 [01:52&lt;01:21,  3.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 718/1000 [01:52&lt;01:08,  4.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/1000 [01:52&lt;00:59,  4.72it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/1000 [01:52&lt;00:53,  5.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/1000 [01:52&lt;00:49,  5.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/1000 [01:52&lt;00:46,  5.99it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 723/1000 [01:53&lt;00:43,  6.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 724/1000 [01:53&lt;00:41,  6.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/1000 [01:53&lt;00:40,  6.79it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/1000 [01:53&lt;00:39,  6.88it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 727/1000 [01:53&lt;00:38,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/1000 [01:53&lt;00:37,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 729/1000 [01:53&lt;00:37,  7.15it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 730/1000 [01:54&lt;00:37,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/1000 [01:54&lt;00:37,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/1000 [01:54&lt;00:37,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/1000 [01:54&lt;00:37,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 734/1000 [01:54&lt;00:37,  7.03it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 735/1000 [01:54&lt;00:38,  6.93it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/1000 [01:54&lt;00:37,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 737/1000 [01:55&lt;00:37,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 738/1000 [01:55&lt;00:36,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739/1000 [01:55&lt;00:36,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/1000 [01:55&lt;00:36,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 741/1000 [01:55&lt;00:36,  7.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 742/1000 [01:55&lt;00:36,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 743/1000 [01:55&lt;00:36,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/1000 [01:55&lt;00:35,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/1000 [01:56&lt;00:35,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/1000 [01:56&lt;00:35,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 747/1000 [01:56&lt;00:35,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/1000 [01:56&lt;00:35,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 749/1000 [01:56&lt;00:34,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750/1000 [01:56&lt;00:34,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/1000 [01:56&lt;00:34,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/1000 [01:57&lt;00:34,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 753/1000 [01:57&lt;00:34,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/1000 [01:57&lt;00:34,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/1000 [01:57&lt;00:33,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 756/1000 [01:57&lt;00:33,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/1000 [01:57&lt;00:33,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 758/1000 [01:57&lt;00:33,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 759/1000 [01:58&lt;00:33,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 760/1000 [01:58&lt;00:33,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/1000 [01:58&lt;00:33,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 762/1000 [01:58&lt;00:33,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 763/1000 [01:58&lt;00:32,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/1000 [01:58&lt;00:31,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/1000 [01:58&lt;00:31,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/1000 [01:59&lt;00:31,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/1000 [01:59&lt;00:31,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/1000 [01:59&lt;00:31,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/1000 [01:59&lt;00:31,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 770/1000 [01:59&lt;00:31,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/1000 [01:59&lt;00:31,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 772/1000 [01:59&lt;00:31,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 773/1000 [01:59&lt;00:31,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/1000 [02:00&lt;00:31,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/1000 [02:00&lt;00:31,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 776/1000 [02:00&lt;00:31,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 777/1000 [02:00&lt;00:30,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/1000 [02:00&lt;00:30,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/1000 [02:00&lt;00:29,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 780/1000 [02:00&lt;00:33,  6.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/1000 [02:01&lt;00:34,  6.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 782/1000 [02:01&lt;00:32,  6.73it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/1000 [02:01&lt;00:31,  6.89it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/1000 [02:01&lt;00:30,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 785/1000 [02:01&lt;00:29,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 786/1000 [02:01&lt;00:29,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 787/1000 [02:01&lt;00:28,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/1000 [02:02&lt;00:28,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/1000 [02:02&lt;00:28,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 790/1000 [02:02&lt;00:28,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 791/1000 [02:02&lt;00:28,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 792/1000 [02:02&lt;00:27,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 793/1000 [02:02&lt;00:27,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 794/1000 [02:02&lt;00:27,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/1000 [02:03&lt;00:27,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/1000 [02:03&lt;00:27,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 797/1000 [02:03&lt;00:27,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 798/1000 [02:03&lt;00:27,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [02:03&lt;00:27,  7.39it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/1000 [02:03&lt;00:27,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/1000 [02:03&lt;00:26,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/1000 [02:03&lt;00:26,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/1000 [02:04&lt;00:26,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/1000 [02:04&lt;00:26,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/1000 [02:04&lt;00:26,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 806/1000 [02:04&lt;00:25,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 807/1000 [02:04&lt;00:25,  7.45it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 808/1000 [02:04&lt;00:25,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 809/1000 [02:04&lt;00:25,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/1000 [02:05&lt;00:25,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 811/1000 [02:05&lt;00:25,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 812/1000 [02:05&lt;00:25,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 813/1000 [02:05&lt;00:25,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/1000 [02:05&lt;00:24,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/1000 [02:05&lt;00:25,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/1000 [02:05&lt;00:26,  6.92it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/1000 [02:06&lt;00:26,  7.02it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/1000 [02:06&lt;00:25,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 819/1000 [02:06&lt;00:24,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/1000 [02:06&lt;00:24,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 821/1000 [02:06&lt;00:24,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/1000 [02:06&lt;00:24,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/1000 [02:06&lt;00:24,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 824/1000 [02:06&lt;00:24,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/1000 [02:07&lt;00:24,  7.27it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 826/1000 [02:07&lt;00:24,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/1000 [02:07&lt;00:23,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/1000 [02:07&lt;00:23,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 829/1000 [02:07&lt;00:23,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 830/1000 [02:07&lt;00:23,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/1000 [02:07&lt;00:23,  7.12it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/1000 [02:08&lt;00:23,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 833/1000 [02:08&lt;00:23,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/1000 [02:08&lt;00:22,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/1000 [02:08&lt;00:22,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/1000 [02:08&lt;00:22,  7.20it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 837/1000 [02:08&lt;00:22,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 838/1000 [02:08&lt;00:22,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 839/1000 [02:09&lt;00:21,  7.42it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/1000 [02:09&lt;00:22,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/1000 [02:09&lt;00:21,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/1000 [02:09&lt;00:21,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/1000 [02:09&lt;00:21,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 844/1000 [02:09&lt;00:21,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 845/1000 [02:09&lt;00:21,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/1000 [02:10&lt;00:21,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 847/1000 [02:10&lt;00:21,  7.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/1000 [02:10&lt;00:21,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/1000 [02:10&lt;00:21,  7.11it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 850/1000 [02:10&lt;00:20,  7.15it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/1000 [02:10&lt;00:20,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 852/1000 [02:10&lt;00:20,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 853/1000 [02:11&lt;00:20,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 854/1000 [02:11&lt;00:20,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/1000 [02:11&lt;00:20,  7.13it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 856/1000 [02:11&lt;00:20,  7.09it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 857/1000 [02:11&lt;00:20,  6.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 858/1000 [02:11&lt;00:20,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 859/1000 [02:11&lt;00:19,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [02:11&lt;00:19,  7.15it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 861/1000 [02:12&lt;00:19,  7.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 862/1000 [02:12&lt;00:19,  7.17it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/1000 [02:12&lt;00:19,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 864/1000 [02:12&lt;00:19,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/1000 [02:12&lt;00:18,  7.23it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 866/1000 [02:12&lt;00:18,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/1000 [02:12&lt;00:17,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 868/1000 [02:13&lt;00:18,  7.21it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 869/1000 [02:13&lt;00:18,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/1000 [02:13&lt;00:18,  6.95it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 871/1000 [02:13&lt;00:18,  7.06it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/1000 [02:13&lt;00:17,  7.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 873/1000 [02:13&lt;00:17,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/1000 [02:13&lt;00:17,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/1000 [02:14&lt;00:17,  7.28it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 876/1000 [02:14&lt;00:16,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877/1000 [02:14&lt;00:16,  7.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/1000 [02:14&lt;00:16,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 879/1000 [02:14&lt;00:16,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 880/1000 [02:14&lt;00:16,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/1000 [02:14&lt;00:15,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 882/1000 [02:15&lt;00:15,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/1000 [02:15&lt;00:15,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 884/1000 [02:15&lt;00:15,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 885/1000 [02:15&lt;00:15,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/1000 [02:15&lt;00:14,  7.64it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 887/1000 [02:15&lt;00:14,  7.56it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/1000 [02:15&lt;00:14,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 889/1000 [02:15&lt;00:14,  7.55it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 890/1000 [02:16&lt;00:14,  7.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/1000 [02:16&lt;00:14,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 892/1000 [02:16&lt;00:14,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 893/1000 [02:16&lt;00:14,  7.51it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/1000 [02:16&lt;00:14,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/1000 [02:16&lt;00:13,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/1000 [02:16&lt;00:14,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 897/1000 [02:17&lt;00:14,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/1000 [02:17&lt;00:13,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [02:17&lt;00:13,  7.30it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/1000 [02:17&lt;00:13,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/1000 [02:17&lt;00:13,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/1000 [02:17&lt;00:13,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 903/1000 [02:17&lt;00:13,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 904/1000 [02:17&lt;00:12,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/1000 [02:18&lt;00:12,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/1000 [02:18&lt;00:12,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 907/1000 [02:18&lt;00:12,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 908/1000 [02:18&lt;00:11,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/1000 [02:18&lt;00:11,  7.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 910/1000 [02:18&lt;00:11,  7.68it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/1000 [02:18&lt;00:11,  7.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 912/1000 [02:19&lt;00:11,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/1000 [02:19&lt;00:11,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 914/1000 [02:19&lt;00:11,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 915/1000 [02:19&lt;00:11,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/1000 [02:19&lt;00:11,  7.41it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/1000 [02:19&lt;00:11,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 918/1000 [02:19&lt;00:10,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/1000 [02:19&lt;00:10,  7.53it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 920/1000 [02:20&lt;00:10,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 921/1000 [02:20&lt;00:10,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/1000 [02:20&lt;00:10,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/1000 [02:20&lt;00:10,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 924/1000 [02:20&lt;00:10,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 925/1000 [02:20&lt;00:10,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/1000 [02:20&lt;00:09,  7.47it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/1000 [02:21&lt;00:09,  7.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/1000 [02:21&lt;00:09,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 929/1000 [02:21&lt;00:09,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 930/1000 [02:21&lt;00:10,  6.67it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 931/1000 [02:21&lt;00:10,  6.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/1000 [02:21&lt;00:09,  6.90it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 933/1000 [02:21&lt;00:09,  6.96it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 934/1000 [02:22&lt;00:09,  7.00it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 935/1000 [02:22&lt;00:09,  7.05it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/1000 [02:22&lt;00:09,  7.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 937/1000 [02:22&lt;00:08,  7.08it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/1000 [02:22&lt;00:08,  7.06it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/1000 [02:22&lt;00:08,  7.10it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 940/1000 [02:22&lt;00:08,  7.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/1000 [02:23&lt;00:08,  7.18it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/1000 [02:23&lt;00:09,  6.38it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 943/1000 [02:23&lt;00:08,  6.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/1000 [02:23&lt;00:08,  6.63it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 945/1000 [02:26&lt;00:56,  1.03s/it, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/1000 [02:26&lt;00:41,  1.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/1000 [02:26&lt;00:30,  1.74it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 948/1000 [02:27&lt;00:23,  2.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 949/1000 [02:27&lt;00:18,  2.83it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/1000 [02:27&lt;00:14,  3.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/1000 [02:27&lt;00:11,  4.16it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 952/1000 [02:27&lt;00:09,  4.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/1000 [02:27&lt;00:09,  5.14it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/1000 [02:27&lt;00:08,  5.57it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 955/1000 [02:27&lt;00:07,  6.04it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 956/1000 [02:28&lt;00:06,  6.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/1000 [02:28&lt;00:06,  6.37it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/1000 [02:28&lt;00:06,  6.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 959/1000 [02:28&lt;00:06,  6.81it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 960/1000 [02:28&lt;00:05,  7.05it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/1000 [02:28&lt;00:05,  7.07it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 962/1000 [02:28&lt;00:05,  7.19it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/1000 [02:29&lt;00:05,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964/1000 [02:29&lt;00:04,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/1000 [02:29&lt;00:04,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 966/1000 [02:29&lt;00:04,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 967/1000 [02:29&lt;00:04,  7.24it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 968/1000 [02:29&lt;00:04,  7.29it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/1000 [02:29&lt;00:04,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 970/1000 [02:30&lt;00:04,  7.46it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 971/1000 [02:30&lt;00:03,  7.52it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/1000 [02:30&lt;00:03,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/1000 [02:30&lt;00:03,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 974/1000 [02:30&lt;00:03,  7.32it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/1000 [02:30&lt;00:03,  7.31it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 976/1000 [02:30&lt;00:03,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/1000 [02:31&lt;00:03,  7.33it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 978/1000 [02:31&lt;00:03,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/1000 [02:31&lt;00:02,  7.25it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 980/1000 [02:31&lt;00:02,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/1000 [02:31&lt;00:02,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 982/1000 [02:31&lt;00:02,  7.44it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 983/1000 [02:31&lt;00:02,  7.50it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 984/1000 [02:31&lt;00:02,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/1000 [02:32&lt;00:01,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/1000 [02:32&lt;00:01,  7.62it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 987/1000 [02:32&lt;00:01,  7.66it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 988/1000 [02:32&lt;00:01,  7.65it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 989/1000 [02:32&lt;00:01,  7.59it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 990/1000 [02:32&lt;00:01,  7.58it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/1000 [02:32&lt;00:01,  7.61it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 992/1000 [02:32&lt;00:01,  7.43it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/1000 [02:33&lt;00:00,  7.22it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/1000 [02:33&lt;00:00,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 995/1000 [02:33&lt;00:00,  7.36it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/1000 [02:33&lt;00:00,  7.40it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 997/1000 [02:33&lt;00:00,  7.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/1000 [02:33&lt;00:00,  7.34it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [02:33&lt;00:00,  7.35it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:34&lt;00:00,  7.26it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]sample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:34&lt;00:00,  6.49it/s, 1023 steps of size 8.69e-289. acc. prob=0.00]\n\n\n`",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#mathematical-details",
    "href": "23. Network with block model.html#mathematical-details",
    "title": "Stochastic Block Models",
    "section": "Mathematical Details",
    "text": "Mathematical Details\n\nMain Formula\nThe model‚Äôs block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the Network model chapter:\n\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\n\n\\log(Y_{ij}) = B_{k(i), k(j)}\n\nwhere:\n\nB is a matrix of intercept parameters unique to the interaction of categories. For example, if there are three groups, then B will be a 3x3 matrix where each element give the rate an individual in group k interacting with an individual in group l.\nWe use the function k, to return the group identity (i.e., the block) of individual i.\n\n\n\nDefining formula sub-equations and prior distributions\nTo account for all link rates between categories, we can define a square matrix B as follows: the off-diagonal elements represent the link rates between categories i and j, while the diagonal elements represent the link rates within category i.\n\nB_{i,j} =\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j}\n\\end{bmatrix}\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks. Basic priors could be:\n\na_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n\n\na_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n\nwhere:\n\nk \\rightarrow k indicates a diagonal element.\nk \\rightarrow \\tilde{k} indicates an off-diagonal element.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#notes",
    "href": "23. Network with block model.html#notes",
    "title": "Stochastic Block Models",
    "section": "Note(s)",
    "text": "Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nBy defining this block model within our network model, we are estimating assortativity üõà and disassortativity üõà for categorical variables.\nSimilarly, for continuous variables, we can generate a block model that includes all continuous variables.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "23. Network with block model.html#references",
    "href": "23. Network with block model.html#references",
    "title": "Stochastic Block Models",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nRoss, Cody T, Richard McElreath, and Daniel Redhead. 2024. ‚ÄúModelling Animal Network Data in r Using STRAND.‚Äù Journal of Animal Ecology 93 (3): 254‚Äì66.",
    "crumbs": [
      "Models",
      "Stochastic Block Models"
    ]
  },
  {
    "objectID": "25. Network Metrics.html",
    "href": "25. Network Metrics.html",
    "title": "Network Metrics",
    "section": "",
    "text": "This overview is from Sosa, Sueur, and Puga-Gonzalez (2021).",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#general-principles",
    "href": "25. Network Metrics.html#general-principles",
    "title": "Network Metrics",
    "section": "General Principles",
    "text": "General Principles\nNetwork metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#nodal-metrics",
    "href": "25. Network Metrics.html#nodal-metrics",
    "title": "Network Metrics",
    "section": "Nodal metrics",
    "text": "Nodal metrics\nNodal metrics* enable the assessment of nodes‚Äô social heterogeneity and the understanding of underlying mechanisms such as individual characteristics (e.g., the ageing process), ecological factors (e.g., demographic variation), and evolutionary processes (e.g., differences in social styles). Node measures are calculated at a nodal level and assess, in different ways and with different meanings, how an individual is connected. Connections can be ego‚Äôs* direct links only (e.g., degree, strength), its alters‚Äô* links as well (e.g., eigenvector, clustering coefficient), or even all the links in the network (e.g., betweenness). Node measures can also be used to describe the overall network structure through distributions, means, and coefficients of variation.\n\nDegree and strength\nThe degree m.net.degree measures the number of links of a node. When computed on an undirected network, the degree represents the number of alters of an ego. When the network is directed, it represents the number of either incoming or outgoing* links of an ego, and it is then called in-degree m.net.indegree or out-degree m.net.outdegree, respectively. Note that degree can also be computed in directed networks; in this case, it represents the sum of incoming and outgoing links and not the number of alters.\n\nD_i = \\sum_{j=1}^N a_{ij}\n\nWhere a_{ij} is the value of the link between nodes i and j. Isolated node(s) can be considered as zero(s).\nStrength (or weighted degree) m.net.strength is the sum of the links‚Äô weights in a weighted network*. When the network comprises directed links, then it is also possible to differentiate between in-strength m.net.instrength (the sum of weights of incoming links) and out-strength m.net.outstrength (the sum of weights of outgoing links). While degree and strength can be considered correlated, it may not always be the case, as individuals can interact frequently with a few social partners or vice versa (Liao, Sosa, Wu, & Zhang, 2018). Therefore, it is necessary to test their correlation prior to the analysis.\n\nS_i = \\sum_{j=1}^N a_{ij} w_{ij}\n\nWhere a_{ij} is the value of the link between nodes i and j. Isolated node(s) can be considered as zero(s).\n\n\nEigenvector centrality\nEigenvector centrality m.net.eigenvector is the first non-negative eigenvector value obtained by transforming an adjacency matrix linearly. It can be computed on weighted, binary, directed, or undirected networks. It measures centrality by examining the connectedness of an ego as well as that of its alters. Thus, a node‚Äôs eigenvector value can be linked either to its own degree or strength or to the degrees or strengths of the nodes to which it is connected. Eigenvector may be interpreted as the social support or social capital of an individual (Brent, Semple, Dubuc, Heistermann, & MacLarnon, 2011), that is, the real or perceived availability of social resources.\n\n\\lambda c = W c\n\nWhere \\lambda is the largest eigenvalue of the adjacency matrix W. Isolated node(s) can be considered as zero(s).\n\n\nLocal clustering coefficient\nThe local clustering coefficient m.net.cc measures the number of closed triplets* over the total theoretical number of triplets (i.e., open and closed), where a triplet is a set of three nodes that are connected by either two (open triplet) or three (closed triplet) edges. This measure aims to examine the links that may exist between the alters of an ego and measures the cohesion of the network. The main topological effect of closed triplets is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity (see corresponding section). The local clustering coefficient can be computed in a binary network by measuring the proportion of links between the nodes of an ego-network* divided by the number of potential links between them. In weighted networks, several versions exist, such as those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or Opsahl and Panzarasa (2009).\n\nBinary Local Clustering Coefficient\n\nC_i^b = \\frac{2L}{N_i (N_i - 1)}\n Where L is the number of links in the ego-network of node i.\n\n\nBarrat‚Äôs Local Clustering Coefficient\n\nC_i^W = \\frac{1}{S_i (D_i - 1)} \\sum_{j \\neq h \\in N} \\frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}\n\nWhere S_i and D_i are the strength and the degree of node i, respectively. w_{ij} and w_{ih} are the weights of the links, and a_{ij}, a_{ih}, a_{jh} are the links between the nodes.\n\n\nOpsahl‚Äôs Local Clustering Coefficient\n\nC^W(G) = \\frac{\\sum_{\\tau_\\Delta} w}{\\sum_\\tau w}\n Where \\tau_\\Delta represents closed triplets, and w is the chosen weighting scheme (maximum, minimum, arithmetic, or geometric mean).\n\n\n\nBetweenness\nBetweenness (WIP) is the number of times a node is included in the shortest paths (geodesic distances) generated by every combination of two nodes. The value of the betweenness indicates the theoretical role of a node in social transmission (information, disease, etc., see Figure 1), as it indicates to what extent a node connects subgroups, as a bridge, and thus is likely to spread an entity across the whole network (Newman, 2005).\n\nb = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\nWhere \\sigma_{st} is the total number of shortest paths from node s to node t, and \\sigma_{st}(v) is the number of those paths that pass through v. As no paths go through isolated nodes, their betweenness value can be considered zero.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#polyadic-metrics",
    "href": "25. Network Metrics.html#polyadic-metrics",
    "title": "Network Metrics",
    "section": "Polyadic metrics",
    "text": "Polyadic metrics\nPatterns of interactions (how and with whom individuals interact) can be examined using specific network measures* that analyse local-scale interactions within a network and make it possible to test hypotheses about the mechanisms underlying network connectivity. These types of measures are generally used to test mechanistic biological questions, such as what factors (e.g., ecological as well as sociodemographic) affect individuals‚Äô interactions/associations.\n\nAssortativity\nAssortativity (Newman, 2003) (WIP) is probably the most used measure to study homophily (preferential associations or interactions among individuals sharing the same characteristics; Lazarsfeld & Merton, 1954). Assortativity values range from ‚àí1 (total disassortativity, i.e., all the nodes associate or interact with those with the opposite characteristic, such as males interacting exclusively with females) to 1 (total assortativity, i.e., all the nodes associate or interact with those with the same characteristic, such as males interacting only with males). The assortativity coefficient measures the proportion of links between and within clusters of nodes with the same characteristics. Individuals‚Äô characteristics can be continuous (e.g., age, individual network measure, personality) or categorical features (e.g., sex, matriline belonging; Figure 2). Assortativity does not consider directionality* and can be measured in weighted (Leung & Chau, 2007) or binary (Newman, 2003) networks using categorical or continuous characteristics (Figure 2). The use of one or the other assortativity variant depends on the type of characteristics being examined and, whenever possible, the weighted version should be preferred since it is more reliable than the binary version (Farine, 2014).\n\nBinary Assortativity\n\nr = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1 - \\sum_i a_i b_i}\n\nWhere e_{ii} is the proportion of specific links, a_i is the proportion of outgoing links, and b_i is the proportion of incoming links.\n\n\nWeighted Continuous Assortativity\n\nr = \\frac{\\sum_i e_{ii}^w - \\sum_i a_i^w b_i^w}{1 - \\sum_i a_i^w b_i^w}\n Where e_{ii}^w is the proportion of weighted links, and a_i^w, b_i^w are the proportions of weighted outgoing and incoming links.\n\n\n\nTransitive triplets\nTransitive triplets (WIP) are closed triplets where the links among the nodes follow a specific temporal pattern of creation, that is, when the establishment of links between nodes A and B and between nodes A and C is followed by the establishment of a link between nodes B and C. This network measure can be computed in directed, binary, or weighted networks. These types of connections can be studied over time based on the creation of links. From a static perspective, directionality can be considered by calculating the number of transitive triplets divided by the number of potential transitive triplets, and weights can also be considered by using Opsahl‚Äôs variants, which are discussed in the section on local clustering coefficient (Opsahl & Panzarasa, 2009). While transitivity is importantly related to the clustering coefficient (the clustering coefficient includes transitive triplets), not all closed triplets are transitive. Transitive triplets are one of the 16 possible configurations of a triplet considering open and closed triplets as well as link directionality (i.e., triad census).",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#global-metrics",
    "href": "25. Network Metrics.html#global-metrics",
    "title": "Network Metrics",
    "section": "Global metrics",
    "text": "Global metrics\nThe structure of this section is based on the distinction between network connectivity and social diffusion (information or disease spread). However, the social diffusion section contains measures specifically designed to study theoretical (i.e., considering the diffusion is perfectly related to network links and link weights) social diffusion features based on geodesic distances (see corresponding section). Aspects of the structure and properties of a group (e.g., cohesion, sub-grouping) can be quantified using global network measures. For instance, one may quantify properties such as network resilience (see Diameter), network clustering* (see Modularity) through network connectivity analysis, or network transmission efficiency* (see Global efficiency) through network theoretical social diffusion analysis.\n\nDensity\nThe density m.net.density is the ratio of existing links to all potential links in a network. This measure is easy to interpret; it assesses how fully connected a network is. Density considers neither directionality nor link weights.\n\nD = \\frac{2|L|}{|N|(|N| - 1)}\n\nWhere L is the number of links and N is the number of nodes. Isolated node(s) can be considered as zero(s).\n\n\nGeodesic Distance\nGeodesic distance m.net.geodesic_distance is the shortest path considering all potential dyads in a network. This measure thereby indicates the fastest path of diffusion. Geodesic distance can be calculated in binary, weighted*, directed, or undirected networks. In weighted networks, it can be normalized (by dividing all links by the network‚Äôs mean weight), and the strongest or the weakest links can be considered as the fastest route between two nodes. This great number of variants of geodesic distance can greatly affect the results and interpretations. Researchers must thus have knowledge of the variants and know which one is the most appropriate according to their research question (Opsahl, Agneessens, & Skvoretz, 2010).\nThe computation uses algorithms like breadth-first search, depth-first search, or Dijkstra‚Äôs algorithm. None handle isolated nodes.\n\n\nDiameter\nThe diameter m.net.diameter of a network represents the longest of the shortest paths in the network. The diameter is used in ASNA to examine aspects such as network cohesion and the rapidness of information or disease transmission. While global efficiency measures the theoretical social diffusion spread, diameter informs on the maximum path length of diffusion required to reach all nodes.\n\n\nGlobal efficiency\nGlobal efficiency (WIP) is the ratio between the number of individuals and the number of connections multiplied by the network diameter. It provides a quantitative measure of how efficiently information is exchanged among the nodes of the network. As global efficiency gives a probability of social diffusion, it may help to better understand social transmission phenomena in the short and long term (Migliano et al., 2017). Pasquaretta et al.¬†(2014) found a positive correlation between the neocortex ratio and global efficiency in primate species with a higher neocortex ratio. By drawing a parallel between cognitive capacities and social network efficiency, this study showed that in species with a higher neocortex ratio, individuals may adjust their social relationships to gain better access to social information and thus optimize network efficiency. Alternatively, studies on epidemiology in ant colonies showed that ants adapt their interaction rate to decrease network efficiency when infected by a pathogen (Stroeymeyt et al., 2018).\n\n\nModularity\nModularity (WIP) is a measure designed to quantify the degree to which a network can be divided into different groups or clusters, and its value ranges from 0 to 1. Networks with high modularity have dense connections within the modules but sparse connections between them. Modularity can be computed in weighted, binary, directed, or undirected networks.\n\nQ = \\sum_{s=1}^m \\left[ \\frac{l_s}{|E|} - \\left(\\frac{d_s}{2|E|}\\right)^2 \\right]\n\nWhere l_s is the number of edges in the s-th community, and d_s is the sum of the degrees of the nodes in the community.\n\n\nGlobal Clustering Coefficient\nThe global clustering coefficient (WIP), like the local clustering coefficient, evaluates how well the alters of an ego are interconnected and measures the cohesion of the network. Its main topological effect is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity. However, it becomes highly correlated with density and less so with modularity as density grows. Several variants of the global clustering coefficient can be found: (a) the ratio of closed triplets to all triplets (open and closed), and (b) the binary local mean clustering coefficient derived from the node level (see Local clustering coefficient). The binary local mean clustering coefficient allows us to consider node heterogeneity and thus should be preferred over the first variant. Weighted versions also exist and are based on the same variants described in the section on the local clustering coefficient and require the same considerations.\n\nC^b(G) = \\frac{\\sum \\tau_\\Delta}{\\sum \\tau}\n\nWhere \\tau is the total number of triplets and \\tau_\\Delta represents closed triplets.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#references",
    "href": "25. Network Metrics.html#references",
    "title": "Network Metrics",
    "section": "Reference(s)",
    "text": "Reference(s)\n\n\nSosa, Sebastian, C√©dric Sueur, and Ivan Puga-Gonzalez. 2021. ‚ÄúNetwork Measures in Animal Social Network Analysis: Their Strengths, Limits, Interpretations and Uses.‚Äù Methods in Ecology and Evolution 12 (1): 10‚Äì21. https://doi.org/https://doi.org/10.1111/2041-210X.13366.",
    "crumbs": [
      "Models",
      "Network Metrics"
    ]
  }
]