% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\usepackage{amsmath}
\usepackage{amssymb}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Gaussian Mixture Models},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Gaussian Mixture Models}
\author{}
\date{}

\begin{document}
\maketitle


\subsection{General Principles}\label{general-principles}

To discover group structures or clusters in data, we can use a
\textbf{Gaussian Mixture Model (GMM)}. This is a parametric clustering
method. A GMM assumes that the data is generated from a mixture of a
\textbf{pre-specified number (\texttt{K})} of different Gaussian
distributions. The model's goal is to figure out:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The properties of each of the \texttt{K} clusters}: For each
  of the \texttt{K} clusters, it estimates its center (mean \(\mu\)) and
  its shape/spread (covariance \(\Sigma\)).
\item
  \textbf{The mixture weights}: It estimates the proportion of the data
  that belongs to each cluster.
\item
  \textbf{The assignment of each data point}: It determines the
  probability of each data point belonging to each of the \(K\)
  clusters.
\end{enumerate}

\subsection{Considerations}\label{considerations}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, colbacktitle=quarto-callout-caution-color!10!white, left=2mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, bottomtitle=1mm, coltitle=black, colback=white, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, breakable, toptitle=1mm, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]

\begin{itemize}
\item
  A GMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its
  parameters, \emph{except for the number of clusters, \(K\)}, which
  must be fixed in advance.
\item
  The key parameters and their priors are:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Number of Clusters \(K\)}: This is a \textbf{fixed
    hyperparameter} that you must choose before running the model.
    Choosing the right \texttt{K} often involves running the model
    multiple times and using model comparison criteria (like
    cross-validation, AIC, or BIC).
  \item
    \textbf{Cluster Weights \texttt{w}}: These are the probabilities of
    drawing a data point from any given cluster. Since there are a fixed
    number \texttt{K} of them and they must sum to 1, they are typically
    given a \texttt{Dirichlet} prior. A symmetric \texttt{Dirichlet}
    prior (e.g., \texttt{Dirichlet(1,\ 1,\ ...,\ 1)}) represents an
    initial belief that all clusters are equally likely.
  \item
    **Cluster Parameters (\(\mu\), \(\Sigma\)): Each of the \texttt{K}
    clusters has a mean \(\mu\) and a covariance matrix \(\Sigma\). We
    place priors on these to define our beliefs about their plausible
    values.
  \end{itemize}
\item
  Like the DPMM, the model is often implemented in its marginalized form
  . Instead of explicitly assigning each data point to a cluster, we
  integrate out this choice. This creates a smoother probability surface
  for the inference algorithm to explore, leading to much more efficient
  computation.
\item
  To increase accuracy we run a k-means algorithm to initialize the
  cluster mean priors.
\end{itemize}

\end{tcolorbox}

\subsection{Example}\label{example}

Below is an example of a GMM implemented in BI. The goal is to cluster a
synthetic dataset into a pre-specified K=4 groups.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ BI }\ImportTok{import}\NormalTok{ bi, jnp}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ make\_blobs}

\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}
\CommentTok{\# Generate synthetic data}
\NormalTok{data, true\_labels }\OperatorTok{=}\NormalTok{ make\_blobs(}
\NormalTok{    n\_samples}\OperatorTok{=}\DecValTok{500}\NormalTok{, centers}\OperatorTok{=}\DecValTok{8}\NormalTok{, cluster\_std}\OperatorTok{=}\FloatTok{0.8}\NormalTok{,}
\NormalTok{    center\_box}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{), random\_state}\OperatorTok{=}\DecValTok{101}
\NormalTok{)}


\CommentTok{\#  The model}
\KeywordTok{def}\NormalTok{ gmm(data, K, initial\_means): }\CommentTok{\# Here K is the *exact* number of clusters}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ data.shape[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# Number of features}
\NormalTok{    alpha\_prior }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ jnp.ones(K)}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ m.dist.dirichlet(concentration}\OperatorTok{=}\NormalTok{alpha\_prior, name}\OperatorTok{=}\StringTok{\textquotesingle{}weights\textquotesingle{}}\NormalTok{) }

    \ControlFlowTok{with}\NormalTok{ m.dist.plate(}\StringTok{"components"}\NormalTok{, K): }\CommentTok{\# Use fixed K}
\NormalTok{        mu }\OperatorTok{=}\NormalTok{ m.dist.multivariate\_normal(loc}\OperatorTok{=}\NormalTok{initial\_means, covariance\_matrix}\OperatorTok{=}\FloatTok{0.1}\OperatorTok{*}\NormalTok{jnp.eye(D), name}\OperatorTok{=}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{)        }
\NormalTok{        sigma }\OperatorTok{=}\NormalTok{ m.dist.half\_cauchy(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(D,), event}\OperatorTok{=}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{        Lcorr }\OperatorTok{=}\NormalTok{ m.dist.lkj\_cholesky(dimension}\OperatorTok{=}\NormalTok{D, concentration}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}Lcorr\textquotesingle{}}\NormalTok{)}

\NormalTok{        scale\_tril }\OperatorTok{=}\NormalTok{ sigma[..., }\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ Lcorr}

\NormalTok{    m.dist.mixture\_same\_family(}
\NormalTok{        mixing\_distribution}\OperatorTok{=}\NormalTok{m.dist.categorical(probs}\OperatorTok{=}\NormalTok{w, create\_obj}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{        component\_distribution}\OperatorTok{=}\NormalTok{m.dist.multivariate\_normal(loc}\OperatorTok{=}\NormalTok{mu, scale\_tril}\OperatorTok{=}\NormalTok{scale\_tril, create\_obj}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{        name}\OperatorTok{=}\StringTok{"obs"}\NormalTok{,}
\NormalTok{        obs}\OperatorTok{=}\NormalTok{data}
\NormalTok{    )}

\CommentTok{\# Kmeans clustering do initiate the means}
\NormalTok{m.ml.KMEANS(data, n\_clusters}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=}\NormalTok{ \{}\StringTok{"data"}\NormalTok{: data,}\StringTok{"K"}\NormalTok{: }\DecValTok{8}\NormalTok{ \}}
\NormalTok{m.data\_on\_model[}\StringTok{\textquotesingle{}initial\_means\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.ml.results[}\StringTok{\textquotesingle{}centroids\textquotesingle{}}\NormalTok{]}


\NormalTok{m.fit(gmm) }\CommentTok{\# Optimize model parameters through MCMC sampling}
\NormalTok{m.plot(X}\OperatorTok{=}\NormalTok{data,sampler}\OperatorTok{=}\NormalTok{m.sampler) }\CommentTok{\# Prebuild plot function for GMM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
jax.local_device_count 32
\end{verbatim}

\begin{verbatim}
  0%|          | 0/1000 [00:00<?, ?it/s]warmup:   0%|          | 1/1000 [00:00<16:10,  1.03it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   4%|â–         | 42/1000 [00:01<00:17, 53.22it/s, 63 steps of size 4.18e-02. acc. prob=0.75]warmup:  10%|â–ˆ         | 101/1000 [00:01<00:06, 133.19it/s, 127 steps of size 4.73e-01. acc. prob=0.77]warmup:  15%|â–ˆâ–        | 149/1000 [00:01<00:04, 194.68it/s, 15 steps of size 6.16e-01. acc. prob=0.78] warmup:  20%|â–ˆâ–ˆ        | 202/1000 [00:01<00:03, 261.57it/s, 15 steps of size 4.38e-01. acc. prob=0.78]warmup:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:01<00:02, 330.37it/s, 3 steps of size 7.85e-02. acc. prob=0.78] warmup:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:01<00:01, 358.63it/s, 15 steps of size 6.22e-01. acc. prob=0.78]warmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [00:01<00:01, 453.33it/s, 7 steps of size 5.77e-01. acc. prob=0.79] warmup:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [00:01<00:01, 520.25it/s, 7 steps of size 5.21e-01. acc. prob=0.79]sample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [00:01<00:00, 524.97it/s, 7 steps of size 4.15e-01. acc. prob=0.89]sample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [00:02<00:00, 553.39it/s, 7 steps of size 4.15e-01. acc. prob=0.89]sample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [00:02<00:00, 571.51it/s, 15 steps of size 4.15e-01. acc. prob=0.89]sample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [00:02<00:00, 567.90it/s, 7 steps of size 4.15e-01. acc. prob=0.90] sample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [00:02<00:00, 589.65it/s, 23 steps of size 4.15e-01. acc. prob=0.89]sample:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [00:02<00:00, 595.75it/s, 15 steps of size 4.15e-01. acc. prob=0.89]sample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [00:02<00:00, 588.65it/s, 15 steps of size 4.15e-01. acc. prob=0.89]sample:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [00:02<00:00, 594.55it/s, 15 steps of size 4.15e-01. acc. prob=0.89]sample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 368.56it/s, 15 steps of size 4.15e-01. acc. prob=0.89]
\end{verbatim}

\begin{verbatim}
âš ï¸This function is still in development. Use it with caution. âš ï¸
âš ï¸This function is still in development. Use it with caution. âš ï¸
\end{verbatim}

\includegraphics{20. GMM_files/figure-pdf/cell-2-output-4.pdf}

\subsection{R}

\begin{center}
\includegraphics{travaux-routiers.png}
\end{center}

\subsection{Julia}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{@BI} \KeywordTok{function} \FunctionTok{gmm}\NormalTok{(data, K, initial\_means)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ data.shape[}\FloatTok{1}\NormalTok{] }
\NormalTok{    alpha\_prior }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ jnp.}\FunctionTok{ones}\NormalTok{(K)}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ m.dist.}\FunctionTok{dirichlet}\NormalTok{(concentration}\OperatorTok{=}\NormalTok{alpha\_prior, name}\OperatorTok{=}\StringTok{"weights"}\NormalTok{) }

    \CommentTok{\# We capture the output of the pywith block}
    \CommentTok{\# The block returns a tuple (mu, scale\_tril)}
\NormalTok{    mu, scale\_tril }\OperatorTok{=} \FunctionTok{pywith}\NormalTok{(m.dist.}\FunctionTok{plate}\NormalTok{(}\StringTok{"components"}\NormalTok{, K)) }\ControlFlowTok{do}\NormalTok{ \_}
\NormalTok{        mu\_inner }\OperatorTok{=}\NormalTok{ m.dist.}\FunctionTok{multivariate\_normal}\NormalTok{(}
\NormalTok{            loc}\OperatorTok{=}\NormalTok{initial\_means, }
\NormalTok{            covariance\_matrix}\OperatorTok{=}\FloatTok{0.1}\OperatorTok{*}\NormalTok{jnp.}\FunctionTok{eye}\NormalTok{(D), }
\NormalTok{            name}\OperatorTok{=}\StringTok{"mu"}
\NormalTok{        )        }
        
\NormalTok{        sigma }\OperatorTok{=}\NormalTok{ m.dist.}\FunctionTok{half\_cauchy}\NormalTok{(}\FloatTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(D,), event}\OperatorTok{=}\FloatTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{"sigma"}\NormalTok{)}
\NormalTok{        Lcorr }\OperatorTok{=}\NormalTok{ m.dist.}\FunctionTok{lkj\_cholesky}\NormalTok{(dimension}\OperatorTok{=}\NormalTok{D, concentration}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, name}\OperatorTok{=}\StringTok{"Lcorr"}\NormalTok{)}

        \CommentTok{\# FIX: Use expand\_dims instead of slicing}
\NormalTok{        scale\_tril\_inner }\OperatorTok{=}\NormalTok{ jnp.}\FunctionTok{expand\_dims}\NormalTok{(sigma, }\OperatorTok{{-}}\FloatTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ Lcorr}
        
        \CommentTok{\# Return them so they are available outside}
\NormalTok{        (mu\_inner, scale\_tril\_inner)}
    \ControlFlowTok{end}

\NormalTok{    m.dist.}\FunctionTok{mixture\_same\_family}\NormalTok{(}
\NormalTok{        mixing\_distribution}\OperatorTok{=}\NormalTok{m.dist.}\FunctionTok{categorical}\NormalTok{(probs}\OperatorTok{=}\NormalTok{w, create\_obj}\OperatorTok{=}\ConstantTok{true}\NormalTok{),}
\NormalTok{        component\_distribution}\OperatorTok{=}\NormalTok{m.dist.}\FunctionTok{multivariate\_normal}\NormalTok{(loc}\OperatorTok{=}\NormalTok{mu, scale\_tril}\OperatorTok{=}\NormalTok{scale\_tril, create\_obj}\OperatorTok{=}\ConstantTok{true}\NormalTok{),}
\NormalTok{        name}\OperatorTok{=}\StringTok{"obs"}\NormalTok{,}
\NormalTok{        obs}\OperatorTok{=}\NormalTok{data}
\NormalTok{    )}
\KeywordTok{end}

\CommentTok{\# Run}
\NormalTok{m.}\FunctionTok{fit}\NormalTok{(gmm)}
\NormalTok{m.}\FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Mathematical Details}\label{mathematical-details}

This section describes the generative process for a GMM.

\[
\begin{pmatrix}
Y_{i,1} \\
\vdots \\
Y_{i,D}
\end{pmatrix}
\sim 
\text{MVN}\left(
\begin{pmatrix}
\mu_{z_i,1} \\
\vdots \\
\mu_{z_i,D}
\end{pmatrix},
\Sigma_{z_i}
\right)
\]

\[
\begin{pmatrix}
\mu_{k,1} \\
\vdots \\
\mu_{k,D}
\end{pmatrix}
\sim 
\text{MVN}\left(
\begin{pmatrix}
A_{k,1} \\
\vdots \\
A_{k,D}
\end{pmatrix},
B
\right)
\]

\[
\Sigma_k = \text{Diag}(\sigma_k) \Omega_k  \text{Diag}(\sigma_k)
\]

\[
\sigma_{[k,d]} &\sim \text{HalfCauchy}(1) 
\]

\[
\Omega_k \sim \text{LKJ}(2) 
\]

\[
z_{i} \sim \text{Categorical}(\pi) 
\]

\[
\pi \sim \text{Dirichlet}(0.5, \dots, 0.5)
\]

Where :

\begin{itemize}
\item
  \(\begin{pmatrix} Y_{[i,1]} \\ \vdots \\ Y_{[i,D]} \end{pmatrix}\) is
  the \(i\)-th observation of a D-dimensional data array.
\item
  \(\begin{pmatrix}\mu_{[k,1]} \\ \vdots \\ \mu_{[k,D]}\end{pmatrix}\)
  is the \(k\)-th parameter vector of dimension D.
\item
  \(\begin{pmatrix} A_{[k,1]} \\ \vdots \\ A_{[k,D]} \end{pmatrix}\) is
  a prior for the \(k\)-th mean vector as derived by a \emph{KMEANS}
  clustering algorithm.
\item
  \(B\) is the prior covariance of the cluster means, and is setup as a
  diagonal matrix with 0.1 along the diagonal.
\item
  \(\Sigma_k\) is the DxD covariance matrix of the \(k\)-th cluster (it
  is composed from \(\sigma_k\) and \(\Omega_k\)).
\item
  \(\text{Diag}(\sigma_k)\) is a diagonal matrix whose diagonal entries
  are the standard deviations: \[
    \text{Diag}(\sigma_k) =
    \begin{pmatrix}
    \sigma_{[k,1]} & 0 & \cdots & 0 \\
    0 & \sigma_{[k,2]} &        & \vdots \\
    \vdots &        & \ddots & 0 \\
    0 & \cdots & 0 & \sigma_{[k,D]}
    \end{pmatrix}.
    \]
\item
  \(\sigma_{k}\) is a \(D\)-vector of standard deviations for the
  \(k\)-th cluster where each element, \(d\), has a half-cauchy prior.
\item
  \(\Omega_k\) is a correlation matrix for the \(k\)-th cluster.
\item
  \(z_i\) is a latent variable that maps observation \(i\) to cluster
  \(k\).
\item
  \(\pi\) is a vector of \(K\) cluster weights.
\end{itemize}

Where :

\begin{itemize}
\item
  \(\begin{pmatrix} Y_{[i,1]} \\ \vdots \\ Y_{[i,D]} \end{pmatrix}\) is
  the \(i\)-th observation of a D-dimensional data array.
\item
  \(\begin{pmatrix}\mu_{[k,1]} \\ \vdots \\ \mu_{[k,D]}\end{pmatrix}\)
  is the \(k\)-th parameter vector of dimension D.
\item
  \(\begin{pmatrix} A_{[k,1]} \\ \vdots \\ A_{[k,D]} \end{pmatrix}\) is
  a prior for the \(k\)-th mean vector as derived by a \emph{KMEANS}
  clustering algorithm.
\item
  \(B\) is the prior covariance of the cluster means, and is setup as a
  diagonal matrix with 0.1 along the diagonal.
\item
  \(\Sigma_k\) is the DxD covariance matrix of the \(k\)-th cluster (it
  is composed from \(\sigma_k\) and \(\Omega_k\)).
\item
  \(\sigma_k\) is a diagonal matrix of standard deviations for the
  \(k\)-th cluster.
\item
  \(\Omega_k\) is a correlation matrix for the \(k\)-th cluster.
\item
  \(z_i\) is a latent variable that maps observation \(i\) to cluster
  \(k\).
\item
  \(\pi\) is a vector of \(K\) cluster weights.
\end{itemize}

\subsection{Notes}\label{notes}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, colbacktitle=quarto-callout-note-color!10!white, left=2mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, bottomtitle=1mm, coltitle=black, colback=white, rightrule=.15mm, colframe=quarto-callout-note-color-frame, breakable, toptitle=1mm, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]

The primary challenge of the GMM compared to the DPMM is the need to
\textbf{manually specify the number of clusters \texttt{K}}. If the
chosen \texttt{K} is too small, the model may merge distinct clusters.
If \texttt{K} is too large, it may split natural clusters into
meaningless sub-groups. Therefore, applying a GMM often involves an
outer loop of model selection where one fits the model for a range of
\texttt{K} values and uses a scoring metric to select the best one.

\end{tcolorbox}

\subsection{Reference(s)}\label{references}

C. M. Bishop (2006). \emph{Pattern Recognition and Machine Learning}.
Springer. (Chapter 9)




\end{document}
