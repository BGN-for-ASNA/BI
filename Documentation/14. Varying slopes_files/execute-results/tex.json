{
  "hash": "208aeb47900070a5805d105e098ba0fe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Varying Slopes Models üöß\"\ndescription: \"Extending mixed-effects models by allowing the relationship (slope) to vary across different groups.\"\ncategories: [Hierarchical Models, Regression]\nimage: \"Figures/14.png\"\norder: 17\n---\n\n\n\n\n## General Principles\n\nTo model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a *varying slopes* model.\n\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.\n\n\n## Considerations\n\n::: callout-note\n-   We have the same considerations as for [12. Varying intercepts](13.%20Varying%20intercepts.qmd).\n\n-   The idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a [<span style=\"color:#0D6EFD\">matrix of covariance üõà</span>]{#MatCov}.\n\n-  To construct the covariance matrix, we use an *SRS* decomposition where *S* is a diagonal matrix of standard deviations and *R* is a correlation matrix. To model the correlation matrix, we use an $LKJcorr$ distribution parametrized with a single control parameter $Œ∑$ that controls the amount of regularization. $Œ∑$ is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near ‚àí1 or 1. When we use $LKJcorr(1)$, the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\n\n-   The standard deviations in *S* are model with a prior that constrains them to strictly positive values.\n\n\n:::\n\n## Example\n\nBelow is an example code snippet demonstrating Bayesian regression with varying effects. This example is based on @mcelreath2018statistical.\n\n### Simulated data\n\n::: {.panel-tabset group=\"language\"}\n## Python (Raw)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multivariate_normal(only_path=True)\nm.data(data_path, sep=',') \nm.data_on_model = dict(\n    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n    N_cafes = len(m.df.cafe.unique()),\n    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n)\n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    Rho = m.dist.lkj(2, 2, name = 'Rho')\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariate_normal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<15:36,  1.07it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   7%|‚ñã         | 71/1000 [00:01<00:09, 93.46it/s, 15 steps of size 1.00e-01. acc. prob=0.77]\rwarmup:  15%|‚ñà‚ñç        | 149/1000 [00:01<00:04, 202.50it/s, 7 steps of size 5.83e-01. acc. prob=0.78]\rwarmup:  24%|‚ñà‚ñà‚ñç       | 238/1000 [00:01<00:02, 329.42it/s, 15 steps of size 3.27e-01. acc. prob=0.78]\rwarmup:  34%|‚ñà‚ñà‚ñà‚ñç      | 339/1000 [00:01<00:01, 471.21it/s, 15 steps of size 3.05e-01. acc. prob=0.78]\rwarmup:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/1000 [00:01<00:00, 613.06it/s, 15 steps of size 3.38e-01. acc. prob=0.79]\rsample:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/1000 [00:01<00:00, 727.08it/s, 15 steps of size 3.22e-01. acc. prob=0.87]\rsample:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/1000 [00:01<00:00, 823.80it/s, 15 steps of size 3.22e-01. acc. prob=0.88]\rsample:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/1000 [00:01<00:00, 902.54it/s, 15 steps of size 3.22e-01. acc. prob=0.88]\rsample:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/1000 [00:01<00:00, 956.00it/s, 15 steps of size 3.22e-01. acc. prob=0.87]\rsample:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/1000 [00:01<00:00, 947.43it/s, 15 steps of size 3.22e-01. acc. prob=0.87]\rsample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 509.34it/s, 15 steps of size 3.22e-01. acc. prob=0.87]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n/home/sosa/work/3.12venv/lib/python3.12/site-packages/arviz/stats/diagnostics.py:991: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Rho[0, 0]</th>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>500.00</td>\n      <td>500.00</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Rho[0, 1]</th>\n      <td>-0.48</td>\n      <td>0.19</td>\n      <td>-0.78</td>\n      <td>-0.19</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>422.99</td>\n      <td>367.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Rho[1, 0]</th>\n      <td>-0.48</td>\n      <td>0.19</td>\n      <td>-0.78</td>\n      <td>-0.19</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>422.99</td>\n      <td>367.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Rho[1, 1]</th>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>454.62</td>\n      <td>436.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>3.52</td>\n      <td>0.20</td>\n      <td>3.24</td>\n      <td>3.87</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>746.37</td>\n      <td>288.40</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[0, 0]</th>\n      <td>3.55</td>\n      <td>0.23</td>\n      <td>3.21</td>\n      <td>3.92</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>452.61</td>\n      <td>381.98</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[0, 1]</th>\n      <td>-1.53</td>\n      <td>0.30</td>\n      <td>-1.98</td>\n      <td>-1.04</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>509.28</td>\n      <td>348.57</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[1, 0]</th>\n      <td>5.32</td>\n      <td>0.23</td>\n      <td>4.97</td>\n      <td>5.69</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>678.84</td>\n      <td>388.59</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[1, 1]</th>\n      <td>-1.37</td>\n      <td>0.32</td>\n      <td>-1.91</td>\n      <td>-0.91</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>717.67</td>\n      <td>407.82</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[2, 0]</th>\n      <td>3.50</td>\n      <td>0.23</td>\n      <td>3.19</td>\n      <td>3.91</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>584.24</td>\n      <td>402.89</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[2, 1]</th>\n      <td>-1.24</td>\n      <td>0.29</td>\n      <td>-1.66</td>\n      <td>-0.78</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>565.87</td>\n      <td>254.37</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[3, 0]</th>\n      <td>4.40</td>\n      <td>0.21</td>\n      <td>4.09</td>\n      <td>4.71</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>595.46</td>\n      <td>337.76</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[3, 1]</th>\n      <td>-1.28</td>\n      <td>0.28</td>\n      <td>-1.73</td>\n      <td>-0.85</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>511.32</td>\n      <td>319.38</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[4, 0]</th>\n      <td>3.56</td>\n      <td>0.22</td>\n      <td>3.22</td>\n      <td>3.92</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>651.57</td>\n      <td>395.53</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[4, 1]</th>\n      <td>-1.58</td>\n      <td>0.29</td>\n      <td>-2.04</td>\n      <td>-1.12</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>580.06</td>\n      <td>351.02</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[5, 0]</th>\n      <td>4.23</td>\n      <td>0.21</td>\n      <td>3.93</td>\n      <td>4.60</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>708.60</td>\n      <td>355.22</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[5, 1]</th>\n      <td>-1.73</td>\n      <td>0.29</td>\n      <td>-2.13</td>\n      <td>-1.21</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>657.42</td>\n      <td>399.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[6, 0]</th>\n      <td>4.08</td>\n      <td>0.23</td>\n      <td>3.70</td>\n      <td>4.42</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>536.86</td>\n      <td>282.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[6, 1]</th>\n      <td>-0.19</td>\n      <td>0.29</td>\n      <td>-0.64</td>\n      <td>0.25</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>501.31</td>\n      <td>399.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[7, 0]</th>\n      <td>3.78</td>\n      <td>0.22</td>\n      <td>3.45</td>\n      <td>4.12</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>842.35</td>\n      <td>436.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[7, 1]</th>\n      <td>-1.08</td>\n      <td>0.30</td>\n      <td>-1.52</td>\n      <td>-0.56</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>741.73</td>\n      <td>425.51</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[8, 0]</th>\n      <td>3.49</td>\n      <td>0.22</td>\n      <td>3.15</td>\n      <td>3.82</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>928.83</td>\n      <td>448.99</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[8, 1]</th>\n      <td>-1.51</td>\n      <td>0.29</td>\n      <td>-1.95</td>\n      <td>-1.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>900.17</td>\n      <td>323.82</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[9, 0]</th>\n      <td>3.25</td>\n      <td>0.21</td>\n      <td>2.93</td>\n      <td>3.57</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>633.95</td>\n      <td>365.86</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[9, 1]</th>\n      <td>-0.34</td>\n      <td>0.28</td>\n      <td>-0.82</td>\n      <td>0.04</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>652.68</td>\n      <td>393.66</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[10, 0]</th>\n      <td>3.29</td>\n      <td>0.22</td>\n      <td>2.92</td>\n      <td>3.58</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>810.20</td>\n      <td>365.91</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[10, 1]</th>\n      <td>-0.60</td>\n      <td>0.28</td>\n      <td>-1.06</td>\n      <td>-0.18</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>659.36</td>\n      <td>385.50</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[11, 0]</th>\n      <td>3.83</td>\n      <td>0.20</td>\n      <td>3.50</td>\n      <td>4.13</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>577.10</td>\n      <td>304.11</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[11, 1]</th>\n      <td>-1.44</td>\n      <td>0.27</td>\n      <td>-1.82</td>\n      <td>-0.96</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>593.52</td>\n      <td>297.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[12, 0]</th>\n      <td>1.92</td>\n      <td>0.22</td>\n      <td>1.57</td>\n      <td>2.23</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>600.90</td>\n      <td>314.72</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[12, 1]</th>\n      <td>-0.77</td>\n      <td>0.30</td>\n      <td>-1.18</td>\n      <td>-0.24</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>724.12</td>\n      <td>342.76</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[13, 0]</th>\n      <td>4.37</td>\n      <td>0.22</td>\n      <td>4.04</td>\n      <td>4.71</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>549.10</td>\n      <td>369.49</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[13, 1]</th>\n      <td>-2.18</td>\n      <td>0.30</td>\n      <td>-2.64</td>\n      <td>-1.67</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>565.11</td>\n      <td>332.48</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[14, 0]</th>\n      <td>2.88</td>\n      <td>0.21</td>\n      <td>2.55</td>\n      <td>3.22</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>708.31</td>\n      <td>473.41</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[14, 1]</th>\n      <td>-0.73</td>\n      <td>0.29</td>\n      <td>-1.15</td>\n      <td>-0.25</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>644.40</td>\n      <td>297.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[15, 0]</th>\n      <td>2.26</td>\n      <td>0.22</td>\n      <td>1.90</td>\n      <td>2.59</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>598.20</td>\n      <td>408.34</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[15, 1]</th>\n      <td>-0.45</td>\n      <td>0.29</td>\n      <td>-0.90</td>\n      <td>-0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>500.02</td>\n      <td>219.35</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[16, 0]</th>\n      <td>4.68</td>\n      <td>0.22</td>\n      <td>4.28</td>\n      <td>5.00</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>601.60</td>\n      <td>399.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[16, 1]</th>\n      <td>-2.08</td>\n      <td>0.30</td>\n      <td>-2.53</td>\n      <td>-1.54</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>584.76</td>\n      <td>437.40</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[17, 0]</th>\n      <td>2.46</td>\n      <td>0.21</td>\n      <td>2.17</td>\n      <td>2.84</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>647.23</td>\n      <td>279.36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[17, 1]</th>\n      <td>-1.22</td>\n      <td>0.28</td>\n      <td>-1.67</td>\n      <td>-0.80</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>634.28</td>\n      <td>211.75</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[18, 0]</th>\n      <td>2.51</td>\n      <td>0.21</td>\n      <td>2.20</td>\n      <td>2.84</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>518.14</td>\n      <td>369.15</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[18, 1]</th>\n      <td>-0.16</td>\n      <td>0.28</td>\n      <td>-0.59</td>\n      <td>0.31</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>514.85</td>\n      <td>364.46</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[19, 0]</th>\n      <td>2.82</td>\n      <td>0.20</td>\n      <td>2.48</td>\n      <td>3.15</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>570.10</td>\n      <td>327.00</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a_b_cafe[19, 1]</th>\n      <td>-0.57</td>\n      <td>0.27</td>\n      <td>-1.02</td>\n      <td>-0.16</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>766.08</td>\n      <td>470.00</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b</th>\n      <td>-1.09</td>\n      <td>0.15</td>\n      <td>-1.34</td>\n      <td>-0.86</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>634.77</td>\n      <td>375.64</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma</th>\n      <td>0.51</td>\n      <td>0.03</td>\n      <td>0.46</td>\n      <td>0.54</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>800.09</td>\n      <td>512.95</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma_cafe[0]</th>\n      <td>0.92</td>\n      <td>0.15</td>\n      <td>0.67</td>\n      <td>1.14</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>541.45</td>\n      <td>397.11</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma_cafe[1]</th>\n      <td>0.68</td>\n      <td>0.13</td>\n      <td>0.47</td>\n      <td>0.87</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>401.57</td>\n      <td>358.18</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Python (Build in function)\n\n``` python\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multivariate_normal(only_path=True)\nm.data(data_path, sep=',') \nm.data_on_model = dict(\n    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n    N_cafes = len(m.df.cafe.unique()),\n    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n)\n\n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n\n    varying_intercept, varying_slope = m.effects.varying_effects(\n        N_vars = 1,\n        N_group = N_cafes,\n        group_id = cafe,\n        group_name = 'cafe',\n        centered = False)\n    \n\n    mu = varying_intercept + varying_slope* afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \n\n```\n## R\n\n``` r\nlibrary(BayesianInference)\njnp = reticulate::import('jax.numpy')\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\nm$data_on_model\n\n# Define model ------------------------------------------------\nmodel <- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariate_normal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, \n    shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  bi.dist.normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n\n```\n\n## Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_multivariate_normal(only_path = true)\n\nm.data(data_path, sep=',') \nm.data_on_model = pybuiltins.dict(\n    cafe = jnp.array(m.df.cafe.values, dtype=jnp.int32),\n    wait = jnp.array(m.df.wait.values, dtype=jnp.float32),\n    N_cafes = length(m.df.cafe.unique()),    \n    afternoon = jnp.array(m.df.afternoon.values, dtype=jnp.float32)\n)\n\n\n# Define model ------------------------------------------------\n@BI function model(cafe, wait, N_cafes, afternoon)\n    a = m.dist.normal(5, 2,  name = \"a\")\n    b = m.dist.normal(-1, 0.5, name = \"b\")\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = \"sigma_cafe\")\n    sigma = m.dist.exponential( 1,  name = \"sigma\")\n    Rho = m.dist.lkj(2, 2, name = \"Rho\")\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariate_normal(jnp.stack([a, b]), cov, shape = [N_cafes], name = \"a_b_cafe\")    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\n###  Centered parameterization\nThe Gaussian Mixture Model is a hierarchical model where each data point is generated from one of $K$ distinct multivariate Gaussian distributions. \n\nThe varying intercepts ($\\alpha_k$) and slopes ($\\beta_k$) are modeled using a *Multivariate Normal distribution*:\n\n$$ \n\\begin{pmatrix} \n\\alpha_k \\\\ \n\\beta_k \n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left( \n\\begin{pmatrix} \n\\bar{\\alpha} \\\\ \n\\bar{\\beta}\n\\end{pmatrix}, \n\\text{diag}(\\varsigma) ~ \\Omega ~ \\text{diag}(\\varsigma)\n\\right) \n$$\n\n$$ \n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n$$ \n$$  \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n$$ \n$$ \n\\varsigma \\sim  \\text{Exponential}(1)\n$$ \n$$ \n\\Omega \\sim \\text{LKJ}(\\eta) \n$$ \n\nWhere:\n\n- $\\left(\\begin{array}{cc} \\bar{\\alpha} \\\\ \\bar{\\beta} \\end{array}\\right)$ is a vector composed from concatenating a parameter for the global intercept and a parameter vector of the global slopes.\n\n- $\\varsigma$ is a vector giving the standard deviation of the random effects for the intercept and slopes across groups.\n\n- $\\Omega$ is the correlation matrix.\n\n###  Non-centered parameterization\nFor computational reasons, it is often better to implement a [<span style=\"color:#0D6EFD\">non-centered parameterization üõà</span>]{#RFcentered} that is equivalent to the *Multivariate Normal distribution* approach:\n\n$$\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n=\n\\left(\\begin{array}{cc} \n\\bar{\\alpha} \\\\ \n\\bar{\\beta}\n\\end{array}\\right) + \\varsigma\\circ \n\\left(\nL \\cdot\n\\left(\n\\begin{array}{cc} \n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\n\\right)\n\\right)\n$$\n\n$$ \n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n$$ \n$$  \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n$$ \n$$ \n\\varsigma \\sim  \\text{Exponential}(1)\n$$ \n$$ \nL \\sim \\text{LKJ Cholesky}(\\eta) \n$$ \n\n$$ \n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n$$ \n\n$$ \n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n$$ \n\n- Where:\n\n  - $\\sigma_\\alpha \\sim \\text{Exponential}(1)$ is the prior standard deviation among intercepts.\n\n  - $\\sigma_\\beta \\sim \\text{Exponential}(1)$ is the prior standard deviation among slopes.\n\n  - $L \\sim \\text{LKJcorr}(\\eta)$ is the a cholesky factor of the correlation matrix matrix using the [<span style=\"color:#0D6EFD\">Cholesky Factor üõà</span>]{#LKJcorr}\n\n\n### *Multivariate Model with One Random Slope for Each Variable*\n\nWe can apply a multivariate model similarly to [Chapter 2](2.%20Multiple%20continuous%20variable.qmd). In this case, we apply the same principle, but with a covariance matrix with a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for $i$ observations in a model with two independent variables $X_1$ and $X_2$, we can define the formula as follows:\n\n$$\nY_{i}  \\sim \\text{Normal}(\\mu_i , \\sigma) \n$$\n\n$$\n\\mu_i =   \\alpha_i + \\beta_{k(i)} X_{1i}  + \\gamma_{k(i)} X_{2i} \n$$\n\n<br>\n\n$$ \n\\begin{pmatrix} \n\\alpha\\\\ \n\\beta\\\\ \n\\gamma\n\\end{pmatrix} \n\\sim \\begin{pmatrix} \n\\bar{\\alpha}\\\\ \n\\bar{\\beta}\\\\ \n\\bar{\\gamma} \n\\end{pmatrix} + \\varsigma \\circ\n\\left(\nL \\cdot \n\\begin{pmatrix} \n\\widehat{\\alpha}_{k} \\\\ \n\\widehat{\\beta}_{k} \\\\ \n\\widehat{\\gamma}_{k} \n\\end{pmatrix} \n\\right)\n$$\n\n$$ \n\\bar{\\alpha} \\sim \\text{Normal}(0, 1)\n$$ \n$$  \n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n$$ \n\n$$  \n\\bar{\\gamma} \\sim \\text{Normal}(0, 1)\n$$ \n\n$$ \n\\varsigma \\sim  \\text{Exponential}(1)\n$$ \n$$ \nL \\sim \\text{LKJ Cholesky}(2) \n$$ \n\n$$ \n\\widehat{\\alpha}_k \\sim \\text{Exponential}(1)\n$$ \n\n$$ \n\\widehat{\\beta}_k \\sim \\text{Exponential}(1)\n$$ \n\n$$ \n\\widehat{\\gamma}_k \\sim \\text{Exponential}(1)\n$$ \n\n\n## Notes\n\n::: callout-note\n\n- We can apply interaction terms similarly to [Chapter 3](/3.%20Interaction%20between%20continuous%20variables.qmd).\n\n- We can apply categorical variables similarly to [Chapter 4](/4.%20Categorical%20variable.qmd).\n\n- We can apply multiple random effects in Bayesian inference as follows:\n\n```python\ndef model(cafe, wait, N_cafes, afternoon, state):\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    \n    # ‚ö†Ô∏è Note: `alpha_bar` and `beta_bar` are set to zero to ensure the same intercepts for all varying effects. ‚ö†Ô∏è\n    varying_intercept_1, varying_slope_1 = m.effects.varying_effects(\n        N_vars = 1,\n        N_group = N_cafes,\n        group_id = cafe, \n        group_name='cafe',\n        alpha_bar = jnp.array([0]),\n        beta_bar = jnp.array([0]),\n        centered=True,\n    )\n\n    varying_intercept_2, varying_slope_2 = m.effects.varying_effects(\n        N_vars = 1,\n        N_group = 4,\n        group_id = states, \n        group_name='states',\n        alpha_bar = jnp.array([0]),\n        beta_bar = jnp.array([0]),\n        centered=True,\n    )\n\n    varying_intercept = varying_intercept_1 + varying_intercept_2\n    varying_slope = varying_slope_1 + varying_slope_2\n\n    mu = varying_intercept + varying_slope[:,0] * afternoon \n    m.dist.normal(mu, sigma, obs=wait\n\n```\n\nWhere `state` is a second varying effect, representing cafes clustered by state.\n\n‚ö†Ô∏è Note: `alpha_bar` and `beta_bar` are set to zero to ensure the same intercepts for all varying effects. ‚ö†Ô∏è\n<!---\n- We can apply varying slopes with any distribution presented in previous chapters. Below is the formula and the code snippet for a Binomial multivariate model with an interaction between two independent variables $X_1$ and $X_2$ and multiple varying effects for each actor and each group.\n\n\n$$\nY_{i} \\sim \\text{Binomial}(n = 1, p_i) \\\\\n$$\n\n$$\n\\text{logit}(p_i) = \\left( \\alpha_{k(i)} + \\pi_{g(i)} \\right) + \\left( \\beta_{k(i)} + \\gamma_{g(i)} \\right) X_i \n$$ \n\n<br>\n\n$$\n\\begin{pmatrix} \n\\alpha_{k} \\\\ \n\\beta_{k} \n\\end{pmatrix} \n\\sim \n\\begin{pmatrix} \n\\bar{\\alpha} \\\\ \n\\bar{\\beta} \n\\end{pmatrix} \n\\varsigma\n\\circ \n\\left(\nL_{k} \\cdot \n\\begin{pmatrix} \n\\widehat{\\alpha}_{k} \\\\ \n\\widehat{\\beta}_{k} \n\\end{pmatrix} \n\\right)\n$$\n\n$$\n\\bar{\\alpha}  \\sim \\text{Normal}(0, 1)\n$$ \n$$\n\\bar{\\beta} \\sim \\text{Normal}(0, 1)\n$$ \n$$\nL_{k} \\sim \\text{LKJ Cholesky}(2)\n$$\n\n$$\n\\hat{\\alpha}  \\sim \\text{Exponential}(1)\n$$ \n$$\n\\hat{\\beta} \\sim \\text{Exponential}(1)\n$$ \n\n<br>\n\n$$ \n\\begin{pmatrix} \n\\pi_{g} \\\\ \n\\gamma_{g} \n\\end{pmatrix} \n\\sim  \n\\begin{pmatrix} \n\\bar{\\pi} \\\\ \n\\bar{\\gamma} \n\\end{pmatrix}  + \\varsigma \\circ\n\\left(\nL_g \\cdot \n\\begin{pmatrix} \n\\widehat{\\pi}_{g} \\\\ \n\\widehat{\\gamma}_{g} \n\\end{pmatrix} \n\\right)\n$$\n\n$$\n\\bar{\\pi}  \\sim \\text{Normal}(0, 1)\n$$ \n$$\n\\bar{\\gamma} \\sim \\text{Normal}(0, 1)\n$$ \n\n$$\nL_{g} \\sim \\text{LKJ Cholesky}(2)\n$$\n\n$$\n\\hat{\\alpha}  \\sim \\text{Exponential}(1)\n$$ \n$$\n\\hat{\\beta} \\sim \\text{Exponential}(1)\n$$ \n\n<br>\n\n``` python\nfrom main import *\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Import data\nm.read_csv(\"../data/chimpanzees.csv\", sep=\";\")\nm.df[\"block_id\"] = m.df.block\nm.df[\"treatment\"] = 1 + m.df.prosoc_left + 2 * m.df.condition\nm.data_to_model(['pulled_left', 'treatment', 'actor', 'block_id'])\n\n\ndef model(tid, actor, block_id, L=None, link=False):\n    # fixed priors\n    g = dist.normal(0, 1, name = 'g', shape = (4,))\n    sigma_actor = dist.exponential(1, name = 'sigma_actor', shape = (4,))\n    L_Rho_actor = dist.lkjcholesky(4, 2, name = \"L_Rho_actor\")\n    sigma_block = dist.exponential(1, name = \"sigma_block\", shape = (4,))\n    L_Rho_block = dist.lkjcholesky(4, 2, name = \"L_Rho_block\")\n\n    # adaptive priors - non-centered\n    z_actor = dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n    z_block = dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n    alpha = deterministic(\n        \"alpha\", ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n    )\n    beta = deterministic(\n        \"beta\", ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n    )\n\n    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n    dist(\"L\", dist.Binomial(logits=logit_p), obs=L)\n\n    # compute ordinary correlation matrices from Cholesky factors\n    if link:\n        deterministic(\"Rho_actor\", L_Rho_actor @ L_Rho_actor.T)\n        deterministic(\"Rho_block\", L_Rho_block @ L_Rho_block.T)\n        deterministic(\"p\", expit(logit_p))\n\n# Run mcmc ------------------------------------------------\nm.fit(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)\n```\n--->\n:::\n\n\n## Reference(s)\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "14. Varying slopes_files/figure-pdf"
    ],
    "filters": []
  }
}