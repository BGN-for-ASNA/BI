{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Categorical Model\"\n",
        "description: \"Modeling a categorical dependent variable with more than two nominal outcomes.\"\n",
        "categories: [Classification, Regression, GLM]\n",
        "image: \"Figures/9.png\"\n",
        "order: 11\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "To model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a *Categorical* model. \n",
        "\n",
        "\n",
        "## Considerations\n",
        "\n",
        "::: callout-caution \n",
        "- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n",
        "  \n",
        "- One way to interpret a *Categorical* model is to consider that we need to build $K - 1$ linear models, where $K$ is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}. To do this, we convert the regression outputs using the [<span style=\"color:#0D6EFD\">softmax function ðŸ›ˆ</span>]{#softmax} (see the \"jax.nn.softmax\" line in the code). \n",
        "\n",
        "- The intercept $\\alpha$ captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\n",
        "  \n",
        "- On the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients $\\beta$ are shared across categories.\n",
        "\n",
        "- The relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.\n",
        "\n",
        ":::\n",
        "\n",
        "## Example\n",
        "Below is an example code snippet demonstrating a Bayesian Categorical model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "### Python"
      ],
      "id": "6e98b104"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi, jnp\n",
        "import jax\n",
        "# Setup device ------------------------------------------------\n",
        "m = bi('cpu')\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "from importlib.resources import files\n",
        "data_path = m.load.sim_multinomial(only_path=True)\n",
        "m.data(data_path, sep=',') \n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "def model(career, income):\n",
        "    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n",
        "    b = m.dist.half_normal(0.5, shape=(1,), name = 'b')\n",
        "    s_1 = a[0] + b * income[0]\n",
        "    s_2 = a[1] + b * income[1]\n",
        "    s_3 = [0] #pivot\n",
        "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
        "    m.dist.categorical(probs=p, obs=career)\n",
        "\n",
        "# Run sampler ------------------------------------------------ \n",
        "m.fit(model)  # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary() # Get posterior distributions"
      ],
      "id": "9ecd9d26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### R\n",
        "```R\n",
        "library(BayesianInference)\n",
        "jax = reticulate::import('jax')\n",
        "\n",
        "# setup platform------------------------------------------------\n",
        "m=importBI(platform='cpu')\n",
        "\n",
        "# import data ------------------------------------------------\n",
        "m$data(m$load$sim_multinomial(only_path=T), sep=',')\n",
        "keys <- c(\"income\", \"career\")\n",
        "income = unique(m$df$income)\n",
        "income = income[order(income)]\n",
        "values <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\n",
        "m$data_on_model = py_dict(keys, values, convert = TRUE)\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "model <- function(income, career){\n",
        "  # Parameter prior distributions\n",
        "  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n",
        "  beta = bi.dist.normal(0.5, name='beta')\n",
        "  \n",
        "  s_1 = alpha[0] + beta * income[0]\n",
        "  s_2 = alpha[1] + beta * income[1]\n",
        "  s_3 = 0 # reference category\n",
        "\n",
        "  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n",
        "\n",
        "  # Likelihood\n",
        "  m$dist$categorical(probs=p[career], obs=career)\n",
        "}\n",
        "\n",
        "# Run MCMC ------------------------------------------------\n",
        "m$fit(model) # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m$summary() # Get posterior distribution\n",
        "\n",
        "```\n",
        "\n",
        "### Julia\n",
        "```julia\n",
        "using BayesianInference\n",
        "\n",
        "# Setup device------------------------------------------------\n",
        "m = importBI(platform=\"cpu\")\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "data_path = m.load.sim_multinomial(only_path = true)\n",
        "m.data(data_path, sep=',')\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "@BI function model(career, income)\n",
        "    a = m.dist.normal(0, 1, shape=(2,), name = \"a\")\n",
        "    b = m.dist.half_normal(0.5, shape=(1,), name = \"b\")\n",
        "    \n",
        "    # indexing works now because of the package update\n",
        "    s_1 = a[0] + b * income[0]\n",
        "    s_2 = a[1] + b * income[1]\n",
        "    \n",
        "    # âš ï¸ Use jnp.array to create a Python object, so [0] indexing works\n",
        "    s_3 = jnp.array([0.0]) \n",
        "    \n",
        "    # Now s_3[0] is valid because it calls Python's __getitem__(0)\n",
        "    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n",
        "    \n",
        "    m.dist.categorical(probs=p, obs=career)\n",
        "end\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m.fit(model)  # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary() # Get posterior distributions\n",
        "```\n",
        ":::\n",
        "\n",
        "## Mathematical Details \n",
        "We can model a *Categorical* model using a $Categorical distribution$. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable \n",
        "ð‘¦ with ð¾ categories, the multinomial likelihood function is:\n",
        "\n",
        "$$\n",
        "Y_i \\sim \\text{Categorical}(\\theta_i) \\\\\n",
        "\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n",
        "\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n",
        "\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n",
        "... \\\\\n",
        "\\phi_{[i,k]} = 0 \\\\\n",
        "\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n",
        "\\beta_{k} \\sim \\text{Normal}(0.1)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Y_i$ is the dependent categorical variable for observation *i* indicating the category of the observation.\n",
        "  \n",
        "- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n",
        "  \n",
        "- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n",
        "  \n",
        "- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n",
        "\n",
        "\n",
        "## Reference(s)\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "5b1dc0da"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}