{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Bayesian Neural Networks\"\n",
        "description: \"Neural networks that incorporate Bayesian inference to quantify uncertainty in their predictions.\"\n",
        "categories: [Neural Networks,  Deep Learning]\n",
        "image: \"Figures/24.png\"\n",
        "order: 30\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "\n",
        "To model complex, non-linear relationships between variables, we can use multiple approaches including, splines, polynomials, gaussian processes, and neural networks. Here, we will focus on a Bayesian Neural Network (BNN). Think of a neural network as a highly flexible function made of interconnected layers of \"neurons.\" Each connection between neurons has a *weight*, and each neuron has a *bias*. These *weights* and *biases* are like a vast set of adjustable knobs. In a standard network, the goal is to find the single best setting for all these knobs to map inputs to outputs. Unlike a standard neural network which learns a single set of optimal weights, a BNN learns distributions over its *weights* and *biases*. This allows it to capture not just the relationship in the data, but also its own uncertainty about that relationship. For this, we need to define:\n",
        "\n",
        "\n",
        "1)  **A Network Architecture**, which specifies the number of layers, the number of neurons in each layer, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the structure of our \"knobs.\"\n",
        "\n",
        "2)  **Priors for Arrays of Weights and Biases**. In a simple model like linear regression, we define a prior for each individual parameter (e.g., one prior for the slope $\\beta$). In a neural network, which can have thousands or millions of weights, we don't define a unique prior for every single one. Instead, we define a prior that acts as a template for an entire **array of parameters**. For example, we might declare that all weights in a specific layer are drawn from the same `Normal(0, 1)` distribution. This allows us to efficiently specify our beliefs about the entire set of network parameters.\n",
        "\n",
        "3)  **An Output Distribution (Likelihood)**, which defines the probability of the data given the network's predictions. For a continuous variable (regression), this is often a Normal distribution with a variance term $\\sigma$ that quantifies the data's noise around the model's predictions.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "::: callout-caution\n",
        "-   Like all Bayesian models, BNNs consider [<span style=\"color:#0D6EFD\">model parameter uncertainty ðŸ›ˆ</span>]{#uncertainty}. The parameters here are the network's **weights (W)** and **biases (b)**. We quantify our uncertainty about them through their [<span style=\"color:#0D6EFD\">posterior distribution ðŸ›ˆ</span>]{#posterior}. Therefore, we must declare [<span style=\"color:#0D6EFD\">prior distributions ðŸ›ˆ</span>]{#prior} for all *weights* and *biases*, as well as for the output variance $\\sigma$.\n",
        "\n",
        "-   Unlike in a linear regression where the coefficient $\\beta$ has a direct interpretation (e.g., the effect of weight on height), the individual *weights* and *biases* in a BNN are not directly interpretable. A single *weight*'s influence is entangled with thousands of other parameters through non-linear functions. Consequently, BNNs are best viewed as powerful **predictive tools** rather than explanatory ones. They excel at learning complex patterns and quantifying predictive uncertainty, but if the goal is to isolate and interpret the effect of a specific variable, a simpler model is often more appropriate.\n",
        "\n",
        "-   Prior distributions are built following these considerations:\n",
        "\n",
        "    -   As the data is typically [<span style=\"color:#0D6EFD\">scaled ðŸ›ˆ</span>]{#scaled} (see introduction), we can use a standard Normal distribution (mean 0, standard deviation 1) as a weakly-informative prior for all weights and biases. This acts as a form of regularization.\n",
        "\n",
        "    -   Since the output variance $\\sigma$ must be positive, we can use a positively-defined distribution, such as the Exponential or Half-Normal.\n",
        "\n",
        "-   BNNs can be used for both *regression* and *classification*. The final layer's activation and the chosen likelihood distribution depend on the task. For binary classification, a *sigmoid* activation is paired with a Bernoulli likelihood, which requires a [<span style=\"color:#0D6EFD\">link function ðŸ›ˆ</span>]{#linkF} (logit) to connect the linear output of the network to the probability space [0, 1]. For regression, the identity activation is often used with a Gaussian likelihood.\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "Below is an example code snippet demonstrating a *Bayesian Neural Network* for regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to predict height from weight using a non-linear model.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "## Python"
      ],
      "id": "efde8749"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi\n",
        "import json\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup device------------------------------------------------\n",
        "m = bi(platform='cpu')\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "\n",
        "with open('BNN.json', 'r', encoding='utf-8') as file:\n",
        "    # Load the JSON data into a Python dictionary\n",
        "    data = json.load(file)\n",
        "# X is already scaled\n",
        "X = jnp.array(data['X']) # Note X shape = (N,2) where first column is the intercept and second column is the predictor\n",
        "Y = jnp.array(data['Y']) # Note Y shape = (N,1) where N is the number of observations\n",
        "\n",
        "m.data_on_model = dict(X = X, Y = Y)\n",
        "# Define model ------------------------------------------------\n",
        "def model(X, Y,  D_H=5, D_Y=1):  \n",
        "    N, D_X = X.shape\n",
        "    \n",
        "    # First hidden layer: Transforms input to N Ã— D_H (hidden units)\n",
        "    w1 = m.bnn.layer_linear(\n",
        "        X, \n",
        "        dist=m.dist.normal(\n",
        "            0, 1,  name='w1',shape=(D_X,D_H)\n",
        "            ),\n",
        "        activation='tanh'\n",
        "        )\n",
        "\n",
        "    # sample final layer of weights and neural network output\n",
        "    # Final layer (z3) computes linear combination of second hidden layer\n",
        "    w2 = m.bnn.layer_linear(\n",
        "        X=w1, \n",
        "        dist=m.dist.normal(0, 1,  name='w2',shape=(D_H,D_Y))\n",
        "        )\n",
        "\n",
        "    sigma = m.dist.exponential(1, name='sigma')\n",
        "\n",
        "    m.dist.normal(w2, sigma, obs=Y,name='Y')\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m.fit(model, num_samples=500, progress_bar=False)   # Approximate posterior distributions for weights, biases, and sigma\n",
        "\n",
        "# Predictions from the model ------------------------------------------------\n",
        "pred = m.sample(samples = 500)['Y']\n",
        "pred = pred[..., 0]\n",
        "mean_prediction = jnp.mean(pred, axis=0)\n",
        "percentiles = jnp.percentile(pred, jnp.array([5.0, 95.0]), axis=0)\n",
        "# make plots\n",
        "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "# plot training data\n",
        "ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
        "# plot 90% confidence level of predictions\n",
        "ax.fill_between(\n",
        "    X[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
        ")\n",
        "# plot mean prediction\n",
        "ax.plot(X[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")"
      ],
      "id": "59928f18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R\n",
        "\n",
        "```r\n",
        "library(BI)\n",
        "m=importbi(platform='cpu')\n",
        "\n",
        "# Load csv file\n",
        "m$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n",
        "\n",
        "# Filter data frame\n",
        "m$df = m$df[m$df$age > 18,]\n",
        "\n",
        "# Scale\n",
        "m$scale(list('weight')) \n",
        "\n",
        "# Convert data to JAX arrays\n",
        "m$data_to_model(list('weight', 'height'))\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "model <- function(height, weight){\n",
        "  # Define the BNN architecture\n",
        "  # 1 input -> 10 hidden neurons (tanh) -> 1 output neuron (identity)\n",
        "  # Priors for weights/biases are Normal(0,1) by default\n",
        "  mu <- bi$bnn$layer_linear(\n",
        "    x = weight, n_neurons = list(10, 1), activations = list('tanh', 'identity'), name = 'bnn')\n",
        "\n",
        "  # Prior for the output standard deviation\n",
        "  s = bi$dist$exponential(1, name = 's')\n",
        "  \n",
        "  # Likelihood\n",
        "  m$normal(mu, s, obs = height)\n",
        "}\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m$fit(model) # Approximate posterior distributions\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m$summary()\n",
        "\n",
        "```\n",
        ":::\n",
        "\n",
        "\n",
        "## Mathematical Details\n",
        "\n",
        "### *Frequentist Formulation*\n",
        "\n",
        "A standard (non-Bayesian) neural network with one hidden layer is defined by forward propagation:\n",
        "\n",
        "$$\n",
        "h_i = f(X_i W_1 + b_1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{Y}_i = g(h_i W_2 + b_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "-   $Y_i$ is the predicted output for observation *i*.\n",
        "-   $X_i$ is the input vector for observation *i*.\n",
        "-   $W_1, b_1$ are the weight matrix and bias vector for the hidden layer.\n",
        "-   $W_2, b_2$ are the weight matrix and bias vector for the output layer.\n",
        "-   $h_i$ is the activation of the hidden layer.\n",
        "-   $f$ and $g$ are activation functions (e.g., ReLU, tanh, sigmoid, identity).\n",
        "-   Parameters $W_1, b_1, W_2, b_2$ are learned as single optimal values via optimization.\n",
        "\n",
        "### *Bayesian Formulation*\n",
        "\n",
        "In the Bayesian formulation, we place [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior} on all weights and biases and define a likelihood for the output. For a regression task with a one-layer BNN:\n",
        "\n",
        "$$\n",
        "Y_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu_i = g(h_i W_2 + b_2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_i = f(X_i W_1 + b_1)\n",
        "$$\n",
        "\n",
        "The parameters are now distributions:\n",
        "$$\n",
        "W_1 \\sim \\text{Normal}(0, 1)\n",
        "$$\n",
        "$$\n",
        "b_1 \\sim \\text{Normal}(0, 1)\n",
        "$$\n",
        "$$\n",
        "W_2 \\sim \\text{Normal}(0, 1)\n",
        "$$\n",
        "$$\n",
        "b_2 \\sim \\text{Normal}(0, 1)\n",
        "$$\n",
        "$$\n",
        "\\sigma \\sim \\text{Exponential}(1)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "-   $Y_i$ is the observed dependent variable for observation *i*.\n",
        "-   $\\mu_i$ is the mean predicted by the network, which is itself a distribution because it is a function of the distributions of weights and biases.\n",
        "-   $W_1, b_1, W_2, b_2$ are the weights and biases, treated as random variables.\n",
        "-   $\\sigma$ is the standard deviation of the normal distribution, quantifying observation noise.\n",
        "\n",
        "## Notes\n",
        "::: callout-note\n",
        "- The primary difference between a *Frequentist* and *Bayesian* neural network lies in how parameters are treated. In the frequentist approach, weights and biases are point estimates found by minimizing a loss function (e.g., via gradient descent). Techniques like *Dropout* or *L2 regularization* are often used to prevent *overfitting*, which can be interpreted as approximations to a Bayesian treatment. In contrast, the *Bayesian* formulation does not seek a single best set of weights. Instead, it uses methods like MCMC or Variational Inference to approximate the entire posterior distribution for every *weight* and *bias*. This provides a principled and direct way to quantify model uncertainty.\n",
        "- While present an example of non-linear regression, the Bayesian Neural Network can be used for linear regressions as well (keeping in mind that interpretation of the weights are impossible).\n",
        "\n",
        ":::\n",
        "\n",
        "## Reference(s)\n",
        "\n",
        "@neal1995bayesian"
      ],
      "id": "a97aca64"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}