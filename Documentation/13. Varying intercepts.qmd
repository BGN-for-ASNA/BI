# Varying Intercepts

## General Principles

To model the relationship between dependent variables and an independent variable while allowing for different intercepts across groups or clusters, we can use a *Varying Intercepts* model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.

## Considerations

::: callout-caution
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- The main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept $\beta$ is defined based on the $k$ declared groups.

- Each intercept has its own prior -i.e., a [<span style="color:#0D6EFD">hyper-prior ðŸ›ˆ</span>]{#hyperP}

- In the code below, the *hyper-prior* is ```a_bar```.

:::

## Example

Below is an example code snippet demonstrating Bayesian regression with varying intercepts using the Bayesian Inference (BI) package. The data consist of a dependent variable representing individuals' survival (*surv*) and an independent categorical variable (*tank*), which indicates the tank where the individual was born, with a total of 48 tanks.

::: {.panel-tabset group="language"}
## Python
```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import Data & Data Manipulation ------------------------------------------------
m.data('../data/reedfrogs.csv', sep=';') # Import
m.df["tank"] = np.arange(m.df.shape[0]) # Manipulate
tank = jnp.array(m.df["tank"].astype('int32').values) # Manipulate
density = jnp.array(m.df["density"].astype('float32').values) # Manipulate
surv = jnp.array(m.df["surv"].astype('int32').values) # Manipulate
m.data_on_model = dict(
    tank = tank,
    surv = surv
) # Send to model (convert to jax array)

# Define model ------------------------------------------------
def model(tank, surv, density):
    sigma = dist.exponential( 1, shape=(1,), name = 'sigma')
    a_bar = dist.normal( 0., 1.5, shape=(1,), name = 'a_bar')
    alpha = dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')
    p = alpha[tank]
    print(p.shape)
    m.lk("y", m.binomial(total_count = density, logits = p), obs=surv)


# Run mcmc ------------------------------------------------
m.run(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m.summary(0.89) # Get posterior distributions
```
## R

```R
library(reticulate)
bi <- import("main")

# Setup device ------------------------------------------------
m = bi$bi(platform='cpu')

# Import Data & Data Manipulation ------------------------------------------------
m$data('../data/reedfrogs.csv', sep=';')  # Import
m$df$tank = c(0:(nrow(m$df)-1)) # Manipulate
m$data_to_model(list('tank', 'surv', 'density')) # Manipulate
m$data_on_model$tank = m$data_on_model$tank$astype(bi$jnp$int32) # Manipulate
m$data_on_model$surv = m$data_on_model$surv$astype(bi$jnp$int32) # Manipulate

# Define model ------------------------------------------------
model <- function(tank, surv, density){
  # Parameters priors distributions
  sigma = bi$dist$exponential( 1,  name = 'sigma')
  a_bar = bi$dist$normal(0, 1.5, name='a_bar')
  alpha = bi$dist$normal(a_bar, sigma, name='alpha', shape = tuple(as.integer(48)))
  p = alpha[tank]
  # Likelihood
  bi$lk("y", bi$Binomial(total_count = density, logits = p), obs=surv)
} 

# Run MCMC ------------------------------------------------
m$run(model) # Optimize model parameters through MCMC sampling

# Summary ------------------------------------------------
m$sampler$print_summary(0.89) # Get posterior distributions
```
:::

## Mathematical Details
### *Frequentist formulation*
We model the relationship between the independent variables $X$ and the outcome variable $Y$ with varying intercepts $\alpha$ for each group $k$ using the following equation:


$$
Y_{ik} = \alpha_k + \beta X_{ik} + \sigma
$$

Where:

- $Y_{ik}$ is the outcome variable for observation $i$ in group $k$.

- $\alpha_k$ is the varying intercept for group $k$.

- $X_{ik}$ is the independent variable for observation $i$ in group $k$.

- $\beta$ is the regression coefficient term.

- $\sigma$ is the error term, typically assumed to be normally distributed and positive.

### Bayesian Model

We can express the Bayesian regression model accounting for prior distributions as follows:

$$
Y_{ik} = Normal(\mu_{ik}, \sigma)
$$
$$
\mu_{ik} = \alpha_j + \beta X_{ik} + \sigma 
$$
$$
\alpha_k \sim \text{Normal}(\mu_{\alpha_k}, \sigma_{\alpha_k}) 
$$
$$
\beta \sim \text{Normal}(0, 1)
$$
$$
\sigma \sim \text{Exponential}(1)
$$
$$
\mu_{\alpha_k} \sim \text{Normal}(0, 1)
$$
$$
\sigma_{\alpha_k} \sim \text{Exponential}(1)
$$

Where:

Where:

- $Y_{ij}$ is the likelihood function for the outcome variable.

- $\alpha_k$ is the varying intercept for group $k$.

- $\mu_{\alpha_k}$ is the overall mean intercept.

- $\sigma_{\alpha_k}$ is the variance of the intercepts across groups.

- $\beta$ is the prior distribution for the regression coefficients.

- $\sigma$ is the prior distribution for the error term.

## Notes
::: callout-note 
- We can apply multiple variables similarly to [Chapter 2](/2.%20Multiple%20Regression%20for%20Continuous%20Variables.qmd).

- We can apply interaction terms similarly to [Chapter 3](/3.%20Interaction%20between%20continuous%20variables.qmd).

- We can apply categorical variables similarly to [Chapter 4](/4.%20Categorical%20variable.qmd).

- We can apply varying intercepts with any distribution developed in previous chapters.
:::

## Reference(s)
@mcelreath2018statistical
