# Binomial model
## General Principles
To model the relationship between a binary outcome -e.g. such as success/failure, yes/no, or 1/0.- variable and one or more independent variables, we can use Binomial model. 

![](https://i.sstatic.net/xHlvv.png)

##  Considerations
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- We have the firs [link function](5.&#32;Binomial&#32;regression.qmd "A link function is a mathematical tool used in statistics to connect the outcome (dependent variable) with the predictors (independent variables) in models like Binomial regression. It transforms the predicted values into a form that makes them easier to model. For example, in Binomial regression, the link function converts probabilities (which range from 0 to 1) into values that can range from negative to positive infinity.") _logit_. The  _logit_ link function in Bayesian binomial model converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution. 


## Example
Below is an example code snippet demonstrating Bayesian binomial regression

```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/chimpanzees.csv', sep=';') 
m.data_to_model(['pulled_left'])

# Define model ------------------------------------------------
def model(pulled_left):
    alpha = dist.normal( 0, 10)
    lk("y", Binomial(logits= alpha[actor] + beta1[side] + beta2[cond]), obs=pulled_left)

# Run mcmc ------------------------------------------------
m.run(model, init_strategy = numpyro.infer.initialization.init_to_mean()) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)

```

## Mathematical Details
### *Formula*
We model the relationship between the independent variable ($X$) and the binary outcome variable ($Y$) using the following equation:
$$
logit(Y)=\alpha + \beta   ğ‘‹ 
$$

Where:

- $Y$ is the probability of success (or the probability of the binary outcome being 1).
- $X$, is an independent variables.
- $\beta$ is the regression coefficients.
- $\alpha$ is the intercept term.
- $\sigma$ is the error term.
- $logit(Y)$ is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variables on the log-odds of success.

### *Bayesian model*
We can express the Bayesian binomial regression model using probability distributions as follows:

$$ 
ğ‘(ğ‘Œâˆ£\alpha, \beta, ğ‘‹) = Binomial(ğ‘›=1, ğ‘=logit(\alpha + \beta  ğ‘‹ ))
$$
$$
ğ‘(\alpha)=Normal(0,1)\\
ğ‘(\beta)=Normal(0,1)\\

$$



Where:

- $p(Y | ğ‘Œâˆ£\alpha, \beta, ğ‘‹)$ is the likelihood function.
- $p(\beta)$ and $p(\alpha)$ are the prior distributions for the regression coefficients and intercept, respectivelly.
- $n=1$ represents the number of trials in the binomial distribution (binary outcome).
- $\logit(\alpha + \beta  ğ‘‹ )$ is the _logit_ link function that is equal to sigmoid function applied to the linear combination of predictors, mapping the log-odds to probabilities.

## Notes

- We can apply multiple variables similarly as [chapter 2](/2.&#32;Multiple&#32;Regression&#32;for&#32;Continuous&#32;Variables.qmd).
  
- We can apply interaction terms  similarly as [chapter 3](\3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).

- We can apply  caterogical variables similarly as [chapter 4](4.&#32;Categorical&#32;variable.qmd). 
  
- Below is an example code snippet demonstrating Bayesian binomial model for multiple caterogical variables :

```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/chimpanzees.csv', sep=';') 
m.df["side"] = m.df.prosoc_left  # right 0, left 1
m.df["cond"] = m.df.condition  # no partner 0, partner 1
m.data_to_model(['pulled_left', "actor", "side", "cond"])

# Define model ------------------------------------------------
def model(pulled_left):
    alpha = bi.dist.normal(0, 10, shape = (7,), "alpha") # generating k intercept  #(one for each actor)
    beta1 = bi.dist.normal(0, 10, shape = (2,), "beta") # generating k regression coefficient  for each k prosoc_left)
    beta2 = bi.dist.normal(0, 10, shape = (2,), "alpha") # generating k regression coefficient for each k condition)
    lk("y", Binomial(logits= alpha[actor] + beta1[side] + beta2[cond]), obs=pulled_left)

# Run mcmc ------------------------------------------------
m.run(model, init_strategy = numpyro.infer.initialization.init_to_mean()) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```
## Reference(s)
@mcelreath2018statistical