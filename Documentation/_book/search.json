[
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "2Â  Introduction",
    "section": "",
    "text": "2.1 1.1 Model set-up\nWe define a likelihood (e.g., a mathematical formula that specifies the plausibility of the data). The likelihood has parameters (e.g., adjustable inputs) for which we define priors (e.g., initial plausibility assignment for each possible value of the parameter). Considering a linear regression with an intercept (e.g., \\(Î¼\\) value when \\(x\\) is at zero, or at the mean if the data is centered), a slope (e.g., \\(Î¼\\) change value when \\(x\\) is incremented by one unit), and assuming the data is centered ( as we will always concider in the next chapters):\n* Toolpit available for each lines of equation\n\\[y \\sim  Normal(Î¼,Ïƒ)\n\\]\n\\[ Î¼ \\sim Î± + Î²x\n\\]\n\\[ Î± \\sim Normal(0,1)\n\\]\n\\[ Î² \\sim Normal(0,1)\n\\]\n\\[ Ïƒ \\sim Uniform(0,1)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fitting",
    "href": "0. Introduction.html#model-fitting",
    "title": "2Â  Introduction",
    "section": "2.2 1.2 Model fitting",
    "text": "2.2 1.2 Model fitting\nBy using probability distributions for parameters, we can better tune the model by describing parameters with â€˜subequationsâ€™ and accounting for correlated varying effects, Gaussian processes, measurement error, and missing data.\nIn addition, we can use Bayesian updating using the Bayesian theorem to â€˜reshapeâ€™ the prior distributions by considering every possible combination of values for Âµ and Ïƒ and scoring each combination by its relative plausibility in light of the data. These relative plausibilities are the posterior probabilities of each combination of values Âµ and Ïƒ: the posterior distributions. Various techniques can be used to approximate the mathematics that follows from the definition of Bayesâ€™ theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC).\n\\[\\frac{likelihood*Priors}{average likelihood}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "2Â  Introduction",
    "section": "2.3 1.3 Model â€˜diagnosticâ€™",
    "text": "2.3 1.3 Model â€˜diagnosticâ€™\nThe posterior distribution can be described using percentile intervals (PI), the highest posterior density interval (HPDI), and point estimates. We can also sample the posterior distribution and generate dummy data, which can help check the model through observations and p uncertainty propagation on the samples. In some aspects, it is the opposite of a null model as it represents an expected model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions",
    "href": "0. Introduction.html#link-functions",
    "title": "2Â  Introduction",
    "section": "2.4 1.4 Link functions",
    "text": "2.4 1.4 Link functions\nWe will see different families of regreessions that have different distribtions. For the moment we just need to know that those different distribtions required _link function (for each specific family we will discuss the corresponding link function):\n\n\n\nimg",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#vocabulary",
    "href": "0. Introduction.html#vocabulary",
    "title": "2Â  Introduction",
    "section": "2.5 Vocabulary",
    "text": "2.5 Vocabulary\nThis method evaluate if variable we want to predict -the dependent variable (Y)- and the variable(s) that may affect(s)-independent variables (Xs)- this dependent variable is",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#conciderations",
    "href": "0. Introduction.html#conciderations",
    "title": "2Â  Introduction",
    "section": "2.6 Conciderations",
    "text": "2.6 Conciderations\nWhen implementing Bayesian linear regression with TensorFlow Probability, itâ€™s important to consider the following: - Specifying appropriate prior distributions for the model parameters. - Choosing an appropriate likelihood function that captures the relationship between the inputs and outputs. - Selecting an inference method to approximate the posterior distribution over parameters, such as Markov chain Monte Carlo (MCMC) or variational inference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "3Â  Linear Regression for continuous vairable",
    "section": "",
    "text": "3.1 General Principles\nTo study relationships between two continuous variables (e.g.Â heigth and weigth), we can use : Linear regression approach. Basically, we draw a line that cross the points clouds of the two tested variables. For this we need to have: 1) an intercept \\(\\alpha\\) which inform us about the starting point of the line, 2) a coefficient \\(\\beta\\) which in inform us about the slope of the line and 3) a error term \\(\\sigma\\) which inform us about spread of points between the line. We can interpret the intercept \\(\\alpha\\) as the mean for of Y for the smaller value of X, the coefficient \\(\\beta\\) as how much Y increase for each increment of X, and \\(\\sigma\\) as the error arround the prediction. So the coefficient \\(\\beta\\) give the strength of the relationship between X and Y and \\(\\sigma\\) the amount of error in the model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear Regression for continuous vairable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#conciderations",
    "href": "1. Linear Regression for continuous variable.html#conciderations",
    "title": "3Â  Linear Regression for continuous vairable",
    "section": "3.2 Conciderations",
    "text": "3.2 Conciderations\n\nBayesian linear regression considers uncertainty in the model parameters and provides a full posterior distribution over them. We thus need to decalre prior distribution for \\(\\alpha\\) , \\(\\beta\\) and \\(\\sigma^2\\) .\nUssually, we use Normal distribution for \\(\\alpha\\) , \\(\\beta\\) and an exponential or Uniform distributiuon for \\(\\sigma\\).\nAs we concider that data is standardized (see introduction) we use a distribution of mean 0 and of standard deviation of 1.\n\\(\\sigma\\) is assumed to be normally distributed and is squared to force positive error and account for values bellow and above the line.\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a link function. This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear Regression for continuous vairable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "3Â  Linear Regression for continuous vairable",
    "section": "3.3 Example",
    "text": "3.3 Example\nBelow is an example code snippet demonstrating Bayesian linear regression using Bayesian Inference (BI) package:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear Regression for continuous vairable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "3Â  Linear Regression for continuous vairable",
    "section": "3.4 Mathematical Details",
    "text": "3.4 Mathematical Details\n\n3.4.1 Formula\nThe following equation allow us to draw a line and is ths one that is most used in statistic clases: \\[\nY = \\alpha + \\beta  X + \\sigma\n\\]\nWhere: - Y is the target variable. - X is the input variable. - \\(\\beta\\) is the regression coefficient. - \\(\\alpha\\) is the intercept term. - \\(\\sigma\\) is the error term.\n\n\n3.4.2 Bayesian model\nThe equivalent version of the Bayesian linear regression model using probability distributions is as follows: \\[\np(Y | X, \\alpha , \\beta ) = Normal(\\alpha + \\beta   X, \\sigma)\n\\]\n\\[\np(\\alpha) = Normal(0, 1)\n\\]\n\\[\np(\\beta) = Normal(0, 1)\n\\]\n\\[\np(\\sigma) = Uniform(0, 50)\n\\]\nWhere: - \\(p(Y |X, \\alpha , \\beta)\\) is the likelihood function (equivalent of the line equation). - \\(p(\\beta)\\) and \\(p(\\alpha)\\) are the prior distributions for the regression coefficients and intercept, respectivelly. - \\(p(\\sigma)\\), controll the variance of the likelihood.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear Regression for continuous vairable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "3Â  Linear Regression for continuous vairable",
    "section": "3.5 Reference(s)",
    "text": "3.5 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear Regression for continuous vairable</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "4Â  Multiple continuous variables model",
    "section": "",
    "text": "4.1 General Principles\nTo study relationships between multiple continuous variables (e.g., effect of height and age on weight), we can use a Multiple Regression approach. Essentially, we extend the Linear Regression for continuous variable by adding a regression coefficient \\(\\beta\\) for each continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "4Â  Multiple continuous variables model",
    "section": "4.2 Considerations",
    "text": "4.2 Considerations\n\nWe have the same considerations as for Regression for continuous variable.\nWe need regression coefficient \\(\\beta\\) for each independent variables ğŸ›ˆ.\nModel interpretation of the regression coefficients \\(\\beta\\) is considered for a fixed value of the other dependent variablesâ€™ regression coefficients â€”i.e., for a given age, a variation of 1 unit in height reflects the value of the regression coefficient \\(\\beta\\) for height.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "4Â  Multiple continuous variables model",
    "section": "4.3 Example",
    "text": "4.3 Example\nBelow is an example code snippet demonstrating Bayesian multiple regression using Bayesian Inference (BI) package:\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import data ------------------------------------------------\nm.data('../data/Howell1.csv', sep=';') \nm.df = m.df[m.df.age &gt; 18]\nm.scale(['weight', 'age'])\nm.data_to_model(['weight', 'height', 'age'])\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    alpha = bi.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = bi.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = bi.dist.normal(0, 0.5, name = 'beta2')\n    sigma = bi.dist.uniform( 1, name = 'sigma')\n\n    lk(\"y\", Normal(alpha + beta1 * weight + beta2 * age, sigma), obs=height)\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "4Â  Multiple continuous variables model",
    "section": "4.4 Mathematical Details",
    "text": "4.4 Mathematical Details\n\n4.4.1 Formula\nWe model the relationship between the independent variables (X1, X2, â€¦, Xn) and the dependent variable (Y) using the following equation:\n\\[\nğ‘Œ = \\alpha +\\beta_1  ğ‘‹_1 + \\beta_2  ğ‘‹_2 + ... + \\beta_n  ğ‘‹_ğ‘› + \\sigma\n\\]\nWhere:\n\n\\(Y\\) is the dependent variable.\n\\(\\alpha\\) is the intercept term.\n\\(X_1\\), \\(X_2\\), â€¦, \\(X_n\\) are the independent variables.\n\\(\\beta_1\\), \\(\\beta_2\\), â€¦, \\(\\beta_n\\) are the regression coefficients.\n\\(sigma\\) is the error term.\n\n\n\n4.4.2 Bayesian model\nWe can express the Bayesian multiple regression model using probability distributions as follows:\n\\[\nğ‘(ğ‘Œâˆ£ğ‘‹, \\alpha,\\beta) = Normal(\\alpha + \\sum_k^n  \\beta_k  X, ÏƒÂ²)\n\\]\n\\[\np(\\alpha) = Normal(0,1)\n\\]\n\\[\np(\\beta_i) = Normal(0,1)\n\\]\n\\[\np(Ïƒ) = Uniform(0, 50)\n\\]\nWhere:\n\n\\(p(Y | ğ‘‹, \\alpha,\\beta)\\) is the likelihood function.\n\\(p(\\alpha)\\) is prior distributions for the intercept\n\\(p(\\beta_k)\\) are the prior distributions for the regression coefficients k distinct regression coefficients.\n\\(p(\\sigma)\\) is the prior distribution for the standard deviation, ensuring - it is positive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "4Â  Multiple continuous variables model",
    "section": "4.5 Reference(s)",
    "text": "4.5 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "",
    "text": "5.1 General Principles\nTo study relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.\nParallel lines indicate that there is no interaction effect while different slopes suggest that one might be present. Below is the plot for Food x Condiment. The crossed lines on the graph suggest that there is an interaction effect, which the significant p-value for the Food*Condiment term confirms. The graph shows that enjoyment levels are higher for chocolate sauce when the food is ice cream. Conversely, satisfaction levels are higher for mustard when the food is a hot dog. If you put mustard on ice cream or chocolate sauce on hot dogs, you wonâ€™t be happy!",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "5.2 Considerations",
    "text": "5.2 Considerations\n\nWe have the same considerations as for Regression for continuous variable.\nModel relationship between Y and R to vary as a function of A. you explicitly model the hypothesis that the slope between Y and R dependsâ€”is conditionalâ€”upon A.\nFor continuous interactions, the intercept becomes the grand mean of the outcome variable. This ease of interpretation alone is a good reason to center predictor variables.\nEstimate interpretation is more difficult as estimate of non-interaction terms become expected change in Y when R increases by one unit and A is at its average value and estimate of interaction terms are expected change in the influence of A on Y when increasing R by one unit and expected change in the influence of R on Y when increasing A by one unit.\nTriptych ğŸ›ˆ plots are very handy for understanding the impact of interactions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "5.3 Example",
    "text": "5.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with an interaction term between two continuous variables with the Bayesian Inference (BI) package:\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import data ------------------------------------------------\nm.data('../data/tulips.csv', sep=';') \nm.scale(['blooms', 'water', 'shade'])\nm.data_to_model(['blooms', 'water', 'shade'])\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    alpha = dist.normal(0.5, 0.25, name = 'alpha')\n    sigma = dist.exponential(1, name = 'sigma')\n    beta1 = dist.normal(0, 0.25, name = 'beta1')\n    beta2 = dist.normal(0, 0.25, name = 'beta2')\n    beta_interaction_ = dist.normal(0, 0.25, name = 'beta_interaction_')    \n    lk(\"y\", Normal(a + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma), obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "5.4 Mathematical Details",
    "text": "5.4 Mathematical Details",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#formula",
    "href": "3. Interaction between continuous variables.html#formula",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "5.5 Formula",
    "text": "5.5 Formula\nWe model the relationship between the input features (X1 and X2) and the target variable (Y) using the following equation: \\[\nğ‘Œ = \\alpha + \\beta_1ğ‘‹_1âˆ—+\\beta_2ğ‘‹_2+\\\\beta_{interaction}ğ‘‹_1ğ‘‹_2 + $\\sigma$\n\\]\nWhere:\n\n\\(Y\\) is the target variable.\n\\(\\alpha\\) is the intercept term.\n\\(X_1\\) and \\(X_2\\) are the two independent continuous variables.\n\\(\\beta_1\\) and \\(\\beta_2\\) are the regression coefficients for \\(X_1\\) and \\(X_2\\), respectively.\n\\(\\beta_{interaction}\\) is the regression coefficient for the interaction term \\((X_1  X_2)\\).\n\\(\\sigma\\) is the error term assumed to be normally distributed.\n\nIn this context, the interaction term \\(X_1 * X_2\\) captures the joint effect of \\(X_1\\) and \\(X_2\\) on the target variable \\(Y\\).\n\n5.5.1 Bayesian model\nWe can express the Bayesian regression model with an interaction term between two continuous variables using probability distributions as follows:\n\\[\np(Yâˆ£X_1â€‹ ,X_2â€‹ , \\beta_1, \\beta_2, \\beta_{interaction} ) = Normal(\\alpha +  \\beta_1  X_1â€‹ + \\beta_2  X_2â€‹â€‹ + \\beta_{interaction}  X_1â€‹ X_2â€‹ ,  $\\sigma$)\n\\]\n\\[\nğ‘(\\alpha)=Normal(0,1)\n\\]\n\\[\nğ‘(\\beta_1)=Normal(0,1)\n\\]\n\\[\nğ‘(\\beta_2)=Normal(0,1)\n\\]\n\\[\nğ‘(\\beta_{interaction})=Normal(0,1)\n\\]\n\\[\nğ‘(Ïƒ)=Exponential(1)\n\\]\nWhere:\n\n\\(p(Y | X_1â€‹ ,X_2â€‹ , \\beta_1, \\beta_2, \\beta_{interaction})\\) is the likelihood function.\n\\(p(\\alpha)\\) is the prior distribution for the intercept\n\\(p(\\beta_1)\\), \\(p(\\beta_2)\\) and \\(\\beta_{interaction}\\) are the prior distributions for the regression coefficients.\n\\(p(\\sigma)\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "5Â  Interaction Term between Two Continuous Variables",
    "section": "5.6 Reference(s)",
    "text": "5.6 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Interaction Term between Two Continuous Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "6Â  Regression for Categorical Variables",
    "section": "",
    "text": "6.1 General Principles\nTo study the relationship between a categorical independent variable and a continuous dependent variable, we use Categorical model wich apply stratification.\nStratification concist in modeling how the different categories of the independent variable affect the target continuous variable, by performing a regression for each categories and asing a regression coefficient for each categories. To realize the stratification, categorical variables are often encoded using one-hot encoding or converting categories to indeces.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "6Â  Regression for Categorical Variables",
    "section": "6.2 Considerations",
    "text": "6.2 Considerations\n\nWe have the same considerations as for Regression for continuous variable.\nAs we generate regression coefficients for each ( k ) category in the code, we need to specify a prior with a shape equal to the number of categories (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare the confidence intervals or p-values directly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "6Â  Regression for Categorical Variables",
    "section": "6.3 Example",
    "text": "6.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with an independent categorical variable:\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import data ------------------------------------------------\nm.data('../data/milk.csv', sep=';') \nm.index([\"clade\"])\nm.scale(['kcal_per_g'])\nm.data_to_model(['kcal_per_g', \"index_clade\"])\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade):    \n    beta = bi.dist.normal(0, 0.5, shape = (4,), name = 'beta') # we specify a vector of length 4 as we have 4 categories\n    sigma = bi.dist.exponential( 1,  name = 'sigma')\n    lk(\"y\", Normal(beta[index_clade], s), obs= kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "6Â  Regression for Categorical Variables",
    "section": "6.4 Mathematical Details",
    "text": "6.4 Mathematical Details\n\n6.4.1 Formula\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\\[ğ‘Œ=\\alpha + \\beta_k X_i + \\sigma\\]\nWhere:\n\n\\(Y\\) is the target variable.\n\\(X\\) is the encoded categorical input variable .\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_k\\) are the regression coefficient for each k categories.\n\\(X\\) is the independet varible\n\\(\\sigma\\) is the error term .\n\nWe can interpret \\(\\beta_i\\) as the effect of each category on \\(Y\\) relative to the baseline (usually one of the categories or the intercept).\n\n\n6.4.2 Bayesian model\nWe can express the Bayesian regression model with a categorical independent variable using probability distributions as follows:\n\\[\nğ‘(ğ‘Œ_iâˆ£\\alpha, ğ‘‹_i,\\beta_i)=Normal(\\alpha +  \\beta_k X,ğœ) \\\\\nğ‘(\\alpha)=Normal(0,1)\\\\\nğ‘(\\beta_k)=Normal(0,1)\\\\\nğ‘(ğœ)=Exponential(1)\n\\]\nWhere:\n\n\\(p(ğ‘Œ_iâˆ£\\alpha, ğ‘‹_i, \\beta_i)\\) is the likelihood function.\n\\(p(\\alpha)\\) is prior distributions for the intercept\n\\(p(\\beta_k)\\) are k prior distributions for k regression coefficients.\n\\(p(\\sigma)\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "6Â  Regression for Categorical Variables",
    "section": "6.5 Notes",
    "text": "6.5 Notes\n\nWe can apply multiple variables similarly as chapter 2: Multiple continuous Variables.\nWe can apply interaction terms similarly as chapter 3: Interaction between continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "6Â  Regression for Categorical Variables",
    "section": "6.6 Reference(s)",
    "text": "6.6 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html",
    "href": "5. Binomial regression.html",
    "title": "7Â  Binomial model",
    "section": "",
    "text": "7.1 General Principles\nTo model the relationship between a binary outcome -e.g.Â such as success/failure, yes/no, or 1/0.- variable and one or more independent variables, we can use Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html#considerations",
    "href": "5. Binomial regression.html#considerations",
    "title": "7Â  Binomial model",
    "section": "7.2 Considerations",
    "text": "7.2 Considerations\n\nWe have the same considerations as for Regression for continuous variable.\nWe have the firs link function logit. The logit link function in Bayesian binomial model converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html#example",
    "href": "5. Binomial regression.html#example",
    "title": "7Â  Binomial model",
    "section": "7.3 Example",
    "text": "7.3 Example\nBelow is an example code snippet demonstrating Bayesian binomial regression\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import data ------------------------------------------------\nm.data('../data/chimpanzees.csv', sep=';') \nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    alpha = dist.normal( 0, 10)\n    lk(\"y\", Binomial(logits= alpha[actor] + beta1[side] + beta2[cond]), obs=pulled_left)\n\n# Run mcmc ------------------------------------------------\nm.run(model, init_strategy = numpyro.infer.initialization.init_to_mean()) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html#mathematical-details",
    "href": "5. Binomial regression.html#mathematical-details",
    "title": "7Â  Binomial model",
    "section": "7.4 Mathematical Details",
    "text": "7.4 Mathematical Details\n\n7.4.1 Formula\nWe model the relationship between the independent variable (\\(X\\)) and the binary outcome variable (\\(Y\\)) using the following equation: \\[\nlogit(Y)=\\alpha + \\beta   ğ‘‹\n\\]\nWhere:\n\n\\(Y\\) is the probability of success (or the probability of the binary outcome being 1).\n\\(X\\), is an independent variables.\n\\(\\beta\\) is the regression coefficients.\n\\(\\alpha\\) is the intercept term.\n\\(\\sigma\\) is the error term.\n\\(logit(Y)\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variables on the log-odds of success.\n\n\n\n7.4.2 Bayesian model\nWe can express the Bayesian binomial regression model using probability distributions as follows:\n\\[\nğ‘(ğ‘Œâˆ£\\alpha, \\beta, ğ‘‹) = Binomial(ğ‘›=1, ğ‘=logit(\\alpha + \\beta  ğ‘‹ ))\n\\] $$ ğ‘()=Normal(0,1)\\ ğ‘()=Normal(0,1)\\\n$$\nWhere:\n\n\\(p(Y | ğ‘Œâˆ£\\alpha, \\beta, ğ‘‹)\\) is the likelihood function.\n\\(p(\\beta)\\) and \\(p(\\alpha)\\) are the prior distributions for the regression coefficients and intercept, respectivelly.\n\\(n=1\\) represents the number of trials in the binomial distribution (binary outcome).\n\\(\\logit(\\alpha + \\beta  ğ‘‹ )\\) is the logit link function that is equal to sigmoid function applied to the linear combination of predictors, mapping the log-odds to probabilities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html#notes",
    "href": "5. Binomial regression.html#notes",
    "title": "7Â  Binomial model",
    "section": "7.5 Notes",
    "text": "7.5 Notes\n\nWe can apply multiple variables similarly as chapter 2.\nWe can apply interaction terms similarly as chapter 3.\nWe can apply caterogical variables similarly as chapter 4.\nBelow is an example code snippet demonstrating Bayesian binomial model for multiple caterogical variables :\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import data ------------------------------------------------\nm.data('../data/chimpanzees.csv', sep=';') \nm.df[\"side\"] = m.df.prosoc_left  # right 0, left 1\nm.df[\"cond\"] = m.df.condition  # no partner 0, partner 1\nm.data_to_model(['pulled_left', \"actor\", \"side\", \"cond\"])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    alpha = bi.dist.normal(0, 10, shape = (7,), \"alpha\") # generating k intercept  #(one for each actor)\n    beta1 = bi.dist.normal(0, 10, shape = (2,), \"beta\") # generating k regression coefficient  for each k prosoc_left)\n    beta2 = bi.dist.normal(0, 10, shape = (2,), \"alpha\") # generating k regression coefficient for each k condition)\n    lk(\"y\", Binomial(logits= alpha[actor] + beta1[side] + beta2[cond]), obs=pulled_left)\n\n# Run mcmc ------------------------------------------------\nm.run(model, init_strategy = numpyro.infer.initialization.init_to_mean()) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "5. Binomial regression.html#references",
    "href": "5. Binomial regression.html#references",
    "title": "7Â  Binomial model",
    "section": "7.6 Reference(s)",
    "text": "7.6 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Binomial model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html",
    "href": "6. Poisson regression.html",
    "title": "8Â  Poisson model",
    "section": "",
    "text": "8.1 General Principles\nTo model the relationship between a count outcome variable -e.g.Â counts of events occurring in a fixed interval of time or space- and one or more independent variables, we can use Poisson model.\nThis is a special shape of the binomial distribution, it is useful because it model binomial events for which the number of trials n is unknown or uncountably large.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html#considerations",
    "href": "6. Poisson regression.html#considerations",
    "title": "8Â  Poisson model",
    "section": "8.2 Considerations",
    "text": "8.2 Considerations\n\nWe have the same considerations as for Regression for continuous variable.\nWe have the second link function. The conventional link function for a Poisson model is the log link (it ensures that Î» i is always positive).\nTo invert the log link function and model linearly the relationship between the predictor variables and the log of the mean rate parameter we can apply the exponential function (see comment in code)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html#example",
    "href": "6. Poisson regression.html#example",
    "title": "8Â  Poisson model",
    "section": "8.3 Example",
    "text": "8.3 Example\nBelow is an example code snippet demonstrating Bayesian Poisson model\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi()\n\n# Import data ------------------------------------------------\nm.data('../data/Kline.csv', sep=';') \nm.sale([\"P\"]) \nm.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n\nm.data_to_model(['total_tools', 'P', 'cid'])\n\n\n# Define model ------------------------------------------------\ndef model(cid, P, total_tools):\n    alpha = dist.normal(3, 0.5, name='alpha')\n    beta = dist.normal(0, 0.2, name='beta')\n    lk(\"y\", Poisson(jnp.exp(alpha[cid] + beta[cid]*P)), obs=total_tools) # Exponential ensure non-negative values and invert the log link function\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html#mathematical-details",
    "href": "6. Poisson regression.html#mathematical-details",
    "title": "8Â  Poisson model",
    "section": "8.4 Mathematical Details",
    "text": "8.4 Mathematical Details\n\n8.4.1 Formula\nWe model the relationship between the predictor variable (\\(X\\)) and the count outcome variable (\\(Y\\)) using the following equation:\n\\[\nlog(ğœ†)=\\alpha + \\beta  X\n\\]\nWhere:\n\n\\(\\lambda\\) is the mean rate parameter of the Poisson distribution (expected count).\n\\(X\\) is the predictor variables.\n\\(\\beta\\) is the regression coefficients.\n\\(\\alpha\\) is the intercept term.\n\\(\\log(\\lambda)\\) is the log of the mean rate parameter, ensuring it is positive.\n\n\n\n8.4.2 Bayesian model\nWe can express the Bayesian Poisson regression model using probability distributions as follows:\n\\[\np(Yâˆ£\\alpha, \\beta, X)=Poisson(Î»=exp(\\alpha + \\beta X))\n\\]\n\\[\np(\\alpha)=Normal(0, 1)\n\\]\n\\[\np(\\beta)=Normal(0, 1)\n\\]\nWhere:\n\n\\(p(Y |alpha, \\beta, X)\\) is the likelihood function.\n\\(p(\\beta)\\) and \\(p(\\alpha)\\) are the prior distributions for the regression coefficients and intercept.\n\\(\\lambda = \\exp(\\alpha + \\beta X)\\) is the mean rate parameter of the Poisson distribution, modeled as the exponential function of the linear combination of predictors.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html#notes",
    "href": "6. Poisson regression.html#notes",
    "title": "8Â  Poisson model",
    "section": "8.5 Notes",
    "text": "8.5 Notes\n\nWe can apply multiple variables similarly as chapter 2.\nWe can apply interaction terms similarly as chapter 3.\nWe can apply caterogical variables similarly as chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "6. Poisson regression.html#references",
    "href": "6. Poisson regression.html#references",
    "title": "8Â  Poisson model",
    "section": "8.6 Reference(s)",
    "text": "8.6 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Negative binomial.html",
    "href": "7. Negative binomial.html",
    "title": "9Â  Negative binomial model",
    "section": "",
    "text": "9.1 General Principles\nTo model the relationship between a count outcome variable and one or more independent variables with overdispersion, we can use Negative Binomial model. Overdispersion is handled because the Negative-binomial model assumes that each Poisson count observation has its own rate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Negative binomial model</span>"
    ]
  },
  {
    "objectID": "7. Negative binomial.html#considerations",
    "href": "7. Negative binomial.html#considerations",
    "title": "9Â  Negative binomial model",
    "section": "9.2 Considerations",
    "text": "9.2 Considerations\n\nWe have the same considerations as for Poisson model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Negative binomial model</span>"
    ]
  },
  {
    "objectID": "7. Negative binomial.html#example",
    "href": "7. Negative binomial.html#example",
    "title": "9Â  Negative binomial model",
    "section": "9.3 Example",
    "text": "9.3 Example\n# Simulate data -------------------------------------------------------------\nimport tensorflow_probability.substrates.jax.distributions as tfd\ninit_key, sample_key = random.split(random.PRNGKey(int(r.randint(0, 10000000))))\ninit_key = jnp.array(init_key)\nnum_days = 30\ny = tfd.Poisson(rate=1.5).sample(seed = init_key, sample_shape=(num_days,))\nnum_weeks = 4\ny_new = tfd.Poisson(rate=0.5 * 7).sample(seed = init_key, sample_shape=(num_weeks,))\ny_all = np.concatenate([y, y_new])\nexposure = np.concatenate([np.repeat(1, 30), np.repeat(7, 4)])\nmonastery = np.concatenate([np.repeat(0, 30), np.repeat(1, 4)])\nd = pd.DataFrame.from_dict(dict(y=y_all, days=exposure, monastery=monastery))\nd[\"log_days\"] = d.days.pipe(np.log)\n\nfrom main import*\n# Setup device------------------------------------------------\nm = bi()\n\n# import data ------------------------------------------------\nm.data_on_model = dict(\n    log_days = jnp.array(d.log_days.values), # rate of each counts data\n    monastery = jnp.array(d.monastery.values),\n    output = jnp.array(d.y.values)\n)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, output):\n    a = dist.normal(0, 1, shape=[1], name = 'a')\n    b = dist.normal(0, 1, shape=[1], name = 'b')\n    l = log_days + a +  b * monastery\n    lk(\"y\", Poisson(rate = l), obs=output)\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Negative binomial model</span>"
    ]
  },
  {
    "objectID": "7. Negative binomial.html#mathematical-details",
    "href": "7. Negative binomial.html#mathematical-details",
    "title": "9Â  Negative binomial model",
    "section": "9.4 Mathematical Details",
    "text": "9.4 Mathematical Details\n\n9.4.1 Formula\nWe model the relationship between the independent variable \\(X\\) and the count outcome variable \\(Y\\) using the following equation:\n\\(logâ¡(ğœ†)= exp(\\alpha + \\beta ğ‘‹)\\)\nWhere:\n\n\\(\\lambda\\) is the mean rate parameter of the negative binomial distribution (expected count).\n\\(X\\) is the predictor variables.\n\\(\\beta\\) is the regression coefficients.\n\\(\\alpha\\) is the intercept term.\n\\(\\log(\\lambda)\\) is the log of the mean rate parameter, ensuring it is positive.\n\nWe can express the Bayesian Negative Binomial regression model using probability distributions as follows:\n\n\n9.4.2 Bayesian model\n\\[\nğ‘(ğ‘Œâˆ£\\alpha, \\beta, X)=Poisson( Î»=exp(rates + \\alpha + \\beta X)))\n\\]\n\\[\nğ‘(\\alpha)=Normal(0,1)\n\\]\n\\[\nğ‘(\\beta)=Normal(0,1)\n\\]\nWhere:\n\n\\(p(Y | \\alpha, \\beta, X)\\) is the likelihood function.\n\\(p(\\beta)\\) and \\(p(\\alpha)\\) are the prior distributions for the regression coefficients and intercept.\n\\(\\lambda = \\exp(rates +\\alpha + \\beta X)\\) is the mean rate parameter of the Poisson distribution, modeled as the exponential function of the linear combination of predictors and assuming that each Poisson count observation has its own rate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Negative binomial model</span>"
    ]
  },
  {
    "objectID": "8. Multinomial.html",
    "href": "8. Multinomial.html",
    "title": "10Â  Multinomial Regression",
    "section": "",
    "text": "10.1 General Principles\nTo model the relationship between a categorical outcome variable with more than two categories and one or more predictor variables, we can use Multinomial Regression. The relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Multinomial Regression</span>"
    ]
  },
  {
    "objectID": "8. Multinomial.html#formula",
    "href": "8. Multinomial.html#formula",
    "title": "10Â  Multinomial Regression",
    "section": "10.2 Formula",
    "text": "10.2 Formula\nWe model the relationship between the predictor variables (X1, X2, â€¦, Xn) and the categorical outcome variable (Y) using the following equation:\n\\[\nlogit(ğ‘_ğ‘˜)=ğ‘‹_1âˆ—ğ‘Š_{1ğ‘˜}+ğ‘‹_2âˆ—ğ‘Š_{2ğ‘˜}+...+ğ‘‹_ğ‘›âˆ—ğ‘Š_{ğ‘›ğ‘˜}+ğ‘_k\n\\] Where:\n\\(p_k\\) is the probability of category \\(k\\). \\(X1, X2, ..., Xn\\) are the predictor variables. \\(W1k, W2k, ..., Wnk\\) are the regression coefficients for category \\(k\\). \\(b_k\\) is the intercept term for category \\(k\\). \\(logit(ğ‘_ğ‘˜)\\) is the log-odds of category \\(k\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Multinomial Regression</span>"
    ]
  },
  {
    "objectID": "8. Multinomial.html#considerations",
    "href": "8. Multinomial.html#considerations",
    "title": "10Â  Multinomial Regression",
    "section": "10.3 Considerations",
    "text": "10.3 Considerations\nIn Bayesian Multinomial regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for \\(W\\) and \\(b_k\\) for each category k.\n  Additional conciderations :  \n\nIn a multinomial model, you need K âˆ’ 1 linear models for K types of events. Estimates need to be converted to a simplex ğŸ›ˆ. For this, we can convert regressions outputs using the softmax function (see â€œnn.softmaxâ€ line in code).\nThe relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Multinomial Regression</span>"
    ]
  },
  {
    "objectID": "8. Multinomial.html#example",
    "href": "8. Multinomial.html#example",
    "title": "10Â  Multinomial Regression",
    "section": "10.4 Example",
    "text": "10.4 Example\nfrom BI import bi.hard\n# Data simulation--------------------------------------------------------------------------\n## simulate career choices among 500 individuals\nN = 500  # number of individuals\nincome = np.array([1, 2, 5])  # expected income of each career\nscore = 0.5 * income  # scores for each career, based on income\n\n## next line converts scores to probabilities\np = jnp.array(tf.nn.softmax(score))\n\n## now simulate choice\n## outcome career holds event type values, not counts\ncareer = tfd.Categorical(probs=p).sample(seed = init_key, sample_shape = N)\nresult = [income[index] for index in career]\ndata = {'career': career, 'income': result}\nd = pd.DataFrame(data)\ncareer = jnp.array(d.career.values)\ncareer_income = jnp.array(d.income.values)\nincome = jnp.array(income)\n\n# Model-------------------------------------------------------------------------------------\n\ndef model():\n    a = yield normal(2, 0, 1)\n    b = yield halfnormal(1,0.5)\n    ##  K âˆ’ 1 linear models\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0]\n\n    # converted to a vector of probabilities \n    p = nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]])) \n    p = p[career] \n\n    y = yield Independent(Categorical(probs =  p))\n\nposterior, sample_stats = NUTS(model, obs = career)",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Multinomial Regression</span>"
    ]
  },
  {
    "objectID": "8. Multinomial.html#mathematical-details",
    "href": "8. Multinomial.html#mathematical-details",
    "title": "10Â  Multinomial Regression",
    "section": "10.5 Mathematical Details",
    "text": "10.5 Mathematical Details\nWe can express the Bayesian Multinomial model as follows:\nIf \\(KâˆˆN ,  NâˆˆN , and  Î¸âˆˆK\\)-simplex , then for \\(yâˆˆNK\\) such that \\(âˆ‘Kk=1yk=N\\):\n\\[\nMultinomial(y|Î¸)=(Ny1,â€¦,yK)Kâˆk=1 Î¸ykk\n\\]\nWhere the multinomial coefficient is defined by:\n\\[\n(\\frac{N}{y_1,â€¦,y_k})=\\frac{N!}{âˆ^K_{k=1}yk!}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Multinomial Regression</span>"
    ]
  },
  {
    "objectID": "9. Beta binomial.html",
    "href": "9. Beta binomial.html",
    "title": "11Â  Beta-Binomial Regression",
    "section": "",
    "text": "11.1 General Principles\nTo model the relationship between a binary outcome variable representing success counts and one or more predictor variables, we can use Beta-Binomial Regression. This approach is suitable when the outcome variable follows a beta-binomial distribution, which accounts for overdispersion relative to the binomial distribution.\nWe model the relationship between the predictor variables (X1, X2, â€¦, Xn) and the probability of success (p) using the following equation:",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Beta-Binomial Regression</span>"
    ]
  },
  {
    "objectID": "9. Beta binomial.html#formula",
    "href": "9. Beta binomial.html#formula",
    "title": "11Â  Beta-Binomial Regression",
    "section": "11.2 Formula",
    "text": "11.2 Formula\n\\[\nlogit(ğ‘)=ğ‘‹_1âˆ—ğ‘Š_1+ğ‘‹_2âˆ—ğ‘Š_2+...+ğ‘‹_ğ‘›âˆ—ğ‘Š_ğ‘›+ğ‘\n\\]\nWhere:\n\n\\(p\\) is the probability of success.\n\\(X1, X2, ..., Xn\\) are the predictor variables.\n\\(W1, W2, ..., Wn\\) are the regression coefficients.\n\\(b\\) is the intercept term.\n\\(\\text{logit}(p)\\) is the log-odds of success.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Beta-Binomial Regression</span>"
    ]
  },
  {
    "objectID": "9. Beta binomial.html#considerations",
    "href": "9. Beta binomial.html#considerations",
    "title": "11Â  Beta-Binomial Regression",
    "section": "11.3 Considerations",
    "text": "11.3 Considerations\nIn Bayesian Beta-Binomial regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for W1, W2, â€¦, Wn, and b.\n  Additional conciderations :  \n\nThe relationship between the predictor variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of success.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Beta-Binomial Regression</span>"
    ]
  },
  {
    "objectID": "9. Beta binomial.html#example",
    "href": "9. Beta binomial.html#example",
    "title": "11Â  Beta-Binomial Regression",
    "section": "11.4 Example",
    "text": "11.4 Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression:\nfrom BI import bi.hard\nd = pd.read_csv('/home/sosa/BI/data/UCBadmit.csv', sep = ';')\nd[\"gid\"] = (d[\"applicant.gender\"] != \"male\").astype(int)\ngid = jnp.array(d[\"gid\"].astype('int32').values)\napplications = jnp.array(d[\"applications\"].astype('float32').values)\nadmit = jnp.array(d[\"admit\"].astype('float32').values)\n\ndef model():\n    phi = yield exponential(1, 1)\n    #phi2 =  tfp.bijectors.Exp().forward(phi)\n    alpha = yield normal(2,0.,1.5)\n    theta = phi + 2\n    pbar = nn.sigmoid(alpha[gid])\n    concentration1 = pbar*theta\n    concentration0 = (1 - pbar) * theta\n    y = yield Independent(BetaBinomial(applications, concentration1 = concentration1, concentration0 = concentration0), reinterpreted_batch_ndims=1)\n\nposterior, sample_stats = NUTSdual(model, obs = admit)",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Beta-Binomial Regression</span>"
    ]
  },
  {
    "objectID": "9. Beta binomial.html#mathematical-details",
    "href": "9. Beta binomial.html#mathematical-details",
    "title": "11Â  Beta-Binomial Regression",
    "section": "11.5 Mathematical Details",
    "text": "11.5 Mathematical Details\nWe can express the Bayesian Beta-Binomial regression model using probability distributions as follows:\n\\[\nğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘)=BetaBinomial(ğ‘›,ğ›¼=ğ‘â‹…ğ‘›,ğ›½=(1âˆ’ğ‘)â‹…ğ‘›)\n\\]\n\\[\nğ‘(ğ‘Šğ‘–)=Normal(0,ğ›¼2)\n\\] \\[\nğ‘(ğ‘)=Normal(0,ğ›½2)\n\\]\nWhere:\n\n\\(p(Y | X, W, b)\\) is the likelihood function.\n\\(p(Wi)\\) and \\(p(b)\\) are the prior distributions for the regression coefficients and intercept.\n$n is the total count of trials.\n\\(\\alpha = p \\cdot n\\) and \\(\\beta = (1 - p) \\cdot n\\) are the concentration parameters of the Beta distribution, with \\(p = \\text{sigmoid}(X * W + b)\\) being the success probability.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Beta-Binomial Regression</span>"
    ]
  },
  {
    "objectID": "10. Negative-binomial.html",
    "href": "10. Negative-binomial.html",
    "title": "12Â  Negative Binomial Regression",
    "section": "",
    "text": "12.1 Formula\nWe model the relationship between the predictor variables (X1, X2, â€¦, Xn) and the count outcome variable (Y) using the following equation: \\[\nlog(ğœ†)=ğ‘‹_1âˆ—ğ‘Š_1+ğ‘‹_2âˆ—ğ‘Š_2+...+ğ‘‹_ğ‘›âˆ—ğ‘Š_ğ‘›+ğ‘\n\\]\nWhere:\nThe relationship between the predictor variables and the log of the mean rate parameter is modeled linearly, similar to Poisson regression.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "10. Negative-binomial.html#formula",
    "href": "10. Negative-binomial.html#formula",
    "title": "12Â  Negative Binomial Regression",
    "section": "",
    "text": "\\(\\lambda\\) is the mean rate parameter of the negative binomial distribution (expected count).\n\\(X1, X2, ..., Xn\\) are the predictor variables.\n\\(W1, W2, ..., Wn\\) are the regression coefficients.\n\\(b\\) is the intercept term.\n\\(\\log(\\lambda)\\) is the log of the mean rate parameter, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "10. Negative-binomial.html#considerations",
    "href": "10. Negative-binomial.html#considerations",
    "title": "12Â  Negative Binomial Regression",
    "section": "12.2 Considerations",
    "text": "12.2 Considerations\nIn Bayesian Negative Binomial regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for W1, W2, â€¦, Wn, and b.\n  Additional conciderations :  \n\nThe relationship between the predictor variables and the log of the mean rate parameter is modeled linearly, similar to Poisson regression.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "10. Negative-binomial.html#example",
    "href": "10. Negative-binomial.html#example",
    "title": "12Â  Negative Binomial Regression",
    "section": "12.3 Example",
    "text": "12.3 Example\nBelow is an example code snippet demonstrating Bayesian Negative Binomial regression:\nfrom BI import bi.hard\nd = pd.read_csv('/home/sosa/BI/data/UCBadmit.csv', sep = ';')\nd[\"gid\"] = (d[\"applicant.gender\"] != \"male\").astype(int)\ngid = jnp.array(d[\"gid\"].astype('int32').values)\napplications = jnp.array(d[\"applications\"].astype('float32').values)\nadmit = jnp.array(d[\"admit\"].astype('float32').values)\n\ndef model():\n    phi = yield exponential(1, 1)\n    alpha = yield normal(2,0.,1.5)\n    theta = phi + 2\n    pbar = nn.sigmoid(alpha[gid])\n    concentration1 = pbar*theta\n    concentration0 = (1 - pbar) * theta\n    y = yield Independent(BetaBinomial(applications, concentration1 = concentration1, concentration0 = concentration0), reinterpreted_batch_ndims=1)\n\nposterior, sample_stats = NUTS(model, obs = admit)",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "10. Negative-binomial.html#mathematical-details",
    "href": "10. Negative-binomial.html#mathematical-details",
    "title": "12Â  Negative Binomial Regression",
    "section": "12.4 Mathematical Details",
    "text": "12.4 Mathematical Details\nWe can express the Bayesian Negative Binomial regression model using probability distributions as follows: \\[\nğ‘(ğ‘Œâˆ£ğ‘‹,ğ‘Š,ğ‘)=NegativeBinomial(ğ‘Ÿ,p = \\frac{1}{1 + \\exp(X * W + b)})\n\\]\n\\[\np(W_i)=Normal(0,Î±^2 )\n\\]\n\\[\np(b)=Normal(0,Î²^2)\n\\]\nWhere:\n\n\\(p(Y | X, W, b)\\) is the likelihood function.\n\\(p(Wi)\\) and \\(p(b)\\) are the prior distributions for the regression coefficients and intercept.\n\\(r\\) represents the shape parameter of the negative binomial distribution.\n\\(p = \\frac{1}{1 + \\exp(X * W + b)}\\) is the success probability parameter of the negative binomial distribution, modeled as the logistic function of the linear combination of predictors.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html",
    "href": "11. Zero inflated.html",
    "title": "13Â  Zero inflated",
    "section": "",
    "text": "13.1 General Principles\nZero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Zero inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#formula",
    "href": "11. Zero inflated.html#formula",
    "title": "13Â  Zero inflated",
    "section": "13.2 Formula",
    "text": "13.2 Formula\nWe model the relationship between the predictor variables (X1, X2, â€¦, Xn) and the count outcome variable (Y) using two components: 1. A logistic regression model to predict the probability of an excess zero. 2. A count model (e.g., Poisson or Negative Binomial) to predict the count outcome.\nThe overall model can be represented as follows:\n\\[\n\\begin{aligned}\n& \\text{logit}(\\pi) = X_1 * W_{1\\pi} + X_2 * W_{2\\pi} + ... + X_n * W_{n\\pi} + b_\\pi+ \\epsilon_{\\pi} \\\\\n& \\text{log}(\\lambda) = X_1 * W_{1\\lambda} + X_2 * W_{2\\lambda} + ... + X_n * W_{n\\lambda} + b_\\lambda + \\epsilon_{\\lambda}\\\\\n& Y \\sim \\begin{cases}\n0 & \\text{with probability } \\pi \\\\\n\\text{CountModel}(\\lambda) & \\text{with probability } (1 - \\pi)\n\\end{cases}\n\\end{aligned}\n\\]\nWhere: -  is the probability of an excess zero. -  is the mean rate parameter of the count model. - X1, X2, â€¦, Xn are the predictor variables. - W{1}, W_{2}, â€¦, W_{n}_ are the regression coefficients for the logistic model. - W{1}, W_{2}, â€¦, W_{n}_ are the regression coefficients for the count model. - b_ and b_ are the intercept terms for the logistic and count models, respectively. - \\(\\epsilon_{\\pi}\\) and \\(\\epsilon_{\\lambda}\\) a the error term, typically assumed to be normally distributed with mean 0 and variance \\(\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Zero inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#considerations",
    "href": "11. Zero inflated.html#considerations",
    "title": "13Â  Zero inflated",
    "section": "13.3 Considerations",
    "text": "13.3 Considerations\nIn Bayesian Zero-Inflated regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for W{1}, W_{2}, â€¦, W_{n}, W{1}, W{2}, â€¦, W_{n}, b, and b_.\n  Additional conciderations :",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Zero inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#example",
    "href": "11. Zero inflated.html#example",
    "title": "13Â  Zero inflated",
    "section": "13.4 Example",
    "text": "13.4 Example\nBelow is an example code snippet demonstrating Bayesian Zero-Inflated Poisson regression using TensorFlow Probability:\nfrom BI import bi.hard\n# Data simulation --------------------\n# Define parameters\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n\n# sample one year of production\nN = 365\n\nnp.random.seed(365)\ndrink = np.random.binomial(1, prob_drink, N)\ny = (1 - drink) * np.random.poisson(rate_work, N)\nd = pd.DataFrame(y)\n\n# Model --------------------\ndef model():\n    al = yield normal(1, 1, 0.5)\n    ap = yield normal(1, -1.5 , 1)\n    y = yield Independent(ZeroInflatedNegativeBinomial(total_count = 365, inflated_loc_logits = al, logits = jnp.log(ap)), reinterpreted_batch_ndims=1)\n\nposterior, sample_stats = NUTSdual(model, obs = jnp.array(d.iloc[:,0].values))",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Zero inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#mathematical-details",
    "href": "11. Zero inflated.html#mathematical-details",
    "title": "13Â  Zero inflated",
    "section": "13.5 Mathematical Details",
    "text": "13.5 Mathematical Details\nWe can express the Bayesian Zero-Inflated Poisson regression model using probability distributions as follows:\n\\[\np(Y | X, W_\\pi, b_\\pi, W_\\lambda, b_\\lambda) = \\pi \\cdot \\delta_0(Y) + (1 - \\pi) \\cdot \\text{Poisson}(\\lambda) \\\\\n\\pi = \\text{sigmoid}(X * W_\\pi + b_\\pi) \\\\\n\\lambda = \\exp(X * W_\\lambda + b_\\lambda) \\\\\np(W_{\\pi_i}) = \\text{Normal}(0, \\alpha^2) \\\\\np(W_{\\lambda_i}) = \\text{Normal}(0, \\alpha^2) \\\\\np(b_\\pi) = \\text{Normal}(0, \\beta^2) \\\\\np(b_\\lambda) = \\text{Normal}(0, \\beta^2)\\]\nWhere: - \\(p(Y | X, W_\\pi, b_\\pi, W_\\lambda, b_\\lambda)\\) is the likelihood function for the zero-inflated model. - \\(p(W_{\\pi_i})\\), \\(p(W_{\\lambda_i})\\), \\(p(b_\\pi)\\), and \\(p(b_\\lambda)\\) are the prior distributions for the regression coefficients and intercept terms. - \\(\\delta_0(Y)\\) represents a Dirac delta function at zero, indicating the probability of an excess zero. - \\(\\text{Poisson}(\\lambda)\\) is the Poisson distribution with rate parameter .",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Zero inflated</span>"
    ]
  },
  {
    "objectID": "12. Varying interceps.html",
    "href": "12. Varying interceps.html",
    "title": "14Â  Varying interceps",
    "section": "",
    "text": "14.1 General Principles\nTo model the relationship between predictor variables and an outcome variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Varying interceps</span>"
    ]
  },
  {
    "objectID": "12. Varying interceps.html#formula",
    "href": "12. Varying interceps.html#formula",
    "title": "14Â  Varying interceps",
    "section": "14.2 Formula",
    "text": "14.2 Formula\nWe model the relationship between the predictor variables (X1, X2, â€¦, Xn) and the outcome variable (Y) with varying intercepts (b_j) for each group (j) using the following equation:\n\\[\nY_{ij} = X_{ij1} * W_1 + X_{ij2} * W_2 + ... + X_{ijn} * W_n + b_j + \\epsilon_{ij}\n\\]\nWhere: - \\(Y_{ij}\\) is the outcome variable for observation i in group j. - \\(X_{ij1}, X_{ij2}, ..., X_{ijn}\\) are the predictor variables for observation i in group j. - \\(W_1, W_2, ..., W_n\\) are the regression coefficients. - \\(b_j\\) is the varying intercept for group j. - \\(\\epsilon_{ij}\\) is the error term, typically assumed to be normally distributed with mean 0 and variance \\(\\sigma^2\\).\nThe varying intercepts b_j are typically modeled as being drawn from a common distribution:\n\\[\nb_j \\sim \\text{Normal}(\\mu_b, \\sigma_b^2)\n\\]\nWhere: - b is the overall mean intercept. - b^2 is the variance of the intercepts across groups.",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Varying interceps</span>"
    ]
  },
  {
    "objectID": "12. Varying interceps.html#considerations",
    "href": "12. Varying interceps.html#considerations",
    "title": "14Â  Varying interceps",
    "section": "14.3 Considerations",
    "text": "14.3 Considerations\nIn Bayesian regression with varying intercepts, we consider uncertainty in both the regression coefficients and the varying intercepts. We need to declare prior distributions for W_1, W_2, â€¦, W_n, b, and b.\nTypically, we use a Normal distribution for W_1, W_2, â€¦, W_n and b, and a Half-Normal or Exponential distribution for b.\n  Additional conciderations :  \n\nBasically, the idea of varying intercepts is to generate an intercept for each group. So the intercept ( b ) is defined based on the declared groups.\nTherefore, there are as many ( b ) values as there are groups.\nThese intercepts are defined using a Normal distribution with a mean specified by another prior (called a hyper-prior): \\[\nb_j \\sim \\text{Normal}(\\mu_b, \\sigma_b^2)\n\\]\nWhere:\n\n\\(\\mu_b\\)is the overall mean intercept.\n\\(\\sigma_b^2\\) is the variance of the intercepts across groups.\n\nIn the code below, the hyper-prior is a_bar.",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Varying interceps</span>"
    ]
  },
  {
    "objectID": "12. Varying interceps.html#example",
    "href": "12. Varying interceps.html#example",
    "title": "14Â  Varying interceps",
    "section": "14.4 Example",
    "text": "14.4 Example\nBelow is an example code snippet demonstrating Bayesian regression with varying intercepts:\nfrom BI import bi.hard\nd = pd.read_csv('/home/sosa/BI/data/reedfrogs.csv', sep = ';')\nd[\"tank\"] = np.arange(d.shape[0])\ntank = jnp.array(d[\"tank\"].astype('int32').values)\ndensity = jnp.array(d[\"density\"].astype('float32').values)\ndef model():\n    sigma = yield exponential(1, 1)\n    a_bar = yield normal(1, 0, 1.5)\n    alpha = yield normal(48, a_bar, sigma)\n    p = jnp.squeeze(alpha[tank])[0]\n    y = yield Independent(Binomial(total_count = density, logits = p), reinterpreted_batch_ndims=1)\n\nposterior, sample_stats = NUTS(model, obs = jnp.array(d.surv.astype('float32').values))",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Varying interceps</span>"
    ]
  },
  {
    "objectID": "12. Varying interceps.html#mathematical-details",
    "href": "12. Varying interceps.html#mathematical-details",
    "title": "14Â  Varying interceps",
    "section": "14.5 Mathematical Details",
    "text": "14.5 Mathematical Details\nWe can express the Bayesian regression model with varying intercepts using probability distributions as follows:\n\\[\np(Y_{ij} | X_{ij}, W, b_j, \\sigma) = \\text{Normal}(X_{ij} * W + b_j, \\sigma^2) \\\\\nb_j \\sim \\text{Normal}(\\mu_b, \\sigma_b^2) \\\\\np(W_i) = \\text{Normal}(0, \\alpha^2) \\\\\np(\\mu_b) = \\text{Normal}(0, \\beta^2) \\\\\np(\\sigma_b) = \\text{Exponential}(\\lambda)\n\\]\nWhere: - \\(p(Y_{ij} | X_{ij}, W, b_j, \\sigma)\\) is the likelihood function for the outcome variable. - \\(b_j \\sim \\text{Normal}(\\mu_b, \\sigma_b^2)\\) models the varying intercepts across groups. - \\(p(W_i)\\), \\(p(\\mu_b)\\), and \\(p(\\sigma_b)\\) are the prior distributions for the regression coefficients, overall mean intercept, and standard deviation of the intercepts, respectively.",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Varying interceps</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html",
    "href": "13. Varying slopes.html",
    "title": "15Â  Varying effects",
    "section": "",
    "text": "15.1 General Principles\nTo model the relationship between predictor variables and an outcome variable while allowing for both varying intercepts and varying slopes (effects) across groups or clusters, we use a Varying Effects model. This approach is useful when we expect the relationship between predictors and the outcome to differ across groups (e.g., different slopes for different subjects, locations, or time periods).This allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance.\n\\[\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha^2 & \\sigma_\\alpha \\sigma_{\\beta \\rho }\\\\\n\\sigma_\\alpha \\sigma_{\\beta \\rho } & \\sigma_\\beta^2\n\\end{array}\\right)\n\\]\nwhere : - \\(\\sigma_\\alpha^2\\) is the variance of intercepts. - \\(\\sigma_\\beta^2\\) is the covariance of intercepts & slopes. - \\(\\sigma_\\alpha \\sigma_{\\beta \\rho }\\) is the covariance between intercepts and slopes -i.e.Â the product of the two standard deviations-.",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html#formula",
    "href": "13. Varying slopes.html#formula",
    "title": "15Â  Varying effects",
    "section": "15.2 Formula",
    "text": "15.2 Formula\n\n15.2.1 Main equation\nWe model the relationship between the predictor variable (\\(X\\)) and the outcome variable (Y) with varying intercepts (\\(\\alpha\\)) and varying slopes (\\(\\beta\\)) for each group (i) using the following equation:\n\\[\nY_{i} = \\alpha_i + \\beta_i X_i + \\epsilon\n\\]\nWhere: - \\(Y_i\\) is the outcome variable for group i. - \\(X_i\\) are the predictor variables for group i. - \\(\\alpha_i\\) is the varying intercept for group i. - \\(\\beta_i\\) are the varying regression coefficients (slope) for group i. - \\(\\epsilon ~\\sim HalfCauchy(0,1)\\) is the error term, typically assumed to be strictly positive.",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html#priors",
    "href": "13. Varying slopes.html#priors",
    "title": "15Â  Varying effects",
    "section": "15.3 Priors",
    "text": "15.3 Priors\nThe varying intercepts (\\(\\beta_i\\)) and slopes (\\(\\alpha_i\\)) are typically modeled using a Multivariate Normal distribution:\n[ ( , S) ]\nWhere: - \\(\\alpha \\sim Normal(0,1)\\), is the prior for average intercept. - \\(\\beta \\sim Normal(0,1)\\) is the prior for average slope. - $S = \\[\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha & 0 \\\\\n0 & \\sigma_\\beta\n\\end{array}\\right)\nR\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha & 0 \\\\\n0 & \\sigma_\\beta\n\\end{array}\\right)\n\\] $ is the covariance matrix where:\n- $\\sigma_\\alpha \\sim Halfcauchy(0,1)$ bewing the prior stddev among intercepts.\n- $\\sigma_\\beta \\sim Halfcauchy(0,1)$ bewing the prior stddev among slopes.\n- $R \\sim LKJcorr( Î·)$ bewing the prior for the correlation matrix.",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html#considerations",
    "href": "13. Varying slopes.html#considerations",
    "title": "15Â  Varying effects",
    "section": "15.4 Considerations",
    "text": "15.4 Considerations\n\n\\(Halfcauchy\\) distribution allow use to specify strictly positive values for a parameter\n\nParameter \\(Î·\\) for \\(LKJcorr\\) distribution is ussually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near âˆ’1 or 1. When we use LKJ- corr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html#example",
    "href": "13. Varying slopes.html#example",
    "title": "15Â  Varying effects",
    "section": "15.5 Example",
    "text": "15.5 Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects:\nfrom BI import bi.hard\n\n# Simulate data-----------------------------------------------------------------\nimport math\nimport os\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image, set_matplotlib_formats\nfrom matplotlib.patches import Ellipse, transforms\n\nimport jax.numpy as jnp\nfrom jax import random, vmap\nfrom jax.scipy.special import expit\n\nimport numpy as onp\nimport numpyro as numpyro\nimport numpyro.distributions as dist\nfrom numpyro.diagnostics import effective_sample_size, print_summary\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\naz.style.use(\"arviz-darkgrid\")\nnumpyro.set_platform(\"cpu\")\nnumpyro.set_host_device_count(4)\na = 3.5  # average morning wait time\nb = -1  # average difference afternoon wait time\nsigma_a = 1  # std dev in intercepts\nsigma_b = 0.5  # std dev in slopes\nrho = -0.7  # correlation between intercepts and slopes\nMu = jnp.array([a, b])\ncov_ab = sigma_a * sigma_b * rho\nSigma = jnp.array([[sigma_a**2, cov_ab], [cov_ab, sigma_b**2]])\n\nsigmas = jnp.array([sigma_a, sigma_b])  # standard deviations\nRho = jnp.array([[1, rho], [rho, 1]])  # correlation matrix\n\n# now matrix multiply to get covariance matrix\nSigma = jnp.diag(sigmas) @ Rho @ jnp.diag(sigmas)\nN_cafes = 20\nseed = random.PRNGKey(5)  # used to replicate example\nvary_effects = dist.MultivariateNormal(Mu, Sigma).sample(seed, (N_cafes,))\na_cafe = vary_effects[:, 0]\nb_cafe = vary_effects[:, 1]\nseed = random.PRNGKey(22)\nN_visits = 10\nafternoon = jnp.tile(jnp.arange(2), N_visits * N_cafes // 2)\ncafe_id = jnp.repeat(jnp.arange(N_cafes), N_visits)\nmu = a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\nsigma = 0.5  # std dev within cafes\nwait = dist.Normal(mu, sigma).sample(seed)\nd = pd.DataFrame(dict(cafe=cafe_id, afternoon=afternoon, wait=wait))\n\n# Define model-----------------------------------------------------------------\ncafe_id = jnp.array(d.cafe.values)\ndef model():    \n    sigma = yield exponential(1,1)\n    a = yield normal(1, 5, 2)\n    b = yield normal(1, -1, 0.5)\n    sigma_cafe = yield exponential(2, 1)    \n    Rho = yield lkj((), 2, 2)\n    a_cafe_b_cafe = yield multivariatenormaltril(N_cafes, loc = jnp.stack([a, b], axis=1)[0], scale_tril =  Rho * sigma_cafe)\n    mu = a_cafe_b_cafe[:, 0][cafe_id] + a_cafe_b_cafe[:, 1][cafe_id] * afternoon\n    y = yield Independent(Normal(mu, sigma), reinterpreted_batch_ndims=[1])\n\nposterior, sample_stats = NUTStrans(model, obs = jnp.array(d.wait.astype('float32').values))",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "13. Varying slopes.html#mathematical-details",
    "href": "13. Varying slopes.html#mathematical-details",
    "title": "15Â  Varying effects",
    "section": "15.6 Mathematical Details",
    "text": "15.6 Mathematical Details\nWe can express the Bayesian regression model with varying effects using probability distributions as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) = \\text{Normal}(\\mu_i , \\sigma) \\\\\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_i X_i \\\\\n\\]\n[ ( , S) ]\n\\[S =\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha & 0 \\\\\n0 & \\sigma_\\beta\n\\end{array}\\right)\nR\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha & 0 \\\\\n0 & \\sigma_\\beta\n\\end{array}\\right)\n\\]\n\\[\n\\alpha \\sim Normal(0,1) \\\\\n\\beta \\sim Normal(0,1) \\\\\n\\sigma_\\alpha \\sim Halfcauchy(0,1) \\\\\n\\sigma_\\beta \\sim Halfcauchy(0,1) \\\\\nR \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Varying effects</span>"
    ]
  },
  {
    "objectID": "19. Modeling Network.html",
    "href": "19. Modeling Network.html",
    "title": "21Â  17. Modeling Network",
    "section": "",
    "text": "21.1 Conciderations",
    "crumbs": [
      "<span class='chapter-number'>21</span>Â  <span class='chapter-title'>17. Modeling Network</span>"
    ]
  },
  {
    "objectID": "19. Modeling Network.html#conciderations",
    "href": "19. Modeling Network.html#conciderations",
    "title": "21Â  17. Modeling Network",
    "section": "",
    "text": "Network links can be modeled using Bernoulli, Binomial, Poisson, or zero-inflated Poisson distributions. So, by replacing the Poisson distribution with a binomial distribution, we can model the existence or absence of link weights â€” i.e., model binary networks.\nNote that if the network is undirected, then accounting for correlation between propensity to emit and receive links is not necessary, and the terms ( _i ), ( j ), and ( {ij} ) are no longer required. (Is it correct?)\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "<span class='chapter-number'>21</span>Â  <span class='chapter-title'>17. Modeling Network</span>"
    ]
  }
]