[
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "2¬† Introduction",
    "section": "",
    "text": "3 Bayesian analysis\nBayesian analysis is a statistical approach that uses probability theory to update beliefs about the parameters of a model as new data becomes available. Bayesian methods have several key advantages as they allow direct uncertainty quantification üõà, incorporation of prior knowledge üõà and flexibility for complex models üõà.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#modeling-likelihood",
    "href": "0. Introduction.html#modeling-likelihood",
    "title": "2¬† Introduction",
    "section": "4.1 Modeling Likelihood",
    "text": "4.1 Modeling Likelihood\nOnce the likelihood is defined, we can know define the mathematical equations that describe our parameters (\\(\\mu\\) and \\(\\sigma\\)) and their relationship with the dependent variable \\(y\\). We can express this relationship in the form of a linear function:\n\\[\n\\mu = \\alpha + \\beta x\n\\]\nWhere \\(\\alpha\\) is the intercept üõà and \\(\\beta\\) is the slope üõà of the line. These parameters are the unknowns that we want to estimate to evaluate the strength and direction of the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions-link",
    "href": "0. Introduction.html#link-functions-link",
    "title": "2¬† Introduction",
    "section": "4.2 Link functions Link",
    "text": "4.2 Link functions Link\nDepending on the type of problem you are trying to solve (classification, regression, etc.) and the type of data you are working with (continuous, discrete, binomial, etc.) you will need to choose the appropriate distribution to describe the relationship between the data. By using a different distribution, you will need to use a different link function üõà.\nFor the moment, we just need to know that those different distributions required link function (for each specific family we will discuss the corresponding link function). however, below is a table summarizing some of the most common link functions, the mathematical form of each, their typical applications, and how to interpret them. Link functions in BI can be aces through the class bi.link.XXX where XXX is the name of the link function.\n\n\n\n\n\n\n\n\n\nLink Function\nMathematical Form\nTypical Use / Model\nInterpretation & Range\n\n\n\n\nIdentity\n\\(g(\\mu) = \\mu\\)\nLinear regression (Normal)\nDirectly models \\(\\mu\\); \\(\\mu\\) can be any real number.\n\n\nLogit\n\\(g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\nLogistic regression (Binomial)\nModels probabilities (0, 1); coefficients reflect log-odds.\n\n\nProbit\n\\(g(\\mu) = \\Phi^{-1}(\\mu)\\)\nProbit regression (Binomial)\nSimilar to logit; uses the inverse standard normal CDF.\n\n\nLog\n\\(g(\\mu) = \\log(\\mu)\\)\nPoisson, Gamma regression (Count data)\nEnsures \\(\\mu &gt; 0\\); coefficients represent multiplicative effects.\n\n\nInverse\n\\(g(\\mu) = \\frac{1}{\\mu}\\)\nGamma regression\nModels positive \\(\\mu\\); relates changes inversely to \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#the-prior-distributions",
    "href": "0. Introduction.html#the-prior-distributions",
    "title": "2¬† Introduction",
    "section": "4.3 The Prior Distributions",
    "text": "4.3 The Prior Distributions\nFor each parameters of our equation that describe \\(\\mu\\) , we need to define a prior distribution üõà that encodes our initial beliefs about the parameter. In the case of the linear regression model, we need to specify prior distributions for (\\(\\alpha\\)), slope (\\(\\beta\\)), and standard deviation (\\(\\sigma\\)):\n\\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nAnd with this we can write our entire model as:\n\\[\ny \\sim \\text{Normal}(\\mu, \\sigma)\n\\] \\[\n\\mu = \\alpha + \\beta x\n\\] \\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nIn BI you will need to define this model within a function in which you will be able to use any probability distributions, ink functions and declare any mathematical operations required for your model. BI have been designed to allow you to declare your model as close as possible to the mathematical notation. For example, the model above can be written in BI as:\nbi = bi(platform='cpu')\ndef model(x, y):    \n    alpha = bi.dist.normal( 0, 1, name = 'alpha',shape= (1,))\n    beta = bi.dist.normal( 0, 1, name = 'beta',shape= (1,))   \n    sigma = bi.dist.uniform( 0, 50, name = 'sigma',shape = (1,))\n    m.normal(alpha + beta * x , , obs=y)\nNotice that for eah parameter declared in the model you will need to give it a unique name as well as a shale. The shape is the number of parameters you want to estimate. For example, if you want to estimate a different \\(\\beta\\) for each predictor, you would need to declare \\(\\beta\\) with a shape equal to the number of predictors. By default, the shape is one those technically you don‚Äôt need to specify it. In this example we only wanted to highlight this feature.\n\n4.3.1 Which prior distribution range to use?\nThe choice of prior ranges can significantly affect Bayesian analysis results. There are several approaches to selecting them:\n\nExpert Knowledge: The prior distributions can be based on expert knowledge or historical data. This approach is useful when there is a lot of information available about the parameters.\nNoninformative Priors: When there is little or no information about the parameters, noninformative priors can be used. These priors are designed to have minimal influence on the posterior distribution, allowing the data to dominate the inference process.\nScaled data: If the data is scaled üõà, the prior distributions can be chosen to reflect this. For example, if the data are scaled, the prior distributions for the intercept and slope can be centered around 0 and 1, respectively. By scaling the independent variable, we obtain a unit of change based on variance; that is, the effect represents a one‚Äìstandard‚Äìdeviation change in \\(x\\) on \\(y\\). Scaling the data improves both numerical stability and interpretability. When all data are scaled to the same range, it leads to more stable numerical behavior during estimation. Additionally, it facilitates setting priors that are both meaningful and relatively uninformative. By aligning the scale of the data with the scale assumed in the priors, we ensure that the posterior distributions exhibit reasonable spread and that our uncertainty quantification is consistent with the data‚Äôs scale. For the remainder of this document, we will assume that the data are scaled.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fit-and-posterior-distribution",
    "href": "0. Introduction.html#model-fit-and-posterior-distribution",
    "title": "2¬† Introduction",
    "section": "4.4 Model fit and posterior distribution",
    "text": "4.4 Model fit and posterior distribution\nOnce data are observed, Bayes‚Äô Theorem üõà to evaluate how well a given set of parameters value fits the data:\n\\[\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\\]\nWhere:\n\n\\(\\theta\\) represents the unknown parameters we are interested in.\n\\(P(\\theta)\\) is the prior distribution, representing our beliefs about \\(\\theta\\) before seeing the data.\n\\(P(\\text{data} \\mid \\theta)\\) is the likelihood, representing the model of how the data are generated given \\(\\theta\\).\n\\(P(\\theta \\mid \\text{data})\\) is the posterior distribution, representing our updated beliefs after observing the data.\n\nThis distribution reflects our updated beliefs about the parameters after observing the data. It tells us not only the most likely value of \\(\\theta\\),(e.g., \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) in our case) but also quantifies the uncertainty in these estimates.\nWe can use Bayesian updating üõà using the Bayesian theorem to ‚Äòreshape‚Äô the prior distributions by considering every possible combination of values for our parameters, and scoring each combination by its relative plausibility in light of the data. These relative plausibilities are the posterior probabilities of each combination of our parameters: the posterior distributions. Various techniques can be used to approximate the mathematical definition of Bayes‚Äô theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Description of this algorithms are out of the scope of this document. For more information, please refer to the Bayesian Inference. In BI, we use MCMC and it can be call as m.run(model) where model is the function that describe the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "2¬† Introduction",
    "section": "4.5 Model ‚Äòdiagnostic‚Äô",
    "text": "4.5 Model ‚Äòdiagnostic‚Äô\nOnce a Bayesian model has been fit, it is crucial to evaluate how well it captures the observed data and to assess whether the Markov chain Monte Carlo (MCMC) sampling has converged. Bayesian model diagnostics help us answer questions like: ‚ÄúAre our uncertainty estimates reliable?‚Äù, ‚ÄúDoes the model generate data similar to what we observed?‚Äù, and ‚ÄúHave the chains mixed well?‚Äù Mulitple diagnostics approaches can be used to assess the model‚Äôs performance. Below are some key diagnostic tools and techniques available in BI within the class bi.diagnostics.XXX where XXX is the name of the diagnostic tool.\n\n\n\n\n\n\n\n\n\nDiagnostic Tool\nPurpose\nKey Indicator\nInterpretation\n\n\n\n\nposterior predictive checks (PPCs) üõà\nAssess if model can reproduce observed data\nGraphs, p-values, summary stats\nGood fit if simulated data resemble observed data\n\n\nCredible Interval (CI)\nQuantify uncertainty in parameter estimates\n95% CI or other percentage\n95% probability the parameter lies within the interval\n\n\nhighest posterior density intervals (HPDI) üõà\nIdentify the narrowest interval containing a given probability mass\n95% HPDI\nSmallest interval capturing 95% of the posterior density\n\n\neffective sample size (ESS) üõà\nMeasure independent information in the chain\nESS value (ideally high)\nLow ESS indicates high autocorrelation (poor mixing)\n\n\npotential scale reduction factor (Rhat) üõà\nCheck convergence across multiple chains\nRhat ‚âà 1 (typically &lt;1.1)\nValues near 1 indicate convergence; &gt;&gt;1 suggests non-convergence\n\n\nTrace plots üõà\nVisualize the sampling path to check convergence and mixing\nPlot showing parameter values over iterations\nStationary, ‚Äòhairy caterpillar‚Äô pattern suggests convergence\n\n\nautocorrelation plots üõà\nAssess dependency between samples over lags\nAutocorrelation values across lags\nRapid decay to zero suggests good mixing; slow decay indicates poor mixing\n\n\ndensity plots üõà\nVisualize the posterior distribution of a parameter\nSmoothness and shape of the curve\nUnimodal and smooth suggests convergence; multimodal or irregular may suggest poor mixing",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-comparison",
    "href": "0. Introduction.html#model-comparison",
    "title": "2¬† Introduction",
    "section": "4.6 Model comparison",
    "text": "4.6 Model comparison\nModel comparison is performed by evaluating how well different models explain the observed data while accounting for model complexity. Multiple criteria can be used to compare models, and are summarized in the table below. In BI, wwe can compare models using Watanabe-Akaike Information Criterion (WAIC) with the function m.diag.aic(model1,model2).\n\n\n\n\n\n\n\n\n\n\nCriterion\nPurpose\nInterpretation\nStrengths\nWeaknesses\n\n\n\n\nDIC (Deviance Information Criterion)\nMeasures model fit while penalizing complexity\nLower values indicate better model fit\nSimple to compute, useful for hierarchical models\nSensitive to the number of parameters, not always reliable in complex models\n\n\nWAIC (Watanabe-Akaike Information Criterion)\nEstimates out-of-sample predictive accuracy while penalizing complexity\nLower values indicate better models\nMore robust than DIC, accounts for overfitting\nComputationally intensive for large models\n\n\nBayes Factor (BF)\nQuantifies relative support for two models based on marginal likelihoods\nBF &gt; 1 favors the numerator model, BF &lt; 1 favors the denominator\nProvides direct evidence comparison, works with different model types\nSensitive to prior choices, requires good model specification",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "",
    "text": "3.1 General Principles\nTo study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#general-principles",
    "href": "1. Linear Regression for continuous variable.html#general-principles",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "",
    "text": "An intercept \\(\\alpha\\), which represents the origin of the line‚Äîthe expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\(\\beta\\), which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA variance term \\(\\sigma\\), which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#considerations",
    "href": "1. Linear Regression for continuous variable.html#considerations",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "3.2 Considerations",
    "text": "3.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nBayesian models consider model parameter uncertainty üõà, allowing for the quantification of confidence or uncertainty through the parameters‚Äô posterior distribution üõà. Therefore, we need to declare prior distributions üõà for each model parameter, in your case for: \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\).\nPrior distributions are built following these considerations:\n\nAs the data is scaled üõà (see introduction), we can use a Normal distribution for \\(\\alpha\\) and \\(\\beta\\), with a mean of 0 and a standard deviation of 1.\nSince \\(\\sigma\\) is strictly positive, we can use any distribution that is positively defined, such as the Exponential or Uniform distribution.\n\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a link function üõà (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "3.3 Example",
    "text": "3.3 Example\nBelow is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height.\n\nPythonR\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm.data('../data/Howell1.csv', sep=';') # Import\nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\nm.data_to_model(['weight', 'height']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.lognormal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.run(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# fileter data frame\nm$df = m$df[m$df$age &gt; 18,]\n\n# Scale\nm$scale(list('weight')) \n\n# convert data to jax arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Parameters priors distributions\n  s = bi.dist.uniform(0, 50, name = 's')\n  a = bi.dist.normal(178, 20,  name = 'a')\n  b = bi.dist.normal(0, 1, name = 'b')\n  \n  # Likelihood\n  m$normal(a + b * weight, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "3.4 Mathematical Details",
    "text": "3.4 Mathematical Details\n\n3.4.1 Frequentist formulation\nThe following equation allows us to draw a line: \\[\nY_i = \\alpha + \\beta  X_i + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma_i\\) is a vector of error terms for observation i.\n\n\n\n3.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this regression model using the following model:\n\\[\nY_i \\sim Normal(\\alpha + \\beta   X_i, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0, 1)\n\\]\n\\[\n\\beta \\sim Normal(0, 1)\n\\]\n\\[\n\\sigma \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is dependent variable for observation i.\n\\(\\alpha\\) and \\(\\beta\\) are the regression coefficients and intercept parameters, respectively.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma\\) is a prior for the variance term standard deviation of the normal distribution that describes the variance in the relationship between the dependent variable \\(Y\\) and the independent variable \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#notes",
    "href": "1. Linear Regression for continuous variable.html#notes",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "3.5 Notes",
    "text": "3.5 Notes\n\n\n\n\n\n\nNote\n\n\n\nWe can observe a difference between the Frequentist and the Bayesian formulation regarding the error term. Indeed, in the Frequentist formulation, the error term \\(\\sigma_i\\) represents random fluctuations around the predicted values. This assumption leads to point estimates for \\(\\alpha\\) and \\(\\beta\\), without accounting for uncertainty in these estimates. In contrast, the Bayesian formulation treats \\(\\sigma\\) as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "3¬† Linear Regression for continuous variable",
    "section": "3.6 Reference(s)",
    "text": "3.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for continuous variable</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "4¬† Multiple continuous variables model",
    "section": "",
    "text": "4.1 General Principles\nTo study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\(\\beta_x\\) for each continuous variable (e.g., \\(\\beta_{weight}\\) and \\(\\beta_{age}\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.2 Considerations",
    "text": "4.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Regression for continuous variable.\nThe model interpretation of the regression coefficients \\(\\beta_x\\) is considered for fixed values of the other independent variable(s)‚Äô regression coefficients ‚Äîi.e., for a given age, \\(\\beta_{weight}\\) represents the expected change in the dependent variable variation (height) for each one-unit increase in weight, holding all other variable(s) constant (age).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.3 Example",
    "text": "4.3 Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (height, weight, age), and the goal is to estimate the effect of weight and age on height.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm.data('../data/Howell1.csv', sep=';')  # Import\nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight', 'age']) # Scale\nm.data_to_model(['weight', 'height', 'age']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameters priors distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0,50, name = 'sigma')\n    # Likelihood\n    m.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run mcmc ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89) # Get posterior distributions\n\n\nlibrary(BI)\nm=importBI(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')# Import\nm$df = m$df[m$df$age &gt; 18,] # Manipulate\nm$scale(list('weight', 'age')) # Scale\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight, age){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 0.5, name = 'a')\n  beta1 = bi.dist.normal( 0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(  0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.4 Mathematical Details",
    "text": "4.4 Mathematical Details\n\n4.4.1 Frequentist formulation\nWe model the relationship between the independent variables \\((X_{1i}, X_{2i}, ..., X_{ni})\\) and the dependent variable Y using the following equation:\n\\[\nùëå_i = \\alpha +\\beta_1  ùëã_{1i} + \\beta_2  ùëã_{2i} + ... + \\beta_n  ùëã_{ni} + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\beta_1\\), \\(\\beta_2\\), ‚Ä¶, \\(\\beta_n\\) are the regression coefficients.\n\\(\\sigma_i\\) is a vector of error terms for observation i.\n\n\n\n4.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian model as follows:\n\\[\nùëå \\sim Normal(\\alpha + \\sum_k^n  \\beta_k  X, œÉ¬≤)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\nœÉ \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are the prior distributions for the regression coefficients k distinct regression coefficients.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring that it is positive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.5 Reference(s)",
    "text": "4.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "5¬† Interaction terms",
    "section": "",
    "text": "5.1 General Principles\nTo study relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "5¬† Interaction terms",
    "section": "5.2 Considerations",
    "text": "5.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same assumptions as for Regression for continuous variable.\nWe wish to model the relationship between independent variable Y and dependent variable \\(X_1\\) to vary as a function of dependent variable \\(X_2\\). To do this, we explicitly model the hypothesis that the slope between Y and \\(X_1\\) depends‚Äîis conditional‚Äîupon \\(X_2\\).\nFor continuous interactions with scaled data, the intercept becomes the grand mean üõà of the outcome variable.\nThe interpretation of estimates is more complex. The estimate of non-interaction terms reflects the expected change in Y when \\(X_1\\) increases by one unit, holding \\(X_2\\) constant at its average value. The estimate of interaction terms represents how the effect of \\(X_1\\) on Y changes depending on the value of \\(X_2\\), and vice versa, showing how the relationship between the two variables influences the outcome Y.\nTriptych üõà plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "5¬† Interaction terms",
    "section": "5.3 Example",
    "text": "5.3 Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. Data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm.data('../data/tulips.csv', sep=';') # Import\nm.scale(['blooms', 'water', 'shade']) # Scale\nm.data_to_model(['blooms', 'water', 'shade']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))   \n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.normal(mu, sigma, obs=blooms)\n    \n# Run mcmc ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89) # Get posterior distributions\n\n\nlibrary(BI)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Scale\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(blooms, water,shade){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')   \n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint') \n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "5¬† Interaction terms",
    "section": "5.4 Mathematical Details",
    "text": "5.4 Mathematical Details",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#frequentist-formulation",
    "href": "3. Interaction between continuous variables.html#frequentist-formulation",
    "title": "5¬† Interaction terms",
    "section": "5.5 Frequentist formulation",
    "text": "5.5 Frequentist formulation\nWe model the relationship between the input features (\\(X_1\\) and \\(X_2\\)) and the target variable (\\(Y\\)) using the following equation: \\[\nùëå_i = \\alpha + \\beta_1 ùëã_{1i} + \\beta_2 ùëã_{2i} + \\beta_{interaction} ùëã_{1i} ùëã_{2i} + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\beta_1\\) and \\(\\beta_2\\) are the regression coefficients for \\(X_{1}\\) and \\(X_{2}\\), respectively.\n\\(\\beta_{interaction}\\) is the regression coefficient for the interaction term \\((X_{1}  X_{2})\\).\n\\(\\sigma\\) is the error term, assumed to be normally distributed.\n\nIn this context, the interaction term \\(X_{1i} * X_{2i}\\) captures the joint effect of \\(X_{1i}\\) and \\(X_{2i}\\) on the target variable \\(Y_i\\).\n\n5.5.1 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distribution as follows:\n\\[\nY \\sim Normal(\\alpha +  \\beta_1  X_{1i}‚Äã + \\beta_2  X_{2i}‚Äã‚Äã + \\beta_{interaction}  X_1{1i} X_{2i}‚Äã ,  \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_1 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_2 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_{interaction} \\sim Normal(0,1)\n\\]\n\\[\nœÉ \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_{interaction}\\) are the prior distributions for the regression coefficients.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "5¬† Interaction terms",
    "section": "5.6 Reference(s)",
    "text": "5.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html",
    "href": "5. Binomial model.html",
    "title": "7¬† Binomial Model",
    "section": "",
    "text": "7.1 General Principles\nTo model the relationship between a binary dependent variable ‚Äîe.g., success/failure, yes/no, or 1/0‚Äî and one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#considerations",
    "href": "5. Binomial model.html#considerations",
    "title": "7¬† Binomial Model",
    "section": "7.2 Considerations",
    "text": "7.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nWe have the first link function üõà logit. The logit link function in the Bayesian binomial model converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution \\(\\in[0,1]\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#example",
    "href": "5. Binomial model.html#example",
    "title": "7¬† Binomial Model",
    "section": "7.3 Example",
    "text": "7.3 Example\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents the side individuals pulled. The goal is to evaluate the probability of pulling the left side.\n\nPythonR\n\n\nfrom main import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\nm.data('../resources/data//chimpanzees.csv', sep=';') \nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.binomial(logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.run(model, num_samples=500) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  m$binomial(logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#mathematical-details",
    "href": "5. Binomial model.html#mathematical-details",
    "title": "7¬† Binomial Model",
    "section": "7.4 Mathematical Details",
    "text": "7.4 Mathematical Details\n\n7.4.1 Frequentist formulation\nWe model the relationship between the independent variable (\\(X_i\\)) and the binary dependent variable (\\(Y_i\\)) using the following equation: \\[\nlogit(Y_i) = \\alpha + \\beta X_i\n\\]\nWhere:\n\n\\(Y_i\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the independent variable for observation i.\n\\(logit(Y_i)\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.\n\n\n\n7.4.2 Bayesian formulation\npriors üõà. We can express the Bayesian regression model accounting for prior distribution as follows:\n\\[\nY_i \\sim Binomial(n = 1, p)\n\\]\n\\[\nlogit(p) \\sim \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\nWhere:\n\n\\(Y\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(n = 1\\) represents the number of trials in the binomial distribution (binary outcome).\n\\(\\beta\\) and \\(\\alpha\\) are the prior distributions for the regression coefficients and intercept, respectively.\n\\(logit\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#notes",
    "href": "5. Binomial model.html#notes",
    "title": "7¬† Binomial Model",
    "section": "7.5 Notes",
    "text": "7.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in chapter 2.\nWe can apply interaction terms similarly as in chapter 3.\nWe can apply categorical variables similarly as in chapter 4.\nBelow is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents the side individuals pulled, and three independent variables (actor, side, cond). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as the different conditions.\n\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';') \nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = dist.normal(0, 1.5, shape = (7,), name='a')\n    b = dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.lk(\"y\", dist.binomial(1, logits=p), obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.run(model) \n# Diagnostic ------------------------------------------------\nm.summary()",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#references",
    "href": "5. Binomial model.html#references",
    "title": "7¬† Binomial Model",
    "section": "7.6 Reference(s)",
    "text": "7.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "6¬† Regression for Categorical Variables",
    "section": "",
    "text": "6.1 General Principles\nTo study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.2 Considerations",
    "text": "6.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nAs we generate regression coefficients for each k category, we need to specify a prior with a shape equal to the number of categories k in the code (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare confidence intervals or p-values directly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.3 Example",
    "text": "6.3 Example\nBelow is example code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (kcal_per_g), representing the caloric value of milk per gram, and a categorical independent variable, representing species clade membership. The goal is to estimate the differences in milk calories between clades.\n\nPythonR\n\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm.data('../data/milk.csv', sep=';') # Import\nm.index([\"clade\"]) # Manipulate\nm.scale(['kcal_per_g']) # Scale\nm.data_to_model(['kcal_per_g', \"index_clade\"]) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade):\n    a = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    s = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]\n    m.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89) # Get posterior distributions\n\n\nlibrary(BI)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(kcal_per_g, index_clade){\n  # Parameters priors distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma =bi.dist.exponential(1, name = 's')\n  # Likelihood\n  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.4 Mathematical Details",
    "text": "6.4 Mathematical Details\n\n6.4.1 Frequentist formulation\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\\[\nY_i = \\alpha + \\beta_k X_i + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_k\\) are the regression coefficients for each k category.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the error term.\n\nWe can interpret \\(\\beta_i\\) as the effect of each category on \\(Y\\) relative to the baseline (usually one of the categories or the intercept).\n\n\n6.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distribution as follows:\n\\[\nY \\sim Normal(\\alpha +  \\beta_k X, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are k prior distributions for k regression coefficients.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.5 Notes",
    "text": "6.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in Chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms similarly as in Chapter 3: Interaction between Continuous Variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.6 Reference(s)",
    "text": "6.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html",
    "href": "14. Varying slopes.html",
    "title": "16¬† Varying slopes",
    "section": "",
    "text": "16.1 General Principles\nTo model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#considerations",
    "href": "14. Varying slopes.html#considerations",
    "title": "16¬† Varying slopes",
    "section": "16.2 Considerations",
    "text": "16.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for 12. Varying intercepts.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance üõà.\nThe covariance matrix requires a correlation matrix distribution which is modeled using a \\(LKJcorr\\) distribution that holds a parameter \\(Œ∑\\). \\(Œ∑\\) is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near ‚àí1 or 1. When we use \\(LKJcorr(1)\\), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\nThe Half-Cauchy distribution is used when modeling the covariance matrix to specify strictly positive values for the diagonal of the covariance matrix, ensuring positive variances.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#example",
    "href": "14. Varying slopes.html#example",
    "title": "16¬† Varying slopes",
    "section": "16.3 Example",
    "text": "16.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects:\n\n16.3.1 Simulated data\n\nPythonR\n\n\nfrom main import*\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.data('Sim data multivariatenormal.csv', sep=',') \nm.data.to_model(['cafe', 'wait', 'N_cafes'])\n\ndef model(cafe, wait, N_cafes):\n    a = dist.normal(5, 2, name = 'a') #Standard Normal Vector \n    b = dist.normal(-1, 0.5, name = 'b') # Standard Normal Vector \n    sigma_cafe = dist.exponential(1, shape=[2], name = 'sigma_cafe')\n    sigma = dist.exponential( 1, name = 'sigma')\n    Rho = dist.lkj(2, 2, name = 'Rho') # Cholesky Factor\n    # Applies the correlation structure between the variables.\n    # Scaling the Cholesky Factor by the standard deviations.\n\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho # In a multivariate normal distribution, we not only have correlations (from sr_L), but also individual standard deviations for each variable (from sr_sigma). The diag_pre_multiply operation ensures that each variable's correlation structure is properly scaled by its standard deviation. This makes it possible to account for both correlations and individual variability when generating samples.\n    # This operation applies the correlation and standard deviation structure (encoded in scaled_sr_L) to the standard normal variables sr_raw. The result is a sample from a multivariate normal distribution that respects both the correlations between variables and the individual standard deviations.\n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.normal(mu, sigma, obs=wait)\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariatenormal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  m$normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details",
    "href": "14. Varying slopes.html#mathematical-details",
    "title": "16¬† Varying slopes",
    "section": "16.4 Mathematical Details",
    "text": "16.4 Mathematical Details",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details-1",
    "href": "14. Varying slopes.html#mathematical-details-1",
    "title": "16¬† Varying slopes",
    "section": "16.5 Mathematical Details",
    "text": "16.5 Mathematical Details\n\n16.5.1 Formula\nWe model the relationship between the independent variable \\(X\\) and the outcome variable \\(Y\\) with varying intercepts (\\(\\alpha\\)) and varying slopes (\\(\\beta\\)) for each group (\\(k\\)) using the following equation:\n\\[\nY_{ik} = \\alpha_k + \\beta_k X_{ik} + \\sigma\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(X_{ik}\\) is the independent variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(\\beta_k\\) is the varying regression coefficient for group \\(k\\).\n\\(\\sigma\\) is the error term, assumed to be strictly positive.\n\n\n\n16.5.2 Bayesian Model\nWe can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik} , \\sigma)\n\\] _{ik} = _k + k X{ik} + \\[\n\\alpha_k \\sim Normal(0,1)\n\\] _k Normal(0,1) \\[\n\\sigma \\sim Exponential(0,1)\n\\]\nThe varying intercepts (\\(\\alpha_k\\)) and slopes (\\(\\beta_k\\)) are modeled using a Multivariate Normal distribution:\n\\[\n\\begin{pmatrix}\n\\alpha_k \\\\\n\\beta_k\n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left(\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & \\sigma_\\pi \\sigma_{\\alpha\\rho} \\\\\n\\sigma_\\alpha \\sigma_{\\pi\\rho} & \\sigma_\\pi\n\\end{pmatrix}\n\\right)\n\\]\nWhere:\n\n\\(\\left(\\begin{array}{cc} 0 \\\\ 0 \\end{array}\\right)\\) is the prior for the average intercept.\n\\(\\left(\\begin{array}{cc} \\sigma_\\alpha^2 & \\sigma_\\alpha \\sigma_{\\pi \\rho} \\\\ \\sigma_\\alpha \\sigma_{\\pi \\rho} & \\sigma_\\pi^2 \\end{array}\\right)\\) is the covariance matrix which specifies the variance and covariance of \\(\\alpha_k\\) and \\(\\beta_k\\),\nwhere:\n\n\\(\\sigma_\\alpha^2\\) is the variance of \\(\\alpha_k\\).\n\\(\\sigma_\\pi^2\\) is the variance of \\(\\beta_k\\).\n\\(\\sigma_\\alpha \\sigma_{\\pi \\rho}\\) is the covariance between \\(\\alpha_k\\) and \\(\\beta_k\\). For computational reasons, it is often better to implement a centered version of the varying intercept üõà that is equivalent to the Multivariate Normal distribution approach:\n\n\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\nL *\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\pi}_k\n\\end{array}\\right)\n\\]\n\nWhere:\n\n\\(\\sigma_\\alpha \\sim \\text{Exponential}(1)\\) is the prior standard deviation among intercepts.\n\\(\\sigma_\\beta \\sim \\text{Exponential}(1)\\) is the prior standard deviation among slopes.\n\\(L \\sim \\text{LKJcorr}(\\eta)\\) is the prior for the correlation matrix using the Cholesky Factor üõà\n\n\nThe full centered version of the model is thus:\n\\[\nY_{i} \\sim \\text{Normal}(\\mu_k , \\sigma) \\\\\n\\]\n\\[\n\\mu_k =   \\alpha_k + \\beta_i X_i \\\\\n\\]\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\nL *\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\pi}_k\n\\end{array}\\right)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta \\sim Normal(0,1)\n\\] \\[\n\\sigma_\\alpha \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\pi \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "href": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "title": "16¬† Varying slopes",
    "section": "16.6 Multivariate Model with One Random Slope for Each Variable",
    "text": "16.6 Multivariate Model with One Random Slope for Each Variable\nWe can apply a multivariate model similarly to Chapter 2. In this case, we apply the same principle, but with a covariance matrix of a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for \\(i\\) actors in a model with two independent variables \\(X_1\\) and \\(X_2\\), we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma)\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{1i} X_{1i}  + \\beta_{1i} X_{2i}\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{i}\\\\\n\\beta_{1i}\\\\\n\\beta_{2i}\n\\end{pmatrix}\n\\sim \\begin{pmatrix}\n\\sigma_{\\alpha}\\\\\n\\sigma_{\\pi}\\\\\n\\sigma_{\\gamma}\n\\end{pmatrix} \\circ L \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{k} \\\\\n\\widehat{\\pi}_{k} \\\\\n\\widehat{\\gamma}_{k}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha} \\sim Exponential(1)\n\\] _{} Exponential(1) \\[\n\\sigma_{\\gamma} \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "href": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "title": "16¬† Varying slopes",
    "section": "16.7 Multivariate Random Slopes on a Single Variable",
    "text": "16.7 Multivariate Random Slopes on a Single Variable\nFor more than two varying effects, we apply the same principle but with a covariance matrix for each varying effect that are summed to generate the varying intercept and slope. For example, if we want to generate random slopes for \\(i\\) actors and \\(k\\) groups, we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma) \\\\\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{i} X_i\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{i} = \\beta + \\beta_{actor[i]} + \\beta_{group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{\\text{actor}}\n\\end{pmatrix}\n\\sim\n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\pi a}\n\\end{pmatrix} \\circ L_a \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\pi}_{ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\pi a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{\\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\pi g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\pi}_{kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\pi g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#notes",
    "href": "14. Varying slopes.html#notes",
    "title": "16¬† Varying slopes",
    "section": "16.8 Notes",
    "text": "16.8 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying slopes with any distribution presented in previous chapters. Below is the formula and the code snippet for a Binomial multivariate model with interaction between two independent variables \\(X_1\\) and \\(X_2\\) and multiple varying effects for each actor and each group\n\n\\[\np(Y_{i} |n , p_i) \\sim \\text{Binomial}(n = 1, p_i) \\\\\n\\]\n\\[\nlogit{p_i}=   \\alpha_i + (\\beta_{1i}  + \\beta_{2i} X_{2i})  X_{1i}\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{1i} = \\beta + \\beta_{1 actor[i]} + \\beta_{ group[i]}\n\\] \\[\n\\beta_{2i} = \\beta + \\beta_{2 actor[i]} + \\beta_{2 group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{1 \\, \\text{actor}} \\\\\n\\beta_{2 \\, \\text{actor}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\pi a} \\\\\n\\sigma_{\\gamma a}\n\\end{pmatrix} \\circ L_a \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\pi}_{ka} \\\\\n\\widehat{\\gamma}_{ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\]\n\\[\n\\sigma_{\\pi a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\gamma a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{1 \\, \\text{group}} \\\\\n\\beta_{2 \\, \\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\pi g} \\\\\n\\sigma_{\\gamma g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\pi}_{kg} \\\\\n\\widehat{\\gamma}_{kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\pi g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\gamma g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]\nfrom main import*\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Import data\nm.read_csv(\"../data/chimpanzees.csv\", sep=\";\")\nm.df[\"block_id\"] = m.df.block\nm.df[\"treatment\"] = 1 + m.df.prosoc_left + 2 * m.df.condition\nm.data_to_model(['pulled_left', 'treatment', 'actor', 'block_id'])\n\n\ndef model(tid, actor, block_id, L=None, link=False):\n    # fixed priors\n    g = dist.normal(0, 1, name = 'g', shape = (4,))\n    sigma_actor = dist.exponential(1, name = 'sigma_actor', shape = (4,))\n    L_Rho_actor = dist.lkjcholesky(4, 2, name = \"L_Rho_actor\")\n    sigma_block = dist.exponential(1, name = \"sigma_block\", shape = (4,))\n    L_Rho_block = dist.lkjcholesky(4, 2, name = \"L_Rho_block\")\n\n    # adaptive priors - non-centered\n    z_actor = dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n    z_block = dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n    alpha = deterministic(\n        \"alpha\", ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n    )\n    beta = deterministic(\n        \"beta\", ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n    )\n\n    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n    dist(\"L\", dist.Binomial(logits=logit_p), obs=L)\n\n    # compute ordinary correlation matrixes from Cholesky factors\n    if link:\n        deterministic(\"Rho_actor\", L_Rho_actor @ L_Rho_actor.T)\n        deterministic(\"Rho_block\", L_Rho_block @ L_Rho_block.T)\n        deterministic(\"p\", expit(logit_p))\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#references",
    "href": "14. Varying slopes.html#references",
    "title": "16¬† Varying slopes",
    "section": "16.9 Reference(s)",
    "text": "16.9 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html",
    "href": "6. Beta binomial model.html",
    "title": "8¬† Beta-Binomial Model",
    "section": "",
    "text": "8.1 General Principles\nTo model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#considerations",
    "href": "6. Beta binomial model.html#considerations",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.2 Considerations",
    "text": "8.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Binomial regression.\nA Beta-Binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.\nA Beta distribution has two parameters: the rates for each probabilities and a shape parameter Œ∏. Œ∏ influence how probabilities are distributed between 0 and 1. Specifically, it consists of two parameters, \\(\\alpha\\) and \\(\\beta\\) , which determine the concentration of probability around 0 and 1.\n\nIf both are equal or greater than 1, the distribution is bell-shaped and centered around 0.5.\nIf ùõº &gt; ùõΩ , the distribution is skewed toward 1 and if \\(\\beta &gt; \\alpha\\), it is skewed toward 0. Thus, the shape parameters \\(\\gamma\\) and \\(\\eta\\) provide flexibility in modeling various types of prior beliefs about probabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#example",
    "href": "6. Beta binomial model.html#example",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.3 Example",
    "text": "8.3 Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression using the Bayesian Inference (BI) package. The data consist of:\n\nOne binary dependent variable (admit), which represents candidates‚Äô admission status.\nOne independent categorical variable representing individuals‚Äô gender (gid).\nAdditionally, we have the number of applications (applications) per individuals‚Äô gender, which will be used to account for independent rates.\n\nThe goal is to evaluate whether the probability of admission is different between genders, while accounting for differences in the number of applications between genders.\n\nPythonR\n\n\nfrom main import*#\n# Setup device------------------------------------------------\nm = bi()\n\n# Import Data & Data Manipulation ------------------------------------------------\nm.data('../data/UCBadmit.csv', sep=';') # Import\n# Data type (int, float is important, specially for indices)\nm.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int) # Manipulate\ngid = jnp.array(m.df[\"gid\"].astype('int32').values) # Manipulate\napplications = jnp.array(m.df[\"applications\"].astype('float32').values) # Manipulate\nadmit = jnp.array(m.df[\"admit\"].astype('float32').values) # Manipulate\n\n# Send to model (convert to jax array)\nm.data_on_model = dict(\n    gid=gid,\n    applications=applications,\n    admit=admit\n) \n\n# Define model ------------------------------------------------\ndef model(gid, applications, admit):\n    phi = m.dist.exponential(1, shape=(1,), name = 'phi')\n    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n    theta = phi + 2\n    pbar = jax.nn.sigmoid(alpha[gid])\n    gamma = pbar*theta\n    eta = (1 - pbar) * theta\n\n    m.betabinomial(total_count = applications, concentration1 = gamma, concentration0 = eta, obs=admit)\n\n# Run MCMC ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\nm$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\nm$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(gid, applications, admit){\n  # Parameters priors distributions\n  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n  t = phi + 2\n  pbar = jax$nn$sigmoid(alpha[gid])\n  gamma = pbar * t\n  eta = (1 - pbar) * t\n  # Likelihood\n  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#mathematical-details",
    "href": "6. Beta binomial model.html#mathematical-details",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.4 Mathematical Details",
    "text": "8.4 Mathematical Details\n\n8.4.1 Frequentist formulation\n???\n\n\n8.4.2 Bayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distribution as follows:\n\\[\nY_i \\sim BetaBinomial(n_i, \\gamma_i, \\eta_i)\n\\]\n\\[\n\\gamma_i = \\overline{\\rho}   \\tau\n\\]\n\\[\n\\eta_i = (1 - \\overline{\\rho} ) \\tau\n\\]\n\\[\n\\overline{\\rho} = logit(\\alpha_i)\n\\]\n\\[\n\\tau = \\phi + 2\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\phi \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the ount of successes for the i-th observation, which follows a beta-binomial distribution with \\(n_i\\) trials.\n\\(\\gamma_i\\) represents the concentration parameter for the number of successes, derived from the probability of success and scaled by \\(\\tau\\).\n\\(\\eta_i\\) represents the concentration parameter for failures, derived from the probability of failure \\((1 - \\overline{\\rho} )\\) and also scaled by \\(\\tau\\).\n\\(\\overline{\\rho}\\) is the probability of success for the i-th observation. The logit function transforms the linear predictor Œ± (which can take any real value) into a probability value between 0 and 1.\n\\(\\tau\\) is derived from ùúô and is used as a scaling factor for the shape parameters ùõæ and ùúÇ.\nŒ± is a vector of parameters, each representing the effect of the group variable *i on the success probability.\nœï is a random variable following an exponential distribution with a rate of 1.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#references",
    "href": "6. Beta binomial model.html#references",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.5 Reference(s)",
    "text": "8.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  }
]