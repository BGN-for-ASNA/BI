[
  {
    "objectID": "0. Introduction.html",
    "href": "0. Introduction.html",
    "title": "2¬† Introduction",
    "section": "",
    "text": "3 Bayesian analysis\nThis document is a guide to Bayesian analysis and the implementation of Bayesian inference (BI). It is intended for users ranging from those with little or no experience to advanced practitioners. In this introduction, we outline the main steps of Bayesian analysis. Subsequent chapters present increasingly complex models. Each following chapter will have the same structure in order to allow users to easily find the information they are looking for. The structure is as follows:\nWe recommend reading the introduction first since some key concepts here will not be revisited in later chapters.\nBayesian analysis is a statistical approach that uses probability theory to update beliefs about the parameters of a model as new data becomes available. Bayesian methods have several key advantages, as they allow direct uncertainty quantification üõà, incorporation of prior knowledge üõà and flexibility for complex models üõà.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#modeling-likelihood",
    "href": "0. Introduction.html#modeling-likelihood",
    "title": "2¬† Introduction",
    "section": "4.1 Modeling Likelihood",
    "text": "4.1 Modeling Likelihood\nOnce the likelihood is defined, we can now define the mathematical equations that describe our parameters (\\(\\mu\\) and \\(\\sigma\\)) and their relationship with the dependent variable \\(y\\). We can express this relationship in the form of a linear function:\n\\[\n\\mu = \\alpha + \\beta x\n\\]\nWhere \\(\\alpha\\) is the intercept üõà and \\(\\beta\\) is the slope üõà of the line. These parameters are the unknowns that we want to estimate to evaluate the strength and direction of the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#link-functions",
    "href": "0. Introduction.html#link-functions",
    "title": "2¬† Introduction",
    "section": "4.2 Link functions",
    "text": "4.2 Link functions\nDepending on the type of problem you are trying to solve (classification, regression, etc.) and the type of data you are working with (continuous, discrete, binomial, etc.), you will need to choose the appropriate distribution to describe the relationship between the data. By using a different distribution, you will need to use a different link function üõà.\nFor the moment, we just need to know that these different distributions require a link function (for each specific family we will discuss the corresponding link function); however, below is a table summarizing some of the most common link functions, the mathematical form of each, their typical applications, and how to interpret them. Link functions in BI can be accessed through the class BI.link.XXX where XXX is the name of the link function.\n\n\n\n\n\n\n\n\n\nLink Function\nMathematical Form\nTypical Use / Model\nInterpretation & Range\n\n\n\n\nIdentity\n\\(g(\\mu) = \\mu\\)\nLinear regression (Normal)\nDirectly models \\(\\mu\\); \\(\\mu\\) can be any real number.\n\n\nLogit\n\\(g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\nLogistic regression (Binomial)\nModels probabilities (0, 1); coefficients reflect log-odds.\n\n\nProbit\n\\(g(\\mu) = \\Phi^{-1}(\\mu)\\)\nProbit regression (Binomial)\nSimilar to logit; uses the inverse standard normal CDF.\n\n\nLog\n\\(g(\\mu) = \\log(\\mu)\\)\nPoisson, Gamma regression (Count data)\nEnsures \\(\\mu &gt; 0\\); coefficients represent multiplicative effects.\n\n\nInverse\n\\(g(\\mu) = \\frac{1}{\\mu}\\)\nGamma regression\nModels positive \\(\\mu\\); relates changes inversely to \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#the-prior-distributions",
    "href": "0. Introduction.html#the-prior-distributions",
    "title": "2¬† Introduction",
    "section": "4.3 The Prior Distributions",
    "text": "4.3 The Prior Distributions\nFor each parameter of our equation that describes \\(\\mu\\), we need to define a prior distribution üõà that encodes our initial beliefs about the parameter. In the case of the linear regression model, we need to specify prior distributions for the intercept \\(\\alpha\\), the slope \\(\\beta\\), and standard deviation \\(\\sigma\\).\n\\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\]\n\\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nAnd with this, we can write our entire model as:\n\\[\ny \\sim \\text{Normal}(\\mu, \\sigma)\n\\] \\[\n\\mu = \\alpha + \\beta x\n\\] \\[\n\\alpha \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma \\sim \\text{Uniform}(0, 1)\n\\]\nIn BI, you will need to define this model within a function in which you will be able to use any probability distributions, link functions, and mathematical operations required for your model. BI has been designed to allow you to declare your model as close as possible to the mathematical notation. For example, the model above can be written in BI as:\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\ndat = dict(\n    x = m.dist.normal(178, 20, sample = True),   \n    y = m.dist.lognormal( 0, 1, sample = True)   \n)\nm.data_on_model = dat\n\ndef model(x, y):    \n    alpha = m.dist.normal( 0, 1, name = 'alpha', shape= (1,))\n    beta = m.dist.normal( 0, 1, name = 'beta', shape= (1,))   \n    sigma = m.dist.uniform( 0, 50, name = 'sigma', shape = (1,))\n    m.normal(alpha + beta * x, sigma, obs=y)\nThe code snippet provides several key features of the BI package:\n\nFirst, you need to initialize a bi object.\nThen, you can store data as a JAX array dictionary using the m.data_on_model function. If all the data can be stored in a data frame (e.g., Linear Regression for continuous variable), you do not need to use m.data_on_model, as the BI object automatically detects the data provided in the model arguments. However, sometimes you may need different data structures such as vectors and 2D arrays (e.g., Network model).\nRegarding distribution parameters, note the difference depending on whether you are generating data outside a function (e.g., for simulation purposes) or specifying priors inside a model function. In the former case, the argument sample should be set to True. However, if you are specifying priors within a model function, this argument must be set to False.\nFinally, note that each parameter declared in the model must have a unique name as well as a shape. The shape refers to the number of parameters you want to estimate. For example, if you want to estimate a different \\(\\beta\\) for each independent variable, you would declare \\(\\beta\\) with a shape equal to the number of independent variables. By default, the shape is one, so technically you don‚Äôt need to specify it. In this example, we highlight this feature explicitly.\n\n\n4.3.1 Which prior distribution range to use?\nThe choice of prior ranges can significantly affect Bayesian analysis results. There are several approaches to selecting them:\n\nExpert Knowledge: The prior distributions can be based on expert knowledge or historical data. This approach is useful when there is a lot of information available about the parameters.\nNoninformative Priors: When there is little or no information about the parameters, noninformative priors can be used. These priors are designed to have minimal influence on the posterior distribution, allowing the data to dominate the inference process.\nScaled data: If the data is scaled üõà, the prior distributions can be chosen to reflect this. For example, if the data are scaled, the prior distributions for the intercept and slope can be centered around 0 and 1, respectively. By scaling the independent variable, we obtain a unit of change based on variance; that is, the effect represents a one‚Äìstandard‚Äìdeviation change in \\(x\\) on \\(y\\). Scaling the data improves both numerical stability and interpretability. When all data are scaled to the same range, it leads to more stable numerical behavior during estimation. Additionally, it facilitates setting priors that are both meaningful and relatively uninformative. By aligning the scale of the data with the scale assumed in the priors, we ensure that the posterior distributions exhibit reasonable spread and that our uncertainty quantification is consistent with the data‚Äôs scale. For the remainder of this document, we will assume that the data are scaled.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-fit-and-posterior-distribution",
    "href": "0. Introduction.html#model-fit-and-posterior-distribution",
    "title": "2¬† Introduction",
    "section": "4.4 Model fit and posterior distribution",
    "text": "4.4 Model fit and posterior distribution\nOnce data are observed, Bayes‚Äô Theorem üõà is used to evaluate how well a given set of parameter values fits the data:\n\\[\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\\]\nWhere:\n\n\\(\\theta\\) represents the unknown parameters we are interested in.\n\\(P(\\theta)\\) is the prior distribution, representing our beliefs about \\(\\theta\\) before seeing the data.\n\\(P(\\text{data} \\mid \\theta)\\) is the likelihood, representing the model of how the data are generated given \\(\\theta\\).\n\\(P(\\theta \\mid \\text{data})\\) is the posterior distribution, representing our updated beliefs after observing the data. It tells us not only the most likely value of \\(\\theta\\) (e.g., \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) in our case) but also quantifies the uncertainty in these estimates.\n\nWe can use Bayesian updating üõà using the Bayesian theorem to ‚Äòreshape‚Äô the prior distributions by considering every possible combination of values for our parameters, and scoring each combination by its relative plausibility in light of the data. These relative plausibilities are the posterior probabilities of each combination of our parameters: the posterior distributions. Various techniques can be used to approximate the mathematical definition of Bayes‚Äô theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Descriptions of these algorithms are out of the scope of this document. For more information, please refer to the Bayesian Inference. In BI, we use MCMC and it can be called as m.run(model) where model is the function that describes the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-diagnostic",
    "href": "0. Introduction.html#model-diagnostic",
    "title": "2¬† Introduction",
    "section": "4.5 Model ‚Äòdiagnostic‚Äô",
    "text": "4.5 Model ‚Äòdiagnostic‚Äô\nOnce a Bayesian model has been fit, it is crucial to evaluate how well it captures the observed data and to assess whether the Markov chain Monte Carlo (MCMC) sampling has converged. Bayesian model diagnostics help us answer questions like: ‚ÄúAre our uncertainty estimates reliable?‚Äù, ‚ÄúDoes the model generate data similar to what we observed?‚Äù, and ‚ÄúHave the chains mixed well?‚Äù Multiple diagnostics approaches can be used to assess the model‚Äôs performance. Below are some key diagnostic tools and techniques available in BI within the class BI.diag.XXX where XXX is the name of the diagnostic tool.\n\n\n\n\n\n\n\n\n\nDiagnostic Tool\nPurpose\nKey Indicator\nInterpretation\n\n\n\n\nposterior predictive checks (PPCs) üõà\nAssess if the model can reproduce observed data\nGraphs, p-values, summary stats\nGood fit if simulated data resemble observed data\n\n\nCredible Interval (CI)\nQuantify uncertainty in parameter estimates\n95% CI or other percentage\n95% probability the parameter lies within the interval\n\n\nhighest posterior density intervals (HPDI) üõà\nIdentify the narrowest interval containing a given probability mass\n95% HPDI\nSmallest interval capturing 95% of the posterior density\n\n\neffective sample size (ESS) üõà\nMeasure independent information in the chain\nESS value (ideally high)\nLow ESS indicates high autocorrelation (poor mixing)\n\n\npotential scale reduction factor (Rhat) üõà\nCheck convergence across multiple chains\nRhat ‚âà 1 (typically &lt;1.1)\nValues near 1 indicate convergence; &gt;&gt;1 suggests non-convergence\n\n\nTrace plots üõà\nVisualize the sampling path to check convergence and mixing\nPlot showing parameter values over iterations\nStationary, ‚Äòhairy caterpillar‚Äô pattern suggests convergence\n\n\nautocorrelation plots üõà\nAssess dependency between samples over lags\nAutocorrelation values across lags\nRapid decay to zero suggests good mixing; slow decay indicates poor mixing\n\n\ndensity plots üõà\nVisualize the posterior distribution of a parameter\nSmoothness and shape of the curve\nUnimodal and smooth suggests convergence; multimodal or irregular may suggest poor mixing",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "0. Introduction.html#model-comparison",
    "href": "0. Introduction.html#model-comparison",
    "title": "2¬† Introduction",
    "section": "4.6 Model comparison",
    "text": "4.6 Model comparison\nModel comparison is performed by evaluating how well different models explain the observed data while accounting for model complexity. Multiple criteria can be used to compare models, and are summarized in the table below. In BI, we can compare models using Watanabe-Akaike Information Criterion (WAIC) with the function m.diag.waic(model1, model2).\n\n\n\n\n\n\n\n\n\n\nCriterion\nPurpose\nInterpretation\nStrengths\nWeaknesses\n\n\n\n\nDIC (Deviance Information Criterion)\nMeasures model fit while penalizing complexity\nLower values indicate better model fit\nSimple to compute, useful for hierarchical models\nSensitive to the number of parameters, not always reliable in complex models\n\n\nWAIC (Watanabe-Akaike Information Criterion)\nEstimates out-of-sample predictive accuracy while penalizing complexity\nLower values indicate better models\nMore robust than DIC, accounts for overfitting\nComputationally intensive for large models\n\n\nBayes Factor (BF)\nQuantifies relative support for two models based on marginal likelihoods\nBF &gt; 1 favors the numerator model, BF &lt; 1 favors the denominator\nProvides direct evidence comparison, works with different model types\nSensitive to prior choices, requires good model specification",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html",
    "href": "1. Linear Regression for continuous variable.html",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "",
    "text": "3.1 General Principles\nTo study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#general-principles",
    "href": "1. Linear Regression for continuous variable.html#general-principles",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "",
    "text": "An intercept \\(\\alpha\\), which represents the origin of the line‚Äîthe expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\nA coefficient \\(\\beta\\), which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\nA variance term \\(\\sigma\\), which informs us about the spread of points around the line, i.e., the variance around the prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#considerations",
    "href": "1. Linear Regression for continuous variable.html#considerations",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "3.2 Considerations",
    "text": "3.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nBayesian models consider model parameter uncertainty üõà, allowing for the quantification of confidence or uncertainty through the parameters‚Äô posterior distribution üõà. Therefore, we need to declare prior distributions üõà for each model parameter, in this case for: \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\).\nPrior distributions are built following these considerations:\n\nAs the data is scaled üõà (see introduction), we can use a Normal distribution for \\(\\alpha\\) and \\(\\beta\\), with a mean of 0 and a standard deviation of 1.\nSince \\(\\sigma\\) is strictly positive, we can use any distribution that is positively defined, such as the Exponential or Uniform distribution.\n\nGaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without needing a link function üõà (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#example",
    "href": "1. Linear Regression for continuous variable.html#example",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "3.3 Example",
    "text": "3.3 Example\nBelow is an example code snippet demonstrating Bayesian linear regression using the Bayesian Inference (BI) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight']) # Scale\n\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.lognormal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.run(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')\n\n# Filter data frame\nm$df = m$df[m$df$age &gt; 18,]\n\n# Scale\nm$scale(list('weight')) \n\n# Convert data to JAX arrays\nm$data_to_model(list('weight', 'height'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight){\n  # Parameter prior distributions\n  s = bi.dist.uniform(0, 50, name = 's')\n  a = bi.dist.normal(178, 20,  name = 'a')\n  b = bi.dist.normal(0, 1, name = 'b')\n  \n  # Likelihood\n  m$normal(a + b * weight, s, obs = height)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#mathematical-details",
    "href": "1. Linear Regression for continuous variable.html#mathematical-details",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "3.4 Mathematical Details",
    "text": "3.4 Mathematical Details\n\n3.4.1 Frequentist Formulation\nThe following equation allows us to draw a line: \\[\nY_i = \\alpha + \\beta  X_i + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma_i\\) is the error term for observation i.\n\n\n\n3.4.2 Bayesian Formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this regression model using the following model:\n\\[\nY_i \\sim Normal(\\alpha + \\beta   X_i, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0, 1)\n\\]\n\\[\n\\beta \\sim Normal(0, 1)\n\\]\n\\[\n\\sigma \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) and \\(\\beta\\) are the intercept and regression coefficient, respectively.\n\\(X_i\\) is the input variable for observation i.\n\\(\\sigma\\) is the standard deviation of the normal distribution, which describes the variance in the relationship between the dependent variable \\(Y\\) and the independent variable \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#notes",
    "href": "1. Linear Regression for continuous variable.html#notes",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "3.5 Notes",
    "text": "3.5 Notes\n\n\n\n\n\n\nNote\n\n\n\nWe can observe a difference between the Frequentist and the Bayesian formulation regarding the error term. Indeed, in the Frequentist formulation, the error term \\(\\sigma_i\\) represents random fluctuations around the predicted values. This assumption leads to point estimates for \\(\\alpha\\) and \\(\\beta\\), without accounting for uncertainty in these estimates. In contrast, the Bayesian formulation treats \\(\\sigma\\) as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "1. Linear Regression for continuous variable.html#references",
    "href": "1. Linear Regression for continuous variable.html#references",
    "title": "3¬† Linear Regression for a Continuous Variable",
    "section": "3.6 Reference(s)",
    "text": "3.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Linear Regression for a Continuous Variable</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html",
    "href": "2. Multiple continuous Variables.html",
    "title": "4¬† Multiple continuous variables model",
    "section": "",
    "text": "4.1 General Principles\nTo study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend Linear Regression for continuous variable by adding a regression coefficient \\(\\beta_x\\) for each continuous variable (e.g., \\(\\beta_{weight}\\) and \\(\\beta_{age}\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#considerations",
    "href": "2. Multiple continuous Variables.html#considerations",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.2 Considerations",
    "text": "4.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Regression for continuous variable.\nThe model interpretation of the regression coefficients \\(\\beta_x\\) is considered for fixed values of the other independent variable(s)‚Äô regression coefficients‚Äîi.e., for a given age, \\(\\beta_{weight}\\) represents the expected change in the dependent variable (height) for each one-unit increase in weight, holding all other variable(s) constant (age).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#example",
    "href": "2. Multiple continuous Variables.html#example",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.3 Example",
    "text": "4.3 Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (height, weight, age), and the goal is to estimate the effect of weight and age on height.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nfrom importlib.resources import files\n# Import\ndata_path = files('BI.resources.data') / 'Howell1.csv'\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age &gt; 18] # Manipulate\nm.scale(['weight', 'age']) # Scale\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0,50, name = 'sigma')\n    # Likelihood\n    m.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run MCMC ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Howell1.csv\", sep = ''), sep=';')# Import\nm$df = m$df[m$df$age &gt; 18,] # Manipulate\nm$scale(list('weight', 'age')) # Scale\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(height, weight, age){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0, 0.5, name = 'a')\n  beta1 = bi.dist.normal( 0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(  0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#mathematical-details",
    "href": "2. Multiple continuous Variables.html#mathematical-details",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.4 Mathematical Details",
    "text": "4.4 Mathematical Details\n\n4.4.1 Frequentist formulation\nWe model the relationship between the independent variables \\((X_{1i}, X_{2i}, ..., X_{ni})\\) and the dependent variable Y using the following equation:\n\\[\nùëå_i = \\alpha +\\beta_1  ùëã_{1i} + \\beta_2  ùëã_{2i} + ... + \\beta_n  ùëã_{ni} + \\sigma_i\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\beta_1\\), \\(\\beta_2\\), ‚Ä¶, \\(\\beta_n\\) are the regression coefficients.\n\\(\\sigma_i\\) is the error term for observation i.\n\n\n\n4.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian model as follows:\n\\[\nùëå \\sim Normal(\\alpha + \\sum_k^n  \\beta_k  X, œÉ¬≤)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\nœÉ \\sim Uniform(0, 50)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are the prior distributions for the k distinct regression coefficients.\n\\(X_{1i}\\), \\(X_{2i}\\), ‚Ä¶, \\(X_{ni}\\) are the values of the independent variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring that it is positive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "2. Multiple continuous Variables.html#references",
    "href": "2. Multiple continuous Variables.html#references",
    "title": "4¬† Multiple continuous variables model",
    "section": "4.5 Reference(s)",
    "text": "4.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Multiple continuous variables model</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html",
    "href": "3. Interaction between continuous variables.html",
    "title": "5¬† Interaction terms",
    "section": "",
    "text": "5.1 General Principles\nTo study the relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#considerations",
    "href": "3. Interaction between continuous variables.html#considerations",
    "title": "5¬† Interaction terms",
    "section": "5.2 Considerations",
    "text": "5.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same assumptions as for Regression for continuous variable.\nWe wish to model the relationship between dependent variable Y and independent variable \\(X_1\\) to vary as a function of independent variable \\(X_2\\). To do this, we explicitly model the hypothesis that the slope between Y and \\(X_1\\) depends‚Äîis conditional‚Äîupon \\(X_2\\).\nFor continuous interactions with scaled data, the intercept becomes the grand mean üõà of the outcome variable.\nThe interpretation of estimates is more complex. The estimate for a non-interaction term reflects the expected change in Y when \\(X_1\\) increases by one unit, holding \\(X_2\\) constant at its average value. The estimate for the interaction term represents how the effect of \\(X_1\\) on Y changes depending on the value of \\(X_2\\), and vice versa, showing how the relationship between the two variables influences the outcome Y.\nTriptych üõà plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#example",
    "href": "3. Interaction between continuous variables.html#example",
    "title": "5¬† Interaction terms",
    "section": "5.3 Example",
    "text": "5.3 Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. The data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'tulips.csv'\nm.data(data_path, sep=';')\nm.scale(['blooms', 'water', 'shade']) # Scale\n\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.normal(mu, sigma, obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/tulips.csv\", sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Scale\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(blooms, water,shade){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')\n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint')\n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#mathematical-details",
    "href": "3. Interaction between continuous variables.html#mathematical-details",
    "title": "5¬† Interaction terms",
    "section": "5.4 Mathematical Details",
    "text": "5.4 Mathematical Details",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#frequentist-formulation",
    "href": "3. Interaction between continuous variables.html#frequentist-formulation",
    "title": "5¬† Interaction terms",
    "section": "5.5 Frequentist formulation",
    "text": "5.5 Frequentist formulation\nWe model the relationship between the input features (\\(X_1\\) and \\(X_2\\)) and the target variable (\\(Y\\)) using the following equation: \\[\nùëå_i = \\alpha + \\beta_1 ùëã_{1i} + \\beta_2 ùëã_{2i} + \\beta_{interaction} ùëã_{1i} ùëã_{2i} + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\beta_1\\) and \\(\\beta_2\\) are the regression coefficients for \\(X_{1}\\) and \\(X_{2}\\), respectively.\n\\(\\beta_{interaction}\\) is the regression coefficient for the interaction term \\((X_{1} X_{2})\\).\n\\(\\sigma\\) is the error term, assumed to be normally distributed.\n\nIn this context, the interaction term \\(X_{1i} * X_{2i}\\) captures the joint effect of \\(X_{1i}\\) and \\(X_{2i}\\) on the target variable \\(Y_i\\).\n\n5.5.1 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Normal(\\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_{interaction} X_{1i} X_{2i}, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_1 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_2 \\sim Normal(0,1)\n\\]\n\\[\n\\beta_{interaction} \\sim Normal(0,1)\n\\]\n\\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_{interaction}\\) are the prior distributions for the regression coefficients.\n\\(X_{1i}\\) and \\(X_{2i}\\) are the two values of the independent continuous variables for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "3. Interaction between continuous variables.html#references",
    "href": "3. Interaction between continuous variables.html#references",
    "title": "5¬† Interaction terms",
    "section": "5.6 Reference(s)",
    "text": "5.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interaction terms</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html",
    "href": "4. Categorical variable.html",
    "title": "6¬† Regression for Categorical Variables",
    "section": "",
    "text": "6.1 General Principles\nTo study the relationship between a categorical independent variable and a continuous dependent variable, we use a Categorical model which applies stratification.\nStratification involves modeling how the k different categories of the independent variable affect the target continuous variable by performing a regression for each k category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using one-hot encoding üõà or by converting categories to indices üõà.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#considerations",
    "href": "4. Categorical variable.html#considerations",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.2 Considerations",
    "text": "6.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a Continuous Variable.\nAs we generate regression coefficients for each k category, we need to specify a prior with a shape equal to the number of categories k in the code (see comments in the code).\nTo compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. Never compare confidence intervals or p-values directly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#example",
    "href": "4. Categorical variable.html#example",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.3 Example",
    "text": "6.3 Example\nBelow is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (kcal_per_g), representing the caloric value of milk per gram, and a categorical independent variable, representing species clade membership. The goal is to estimate the differences in milk calories between clades.\n\nPythonR\n\n\nfrom main import*\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'milk.csv'\nm.data(data_path, sep=';') \nm.index([\"clade\"]) # Manipulate\nm.scale(['kcal_per_g']) # Scale\nm.data_to_model(['kcal_per_g', \"index_clade\"]) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade):\n    a = m.bi.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    s = m.bi.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]\n    m.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\nm=importbi(platform='cpu')\n\n# Load csv file\nm$data(paste(system.file(package = \"BI\"),\"/data/milk.csv\", sep = ''), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(kcal_per_g, index_clade){\n  # Parameter prior distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma =bi.dist.exponential(1, name = 's')\n  # Likelihood\n  m$normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#mathematical-details",
    "href": "4. Categorical variable.html#mathematical-details",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.4 Mathematical Details",
    "text": "6.4 Mathematical Details\n\n6.4.1 Frequentist formulation\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\\[\nY_i = \\alpha + \\beta_k X_i + \\sigma\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_k\\) are the regression coefficients for each k category.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the error term.\n\nWe can interpret \\(\\beta_i\\) as the effect of each category on \\(Y\\) relative to the baseline (usually one of the categories or the intercept).\n\n\n6.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Normal(\\alpha +  \\beta_k X, \\sigma)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta_k \\sim Normal(0,1)\n\\]\n\\[\n\\sigma \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\alpha\\) is the prior distribution for the intercept.\n\\(\\beta_k\\) are k prior distributions for k regression coefficients.\n\\(X_i\\) is the encoded categorical input variable for observation i.\n\\(\\sigma\\) is the prior distribution for the standard deviation, ensuring it is positive.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#notes",
    "href": "4. Categorical variable.html#notes",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.5 Notes",
    "text": "6.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms similarly to Chapter 3: Interaction between Continuous Variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "4. Categorical variable.html#references",
    "href": "4. Categorical variable.html#references",
    "title": "6¬† Regression for Categorical Variables",
    "section": "6.6 Reference(s)",
    "text": "6.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression for Categorical Variables</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html",
    "href": "5. Binomial model.html",
    "title": "7¬† Binomial Model",
    "section": "",
    "text": "7.1 General Principles\nTo model the relationship between a binary dependent variable‚Äîe.g., success/failure, yes/no, or 1/0‚Äîand one or more independent variables, we can use a Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#considerations",
    "href": "5. Binomial model.html#considerations",
    "title": "7¬† Binomial Model",
    "section": "7.2 Considerations",
    "text": "7.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nThe first link function is the logit. The logit link function in the Bayesian binomial model converts the linear combination of predictor variables into probabilities, making it suitable for modeling binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring results fall within the bounds of the binomial distribution \\(\\in[0,1]\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#example",
    "href": "5. Binomial model.html#example",
    "title": "7¬† Binomial Model",
    "section": "7.3 Example",
    "text": "7.3 Example\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled. The goal is to evaluate the probability of pulling the left side.\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'chimpanzees.csv'\nm.data(data_path, sep=';')\nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.binomial(logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.run(model, num_samples=500)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  m$binomial(logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#mathematical-details",
    "href": "5. Binomial model.html#mathematical-details",
    "title": "7¬† Binomial Model",
    "section": "7.4 Mathematical Details",
    "text": "7.4 Mathematical Details\n\n7.4.1 Frequentist formulation\nWe model the relationship between the independent variable (\\(X_i\\)) and the binary dependent variable (\\(Y_i\\)) using the following equation: \\[\nlogit(Y_i) = \\alpha + \\beta X_i\n\\]\nWhere:\n\n\\(Y_i\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the independent variable for observation i.\n\\(logit(Y_i)\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.\n\n\n\n7.4.2 Bayesian formulation\npriors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim Binomial(n = 1, p)\n\\]\n\\[\nlogit(p) \\sim \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\nWhere:\n\n\\(Y_i\\) is the probability of success (i.e., the probability of the binary outcome being 1) for observation i.\n\\(n = 1\\) represents the number of trials in the binomial distribution (binary outcome).\n\\(\\beta\\) and \\(\\alpha\\) are the prior distributions for the regression coefficient and intercept, respectively.\n\\(logit\\) is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation i.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#notes",
    "href": "5. Binomial model.html#notes",
    "title": "7¬† Binomial Model",
    "section": "7.5 Notes",
    "text": "7.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\nBelow is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (pulled_left), which represents which side individuals pulled, and three independent variables (actor, side, cond). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as the different conditions.\n\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';')\nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.lk(\"y\", m.dist.binomial(1, logits=p), obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.run(model)\n# Diagnostic ------------------------------------------------\nm.summary()",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "5. Binomial model.html#references",
    "href": "5. Binomial model.html#references",
    "title": "7¬† Binomial Model",
    "section": "7.6 Reference(s)",
    "text": "7.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html",
    "href": "6. Beta binomial model.html",
    "title": "8¬† Beta-Binomial Model",
    "section": "",
    "text": "8.1 General Principles\nTo model the relationship between a binary outcome variable representing success counts and one or more independent variables with overdispersion üõà, we can use the Beta-Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#considerations",
    "href": "6. Beta binomial model.html#considerations",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.2 Considerations",
    "text": "8.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Binomial regression.\nA Beta-Binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.\nA Beta distribution has two parameters: the rates for each probability and a shape parameter Œ∏. Œ∏ influences how probabilities are distributed between 0 and 1. Specifically, it consists of two parameters, \\(\\alpha\\) and \\(\\beta\\), which determine the concentration of probability around 0 and 1.\n\nIf both are equal to or greater than 1, the distribution is bell-shaped and centered around 0.5.\nIf \\(\\alpha &gt; \\beta\\), the distribution is skewed toward 1, and if \\(\\beta &gt; \\alpha\\), it is skewed toward 0. Thus, the shape parameters \\(\\gamma\\) and \\(\\eta\\) provide flexibility in modeling various types of prior beliefs about probabilities.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#example",
    "href": "6. Beta binomial model.html#example",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.3 Example",
    "text": "8.3 Example\nBelow is an example code snippet demonstrating Bayesian Beta-Binomial regression using the Bayesian Inference (BI) package. The data consist of:\n\nOne binary dependent variable (admit), which represents candidates‚Äô admission status.\nOne independent categorical variable representing individuals‚Äô gender (gid).\nAdditionally, we have the number of applications (applications) per gender, which will be used to account for independent rates.\n\nThe goal is to evaluate whether the probability of admission is different between genders, while accounting for differences in the number of applications between genders.\n\nPythonR\n\n\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'UCBadmit.csv'\nm.data(data_path, sep=';') \nm.df[\"gid\"] = (m.df[\"applicant.gender\"] != \"male\").astype(int)\n\n# Define model ------------------------------------------------\ndef model(gid, applications, admit):\n    phi = m.dist.exponential(1,  name = 'phi')\n    alpha = m.dist.normal( 0., 1.5, shape=(2,), name = 'alpha')\n    theta =  phi + 2\n    pbar = jax.nn.sigmoid(alpha[gid])\n    concentration1 = pbar*theta\n    concentration0 = (1 - pbar) * theta\n\n    m.dist.betabinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0, obs=admit)\n\n# Run MCMC ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/UCBadmit.csv\", sep = ''), sep=';')\nm$df[\"gid\"] = as.integer(ifelse(m$df[\"applicant.gender\"] == \"male\", 0, 1)) # Manipulate\nm$data_to_model(list('gid', 'applications', 'admit' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(gid, applications, admit){\n  # Parameter prior distributions\n  phi = bi.dist.exponential(1, name = 'phi',shape=c(1))\n  alpha = bi.dist.normal(0., 1.5, shape= c(2), name='alpha')\n  t = phi + 2\n  pbar = jax$nn$sigmoid(alpha[gid])\n  gamma = pbar * t\n  eta = (1 - pbar) * t\n  # Likelihood\n  m$betabinomial(total_count=applications, concentration1=gamma, concentration0=eta, obs=admit)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#mathematical-details",
    "href": "6. Beta binomial model.html#mathematical-details",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.4 Mathematical Details",
    "text": "8.4 Mathematical Details\n\n8.4.1 Bayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim BetaBinomial(n_i, \\gamma_i, \\eta_i)\n\\]\n\\[\n\\gamma_i = \\overline{\\rho}   \\tau\n\\]\n\\[\n\\eta_i = (1 - \\overline{\\rho} ) \\tau\n\\]\n\\[\n\\overline{\\rho} = logit(\\alpha_i)\n\\]\n\\[\n\\tau = \\phi + 2\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\phi \\sim Exponential(1)\n\\]\nWhere:\n\n\\(Y_i\\) is the count of successes for the i-th observation, which follows a beta-binomial distribution with \\(n_i\\) trials.\n\\(\\gamma_i\\) represents the concentration parameter for the number of successes, derived from the probability of success and scaled by \\(\\tau\\).\n\\(\\eta_i\\) represents the concentration parameter for failures, derived from the probability of failure \\((1 - \\overline{\\rho} )\\) and also scaled by \\(\\tau\\).\n\\(\\overline{\\rho}\\) is the probability of success for the i-th observation. The logit function transforms the linear predictor Œ± (which can take any real value) into a probability value between 0 and 1.\n\\(\\tau\\) is derived from ùúô and is used as a scaling factor for the shape parameters ùõæ and ùúÇ.\nŒ± is a vector of parameters, each representing the effect of group i on the success probability.\nœï is a random variable following an exponential distribution with a rate of 1.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "6. Beta binomial model.html#references",
    "href": "6. Beta binomial model.html#references",
    "title": "8¬† Beta-Binomial Model",
    "section": "8.5 Reference(s)",
    "text": "8.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Beta-Binomial Model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html",
    "href": "7. Poisson model.html",
    "title": "9¬† Poisson model",
    "section": "",
    "text": "9.1 General Principles\nTo model the relationship between a count outcome variable‚Äîe.g., counts of events occurring in a fixed interval of time or space‚Äîand one or more independent variables, we can use the Poisson model.\nThis is a special shape of the binomial distribution; it is useful because it models binomial events for which the number of trials \\(n\\) is unknown or uncountably large.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html#considerations",
    "href": "7. Poisson model.html#considerations",
    "title": "9¬† Poisson model",
    "section": "9.2 Considerations",
    "text": "9.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nWe have the second link function üõà: log. The log link ensures that Œª is always positive.\nTo invert the log link function and linearly model the relationship between the predictor variables and the log of the mean rate parameter, we can apply the exponential function (see comment in code).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html#example",
    "href": "7. Poisson model.html#example",
    "title": "9¬† Poisson model",
    "section": "9.3 Example",
    "text": "9.3 Example\nBelow is an example code snippet demonstrating a Bayesian Poisson model using the Bayesian Inference (BI) package. Data consist of:\n\nA continuous dependent variable total_tools, which represents the number of tools produced by a civilization.\nA continuous independent variable population representing population size.\nA categorical independent variable cid representing different civilizations.\n\nThe goal is to estimate the production of tools based on population size, accounting for each civilization.\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline.csv'\nm.data(data_path, sep=';')\nm.scale(['population'])\nm.df[\"cid\"] = (m.df.contact == \"high\").astype(int)\n#m.data_to_model(['total_tools', 'population', 'cid'])\ndef model(cid, population, total_tools):\n    a = m.dist.normal(3, 0.5, shape= (2,), name='a')\n    b = m.dist.normal(0, 0.2, shape=(2,), name='b')\n    l = jnp.exp(a[cid] + b[cid]*population)\n    m.poisson(l, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.run(model)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline.csv\", sep = ''), sep=';')\nm$scale(list('population'))# Scale\nm$df[\"cid\"] =  as.integer(ifelse(m$df$contact == \"high\", 1, 0)) # Manipulate\nm$data_to_model(list('total_tools', 'population', 'cid' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(total_tools, population, cid){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(3, 0.5, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0, 0.2, name='beta', shape = c(2))\n  l = jnp$exp(alpha[cid] + beta[cid]*population)\n  # Likelihood\n  m$poisson(l, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html#mathematical-details",
    "href": "7. Poisson model.html#mathematical-details",
    "title": "9¬† Poisson model",
    "section": "9.4 Mathematical Details",
    "text": "9.4 Mathematical Details\n\n9.4.1 Frequentist formulation\nWe model the relationship between the predictor variable (\\(X_i\\)) and the count outcome variable (\\(Y_i\\)) using the following equation:\n\\[\n\\log(\\lambda_i) = \\alpha + \\beta  X_i\n\\]\nWhere:\n\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution (expected count) for observation i, modeled as the exponential function of the linear combination of predictors.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i, ensuring it is positive.\n\\(\\beta\\) is the regression coefficient.\n\\(\\alpha\\) is the intercept term.\n\\(X_i\\) is the value of the independent variable for observation i.\n\n\n\n9.4.2 Bayesian formulation\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY \\sim Poisson(\\lambda_i)\n\\]\n\\[\n\\log(\\lambda_i) = \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim Normal(0, 1)\n\\]\n\\[\n\\beta \\sim Normal(0, 1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution for observation i, modeled as the exponential function of the linear combination of predictors.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i.\n\\(\\alpha\\) and \\(\\beta\\) are the prior distributions for the intercept and the regression coefficients, respectively.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution, modeled as the exponential function of the linear combination of predictors.\n\\(X_i\\) is the value of the independent variable for observation i.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html#notes",
    "href": "7. Poisson model.html#notes",
    "title": "9¬† Poisson model",
    "section": "9.5 Notes",
    "text": "9.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to chapter 2.\nWe can apply interaction terms similarly to chapter 3.\nWe can apply categorical variables similarly to chapter 4.\n\n\n9.6 Reference(s)\nMcElreath (2018)\n\n\n\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "7. Poisson model.html#references",
    "href": "7. Poisson model.html#references",
    "title": "9¬† Poisson model",
    "section": "9.6 Reference(s)",
    "text": "9.6 Reference(s)\nMcElreath (2018)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html",
    "href": "8. Gamma-Poisson.html",
    "title": "10¬† Gamma-Poisson model",
    "section": "",
    "text": "10.1 General Principles\nTo model the relationship between a count outcome variable and one or more independent variables with overdispersion üõà, we can use the Negative Binomial model.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#considerations",
    "href": "8. Gamma-Poisson.html#considerations",
    "title": "10¬† Gamma-Poisson model",
    "section": "10.2 Considerations",
    "text": "10.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Poisson model.\nOverdispersion is handled because the Negative Binomial model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is log_days).",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#example",
    "href": "8. Gamma-Poisson.html#example",
    "title": "10¬† Gamma-Poisson model",
    "section": "10.3 Example",
    "text": "10.3 Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package:\n\nPythonR\n\n\nfrom BI import bi\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim dat Gamma poisson.csv'\nm.data(data_path, sep=',') \nm.data_to_model(['log_days', 'monastery', 'y']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    l = m.jnp.exp(log_days + a + b * monastery)\n    m.poisson(rate = l, obs=y)\n# Run MCMC ------------------------------------------------\nm.run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  l = jnp$exp(log_days + alpha + beta * monastery)\n  # Likelihood\n  m$poisson(rate=l, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#mathematical-details",
    "href": "8. Gamma-Poisson.html#mathematical-details",
    "title": "10¬† Gamma-Poisson model",
    "section": "10.4 Mathematical Details",
    "text": "10.4 Mathematical Details\n\n10.4.1 Frequentist formulation\nWe model the relationship between the independent variable \\(X\\) and the count outcome variable \\(Y\\) using the following equation:\n\\[\n\\log(\\lambda_i) = \\exp(\\text{rates}_i + \\alpha + \\beta X_i)\n\\]\nWhere:\n\n\\(\\lambda_i\\) is the mean rate parameter of the negative binomial distribution (expected count) for observation i.\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter, ensuring it is positive for observation i.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the predictor variable for observation i.\n\n\n\n10.4.2 Bayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\n\\[\n\\log(\\lambda_i) = \\text{rates}_i + \\alpha + \\beta X_i\n\\]\n\\[\n\\alpha \\sim \\text{Normal}(0,1)\n\\]\n\\[\n\\beta \\sim \\text{Normal}(0,1)\n\\]\nWhere:\n\n\\(Y_i\\) is the dependent variable for observation i.\n\\(\\lambda_i\\) is the mean rate parameter of the Poisson distribution for observation i, assuming that each Poisson count observation has its own \\(rate_i\\).\n\\(\\log(\\lambda_i)\\) is the log of the mean rate parameter for observation i, ensuring it is positive.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient.\n\\(X_i\\) is the value of the predictor variable for observation i.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#notes",
    "href": "8. Gamma-Poisson.html#notes",
    "title": "10¬† Gamma-Poisson model",
    "section": "10.5 Notes",
    "text": "10.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly as in chapter 2.\nWe can apply interaction terms similarly as in chapter 3.\nWe can apply categorical variables similarly as in chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "8. Gamma-Poisson.html#references",
    "href": "8. Gamma-Poisson.html#references",
    "title": "10¬† Gamma-Poisson model",
    "section": "10.6 Reference(s)",
    "text": "10.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Gamma-Poisson model</span>"
    ]
  },
  {
    "objectID": "9. Multinomial model.html",
    "href": "9. Multinomial model.html",
    "title": "11¬† Multinomial model",
    "section": "",
    "text": "11.1 General Principles\nTo model the relationship between a categorical outcome variable with more than two categories and one or more independent variables, we can use a Multinomial model.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Multinomial model</span>"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#considerations",
    "href": "9. Multinomial model.html#considerations",
    "title": "11¬† Multinomial model",
    "section": "11.2 Considerations",
    "text": "11.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for continuous variable.\nOne way to interpret a multinomial model is to consider that we need to build \\(K - 1\\) linear models, where \\(K\\) is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a simplex üõà. To do this, we convert the regression outputs using the softmax function üõà (see the ‚Äújax.nn.softmax‚Äù line in the code).\nThe intercept \\(\\alpha\\) captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\nOn the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients \\(\\beta\\) are shared across categories.\nThe relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Multinomial model</span>"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#example",
    "href": "9. Multinomial model.html#example",
    "title": "11¬† Multinomial model",
    "section": "11.3 Example",
    "text": "11.3 Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package:\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(career, income):\n    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n    b = m.dist.halfnormal(0.5, shape=(1,), name = 'b')\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0] #pivot\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    m.dist.categorical(probs=p, obs=career)\n\n# Run sampler ------------------------------------------------ \nm.run(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multinomial.csv\", sep = ''), sep=',')\nkeys &lt;- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues &lt;- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel &lt;- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.halfnormal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n\n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n\n  # Likelihood\n  m$categorical(probs=p[career], obs=career)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Multinomial model</span>"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#mathematical-details",
    "href": "9. Multinomial model.html#mathematical-details",
    "title": "11¬† Multinomial model",
    "section": "11.4 Mathematical Details",
    "text": "11.4 Mathematical Details\n\n11.4.1 Frequentist formulation\nWe model the relationship between the predictor variables (X1, X2, ‚Ä¶, Xn) and the categorical outcome variable (\\(Y_i\\)) using the following equation:\n\\[\nlogit(p_{ik}) = Œ±_k + Œ≤ X_i\n\\]\nWhere:\n\n\\(p_{ik}\\) is the probability of the ùëñ-th observation being in category ùëò.\n\\(Œ±_k\\) is the intercept for category ùëò.\n\\(Œ≤\\) is the regression coefficients common to all categories.\n\\(X_i\\) is the vector of independent variables for the ùëñ-th observation.\nA reference category is often chosen to simplify the model.\n\n\n\n11.4.2 Bayesian model\nIn Bayesian multinomial modeling, the likelihood function of the data is specified using a multinomial distribution. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable ùë¶ with ùêæ categories, the multinomial likelihood function is: \\[\nMultinomial(y|Œ∏)=\\frac{N!}{\\prod^K_{k=1}y_k!} \\prod_{k=1}^{K} Œ∏_{k}^{y_k}\n\\]\nWhere:\n\n\\(y=(y_1, y_2,‚Ä¶,y_K)\\) represents the counts of observations in each of the ùêæ categories.\n\\(N\\) is the total number of observations or trials.\n\\(Œ∏=(Œ∏_1,Œ∏_2,‚Ä¶,Œ∏_K)\\) is a simplex of category probabilities, with \\(Œ∏_k\\) representing the probability of category ùëò.\n\\(\\frac{N!}{\\prod^K_{k=1}y_k!}\\) is the multinomial coefficient that accounts for the number of ways to arrange the observations into the categories. This coefficient ensures that the likelihood function properly accounts for the permutations of the counts across different categories.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Multinomial model</span>"
    ]
  },
  {
    "objectID": "9. Multinomial model.html#references",
    "href": "9. Multinomial model.html#references",
    "title": "11¬† Multinomial model",
    "section": "11.5 Reference(s)",
    "text": "11.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Multinomial model</span>"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html",
    "href": "10. Dirichlet model (wip).html",
    "title": "12¬† Dirichlet Model",
    "section": "",
    "text": "12.1 General Principles\nTo model the relationship between a categorical outcome variable with more than two categories and one or more independent variables with overdispersion üõà, we can use a Dirichlet model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dirichlet Model</span>"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#considerations",
    "href": "10. Dirichlet model (wip).html#considerations",
    "title": "12¬† Dirichlet Model",
    "section": "12.2 Considerations",
    "text": "12.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for the Multinomial model.\nOne major difference from the multinomial model is that the Dirichlet model doesn‚Äôt require a simplex but rather strictly positive values. We can thus exponentiate the outputs from the categorical regressions instead of using the softmax function.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dirichlet Model</span>"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#example",
    "href": "10. Dirichlet model (wip).html#example",
    "title": "12¬† Dirichlet Model",
    "section": "12.3 Example",
    "text": "12.3 Example\nfrom BI import bi\nimport jax.numpy as jnp\nimport pandas as pd\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multinomial.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.halfnormal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = alpha[0] + beta * income[0]\n    p = jax.nn.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dirichletmultinomial(p[career], lambda_, obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.run(model)  \n\n# Summary ------------------------------------------------\nm.summary()",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dirichlet Model</span>"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#mathematical-details",
    "href": "10. Dirichlet model (wip).html#mathematical-details",
    "title": "12¬† Dirichlet Model",
    "section": "12.4 Mathematical Details",
    "text": "12.4 Mathematical Details\n\n12.4.1 Formula\n\n\n12.4.2 Bayesian Model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express the Bayesian regression model accounting for prior distributions as follows:",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dirichlet Model</span>"
    ]
  },
  {
    "objectID": "10. Dirichlet model (wip).html#references",
    "href": "10. Dirichlet model (wip).html#references",
    "title": "12¬† Dirichlet Model",
    "section": "12.5 Reference(s)",
    "text": "12.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dirichlet Model</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html",
    "href": "11. Zero inflated.html",
    "title": "13¬† Zero-Inflated",
    "section": "",
    "text": "13.1 General Principles\nZero-Inflated Regression models are used when the outcome variable is a count variable with an excess of zero counts. These models combine a count model (e.g., Poisson or Negative Binomial) with a separate model for predicting the probability of excess zeros.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Zero-Inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#considerations",
    "href": "11. Zero inflated.html#considerations",
    "title": "13¬† Zero-Inflated",
    "section": "13.2 Considerations",
    "text": "13.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nIn Bayesian Zero-Inflated regression, we consider uncertainty in the model parameters and provide a full posterior distribution over them. We need to declare prior distributions for \\(W_{1\\pi}, W_{2\\pi}, ..., W_{n\\pi}\\), \\(W_{1\\lambda}, W_{2\\lambda}, ..., W_{n\\lambda}\\), \\(b_\\pi\\), and \\(b_\\lambda\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Zero-Inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#example",
    "href": "11. Zero inflated.html#example",
    "title": "13¬† Zero-Inflated",
    "section": "13.3 Example",
    "text": "13.3 Example\nBelow is an example code snippet demonstrating Bayesian Zero-Inflated Poisson regression using the Bayesian Inference (BI) package. The data represent the production of books in a monastery (y), which is affected by the number of days that individuals work, as well as the number of days individuals drink.\n\nPythonR\n\n\nfrom BI import bi\nfrom jax.scipy.special import expit\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Simulated data------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n\n# Sample one year of production\nN = 365\ndrink = m.dist.binomial(1, prob_drink, shape = (N,), sample = True)\ny = (1 - drink) * m.dist.poisson(rate_work, shape = (N,), sample = True)\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\nm.data_on_model = dict(\n    y=jnp.array(y)\n)\n\n# Define model ------------------------------------------------\ndef model(y):\n    al = dist.normal(1, 0.5, name='al')\n    ap = dist.normal(-1.5, 1, name='ap')\n    p = expit(ap)\n    lambda_ = jnp.exp(al)\n    m.zeroinflatedpoisson(p, lambda_, obs=y)\n\n# Run MCMC ------------------------------------------------\nm.run(model)\n\n# Summary ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Simulate data ------------------------------------------------\nprob_drink = 0.2  # 20% of days\nrate_work = 1     # average 1 manuscript per day\n# sample one year of production\nN = as.integer(365)\ndrink = bi.dist.binomial(total_count = as.integer(1), probs = prob_drink, shape = c(N), sample = T ) # An example of sampling a distribution with BI\ny = (1 - drink) *  bi.dist.poisson(rate_work, shape = c(N), sample = T)\ndata = list()\ndata$y = y\nm$data_on_model = data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(y){\n  al = bi.dist.normal(1, 0.5, name='al', shape=c(1))\n  ap = bi.dist.normal(-1, 1, name='ap', shape=c(1))\n  p = jax$scipy$special$expit(ap)\n  lambda_ = jnp$exp(al)\n  m$zeroinflatedpoisson(p, lambda_, obs=y)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Zero-Inflated</span>"
    ]
  },
  {
    "objectID": "11. Zero inflated.html#references",
    "href": "11. Zero inflated.html#references",
    "title": "13¬† Zero-Inflated",
    "section": "14.1 Reference(s)",
    "text": "14.1 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Zero-Inflated</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html",
    "href": "12. Survival analysis.html",
    "title": "14¬† Survival Analysis",
    "section": "",
    "text": "14.1 General Principles\nSurvival analysis studies the time until an event of interest (e.g., death, recovery, information acquisition) occurs. When analyzing binary survival outcomes (e.g., alive or dead), we can use models such as Cox proportional hazards to evaluate the effect of predictors on survival probabilities.\nKey concepts include:",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#general-principles",
    "href": "12. Survival analysis.html#general-principles",
    "title": "14¬† Survival Analysis",
    "section": "",
    "text": "Hazard Function: The instantaneous risk of the event occurring at a given time.\nSurvival Function: The probability of surviving beyond a given time.\nCovariates: Variables (e.g., age, treatment) that may affect survival probabilities.\nBaseline Hazard: The hazard when all covariates are zero, which forms the reference for comparing different conditions.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#considerations",
    "href": "12. Survival analysis.html#considerations",
    "title": "14¬† Survival Analysis",
    "section": "14.2 Considerations",
    "text": "14.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nBayesian models provide a framework to account for uncertainty üõà in parameter estimates through posterior distributions. You will need to define prior distributions üõà for all model parameters, such as baseline hazard, covariate effects, and variance terms.\nIn survival analysis:\n\nThe baseline hazard can follow distributions like Exponential, Weibull, or Gompertz, depending on the data.\nCensoring (when the event is not observed for some subjects) must be accounted for in the likelihood function. Proper handling is essential for unbiased results.\n\nBayesian survival models allow flexible handling of time-dependent covariates, random effects, and incorporate uncertainty more naturally than Frequentist methods.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#example",
    "href": "12. Survival analysis.html#example",
    "title": "14¬† Survival Analysis",
    "section": "14.3 Example",
    "text": "14.3 Example\nHere‚Äôs an example of a Bayesian survival analysis using the Bayesian Inference (BI) package. The data come from a clinical trial of mastectomy for breast cancer. The goal is to estimate the effect of the metastasized covariate, coded as 0 (no metastasis) and 1 (metastasis), on the survival outcome event for each patient. Time is continuous and censoring is indicated by the event variable.\n\nPythonR\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'mastectomy.csv'\nm.data(data_path, sep=',') \n\nm.df.metastasized = (m.df.metastasized == \"yes\").astype(np.int64)\nm.df.event = jnp.array(m.df.event.values, dtype=jnp.int32)\n\n## Create survival object\nm.surv_object(time='time', event='event', cov='metastasized', interval_length=3)\n\n# Plot censoring ------------------------------------------------\nm.plot_censoring(cov='metastasized')\n\n# Model ------------------------------------------------\ndef model(intervals, death, metastasized, exposure):\n    # Parameter prior distributions-------------------------\n    ## Base hazard distribution\n    lambda0 = m.dist.gamma(0.01, 0.01, shape= intervals.shape, name = 'lambda0')\n    ## Covariate effect distribution\n    beta = m.dist.normal(0, 1000, shape = (1,),  name='beta')\n    ### Likelihood\n    #### Compute hazard rate based on covariate effect\n    lambda_ = m.hazard_rate(cov = metastasized, beta = beta, lambda0 = lambda0)\n    #### Compute exposure rates\n    mu = exposure * lambda_\n\n    # Likelihood calculation\n    y = m.poisson(mu + jnp.finfo(mu.dtype).tiny, obs = death)\n\n# Run mcmc ------------------------------------------------\nm.run(model, num_samples=500) \n\n# Summary ------------------------------------------------\nprint(m.summary())\n\n# Plot hazards and survival function ------------------------------------------------\nm.plot_surv()",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#mathematical-details",
    "href": "12. Survival analysis.html#mathematical-details",
    "title": "14¬† Survival Analysis",
    "section": "14.4 Mathematical Details",
    "text": "14.4 Mathematical Details\n\n14.4.1 Frequentist formulation\nThe Cox proportional hazards model can be expressed as: \\[\nh(t | X) = h_0(t) \\exp(\\beta^T X)\n\\]\n\nWhere:\n\n\\(h(t | X)\\) is the hazard at time \\(t\\) for covariates \\(X\\).\n\\(h_0(t)\\) is the baseline hazard function (e.g., exponential, Weibull).\n\\(X\\) represents the covariates (such as age, treatment).\n\\(\\beta\\) are the regression coefficients to be estimated.\n\nCensoring is accounted for by multiplying the hazard function by a factor that depends on the censoring distribution, usually modeled as independent censoring with a rate Œ¥(t):\n\n\\(Y_i(t) = Poisson(h(t | X) * Œ¥(t))\\)\n\n\n\n\n14.4.2 Bayesian formulation\nIn Bayesian survival analysis, we define priors for each parameter:\n\nHazard Function: The hazard rate at time \\(t\\) for an individual is given by: \\[Y_i(t) = Poisson(\\lambda(t) * censoring(t))\\] \\[\\lambda(t) = \\lambda_0(t)\\exp(x\\beta)\\]\n\\[ \\beta \\sim Normal(\\mu_\\beta, \\sigma^2_\\beta) \\]\n\\[\\mu_\\beta \\sim Normal(0, 10^2)\\]\n\\[\\sigma^2_\\beta \\sim Uniform(0, 10)\\]\nWhere:\n\n\\(Y_i(t)\\) is the status of the \\(i\\)-th subject at time \\(t\\) coded as a binary variable: \\[\nY_i(t) =\n\\begin{cases}\n1 & \\text{if subject } i \\text{ died at time $t$  }, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\n\\(\\lambda(t)\\): Hazard function at time \\(t\\).\n\\(\\lambda_0(t)\\): Baseline hazard function (e.g., exponential or Weibull).\n\\(x\\): Covariates (e.g., age, treatment).\n\\(\\beta\\): Regression coefficients capturing the effect of \\(x\\) on the hazard are assigned a normal prior.\n\\(\\mu_\\beta\\): Mean of the normal distribution.\n\\(\\sigma^2_\\beta\\): Variance of the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "12. Survival analysis.html#references",
    "href": "12. Survival analysis.html#references",
    "title": "14¬† Survival Analysis",
    "section": "14.5 Reference(s)",
    "text": "14.5 Reference(s)",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html",
    "href": "13. Varying intercepts.html",
    "title": "15¬† Varying Intercepts",
    "section": "",
    "text": "15.1 General Principles\nTo model the relationship between a dependent variable and an independent variable while allowing for different intercepts across groups or clusters, we can use a Varying Intercepts model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#considerations",
    "href": "13. Varying intercepts.html#considerations",
    "title": "15¬† Varying Intercepts",
    "section": "15.2 Considerations",
    "text": "15.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for Regression for a continuous variable.\nThe main idea of varying intercepts is to generate an intercept for each group, allowing each group to start at different levels. Thus, the intercept \\(\\alpha_k\\) is defined based on the \\(k\\) declared groups.\nEach intercept has its own prior - i.e., a hyper-prior üõà.\nIn the code below, the hyper-prior is a_bar.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#example",
    "href": "13. Varying intercepts.html#example",
    "title": "15¬† Varying Intercepts",
    "section": "15.3 Example",
    "text": "15.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with varying intercepts using the Bayesian Inference (BI) package. The data consists of a dependent variable representing individuals‚Äô survival (surv) and an independent categorical variable (tank), which indicates the tank where the individual was born, with a total of 48 tanks.\n\nPythonR\n\n\nfrom BI import bi\nimport numpy as np\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'reedfrogs.csv'\nm.data(data_path, sep=';') \n# Manipulate\nm.df[\"tank\"] = np.arange(m.df.shape[0]) \n\n# Define model ------------------------------------------------\ndef model(tank, surv, density):\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    a_bar = m.dist.normal( 0., 1.5,  name = 'a_bar')\n    alpha = m.dist.normal( a_bar, sigma, shape= tank.shape, name = 'alpha')\n    p = alpha[tank]\n    m.dist.binomial(total_count = density, logits = p, obs=surv)\n\n# Run sampler ------------------------------------------------\nm.run(model) \n\n# Diagnostic ------------------------------------------------\nm.summary()\n\n\nlibrary(BI)\n\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/reedfrogs.csv\", sep = ''), sep=';')\nm$df$tank = c(0:(nrow(m$df)-1)) # Manipulate\nm$data_to_model(list('tank', 'surv', 'density')) # Manipulate\nm$data_on_model$tank = m$data_on_model$tank$astype(jnp$int32) # Manipulate\nm$data_on_model$surv = m$data_on_model$surv$astype(jnp$int32) # Manipulate\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(tank, surv, density){\n  # Parameter prior distributions\n  sigma = bi.dist.exponential( 1,  name = 'sigma',shape=c(1))\n  a_bar =  bi.dist.normal(0, 1.5, name='a_bar',shape=c(1))\n  alpha = bi.dist.normal(a_bar, sigma, name='alpha', shape =c(48))\n  p = alpha[tank]\n  # Likelihood\n  m$binomial(total_count = density, logits = p, obs=surv)\n} \n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#mathematical-details",
    "href": "13. Varying intercepts.html#mathematical-details",
    "title": "15¬† Varying Intercepts",
    "section": "15.4 Mathematical Details",
    "text": "15.4 Mathematical Details\n\n15.4.1 Frequentist formulation\nWe model the relationship between the independent variable \\(X\\) and the outcome variable \\(Y\\) with varying intercepts \\(\\alpha\\) for each group \\(k\\) using the following equation:\n\\[\nY_{ik} = \\alpha_k + \\beta X_{ik} + \\sigma\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(X_{ik}\\) is the independent variable for observation \\(i\\) in group \\(k\\).\n\\(\\beta\\) is the regression coefficient.\n\\(\\sigma\\) is the error term, typically assumed to be normally distributed and positive.\n\n\n\n15.4.2 Bayesian Model\nWe can express the Bayesian regression model accounting for priors üõà distributions as follows:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik}, \\sigma)\n\\] \\[\n\\mu_{ik} = \\alpha_k + \\beta X_{ik}\n\\] \\[\n\\alpha_k \\sim \\text{Normal}(\\mu_{\\alpha_k}, \\sigma_{\\alpha_k})\n\\] \\[\n\\beta \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma \\sim \\text{Exponential}(1)\n\\] \\[\n\\mu_{\\alpha_k} \\sim \\text{Normal}(0, 1)\n\\] \\[\n\\sigma_{\\alpha_k} \\sim \\text{Exponential}(1)\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(\\mu_{\\alpha_k}\\) is the overall mean intercept.\n\\(\\sigma_{\\alpha_k}\\) is the variance of the intercepts across groups.\n\\(\\beta\\) is the regression coefficient.\n\\(\\sigma\\) is the standard deviation of the error term.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#notes",
    "href": "13. Varying intercepts.html#notes",
    "title": "15¬† Varying Intercepts",
    "section": "15.5 Notes",
    "text": "15.5 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply multiple variables similarly to Chapter 2.\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying intercepts with any distribution developed in previous chapters.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "13. Varying intercepts.html#references",
    "href": "13. Varying intercepts.html#references",
    "title": "15¬† Varying Intercepts",
    "section": "15.6 Reference(s)",
    "text": "15.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Varying Intercepts</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html",
    "href": "14. Varying slopes.html",
    "title": "16¬† Varying slopes",
    "section": "",
    "text": "16.1 General Principles\nTo model the relationship between predictor variables and a dependent variable while allowing for varying effects across groups or clusters, we use a varying slopes model.\nThis approach is useful when we expect the relationship between predictors and the dependent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods). This allows every unit in the data to have its own unique response to any treatment, exposure, or event, while also improving estimates via pooling.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#considerations",
    "href": "14. Varying slopes.html#considerations",
    "title": "16¬† Varying slopes",
    "section": "16.2 Considerations",
    "text": "16.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nWe have the same considerations as for 12. Varying intercepts.\nThe idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a matrix of covariance üõà.\nThe covariance matrix requires a correlation matrix distribution which is modeled using an \\(LKJcorr\\) distribution that holds a parameter \\(Œ∑\\). \\(Œ∑\\) is usually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near ‚àí1 or 1. When we use \\(LKJcorr(1)\\), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.\nThe Half-Cauchy distribution is used when modeling the covariance matrix to specify strictly positive values for the diagonal of the covariance matrix, ensuring positive variances.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#example",
    "href": "14. Varying slopes.html#example",
    "title": "16¬† Varying slopes",
    "section": "16.3 Example",
    "text": "16.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with varying effects:\n\n16.3.1 Simulated data\n\nPythonR\n\n\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim data multivariatenormal.csv'\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(cafe, wait, N_cafes, afternoon):\n    a = m.dist.normal(5, 2,  name = 'a')\n    b = m.dist.normal(-1, 0.5, name = 'b')\n    sigma_cafe = m.dist.exponential(1, shape=(2,),  name = 'sigma_cafe')\n    sigma = m.dist.exponential( 1,  name = 'sigma')\n    Rho = m.dist.lkj(2, 2, name = 'Rho')\n    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho\n    a_cafe_b_cafe = m.dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_b_cafe')    \n\n    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]\n    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n    m.dist.normal(mu, sigma, obs=wait)\n\n# Run sampler ------------------------------------------------\nm.run(model) \n\n\nlibrary(BI)\n\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Sim data multivariatenormal.csv\", sep = ''), sep=',')\nm$data_to_model(list('cafe', 'wait', 'afternoon'))\n\n# Define model ------------------------------------------------\nmodel &lt;- function(cafe, afternoon, wait, N_cafes = as.integer(20) ){\n  a = bi.dist.normal(5, 2, name = 'a')\n  b = bi.dist.normal(-1, 0.5, name = 'b')\n  sigma_cafe = bi.dist.exponential(1, shape= c(2), name = 'sigma_cafe')\n  sigma = bi.dist.exponential( 1, name = 'sigma')\n  Rho = bi.dist.lkj(as.integer(2), as.integer(2), name = 'Rho')\n  cov = jnp$outer(sigma_cafe, sigma_cafe) * Rho\n  \n  a_cafe_b_cafe = bi.dist.multivariatenormal(\n    jnp$squeeze(jnp$stack(list(a, b))), \n    cov, shape = c(N_cafes), name = 'a_cafe')  \n  \n  a_cafe = a_cafe_b_cafe[, 0]\n  b_cafe = a_cafe_b_cafe[, 1]\n  \n  mu = a_cafe[cafe] + b_cafe[cafe] * afternoon\n  \n  m$normal(mu, sigma, obs=wait)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#mathematical-details",
    "href": "14. Varying slopes.html#mathematical-details",
    "title": "16¬† Varying slopes",
    "section": "16.4 Mathematical Details",
    "text": "16.4 Mathematical Details\n\n16.4.1 Formula\nWe model the relationship between the independent variable \\(X\\) and the outcome variable \\(Y\\) with varying intercepts (\\(\\alpha\\)) and varying slopes (\\(\\beta\\)) for each group (\\(k\\)) using the following equation:\n\\[\nY_{ik} = \\alpha_k + \\beta_k X_{ik} + \\sigma\n\\]\nWhere:\n\n\\(Y_{ik}\\) is the outcome variable for observation \\(i\\) in group \\(k\\).\n\\(X_{ik}\\) is the independent variable for observation \\(i\\) in group \\(k\\).\n\\(\\alpha_k\\) is the varying intercept for group \\(k\\).\n\\(\\beta_k\\) is the varying regression coefficient for group \\(k\\).\n\\(\\sigma\\) is the error term, assumed to be strictly positive.\n\n\n\n16.4.2 Bayesian Model\nWe can express the Bayesian regression model accounting for prior distributions as follows:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik} , \\sigma)\n\\] \\[\n\\mu_{ik} = \\alpha_k + \\beta_k X_{ik}\n\\] \\[\n\\alpha_k \\sim Normal(0,1)\n\\] \\[\n\\beta_k \\sim Normal(0,1)\n\\] \\[\n\\sigma \\sim Exponential(1)\n\\]\nThe varying intercepts (\\(\\alpha_k\\)) and slopes (\\(\\beta_k\\)) are modeled using a Multivariate Normal distribution:\n\\[\n\\begin{pmatrix}\n\\alpha_k \\\\\n\\beta_k\n\\end{pmatrix} \\sim \\text{MultivariateNormal}\\left(\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta \\\\\n\\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta & \\sigma_\\beta^2\n\\end{pmatrix}\n\\right)\n\\]\nWhere:\n\n\\(\\left(\\begin{array}{cc} 0 \\\\ 0 \\end{array}\\right)\\) is the prior for the average intercept.\n\\(\\left(\\begin{array}{cc} \\sigma_\\alpha^2 & \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta \\\\ \\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta & \\sigma_\\beta^2 \\end{array}\\right)\\) is the covariance matrix which specifies the variance and covariance of \\(\\alpha_k\\) and \\(\\beta_k\\),\nwhere:\n\n\\(\\sigma_\\alpha^2\\) is the variance of \\(\\alpha_k\\).\n\\(\\sigma_\\beta^2\\) is the variance of \\(\\beta_k\\).\n\\(\\rho_{\\alpha\\beta} \\sigma_\\alpha \\sigma_\\beta\\) is the covariance between \\(\\alpha_k\\) and \\(\\beta_k\\).\n\n\nFor computational reasons, it is often better to implement a non-centered parameterization üõà that is equivalent to the Multivariate Normal distribution approach:\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\beta\n\\end{array}\\right) \\circ\nL \\cdot\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\\right)\n\\]\n\nWhere:\n\n\\(\\sigma_\\alpha \\sim \\text{Exponential}(1)\\) is the prior standard deviation among intercepts.\n\\(\\sigma_\\beta \\sim \\text{Exponential}(1)\\) is the prior standard deviation among slopes.\n\\(L \\sim \\text{LKJcorr}(\\eta)\\) is the prior for the correlation matrix using the Cholesky Factor üõà\n\n\nThe full non-centered version of the model is thus:\n\\[\nY_{ik} \\sim \\text{Normal}(\\mu_{ik} , \\sigma) \\\\\n\\]\n\\[\n\\mu_{ik} =   \\alpha_k + \\beta_k X_{ik} \\\\\n\\]\n\\[\n\\left(\\begin{array}{cc} \\alpha_k \\\\ \\beta_k\\end{array}\\right)\n\\sim\n\\left(\\begin{array}{cc}\n\\sigma_\\alpha\\\\\n\\sigma_\\beta\n\\end{array}\\right) \\circ\nL \\cdot\n\\left(\\begin{array}{cc}\n\\widehat{\\alpha}_k \\\\\n\\widehat{\\beta}_k\n\\end{array}\\right)\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta \\sim Normal(0,1)\n\\] \\[\n\\sigma_\\alpha \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\beta \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "href": "14. Varying slopes.html#multivariate-model-with-one-random-slope-for-each-variable",
    "title": "16¬† Varying slopes",
    "section": "16.5 Multivariate Model with One Random Slope for Each Variable",
    "text": "16.5 Multivariate Model with One Random Slope for Each Variable\nWe can apply a multivariate model similarly to Chapter 2. In this case, we apply the same principle, but with a covariance matrix with a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for \\(i\\) observations in a model with two independent variables \\(X_1\\) and \\(X_2\\), we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma)\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{1i} X_{1i}  + \\beta_{2i} X_{2i}\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{i}\\\\\n\\beta_{1i}\\\\\n\\beta_{2i}\n\\end{pmatrix}\n\\sim \\begin{pmatrix}\n\\sigma_{\\alpha}\\\\\n\\sigma_{\\beta_1}\\\\\n\\sigma_{\\beta_2}\n\\end{pmatrix} \\circ L \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{i} \\\\\n\\widehat{\\beta}_{1i} \\\\\n\\widehat{\\beta}_{2i}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_1} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2} \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "href": "14. Varying slopes.html#multivariate-random-slopes-on-a-single-variable",
    "title": "16¬† Varying slopes",
    "section": "16.6 Multivariate Random Slopes on a Single Variable",
    "text": "16.6 Multivariate Random Slopes on a Single Variable\nFor more than two varying effects, we apply the same principle but with a covariance matrix for each varying effect that is summed to generate the varying intercept and slope. For example, if we want to generate random slopes for \\(i\\) actors and \\(k\\) groups, we can define the formula as follows:\n\\[\np(Y_{i} |\\mu_i , \\sigma) \\sim \\text{Normal}(\\mu_i , \\sigma) \\\\\n\\]\n\\[\n\\mu_i =   \\alpha_i + \\beta_{i} X_i\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{i} = \\beta + \\beta_{actor[i]} + \\beta_{group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\beta \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{\\text{actor}}\n\\end{pmatrix}\n\\sim\n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\beta a}\n\\end{pmatrix} \\circ L_a \\cdot \\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\beta}_{ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{\\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\beta g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\beta}_{kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#notes",
    "href": "14. Varying slopes.html#notes",
    "title": "16¬† Varying slopes",
    "section": "16.7 Notes",
    "text": "16.7 Notes\n\n\n\n\n\n\nNote\n\n\n\n\nWe can apply interaction terms similarly to Chapter 3.\nWe can apply categorical variables similarly to Chapter 4.\nWe can apply varying slopes with any distribution presented in previous chapters. Below is the formula and the code snippet for a Binomial multivariate model with an interaction between two independent variables \\(X_1\\) and \\(X_2\\) and multiple varying effects for each actor and each group.\n\n\\[\np(Y_{i} |n , p_i) \\sim \\text{Binomial}(n = 1, p_i) \\\\\n\\]\n\\[\n\\text{logit}(p_i)=   \\alpha_i + \\beta_{1i}X_{1i}  + \\beta_{2i} X_{1i}X_{2i}\n\\] \\[\n\\alpha_i = \\alpha + \\alpha_{actor[i]} + \\alpha_{group[i]}\n\\] \\[\n\\beta_{1i} = \\beta_1 + \\beta_{1, actor[i]} + \\beta_{1, group[i]}\n\\] \\[\n\\beta_{2i} = \\beta_2 + \\beta_{2, actor[i]} + \\beta_{2, group[i]}\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\] \\[\n\\beta_1 \\sim Normal(0,1)\n\\] \\[\n\\beta_2 \\sim Normal(0,1)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{actor}} \\\\\n\\beta_{1, \\text{actor}} \\\\\n\\beta_{2, \\text{actor}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha a} \\\\\n\\sigma_{\\beta_1 a} \\\\\n\\sigma_{\\beta_2 a}\n\\end{pmatrix} \\circ L_a \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{ka} \\\\\n\\widehat{\\beta}_{1,ka} \\\\\n\\widehat{\\beta}_{2,ka}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha a} \\sim Exponential(1)\n\\]\n\\[\n\\sigma_{\\beta_1 a} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2 a} \\sim Exponential(1)\n\\] \\[\nL_{a} \\sim LKJcorr(2)\n\\]\n\\[\n\\begin{pmatrix}\n\\alpha_{\\text{group}} \\\\\n\\beta_{1, \\text{group}} \\\\\n\\beta_{2, \\text{group}}\n\\end{pmatrix}\n\\sim  \n\\begin{pmatrix}\n\\sigma_{\\alpha g} \\\\\n\\sigma_{\\beta_1 g} \\\\\n\\sigma_{\\beta_2 g}\n\\end{pmatrix} \\circ L_g \\cdot\n\\begin{pmatrix}\n\\widehat{\\alpha}_{kg} \\\\\n\\widehat{\\beta}_{1,kg} \\\\\n\\widehat{\\beta}_{2,kg}\n\\end{pmatrix}\n\\]\n\\[\n\\sigma_{\\alpha g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_1 g} \\sim Exponential(1)\n\\] \\[\n\\sigma_{\\beta_2 g} \\sim Exponential(1)\n\\] \\[\nL_{g} \\sim LKJcorr(2)\n\\]\nfrom main import *\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Import data\nm.read_csv(\"../data/chimpanzees.csv\", sep=\";\")\nm.df[\"block_id\"] = m.df.block\nm.df[\"treatment\"] = 1 + m.df.prosoc_left + 2 * m.df.condition\nm.data_to_model(['pulled_left', 'treatment', 'actor', 'block_id'])\n\n\ndef model(tid, actor, block_id, L=None, link=False):\n    # fixed priors\n    g = dist.normal(0, 1, name = 'g', shape = (4,))\n    sigma_actor = dist.exponential(1, name = 'sigma_actor', shape = (4,))\n    L_Rho_actor = dist.lkjcholesky(4, 2, name = \"L_Rho_actor\")\n    sigma_block = dist.exponential(1, name = \"sigma_block\", shape = (4,))\n    L_Rho_block = dist.lkjcholesky(4, 2, name = \"L_Rho_block\")\n\n    # adaptive priors - non-centered\n    z_actor = dist.normal(0, 1, name = \"z_actor\", shape = (4,7))\n    z_block = dist.normal(0, 1, name = \"z_block\", shape = (4,3))\n    alpha = deterministic(\n        \"alpha\", ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T\n    )\n    beta = deterministic(\n        \"beta\", ((sigma_block[..., None] * L_Rho_block) @ z_block).T\n    )\n\n    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]\n    dist(\"L\", dist.Binomial(logits=logit_p), obs=L)\n\n    # compute ordinary correlation matrices from Cholesky factors\n    if link:\n        deterministic(\"Rho_actor\", L_Rho_actor @ L_Rho_actor.T)\n        deterministic(\"Rho_block\", L_Rho_block @ L_Rho_block.T)\n        deterministic(\"p\", expit(logit_p))\n\n# Run mcmc ------------------------------------------------\nm.run(model) \n\n# Summary ------------------------------------------------\nm.sampler.print_summary(0.89)",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "14. Varying slopes.html#references",
    "href": "14. Varying slopes.html#references",
    "title": "16¬† Varying slopes",
    "section": "16.8 Reference(s)",
    "text": "16.8 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Varying slopes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html",
    "href": "15. Gaussian processes.html",
    "title": "17¬† Gaussian Processes",
    "section": "",
    "text": "17.1 General Principles\nThrough varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. Basically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a kernel function üõà.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#considerations",
    "href": "15. Gaussian processes.html#considerations",
    "title": "17¬† Gaussian Processes",
    "section": "17.2 Considerations",
    "text": "17.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nTo capture complex, non-linear relationships in data where the underlying function is smooth but has an unknown functional form, GPs use a kernel üõà.\nGPs assume normally distributed errors and may not be appropriate for all types of noise.\nThe choice of kernel hyperparameters can significantly impact results; thus, GPs require choosing an appropriate kernel function that captures the expected behavior of your data.\nThrough kernel definition, we can incorporate domain knowledge.\nThey scale poorly with dataset size (O(n¬≥) complexity) due to matrix operations; thus, memory requirements can be substantial for large datasets, which has led to neural networks being used instead to resolve large non-linear problems.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#example",
    "href": "15. Gaussian processes.html#example",
    "title": "17¬† Gaussian Processes",
    "section": "17.3 Example",
    "text": "17.3 Example\nBelow is an example code snippet demonstrating Gaussian Process regression using the Bayesian Inference (BI) package. Data consist of a continuous dependent variable (total_tools), representing the number of tools invented in the islands, and a continuous independent variable (population), representing the population of the islands. The goal is to estimate the effect of population on the total tools. We use the distance matrix of the islands for the kernel function in order to capture the spatial dependence of the relationship.\n\nPythonR\n\n\nfrom BI import bi\nimport jax.numpy as jnp\nimport numpyro\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Kline2.csv'\nm.data(data_path, sep=';') \n\ndata_path2 = files('BI.resources.data') / 'Kline2.csv'\nislandsDistMatrix = pd.read_csv(data_path2, index_col=0)\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    # non-centered Gaussian Process prior\n    etasq = m.dist.exponential(2, name = 'etasq')\n    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n    SIGMA = cov_GPL2(Dmat, etasq, rhosq, 0.01)\n    k = m.dist.multivariatenormal(0, SIGMA, name = 'k')\n    #k = m.gaussian.gaussian_process(Dmat, etasq, rhosq, 0.01, shape = (10,))\n    k = m.gaussian.kernel_L2(Dmat, etasq, rhosq, 0.01)\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.run(model) \nm.summary()\n\n\nlibrary(BI)\npd=import('pandas')\n# setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BI\"),\"/data/Kline2.csv\", sep = ''), sep=';')\nislandsDistMatrix = pd$read_csv(paste(system.file(package = \"BI\"),\"/data/islandsDistMatrix.csv\", sep = ''), index_col=as.integer(0))\nm$data_to_model(list('total_tools', 'population'))\nm$data_on_model$society = jnp$arange(0,10, dtype='int64')\nm$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n\n\n# Define model ------------------------------------------------\nmodel &lt;- function(Dmat, population, society, total_tools){\n  a = bi.dist.exponential(1, name = 'a')\n  b = bi.dist.exponential(1, name = 'b')\n  g = bi.dist.exponential(1, name = 'g')\n  \n  # non-centered Gaussian Process prior\n  etasq = bi.dist.exponential(2, name = 'etasq')\n  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n  z = bi.dist.normal(0,1, name = 'z', shape = c(10))\n  r = m$kernel_sq_exp(Dmat, z, etasq, rhosq, 0.01)\n  SIGMA = r[[1]]\n  L_SIGMA = r[[2]]\n  k = r[[3]]\n  lambda_ = a * population**b / g * jnp$exp(k[society])\n  m$poisson(lambda_, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#mathematical-details",
    "href": "15. Gaussian processes.html#mathematical-details",
    "title": "17¬† Gaussian Processes",
    "section": "17.4 Mathematical Details",
    "text": "17.4 Mathematical Details\n\n17.4.1 Formula\nThe following equation allows us to evaluate the relationship between the dependent variable \\(Y\\) and the independent variable \\(X\\) while incorporating a GP for variable \\(Z\\):\n\\[\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\\]\nwhere: - \\(Y_i\\) is the i-th value for the dependent variable \\(Y\\).\n\n\\(\\alpha\\) is the intercept term.\n\\(\\beta\\) is the regression coefficient term.\n\\(X_i\\) is the i-th value for the independent variable \\(X\\).\n\\(\\gamma_{Z_i}\\) is the Gaussian process i-th value for the independent variable \\(Z\\).\n\nThe GP \\(\\gamma_{Z_i}\\) follows a multivariate normal distribution:\n\\[\n\\begin{pmatrix}\n    Z_1 \\\\\n    \\vdots \\\\\n    Z_{n}\n\\end{pmatrix}\n\\sim MVNormal \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\\]\nwhere:\n\n\\((Z_1, ..., Z_n)\\) represents a collection of all values of the random variable \\(Z\\).\n\\((0, ..., 0)\\) represents the mean vector of the multivariate normal distribution of the same size as the number of random variables and set to zero üõà.\n\\(K\\) is the covariance matrix of the random variable \\(Z\\). Each element \\(K_{ij}\\) of the matrix is given by the kernel function evaluated at the corresponding points: \\(K_{ij} = k(Z_i, Z_j)\\)\n\n\\[\nK = \\begin{pmatrix}\n    k(Z_1, Z_1) & k(Z_1, Z_2) & \\cdots & k(Z_1, Z_{n}) \\\\\n    k(Z_2, Z_1) & k(Z_2, Z_2) & \\cdots & k(Z_2, Z_{n}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    k(Z_{n}, Z_1) & k(Z_{n}, Z_2) & \\cdots & k(Z_{n}, Z_{n})\n\\end{pmatrix}\n\\]\n\nMultiple kernel functions exist and will be discussed in the Note(s) section. But the most common one is the quadratic kernel:\n\n\\[\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\\]\nWhere:\n\n\\(\\eta\\) is the signal variance, representing the overall variance of the outputs of the Gaussian process. It scales the influence of the kernel function. A larger \\(\\eta^2\\) indicates a wider range of values the function can take.\n\\(p\\) determines the rate of decline.\n\\(D_{ij}\\) is the distance between the \\(i\\)-th and \\(j\\)-th points.\n\\(\\delta_{ij}\\) is the Kronecker delta, taking a value of one when \\(i = j\\) and zero otherwise, allowing the self-covariance to be included in the calculation.\n\\(\\sigma^2\\) is the noise variance, which accounts for the observation noise in the data. It represents the uncertainty or variability in the measurements or outputs at each point. The term effectively adds this noise variance only when \\(i = j\\), ensuring that the diagonal elements of the covariance matrix represent the total variance at each input point.\n\n\n\n17.4.2 Bayesian model\nIn the Bayesian formulation, we define each parameter with priors üõà. We can express a Bayesian version of this GP using the following model:\n\\[\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n\\]\n\\[\n\\gamma \\sim MVNormal \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n\\]\n\\[\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2\n\\]\n\\[\n\\alpha \\sim Normal(0,1)\n\\]\n\\[\n\\eta^2 \\sim HalfCauchy(0,1)\n\\]\n\\[\np^2 \\sim HalfCauchy(0,1)\n\\]\nwhere:\n\n\\(Y_i\\) is the i-th value for the dependent variable \\(Y\\).\n\\(\\alpha\\) is the intercept term with a prior of \\(Normal(0,1)\\).\n\\(\\beta\\) is the regression coefficient term with a prior of \\(Normal(0,1)\\).\n\\(X_i\\) is the i-th value for the independent variable \\(X\\).\n\\(\\gamma_{Z_i}\\) is the Gaussian process i-th value for the independent variable \\(Z\\).\n\\(\\gamma\\) is the latent function modeled by the GP.\n\\(K_{ij}\\) is the kernel function evaluated at the corresponding points, \\(K_{ij} = k(Z_i, Z_j)\\), with priors of HalfCauchy(0,1) for \\(\\eta^2\\) and \\(p^2\\) to ensure positive values.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#notes",
    "href": "15. Gaussian processes.html#notes",
    "title": "17¬† Gaussian Processes",
    "section": "17.5 Notes",
    "text": "17.5 Notes\n\n\n\n\n\n\nNote\n\n\n\nCommon kernel functions include:\n\nRadial Basis Function (RBF) or Squared Exponential Kernel: \\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\\]\nRational Quadratic Kernel, this kernel is equivalent to adding together many RBF kernels with different length scales: \\[k(x,x') = \\sigma^2 \\left(1 + \\frac{||x-x'||^2}{2l^2}\\right)^{-\\alpha}\\]\nPeriodic kernel allows for modeling functions that repeat themselves exactly: \\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)\\]\nLocally Periodic Kernel:\n\n\\[k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right) \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)\\]\n\nGPs can be extended to classification problems using link functions.\nMulti-output problems can be addressed using matrix-valued kernels.\nDeep learning can be combined with GPs through Deep Kernel Learning.\nComputational tricks for large datasets include:\n\nSparse approximations (e.g., FITC, VFE)\nInducing points methods\nRandom Fourier features",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "15. Gaussian processes.html#references",
    "href": "15. Gaussian processes.html#references",
    "title": "17¬† Gaussian Processes",
    "section": "17.6 Reference(s)",
    "text": "17.6 Reference(s)\nMcElreath (2018)\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "16. Measuring error.html",
    "href": "16. Measuring error.html",
    "title": "18¬† Measuring error",
    "section": "",
    "text": "18.1 General Principles\nMeasurement error refers to the variability in the measurement of a variable, and measurement error can be generated by several factors, such as sampling bias, censoring bias, and group size heterogeneity. It is an important consideration in many fields, including statistics, economics, and engineering, where accurate measurements are crucial for making informed decisions. To account for measurement error, we can use a measurement error model. This model assumes that the measurement of a variable is subject to an error, which can be modeled using a probability distribution. The model can be used to estimate the parameters of the measurement error distribution, such as the mean and variance, and to make predictions about the measurements based on the estimated parameters. Measurement error models are composed models (i.e., models with sub-models) that evaluate different generative processes, starting with the measurement error process, which is then used to generate the observed data.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Measuring error</span>"
    ]
  },
  {
    "objectID": "16. Measuring error.html#example",
    "href": "16. Measuring error.html#example",
    "title": "18¬† Measuring error",
    "section": "18.2 Example",
    "text": "18.2 Example\nBelow is an example code snippet demonstrating a Bayesian measurement error model using the Bayesian Inference (BI) package. The data consist of three continuous variables (marriage rate, divorce rate, age), and the goal is to estimate the effect of age and marriage rate on the divorce rate while considering that the divorce rate has a measurement error.\n\nPython\n\n\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'WaffleDivorce.csv'\nm.data(data_path, sep=';') \nm.scale(['Divorce', 'Divorce SE', 'MedianAgeMarriage']) # Scale\ndat = dict(\n    D_obs = jnp.array(m.df['Divorce'].values),   \n    D_sd = jnp.array(m.df['Divorce'].values), \n    A = jnp.array(m.df['MedianAgeMarriage'].values), \n    N = m.df.shape[0]   \n)\nm.data_on_model = dat # Send to model (convert to jax array)\n\n\n# Define model ------------------------------------------------\ndef model(D_obs, D_sd, A, N):  \n    a = m.dist.normal(0, 0.2, name = 'a') \n    bA = m.dist.normal(0, 0.5, name = 'bA') \n    s = m.dist.exponential(1, name = 's') \n    mu = a + bA * A + bM * M\n    D_true = m.dist.normal(mu, s, name = 'D_true') \n    m.normal(D_true , D_sd, obs = D_obs) \n\n# Run MCMC ------------------------------------------------\nm.run(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Measuring error</span>"
    ]
  },
  {
    "objectID": "16. Measuring error.html#mathematical-details",
    "href": "16. Measuring error.html#mathematical-details",
    "title": "18¬† Measuring error",
    "section": "18.3 Mathematical Details",
    "text": "18.3 Mathematical Details\n\n18.3.1 Bayesian formulation\n\\[\nD_i^* \\sim Normal(D_i, \\sigma_i)\n\\]\n\\[\nD_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\alpha + \\beta_A A_i + \\beta_M M_i\n\\]\n\\[\n\\sigma \\sim Normal(1)\n\\]\nwhere:\n\n\\(D_i^*\\) is the observed divorce rate.\n\\(D_i\\) is the true divorce rate.\n\\(\\mu_i\\) is the mean of the true divorce rate.\n\\(\\sigma\\) is the standard deviation of the true divorce rate.\n\\(\\alpha\\) is the intercept term.\n\\(\\beta_A\\) is the regression coefficient for age.\n\\(\\beta_M\\) is the regression coefficient for marriage rate.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Measuring error</span>"
    ]
  },
  {
    "objectID": "16. Measuring error.html#notes",
    "href": "16. Measuring error.html#notes",
    "title": "18¬† Measuring error",
    "section": "18.4 Notes",
    "text": "18.4 Notes\n\n\n\n\n\n\nNote\n\n\n\nThis is an approach that can be extended to any kind of model previously described. For example, one could generate a Bernoulli measurement error model by generating a process for the probabilities of success and failure. We can even go further by potentially having an error rate that is present only in one of the two outcomes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Measuring error</span>"
    ]
  },
  {
    "objectID": "16. Measuring error.html#references",
    "href": "16. Measuring error.html#references",
    "title": "18¬† Measuring error",
    "section": "18.5 Reference(s)",
    "text": "18.5 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Measuring error</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html",
    "href": "17. Missing data (wip).html",
    "title": "19¬† Missing data",
    "section": "",
    "text": "19.1 General Principles",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#considerations",
    "href": "17. Missing data (wip).html#considerations",
    "title": "19¬† Missing data",
    "section": "19.2 Considerations",
    "text": "19.2 Considerations\n\n\n\n\n\n\nCaution",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#example",
    "href": "17. Missing data (wip).html#example",
    "title": "19¬† Missing data",
    "section": "19.3 Example",
    "text": "19.3 Example\nBelow is an example code snippet demonstrating Bayesian Missing data model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#mathematical-details",
    "href": "17. Missing data (wip).html#mathematical-details",
    "title": "19¬† Missing data",
    "section": "19.4 Mathematical Details",
    "text": "19.4 Mathematical Details\n\n19.4.1 Frequentist formulation\n\n\n19.4.2 Bayesian formulation",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#notes",
    "href": "17. Missing data (wip).html#notes",
    "title": "19¬† Missing data",
    "section": "19.5 Notes",
    "text": "19.5 Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "17. Missing data (wip).html#references",
    "href": "17. Missing data (wip).html#references",
    "title": "19¬† Missing data",
    "section": "19.6 Reference(s)",
    "text": "19.6 Reference(s)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html",
    "href": "18. Latent variable (wip).html",
    "title": "20¬† Latent Variables",
    "section": "",
    "text": "20.1 General Principles\nIn some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, latent variables‚Äîvariables that are not directly observed but are inferred from the data‚Äîcan help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (X) and the outcome (Y).\nWe model the relationship between the predictor variables (X) and the outcome variable (Y) with a latent variable (Z) as follows:\n\\[\nY = f(X, Z) + \\epsilon\n\\]\nWhere: - Y is the observed outcome variable. - X is the observed predictor variable(s). - Z is the latent (unobserved) variable, which we aim to infer. - f(X, Z) is the function that relates X and Z to Y. -  is the error term, typically assumed to be normally distributed with mean 0 and variance ^2.\nThe latent variable Z can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#considerations",
    "href": "18. Latent variable (wip).html#considerations",
    "title": "20¬† Latent Variables",
    "section": "20.2 Considerations",
    "text": "20.2 Considerations\nIn Bayesian regression with latent variables, we consider the uncertainty in both the observed and latent variables. We declare prior distributions for the latent variables, in addition to the usual priors for regression coefficients and intercepts. These latent variables are often modeled using Gaussian distributions (Normal priors) or more flexible distributions such as Multivariate Normal for correlations among the latent variables.\nThe goal is to infer the posterior distribution over both the parameters and the latent variables, given the observed data.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#example",
    "href": "18. Latent variable (wip).html#example",
    "title": "20¬† Latent Variables",
    "section": "20.3 Example",
    "text": "20.3 Example\nBelow is an example code snippet demonstrating Bayesian regression with latent variables using TensorFlow Probability:\nfrom BI import bi\nimport numpy as np\nimport jax.numpy as jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Data Simulation ------------------------------------------------\nNY = 4  # Number of dependent variables or outcomes (e.g., dimensions for latent variables)\nNV = 8  # Number of observations or individual-level data points (e.g., subjects)\n\n# Initialize the matrix Y2 with shape (NV, NY) filled with NaN values, to be filled later\nY2 = np.full((NV, NY), np.nan)\n\n# Generate the means and offsets for the data\n# means: Generate random normal means for each of the NY outcomes\n# offsets: Generate random normal offsets for each of the NV observations\nmeans = m.dist.normal(0, 1, shape=(NY,), sample=True, seed=10)\noffsets = m.dist.normal(0, 1, shape=(NV, 1), sample=True, seed=20)\n\n# Fill the matrix Y2 with simulated data based on the generated means and offsets\n# Each observation (i) is the sum of an individual-specific offset and an outcome-specific mean\nfor i in range(NV):\n    for k in range(NY):\n        Y2[i, k] = means[k] + offsets[i]\n\n# Simulate individual-level random effects (e.g., random slopes or intercepts)\n# b_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates\nb_individual = BI.distribution.normal(0, 1, shape=(N, K), sample=True, seed=0)\n\n# mu: Add an additional effect 'a' to the individual-level random effects 'b_individual'\n# 'a' could represent a population-level effect or a baseline\nmu = b_individual + a\n\n# Convert Y2 to a JAX array for further computation in a JAX-based framework\nY2 = jnp.array(Y2)\n\n\n# Set data ------------------------------------------------\ndat = dict(\n    NY = NY,\n    NV = NV,\n    Y2 = Y2\n)\nm.data_on_model = dat\n\n# Define model ------------------------------------------------\ndef model(NY, NV, Y2):\n    means = m.dist.normal(0, 1, shape=(NY,), name='means')\n    offset = m.dist.normal(0, 1, shape=(NV, 1), name='offset')\n    sigma = m.dist.exponential(1, shape=(NY,), name='sigma')\n    tmp = jnp.tile(means, (NV, 1)).reshape(NV, NY)\n    mu_l = tmp + offset\n    m.normal(mu_l, jnp.tile(sigma, [NV, 1]), obs=Y2)\n\n# Run sampler ------------------------------------------------\nm.run(model)\n\n# Summary ------------------------------------------------\nm.summary()",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#mathematical-details",
    "href": "18. Latent variable (wip).html#mathematical-details",
    "title": "20¬† Latent Variables",
    "section": "20.4 Mathematical Details",
    "text": "20.4 Mathematical Details\nWe can express the Bayesian latent variable model using probability distributions as follows:\n\\[\n\\begin{aligned}\n& p(Y | X, Z, W, \\sigma) = \\text{Normal}(X \\cdot W + Z, \\sigma^2) \\\\\n& p(Z) = \\text{Normal}(0, \\tau^2) \\\\\n& p(W) = \\text{Normal}(0, \\alpha^2) \\\\\n\\end{aligned}\n\\]\nWhere: - p(Y | X, Z, W, ) is the likelihood function for the observed outcome variable, which depends on both the observed predictor X and the latent variable Z. - p(Z) is the prior distribution for the latent variable Z, often modeled as Normal with a mean of 0 and variance ^2. - p(W) is the prior distribution for the regression coefficient(s) W, typically assumed to follow a Normal distribution with mean 0 and variance ^2.\nThe latent variable Z introduces additional flexibility to the model, capturing unobserved influences on the outcome Y.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "href": "18. Latent variable (wip).html#interpretation-of-latent-variables",
    "title": "20¬† Latent Variables",
    "section": "20.5 Interpretation of Latent Variables",
    "text": "20.5 Interpretation of Latent Variables\n\nLatent Variable (Z): Represents hidden factors not captured by the observed variables, allowing the model to explain more of the variance in the outcome. For instance, in a psychological model, Z might represent a latent trait such as intelligence or anxiety that influences the outcome.\nPosterior Inference: The posterior distribution of the latent variable Z can give insights into how much the unobserved factors contribute to the outcome.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "18. Latent variable (wip).html#use-cases",
    "href": "18. Latent variable (wip).html#use-cases",
    "title": "20¬† Latent Variables",
    "section": "20.6 Use Cases",
    "text": "20.6 Use Cases\n\nLatent Factors in Psychometrics: In psychometric models, latent variables represent traits or abilities that are not directly observed, such as cognitive ability or personality traits.\nTime-Varying Effects: Latent variables can represent unobserved time trends or individual-specific effects in time-series or longitudinal models.\nMixed Models: In hierarchical or mixed models, latent variables can represent group-specific intercepts or slopes.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Latent Variables</span>"
    ]
  },
  {
    "objectID": "19. PCA.html",
    "href": "19. PCA.html",
    "title": "21¬† Principal Component Analysis",
    "section": "",
    "text": "21.1 General Principles\nPrincipal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto the first coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\nIn Bayesian PCA, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#general-principles",
    "href": "19. PCA.html#general-principles",
    "title": "21¬† Principal Component Analysis",
    "section": "",
    "text": "21.1.1 Goal:\n\nReduce dimensionality while retaining as much variance as possible.\nInfer posterior distributions over the principal components, instead of point estimates, by incorporating prior distributions over the parameters.\n\n\n\n21.1.2 Use Cases\n\nDimensionality Reduction: Bayesian PCA is commonly used to reduce the dimensionality of high-dimensional datasets while incorporating uncertainty about the latent structure.\nData Visualization: By projecting data into a lower-dimensional space, PCA helps in visualizing high-dimensional datasets in 2D or 3D plots.\nNoise Modeling: Bayesian PCA provides an advantage over classical PCA by explicitly modeling noise and accounting for uncertainty in the data.\nFeature Extraction: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.\nLatent Variable Modeling: The latent variables learned by Bayesian PCA can serve as new features for downstream tasks, such as classification or clustering.",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#considerations",
    "href": "19. PCA.html#considerations",
    "title": "21¬† Principal Component Analysis",
    "section": "21.2 Considerations",
    "text": "21.2 Considerations\nIn Bayesian PCA, we assume prior distributions for the latent variables Z and the principal component loadings W. We place Gaussian priors on both Z and W and learn their posterior distributions using the observed data X.\nThis approach differs from traditional PCA by allowing the posterior distributions to reflect uncertainty in the model parameters.",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#example",
    "href": "19. PCA.html#example",
    "title": "21¬† Principal Component Analysis",
    "section": "21.3 Example",
    "text": "21.3 Example\nHere is an example code snippet demonstrating Bayesian PCA using TensorFlow Probability:\n\nfrom main import *\nimport seaborn as sns\n\nm = bi(platform='cpu')\n\n# Data simulation -------------------------------------------\n\nplt.style.use(\"ggplot\")\nwarnings.filterwarnings('ignore')\n\nnum_datapoints = 5000\ndata_dim = 2\nlatent_dim = 1\nstddv_datapoints = 0.5\n\n# Simulate data\ndef sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): \n    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w', sample=True, seed=seed)\n    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z', sample=True, seed=seed)\n    x = bi.dist.normal(w @ z, stddv_datapoints, name='x', sample=True, seed=seed)\n    return w, z, x\n\nactual_w, actual_z, x_train =sim_data(data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 20)\nplt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1)\nplt.axis([-20, 20, -20, 20])\nplt.title(\"Dataset\")\nplt.show()\n\n\n# Model using simulated data\ndef model(x_train, data_dim, latent_dim, num_datapoints, stddv_datapoints, seed = 0): \n    w = bi.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')\n    z = bi.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')\n    lk('Y', Normal(w @ z, stddv_datapoints), obs = x_train)  \n    \nm.data_on_model = dict(\n    x_train = x_train, \n    data_dim = data_dim, \n    latent_dim = latent_dim, \n    num_datapoints = num_datapoints, \n    stddv_datapoints = stddv_datapoints\n)\n\nm.run(model) \nsummary = m.summary()\nreal_data = jnp.concatenate([actual_w.flatten(), actual_z.flatten()]) # concatenate the actual values of w and z\nposteriors = summary.iloc[:,0]\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(real_data, posteriors, marker='o', linestyle='None', color='b', label='Posteriors')",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#mathematical-details",
    "href": "19. PCA.html#mathematical-details",
    "title": "21¬† Principal Component Analysis",
    "section": "21.4 Mathematical Details",
    "text": "21.4 Mathematical Details\n\n21.4.1 Formulation\nGiven an observed data matrix \\(X \\in \\mathbb{R}^{N \\times D}\\) (where N is the number of samples and D is the number of dimensions), we assume the data is generated by a lower-dimensional latent variable model:\n\\[\nX = ZW^T + \\epsilon\n\\]\n\\[\nZ \\sim \\mathcal{N}(0, I)\n\\]\n\\[\nW \\sim \\mathcal{N}(0, I)\n\\]\n\\[\n\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\n\\]\nWhere:\n\n\\(X\\) is the observed data matrix.\n\\(Z \\in \\mathbb{R}^{N \\times K}\\) is the latent variable matrix (latent features with \\(K \\ll D\\)). \\(Z\\) is defined by a Normal distribution with mean 0 and variance 1.\n\\(W \\in \\mathbb{R}^{D \\times K}\\) is the matrix of principal components (projection matrix). \\(W\\) is defined by a Normal distribution with mean 0 and variance 1.\n\\(\\epsilon\\) is Gaussian noise, assumed to be normally distributed: \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\).",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#note",
    "href": "19. PCA.html#note",
    "title": "21¬† Principal Component Analysis",
    "section": "21.5 Note",
    "text": "21.5 Note\n\nTo account for sign ambiguity üõà in PCA, we can align the signs of the estimated parameters with the true parameters before comparison. To do this, calculate the dot product between the true and estimated parameters. If it is negative, multiply the estimated parameters by -1 to align them with the true parameters. Below, a code snippet highlights how to do this:\n\n\ntrue_params = jnp.array(real_data)      \nestimated_params = jnp.array(posteriors) \n\n# Compute dot product\ndot_product = jnp.dot(true_params, estimated_params)\n\n# Align signs if necessary\nif dot_product &lt; 0:\n    estimated_params = -estimated_params\n\n# Plot the aligned parameters\nplt.scatter(true_params, estimated_params, alpha=0.7)\nplt.plot([min(true_params), max(true_params)], [min(true_params), max(true_params)], 'r--')\nplt.xlabel('True Parameters')\nplt.ylabel('Estimated Parameters')\nplt.title('True vs. Estimated Parameters After Sign Alignment')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "19. PCA.html#references",
    "href": "19. PCA.html#references",
    "title": "21¬† Principal Component Analysis",
    "section": "21.6 Reference(s)",
    "text": "21.6 Reference(s)\nhttps://www.tensorflow.org/probability/examples/Probabilistic_PCA",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "20. Network model.html",
    "href": "20. Network model.html",
    "title": "22¬† Modeling Network",
    "section": "",
    "text": "22.1 Considerations\nA network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random intercepts and slopes. By adding such parameters to the model, we can account for the correlations between node link relationships.",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#considerations",
    "href": "20. Network model.html#considerations",
    "title": "22¬† Modeling Network",
    "section": "",
    "text": "Caution\n\n\n\n\nThe particularity here is that varying intercepts and slopes are generated for both nodal effects üõà and dyadic effects üõà. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#example",
    "href": "20. Network model.html#example",
    "title": "22¬† Modeling Network",
    "section": "22.2 Example",
    "text": "22.2 Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect:",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#python",
    "href": "20. Network model.html#python",
    "title": "22¬† Modeling Network",
    "section": "22.3 Python",
    "text": "22.3 Python\nfrom BI import bi\n\n# Setup device------------------------------------------------\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nm.run(model2) \nsummary = m.summary()\nsummary",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#r",
    "href": "20. Network model.html#r",
    "title": "22¬† Modeling Network",
    "section": "22.4 R",
    "text": "22.4 R\nlibrary(BI)\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\n\nload(paste(system.file(package = \"BI\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nids = 0:(model_dat$N_id-1)\nidx = m$net$vec_node_to_edgle(jnp$stack(jnp$array(list(ids, ids)), axis = -as.integer(1)))\n\nkeys &lt;- c(\"idx\",\n          'idxShape',\n          \"result_outcomes\",\n          'focal_individual_predictors',\n          'target_individual_predictors')\n\nvalues &lt;- list(\n  idx,\n  idx$shape[[1]],\n  m$net$mat_to_edgl(model_dat$outcomes[,,1]),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50)),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50))\n)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(idx, idxShape, result_outcomes,focal_individual_predictors, target_individual_predictors){\n  N_id = 50\n  x=0.1/jnp$sqrt(N_id)\n  tmp=jnp$log(x / (1 - x))\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(tmp, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(idxShape))\n\n  ## SR ---------------------------------------                                                      \n  m$poisson(jnp$exp(B + sr + dr), obs=result_outcomes)  \n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\nsummary[rownames(summary) %in% c('focal_effects[0]', 'target_effects[0]', 'block[0]'),]",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#mathematical-details",
    "href": "20. Network model.html#mathematical-details",
    "title": "22¬† Modeling Network",
    "section": "22.5 Mathematical Details",
    "text": "22.5 Mathematical Details\n\n22.5.1 Main Formula\nThe simple model that can be built to model link weights between nodes i and j can be defined using a Poisson distribution:\n\\[\nG_{ij} \\sim Poisson(Y_{ij})\n\\]\n\\[\nlog(Y_{ij}) =  \\lambda_i + \\pi_j + \\delta_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\) is the weight of the link between i and j.\n\\(\\lambda_i\\) is the sender effect üõà.\n\\(\\pi_j\\) is the receiver effect üõà.\n\\(\\delta_{ij}\\) is the dyadic effect üõà.\n22.5.2 Defining formula sub-equations and prior distributions\n\n\\(\\lambda_i\\) and \\(\\pi_j\\) are varying intercepts and slopes identical to those described in previous chapters and are defined through the following equations:\n\\[\n\\left(\\begin{array}{cc}\n\\lambda_i \\\\\n\\pi_j\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL *\n\\left(\\begin{array}{cc}\n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\lambda \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\pi \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJ(2)\n\\]\nSimilarly, for each dyad we can define varying intercepts and slopes to account for the correlation between the propensity to emit and receive links of a dyad:\n\\[\n\\left(\\begin{array}{cc}\n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL_\\delta *\n\\left(\\begin{array}{cc}\n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\nL_\\delta \\sim LKJ(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "20. Network model.html#notes",
    "href": "20. Network model.html#notes",
    "title": "22¬† Modeling Network",
    "section": "22.6 Note(s)",
    "text": "22.6 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nNote that any additional covariates can be summed with a regression coefficient to \\(\\lambda_i\\), \\(\\pi_j\\) and \\(\\delta_{ij}\\). Of course, for \\(\\lambda_i\\) and \\(\\pi_j\\), as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for \\(\\delta_{ij}\\), as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, individual_predictors, and a matrix of dyadic characteristics, kinship, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively, as follows:\n\ndef model2(idx, result_outcomes, dyad_effects, focal_individual_predictors, target_individual_predictors):\n    N_id = ids.shape[0]\n\n    # Sender Receiver effect (SR), its shape is equal to N_id ---------------------------------------\n    ## Covariates for SR\n    sr_terms, focal_effects, target_effects = m.net.nodes_terms(focal_individual_predictors, target_individual_predictors) \n\n    ## Varying intercept and slope for SR\n    sr_rf, sr_raw, sr_sigma, sr_L = m.net.nodes_random_effects(N_id, cholesky_density = 2) \n\n    sender_receiver = sr_terms + sr_rf\n\n    # Dyadic effect (D), its shape is equal to n dyads -----------------------------------------\n    ## Covariates for D\n    dr_terms, dyad_effects = m.net.dyadic_terms(dyad_effects)\n\n    ## Varying intercept and slope for D\n    rf, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(sender_receiver.shape[0], cholesky_density = 2)\n    dr = dr_terms + rf\n\n    lk('Y', Poisson(jnp.exp( sender_receiver + dr ), is_sparse = False), obs=result_outcomes) # is_sparse = True; if the matrix has many zeros, it can help speed up computation.\n\nm.data_on_model = dict(\n    idx = idx,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    dyad_effects = m.net.prepare_dyadic_effect(kinship), # Can be a jax array of multiple dimensions\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n)\n\nm.run(model2) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nWe can apply multiple variables as in chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms as in chapter 3: Interaction Between Continuous Variables.\nNetwork links can be modeled using Bernoulli, Binomial, Poisson, or zero-inflated Poisson distributions. So, by replacing the Poisson distribution with a binomial distribution, we can model the existence or absence of a link ‚Äî i.e., model binary networks.\nIf the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms \\(\\lambda_i\\), \\(\\pi_j\\), and \\(\\delta_{ij}\\) are no longer required. (Is it correct?)\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "21. Network with block model.html",
    "href": "21. Network with block model.html",
    "title": "23¬† Network with block model",
    "section": "",
    "text": "23.1 Considerations\nWithin networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "21. Network with block model.html#considerations",
    "href": "21. Network with block model.html#considerations",
    "title": "23¬† Network with block model",
    "section": "",
    "text": "Caution\n\n\n\n\nWe consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\nIn addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (Any in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "21. Network with block model.html#example",
    "href": "21. Network with block model.html#example",
    "title": "23¬† Network with block model",
    "section": "23.2 Example",
    "text": "23.2 Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the Network model example, with the addition of covariates Any, Merica, and Quantum, representing the block membership of each node.\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\n    m.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs=result_outcomes)\n\nm.data_on_model = dict(\n    idx=idx,\n    Any=Any-1, \n    Merica=Merica-1, \n    Quantum=Quantum-1,\n    result_outcomes=m.net.mat_to_edgl(data['outcomes']), \n    kinship=m.net.mat_to_edgl(kinship),\n    focal_individual_predictors=data['individual_predictors'],\n    target_individual_predictors=data['individual_predictors']\n)\n\nm.run(model3) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "21. Network with block model.html#mathematical-details",
    "href": "21. Network with block model.html#mathematical-details",
    "title": "23¬† Network with block model",
    "section": "23.3 Mathematical Details",
    "text": "23.3 Mathematical Details\n\n23.3.1 Main Formula\nThe model‚Äôs block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the Network model chapter:\n\\[\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\\]\n\\[\n\\log(Y_{ij}) = B_{ij} + B_{ji}\n\\]\nwhere:\n\n\\(B_{ij}\\) is the link probability between category \\(i\\) and \\(j\\).\n\\(B_{ji}\\) is the link probability between category \\(j\\) to \\(i\\).\n\n\n\n23.3.2 Defining formula sub-equations and prior distributions\nTo account for all link probabilities between categories, we can define a square matrix \\(B\\) as follows: the off-diagonal elements represent the link probabilities between categories \\(i\\) and \\(j\\), while the diagonal elements represent the link probabilities within category \\(i\\).\n\\[\nB_{i,j} =\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j}\n\\end{bmatrix}\n\\]\nWhere:\n\n\\(B[i,j]\\) is the link probability between category \\(i\\) and \\(j\\) when \\(i \\neq j\\).\n\\(B[i,j]\\) is the link probability within category \\(i\\) when \\(i = j\\).\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks. Basic priors could be:\n\\[\n\\beta_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n\\]\n\\[\n\\beta_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n\\]\nwhere:\n\n\\(k \\rightarrow k\\) indicates a diagonal element.\n\\(k \\rightarrow \\tilde{k}\\) indicates an off-diagonal element.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "21. Network with block model.html#notes",
    "href": "21. Network with block model.html#notes",
    "title": "23¬† Network with block model",
    "section": "23.4 Note(s)",
    "text": "23.4 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nBy defining this block model within our network model, we are estimating assortativity üõà and disassortativity üõà for categorical variables.\nSimilarly, for continuous variables, we can generate a block model that includes all continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html",
    "href": "22. Network control for data collection biases (wip).html",
    "title": "24¬† Network with data collection biases",
    "section": "",
    "text": "24.1 Considerations\nData collection biases are a persistent issue in studies of social networks. Two main types of biases can be considered: exposure biases üõà and censoring biases üõà.\nTo account for exposure biases, we can switch the network link probability model from a Poisson distribution to a Binomial distribution, as the binomial distribution allows us to account for the number of trials for each data estimation.\nTo address censoring biases, we need to add an additional equation to account for the probability of missing an interaction during observation when modeling the interaction between individuals i and j.",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html#considerations",
    "href": "22. Network control for data collection biases (wip).html#considerations",
    "title": "24¬† Network with data collection biases",
    "section": "",
    "text": "Caution",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html#example-1",
    "href": "22. Network control for data collection biases (wip).html#example-1",
    "title": "24¬† Network with data collection biases",
    "section": "24.2 Example 1",
    "text": "24.2 Example 1\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases:\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure_mat,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.binomial(total_count = m.net.mat_to_edgl(exposure_mat), logits = jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html#example-2",
    "href": "22. Network control for data collection biases (wip).html#example-2",
    "title": "24¬† Network with data collection biases",
    "section": "24.3 Example 2",
    "text": "24.3 Example 2\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases and censoring biases:",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html#mathematical-details",
    "href": "22. Network control for data collection biases (wip).html#mathematical-details",
    "title": "24¬† Network with data collection biases",
    "section": "24.4 Mathematical Details",
    "text": "24.4 Mathematical Details\n\n24.4.1 Main Formula\n\\[\nY_{[i,j]} \\sim \\text{Binomial}\\Big(E_{[i,j]}, Q_{[i,j]}  \\Big)\n\\]\n\\[\nQ_{[i,j]} = \\phi_{[i,j]}\\eta_{[i]}\\eta_{[j]}\n\\]\nWhere:\n\n\\(E_{[i,j]}\\) is the number of trials for each observation (i.e., the sampling effort).\n\\(Q_{[i,j]}\\) is the indicator of a true tie between \\(i\\) and \\(j\\), defined as: \\[\nQ_{[i,j]} \\sim \\begin{cases}\n0 & \\text{if no interaction occurs or if } i \\text{ or } j \\text{ is not detectable} \\\\\n1 & \\text{if } i \\text{ and } j \\text{ are both detectable}\n\\end{cases}\n\\]\n\\(\\phi_{[i,j]}\\) is the probability of a true tie between \\(i\\) and \\(j\\).\n\\(\\eta_{[i]}\\) is the probability of individual \\(i\\) being detectable.\n\\(\\eta_{[j]}\\) is the probability of individual \\(j\\) being detectable.\n\n\n\n24.4.2 Defining formula sub-equations and prior distributions\nWe can let \\(\\eta_{[i]}\\) depend on individual-specific covariates. To model the probability of censoring, we can model \\(1-\\eta_{[i]}\\): \\[\n\\text{logit}(1-\\eta_{[i]}) = \\mu_\\psi + \\hat\\psi_{[i]}  \\sigma_\\psi + \\dots\n\\]\nWhere:\n\n\\(\\mu_\\psi\\) is the intercept.\n\\(\\sigma_\\psi\\) is a scalar for the variance of random effects.\n\\(\\hat\\psi_{[i]}\\sim \\text{Normal}(0,1)\\), and the ellipsis signifies any linear model of coefficients and individual-level covariates. For example, if \\(C\\) is an animal-specific measure, like a binary variable for cryptic coloration, then the ellipsis may be replaced with \\(\\kappa_{[5]}C_{[i]}\\) to give the effects of coloration on censoring probability.",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "22. Network control for data collection biases (wip).html#notes",
    "href": "22. Network control for data collection biases (wip).html#notes",
    "title": "24¬† Network with data collection biases",
    "section": "24.5 Note(s)",
    "text": "24.5 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nOne major limitation of this model is the necessity of having an estimation of the censoring bias for each individual.",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "23. Network Metrics.html",
    "href": "23. Network Metrics.html",
    "title": "25¬† Network metrics",
    "section": "",
    "text": "25.1 General Principles\nNetwork metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "23. Network Metrics.html#nodal-metrics",
    "href": "23. Network Metrics.html#nodal-metrics",
    "title": "25¬† Network metrics",
    "section": "25.2 Nodal metrics",
    "text": "25.2 Nodal metrics\nNodal metrics* enable the assessment of nodes‚Äô social heterogeneity and the understanding of underlying mechanisms such as individual characteristics (e.g., the ageing process), ecological factors (e.g., demographic variation), and evolutionary processes (e.g., differences in social styles). Node measures are calculated at a nodal level and assess, in different ways and with different meanings, how an individual is connected. Connections can be ego‚Äôs* direct links only (e.g., degree, strength), its alters‚Äô* links as well (e.g., eigenvector, clustering coefficient), or even all the links in the network (e.g., betweenness). Node measures can also be used to describe the overall network structure through distributions, means, and coefficients of variation.\n\n25.2.1 Degree and strength\nThe degree m.net.degree measures the number of links of a node. When computed on an undirected network, the degree represents the number of alters of an ego. When the network is directed, it represents the number of either incoming or outgoing* links of an ego, and it is then called in-degree m.net.indegree or out-degree m.net.outdegree, respectively. Note that degree can also be computed in directed networks; in this case, it represents the sum of incoming and outgoing links and not the number of alters.\n\\[\nD_i = \\sum_{j=1}^N a_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\nStrength (or weighted degree) m.net.strength is the sum of the links‚Äô weights in a weighted network*. When the network comprises directed links, then it is also possible to differentiate between in-strength m.net.instrength (the sum of weights of incoming links) and out-strength m.net.outstrength (the sum of weights of outgoing links). While degree and strength can be considered correlated, it may not always be the case, as individuals can interact frequently with a few social partners or vice versa (Liao, Sosa, Wu, & Zhang, 2018). Therefore, it is necessary to test their correlation prior to the analysis.\n\\[\nS_i = \\sum_{j=1}^N a_{ij} w_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\n\n\n25.2.2 Eigenvector centrality\nEigenvector centrality m.net.eigenvector is the first non-negative eigenvector value obtained by transforming an adjacency matrix linearly. It can be computed on weighted, binary, directed, or undirected networks. It measures centrality by examining the connectedness of an ego as well as that of its alters. Thus, a node‚Äôs eigenvector value can be linked either to its own degree or strength or to the degrees or strengths of the nodes to which it is connected. Eigenvector may be interpreted as the social support or social capital of an individual (Brent, Semple, Dubuc, Heistermann, & MacLarnon, 2011), that is, the real or perceived availability of social resources.\n\\[\n\\lambda c = W c\n\\]\nWhere \\(\\lambda\\) is the largest eigenvalue of the adjacency matrix \\(W\\). Isolated node(s) can be considered as zero(s).\n\n\n25.2.3 Local clustering coefficient\nThe local clustering coefficient m.net.cc measures the number of closed triplets* over the total theoretical number of triplets (i.e., open and closed), where a triplet is a set of three nodes that are connected by either two (open triplet) or three (closed triplet) edges. This measure aims to examine the links that may exist between the alters of an ego and measures the cohesion of the network. The main topological effect of closed triplets is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity (see corresponding section). The local clustering coefficient can be computed in a binary network by measuring the proportion of links between the nodes of an ego-network* divided by the number of potential links between them. In weighted networks, several versions exist, such as those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or Opsahl and Panzarasa (2009).\n\n25.2.3.1 Binary Local Clustering Coefficient\n\\[\nC_i^b = \\frac{2L}{N_i (N_i - 1)}\n\\] Where \\(L\\) is the number of links in the ego-network of node \\(i\\).\n\n\n25.2.3.2 Barrat‚Äôs Local Clustering Coefficient\n\\[\nC_i^W = \\frac{1}{S_i (D_i - 1)} \\sum_{j \\neq h \\in N} \\frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}\n\\]\nWhere \\(S_i\\) and \\(D_i\\) are the strength and the degree of node \\(i\\), respectively. \\(w_{ij}\\) and \\(w_{ih}\\) are the weights of the links, and \\(a_{ij}\\), \\(a_{ih}\\), \\(a_{jh}\\) are the links between the nodes.\n\n\n25.2.3.3 Opsahl‚Äôs Local Clustering Coefficient\n\\[\nC^W(G) = \\frac{\\sum_{\\tau_\\Delta} w}{\\sum_\\tau w}\n\\] Where \\(\\tau_\\Delta\\) represents closed triplets, and \\(w\\) is the chosen weighting scheme (maximum, minimum, arithmetic, or geometric mean).\n\n\n\n25.2.4 Betweenness\nBetweenness (WIP) is the number of times a node is included in the shortest paths (geodesic distances) generated by every combination of two nodes. The value of the betweenness indicates the theoretical role of a node in social transmission (information, disease, etc., see Figure 1), as it indicates to what extent a node connects subgroups, as a bridge, and thus is likely to spread an entity across the whole network (Newman, 2005).\n\\[\nb = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\\]\nWhere \\(\\sigma_{st}\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\), and \\(\\sigma_{st}(v)\\) is the number of those paths that pass through \\(v\\). As no paths go through isolated nodes, their betweenness value can be considered zero.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "23. Network Metrics.html#polyadic-metrics",
    "href": "23. Network Metrics.html#polyadic-metrics",
    "title": "25¬† Network metrics",
    "section": "25.3 Polyadic metrics",
    "text": "25.3 Polyadic metrics\nPatterns of interactions (how and with whom individuals interact) can be examined using specific network measures* that analyse local-scale interactions within a network and make it possible to test hypotheses about the mechanisms underlying network connectivity. These types of measures are generally used to test mechanistic biological questions, such as what factors (e.g., ecological as well as sociodemographic) affect individuals‚Äô interactions/associations.\n\n25.3.1 Assortativity\nAssortativity (Newman, 2003) (WIP) is probably the most used measure to study homophily (preferential associations or interactions among individuals sharing the same characteristics; Lazarsfeld & Merton, 1954). Assortativity values range from ‚àí1 (total disassortativity, i.e., all the nodes associate or interact with those with the opposite characteristic, such as males interacting exclusively with females) to 1 (total assortativity, i.e., all the nodes associate or interact with those with the same characteristic, such as males interacting only with males). The assortativity coefficient measures the proportion of links between and within clusters of nodes with the same characteristics. Individuals‚Äô characteristics can be continuous (e.g., age, individual network measure, personality) or categorical features (e.g., sex, matriline belonging; Figure 2). Assortativity does not consider directionality* and can be measured in weighted (Leung & Chau, 2007) or binary (Newman, 2003) networks using categorical or continuous characteristics (Figure 2). The use of one or the other assortativity variant depends on the type of characteristics being examined and, whenever possible, the weighted version should be preferred since it is more reliable than the binary version (Farine, 2014).\n\n25.3.1.1 Binary Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1 - \\sum_i a_i b_i}\n\\]\nWhere \\(e_{ii}\\) is the proportion of specific links, \\(a_i\\) is the proportion of outgoing links, and \\(b_i\\) is the proportion of incoming links.\n\n\n25.3.1.2 Weighted Continuous Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii}^w - \\sum_i a_i^w b_i^w}{1 - \\sum_i a_i^w b_i^w}\n\\] Where \\(e_{ii}^w\\) is the proportion of weighted links, and \\(a_i^w\\), \\(b_i^w\\) are the proportions of weighted outgoing and incoming links.\n\n\n\n25.3.2 Transitive triplets\nTransitive triplets (WIP) are closed triplets where the links among the nodes follow a specific temporal pattern of creation, that is, when the establishment of links between nodes A and B and between nodes A and C is followed by the establishment of a link between nodes B and C. This network measure can be computed in directed, binary, or weighted networks. These types of connections can be studied over time based on the creation of links. From a static perspective, directionality can be considered by calculating the number of transitive triplets divided by the number of potential transitive triplets, and weights can also be considered by using Opsahl‚Äôs variants, which are discussed in the section on local clustering coefficient (Opsahl & Panzarasa, 2009). While transitivity is importantly related to the clustering coefficient (the clustering coefficient includes transitive triplets), not all closed triplets are transitive. Transitive triplets are one of the 16 possible configurations of a triplet considering open and closed triplets as well as link directionality (i.e., triad census).",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "23. Network Metrics.html#global-metrics",
    "href": "23. Network Metrics.html#global-metrics",
    "title": "25¬† Network metrics",
    "section": "25.4 Global metrics",
    "text": "25.4 Global metrics\nThe structure of this section is based on the distinction between network connectivity and social diffusion (information or disease spread). However, the social diffusion section contains measures specifically designed to study theoretical (i.e., considering the diffusion is perfectly related to network links and link weights) social diffusion features based on geodesic distances (see corresponding section). Aspects of the structure and properties of a group (e.g., cohesion, sub-grouping) can be quantified using global network measures. For instance, one may quantify properties such as network resilience (see Diameter), network clustering* (see Modularity) through network connectivity analysis, or network transmission efficiency* (see Global efficiency) through network theoretical social diffusion analysis.\n\n25.4.1 Density\nThe density m.net.density is the ratio of existing links to all potential links in a network. This measure is easy to interpret; it assesses how fully connected a network is. Density considers neither directionality nor link weights.\n\\[\nD = \\frac{2|L|}{|N|(|N| - 1)}\n\\]\nWhere \\(L\\) is the number of links and \\(N\\) is the number of nodes. Isolated node(s) can be considered as zero(s).\n\n\n25.4.2 Geodesic Distance\nGeodesic distance m.net.geodesic_distance is the shortest path considering all potential dyads in a network. This measure thereby indicates the fastest path of diffusion. Geodesic distance can be calculated in binary, weighted*, directed, or undirected networks. In weighted networks, it can be normalized (by dividing all links by the network‚Äôs mean weight), and the strongest or the weakest links can be considered as the fastest route between two nodes. This great number of variants of geodesic distance can greatly affect the results and interpretations. Researchers must thus have knowledge of the variants and know which one is the most appropriate according to their research question (Opsahl, Agneessens, & Skvoretz, 2010).\nThe computation uses algorithms like breadth-first search, depth-first search, or Dijkstra‚Äôs algorithm. None handle isolated nodes.\n\n\n25.4.3 Diameter\nThe diameter m.net.diameter of a network represents the longest of the shortest paths in the network. The diameter is used in ASNA to examine aspects such as network cohesion and the rapidness of information or disease transmission. While global efficiency measures the theoretical social diffusion spread, diameter informs on the maximum path length of diffusion required to reach all nodes.\n\n\n25.4.4 Global efficiency\nGlobal efficiency (WIP) is the ratio between the number of individuals and the number of connections multiplied by the network diameter. It provides a quantitative measure of how efficiently information is exchanged among the nodes of the network. As global efficiency gives a probability of social diffusion, it may help to better understand social transmission phenomena in the short and long term (Migliano et al., 2017). Pasquaretta et al.¬†(2014) found a positive correlation between the neocortex ratio and global efficiency in primate species with a higher neocortex ratio. By drawing a parallel between cognitive capacities and social network efficiency, this study showed that in species with a higher neocortex ratio, individuals may adjust their social relationships to gain better access to social information and thus optimize network efficiency. Alternatively, studies on epidemiology in ant colonies showed that ants adapt their interaction rate to decrease network efficiency when infected by a pathogen (Stroeymeyt et al., 2018).\n\n\n25.4.5 Modularity\nModularity (WIP) is a measure designed to quantify the degree to which a network can be divided into different groups or clusters, and its value ranges from 0 to 1. Networks with high modularity have dense connections within the modules but sparse connections between them. Modularity can be computed in weighted, binary, directed, or undirected networks.\n\\[\nQ = \\sum_{s=1}^m \\left[ \\frac{l_s}{|E|} - \\left(\\frac{d_s}{2|E|}\\right)^2 \\right]\n\\]\nWhere \\(l_s\\) is the number of edges in the \\(s\\)-th community, and \\(d_s\\) is the sum of the degrees of the nodes in the community.\n\n\n25.4.6 Global Clustering Coefficient\nThe global clustering coefficient (WIP), like the local clustering coefficient, evaluates how well the alters of an ego are interconnected and measures the cohesion of the network. Its main topological effect is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity. However, it becomes highly correlated with density and less so with modularity as density grows. Several variants of the global clustering coefficient can be found: (a) the ratio of closed triplets to all triplets (open and closed), and (b) the binary local mean clustering coefficient derived from the node level (see Local clustering coefficient). The binary local mean clustering coefficient allows us to consider node heterogeneity and thus should be preferred over the first variant. Weighted versions also exist and are based on the same variants described in the section on the local clustering coefficient and require the same considerations.\n\\[\nC^b(G) = \\frac{\\sum \\tau_\\Delta}{\\sum \\tau}\n\\]\nWhere \\(\\tau\\) is the total number of triplets and \\(\\tau_\\Delta\\) represents closed triplets.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "23. Network Metrics.html#references",
    "href": "23. Network Metrics.html#references",
    "title": "25¬† Network metrics",
    "section": "25.5 Reference(s)",
    "text": "25.5 Reference(s)\nSosa, Sueur, and Puga-Gonzalez (2021)\n\n\n\n\nSosa, Sebastian, C√©dric Sueur, and Ivan Puga-Gonzalez. 2021. ‚ÄúNetwork Measures in Animal Social Network Analysis: Their Strengths, Limits, Interpretations and Uses.‚Äù Methods in Ecology and Evolution 12 (1): 10‚Äì21. https://doi.org/https://doi.org/10.1111/2041-210X.13366.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html",
    "href": "24. Network Based Diffusion analysis (wip).html",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "",
    "text": "26.1 General Principles\nThe principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links. The basic model underlying NBDA states that at time \\(t\\) an individual, \\(i\\), learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#general-principles",
    "href": "24. Network Based Diffusion analysis (wip).html#general-principles",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "",
    "text": "Where the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time \\(t-1\\)).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#considerations",
    "href": "24. Network Based Diffusion analysis (wip).html#considerations",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "26.2 Considerations",
    "text": "26.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThere are two main NBDA variants: order-of-acquisition diffusion analysis (OADA), which takes as data the order in which individuals acquired the target behaviour, and time-of-acquisition diffusion analysis (TADA), which uses the times of acquisition of the target behaviour.",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#example",
    "href": "24. Network Based Diffusion analysis (wip).html#example",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "26.3 Example",
    "text": "26.3 Example\nBelow is an example code snippet demonstrating Bayesian Multiplex network model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#mathematical-details",
    "href": "24. Network Based Diffusion analysis (wip).html#mathematical-details",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "26.4 Mathematical Details",
    "text": "26.4 Mathematical Details\n\n26.4.1 Formulation\nThere are two parameters of interest in the basic time of acquisition diffusion analysis model: the rate of social transmission be-tween individuals per unit of network connection,s, and the baseline rate of trait performance in the absence of social transmission, \\(Œª_0\\).\n\\[\n\\lambda_i(t) = \\lambda_0(t) (1- z_i(t))  \\left[ s \\sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \\right]\n\\]\nWhere:\n\n\\(\\lambda_i(t)\\) is the rate at which individuals i acquire the task solution at time t.\n\\(\\lambda_0(t)\\) is a baseline acquisition function determining the distribution of latencies to acquisition in the absence of social transmission (that is, through asocial learning). It can be specify by an exponential or Weibull distrbution.\n\\(z_i(t)\\) gives the status (1 = informed, 0 = na√Øve) of individual i at time t.\n\\(s\\) is the regression coefficients capturing the effect of \\(x\\) on the hazard have an assigned a normal prior.\n\\((1- z_i(t))\\) and \\(z_j (-1)\\) terms ensure that the task solution is only transmitted from informed to uninformed individuals:\n\n\\[\nz_j(t) =  Y_i \\sim \\begin{cases}\n0, & \\text{if j is naive} \\\\\n1, & \\text{if j is informed}\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#notes",
    "href": "24. Network Based Diffusion analysis (wip).html#notes",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "26.5 Notes",
    "text": "26.5 Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "24. Network Based Diffusion analysis (wip).html#references",
    "href": "24. Network Based Diffusion analysis (wip).html#references",
    "title": "26¬† Network Based Diffusion analysis",
    "section": "26.6 Reference(s)",
    "text": "26.6 Reference(s)\nhttps://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13307\n\n(PDF) Quantifying diffusion in social networks: a Bayesian approach. Available from: https://www.researchgate.net/publication/270048687_Quantifying_diffusion_in_social_networks_a_Bayesian_approach [accessed Oct 24 2024].",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html",
    "href": "21. DPGMM.html",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "",
    "text": "23.1 General Principles\nTo discover group structures or clusters in data without pre-specifying the number of groups, we can use a Dirichlet Process Gaussian Mixture Model (DPMM). This is a non-parametric üõà clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian (bell curve) distributions, and it simultaneously tries to figure out:",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#general-principles",
    "href": "21. DPGMM.html#general-principles",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "",
    "text": "How many clusters (K) exist: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\nThe properties of each cluster: For each inferred cluster, it estimates its center (mean Œº) and its shape/spread (covariance Œ£).\nThe assignment of each data point: It determines the probability of each data point belonging to each cluster.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#considerations",
    "href": "21. DPGMM.html#considerations",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "23.2 Considerations",
    "text": "23.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nA DPMM is a Bayesian model üõà that considers uncertainty in all its parameters. The core idea is the Dirichlet Process üõà, a prior over distributions that allows for a potentially infinite number of clusters. In practice, we use a finite approximation called the Stick-Breaking Process üõà.\nThe key parameters and their priors are:\n\nConcentration Œ±: This single parameter controls the tendency to create new clusters. A low Œ± favors fewer, larger clusters, while a high Œ± allows for many smaller clusters. We typically place a Gamma prior on Œ± to learn its value from the data.\nCluster Weights w: Generated via the Stick-Breaking process from Œ±. These are the probabilities of drawing a data point from any given cluster.\nCluster Parameters (Œº, Œ£): Each potential cluster has a mean Œº and a covariance matrix Œ£. We must define priors for them. For numerical stability, the covariance Œ£ is often decomposed into standard deviations (œÉ) and a correlation matrix (Lcorr), using a LKJ prior üõà for the correlation part.\n\nThe model is often implemented in its marginalized form üõà. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm (e.g., NUTS) to explore, leading to much more efficient computation.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#example",
    "href": "21. DPGMM.html#example",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "23.3 Example",
    "text": "23.3 Example\nBelow is an example of a DPMM implemented in Python. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\nPythonR (Conceptual)\n\n\nimport jax, jax.numpy as jnp\nimport numpy as onp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nfrom sklearn.datasets import make_blobs\n\n# 1) Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=4, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\nN, D = data.shape\n\n# 2) Define the DPMM model\ndef dpmm_marginal(data, T=10):\n    # Priors\n    # Concentration parameter prior. Smaller Gamma rate encourages fewer clusters.\n    alpha = numpyro.sample(\"alpha\", dist.Gamma(1.0, 10.0))\n    \n    # Stick-breaking process for cluster weights\n    beta  = numpyro.sample(\"beta\", dist.Beta(1, alpha).expand([T-1]).to_event(1))\n    w     = numpyro.deterministic(\"w\", dist.transforms.StickBreakingTransform()(beta))\n\n    # Priors for cluster parameters (mean and covariance)\n    data_mean = jnp.mean(data, axis=0)\n    with numpyro.plate(\"components\", T):\n        mu    = numpyro.sample(\"mu\", dist.MultivariateNormal(loc=data_mean, covariance_matrix=10*jnp.eye(D)))\n        sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(1.).expand([D]).to_event(1))\n        Lcorr = numpyro.sample(\"Lcorr\", dist.LKJCholesky(dimension=D, concentration=1.0))\n    \n    # Reconstruct covariance Cholesky factor\n    scale_tril = sigma[..., None] * Lcorr\n\n    # Likelihood\n    # The data is drawn from a mixture of the Gaussian components\n    from numpyro.distributions import MixtureSameFamily, Categorical, MultivariateNormal\n    mix = MixtureSameFamily(Categorical(probs=w), MultivariateNormal(loc=mu, scale_tril=scale_tril))\n    numpyro.sample(\"obs\", mix, obs=data)\n\n# 3) Run MCMC to infer parameters\nkernel = NUTS(dpmm_marginal)\nmcmc   = MCMC(kernel, num_warmup=1000, num_samples=1000)\nmcmc.run(jax.random.PRNGKey(0), data=data)\n\n# 4) Get a summary of the inferred parameters\nmcmc.print_summary()\n\n\nlibrary(reticulate) # To use sklearn for data generation\nnp &lt;- import(\"numpy\")\nsklearn_datasets &lt;- import(\"sklearn.datasets\")\n\n# 1) Generate synthetic data\npy_obj &lt;- sklearn_datasets$make_blobs(\n    n_samples=500L, centers=4L, cluster_std=0.8,\n    center_box=c(-10,10), random_state=101L\n)\ndata &lt;- py_obj[[1]]\ntrue_labels &lt;- py_obj[[2]]\n\n# Hypothetical 'BI' package for Bayesian Inference\nlibrary(BI)\nm = importbi(platform='cpu')\nm$data_to_model(list(data=data))\n\n# 2) Define the DPMM model\nmodel &lt;- function(data){\n  D = ncol(data)\n  T = 10L # Truncation level\n\n  # Priors\n  alpha = bi.dist.gamma(1, 10, name='alpha')\n  beta = bi.dist.beta(1, alpha, shape=T-1, name='beta')\n  w = bi.deterministic(bi.stick_breaking(beta), name='w')\n  \n  # Priors for cluster parameters\n  m$plate(name='components', size=T, {\n    mu = bi.dist.multi_normal(loc=colMeans(data), cov=10*diag(D), name='mu')\n    sigma = bi.dist.half_cauchy(1, shape=D, name='sigma')\n    Lcorr = bi.dist.lkj_cholesky(dim=D, eta=1, name='Lcorr')\n  })\n  \n  # Likelihood\n  m$mixture_same_family(\n    mix_dist = bi.dist.categorical(probs=w),\n    comp_dist = bi.dist.multi_normal(loc=mu, scale_tril=sigma * Lcorr),\n    obs = data\n  )\n}\n\n# 3) Run MCMC\nm$run(model)\n\n# 4) Summary\nm$summary()",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#mathematical-details",
    "href": "21. DPGMM.html#mathematical-details",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "23.4 Mathematical Details",
    "text": "23.4 Mathematical Details\n\n23.4.1 Parametric Formulation: Finite Gaussian Mixture Model\nA standard, parametric approach to clustering is the Finite Gaussian Mixture Model (GMM), where the number of clusters K must be specified in advance. The model is:\n\\[\ny_i \\sim \\sum_{k=1}^{K} w_k \\cdot \\text{Normal}(\\mu_k, \\Sigma_k)\n\\]\nWhere: - \\(y_i\\) is the data for observation i. - K is the fixed number of clusters. - \\(w_k\\) are fixed mixing weights that sum to 1. - \\(\\mu_k\\) and \\(\\Sigma_k\\) are the mean and covariance of the k-th cluster.\n\n\n23.4.2 Non-Parametric Bayesian Formulation: DPMM\nThe DPMM replaces the fixed K with a flexible, data-driven process. The model is specified hierarchically:\n1. Stick-Breaking Construction for Weights \\[\n\\begin{align*}\n\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\quad \\text{for } k=1, \\dots, T-1 \\\\\nw_k &= \\beta_k \\prod_{j=1}^{k-1} (1-\\beta_j)\n\\end{align*}\n\\]\n2. Priors on Parameters \\[\n\\begin{align*}\n\\alpha &\\sim \\text{Gamma}(1, 10) && \\text{(Concentration hyperprior)} \\\\\n\\mu_k &\\sim \\text{Normal}(\\mu_0, \\Sigma_0) && \\text{(Prior for cluster means)} \\\\\n\\Sigma_k &\\sim \\text{InverseWishart}(\\nu, \\Psi) && \\text{(A common prior for covariance)} \\\\\n&\\text{or decomposed as:} \\\\\nL_{k} &\\sim \\text{LKJCholesky}(\\eta) && \\text{(Prior for correlation structure)} \\\\\n\\sigma_{k,d} &\\sim \\text{HalfCauchy}(1) && \\text{(Prior for standard deviations)}\n\\end{align*}\n\\]\n3. Likelihood \\[\ny_i \\sim \\sum_{k=1}^{T} w_k \\cdot \\text{Normal}(y_i | \\mu_k, \\Sigma_k)\n\\] Where T is a truncation level chosen to be larger than the expected number of clusters.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#notes",
    "href": "21. DPGMM.html#notes",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "23.5 Notes",
    "text": "23.5 Notes\n\n\n\n\n\n\nNote\n\n\n\nThe primary advantage of the DPMM over methods like K-Means or a finite GMM is the automatic inference of the number of clusters. Instead of running the model multiple times with different values of K and comparing them, the DPMM explores different numbers of clusters as part of its fitting process. The posterior distribution of the weights w reveals which components are ‚Äúactive‚Äù (have significant weight) and thus gives a probabilistic estimate of the number of clusters supported by the data.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "21. DPGMM.html#references",
    "href": "21. DPGMM.html#references",
    "title": "23¬† Dirichlet Process Gaussian Mixture Model",
    "section": "23.6 Reference(s)",
    "text": "23.6 Reference(s)\n(gershman2012tutorial?)\nMcElreath (2018)\n\n\n\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Dirichlet Process Gaussian Mixture Model</span>"
    ]
  },
  {
    "objectID": "22. Network model.html",
    "href": "22. Network model.html",
    "title": "24¬† Modeling Network",
    "section": "",
    "text": "24.1 Considerations\nA network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random intercepts and slopes. By adding such parameters to the model, we can account for the correlations between node link relationships.",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#considerations",
    "href": "22. Network model.html#considerations",
    "title": "24¬† Modeling Network",
    "section": "",
    "text": "Caution\n\n\n\n\nThe particularity here is that varying intercepts and slopes are generated for both nodal effects üõà and dyadic effects üõà. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#example",
    "href": "22. Network model.html#example",
    "title": "24¬† Modeling Network",
    "section": "24.2 Example",
    "text": "24.2 Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect:",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#python",
    "href": "22. Network model.html#python",
    "title": "24¬† Modeling Network",
    "section": "24.3 Python",
    "text": "24.3 Python\nfrom BI import bi\n\n# Setup device------------------------------------------------\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nm.run(model2) \nsummary = m.summary()\nsummary",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#r",
    "href": "22. Network model.html#r",
    "title": "24¬† Modeling Network",
    "section": "24.4 R",
    "text": "24.4 R\nlibrary(BI)\n# Setup platform------------------------------------------------\nm=importbi(platform='cpu')\n\n# Import data ------------------------------------------------\n\nload(paste(system.file(package = \"BI\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nids = 0:(model_dat$N_id-1)\nidx = m$net$vec_node_to_edgle(jnp$stack(jnp$array(list(ids, ids)), axis = -as.integer(1)))\n\nkeys &lt;- c(\"idx\",\n          'idxShape',\n          \"result_outcomes\",\n          'focal_individual_predictors',\n          'target_individual_predictors')\n\nvalues &lt;- list(\n  idx,\n  idx$shape[[1]],\n  m$net$mat_to_edgl(model_dat$outcomes[,,1]),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50)),\n  jnp$array(model_dat$individual_predictors)$reshape(as.integer(1),as.integer(50))\n)\ndata = py_dict(keys, values, convert = TRUE)\nm$data_on_model=data\n\n# Define model ------------------------------------------------\nmodel &lt;- function(idx, idxShape, result_outcomes,focal_individual_predictors, target_individual_predictors){\n  N_id = 50\n  x=0.1/jnp$sqrt(N_id)\n  tmp=jnp$log(x / (1 - x))\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(tmp, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(idxShape))\n\n  ## SR ---------------------------------------                                                      \n  m$poisson(jnp$exp(B + sr + dr), obs=result_outcomes)  \n}\n\n# Run MCMC ------------------------------------------------\nm$run(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\nsummary[rownames(summary) %in% c('focal_effects[0]', 'target_effects[0]', 'block[0]'),]",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#mathematical-details",
    "href": "22. Network model.html#mathematical-details",
    "title": "24¬† Modeling Network",
    "section": "24.5 Mathematical Details",
    "text": "24.5 Mathematical Details\n\n24.5.1 Main Formula\nThe simple model that can be built to model link weights between nodes i and j can be defined using a Poisson distribution:\n\\[\nG_{ij} \\sim Poisson(Y_{ij})\n\\]\n\\[\nlog(Y_{ij}) =  \\lambda_i + \\pi_j + \\delta_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\) is the weight of the link between i and j.\n\\(\\lambda_i\\) is the sender effect üõà.\n\\(\\pi_j\\) is the receiver effect üõà.\n\\(\\delta_{ij}\\) is the dyadic effect üõà.\n24.5.2 Defining formula sub-equations and prior distributions\n\n\\(\\lambda_i\\) and \\(\\pi_j\\) are varying intercepts and slopes identical to those described in previous chapters and are defined through the following equations:\n\\[\n\\left(\\begin{array}{cc}\n\\lambda_i \\\\\n\\pi_j\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL *\n\\left(\\begin{array}{cc}\n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\lambda \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\pi \\sim Exponential(1)\n\\]\n\\[\nL \\sim LKJ(2)\n\\]\nSimilarly, for each dyad we can define varying intercepts and slopes to account for the correlation between the propensity to emit and receive links of a dyad:\n\\[\n\\left(\\begin{array}{cc}\n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right)\n\\sim\nMultivariateNormal\\left(\\begin{array}{cc}\n\\left(\\begin{array}{cc}\n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ\n\\left(\\begin{array}{cc}\nL_\\delta *\n\\left(\\begin{array}{cc}\n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\\right)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\n\\sigma_\\delta \\sim Exponential(1)\n\\]\n\\[\nL_\\delta \\sim LKJ(2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "22. Network model.html#notes",
    "href": "22. Network model.html#notes",
    "title": "24¬† Modeling Network",
    "section": "24.6 Note(s)",
    "text": "24.6 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nNote that any additional covariates can be summed with a regression coefficient to \\(\\lambda_i\\), \\(\\pi_j\\) and \\(\\delta_{ij}\\). Of course, for \\(\\lambda_i\\) and \\(\\pi_j\\), as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for \\(\\delta_{ij}\\), as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, individual_predictors, and a matrix of dyadic characteristics, kinship, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively, as follows:\n\ndef model2(idx, result_outcomes, dyad_effects, focal_individual_predictors, target_individual_predictors):\n    N_id = ids.shape[0]\n\n    # Sender Receiver effect (SR), its shape is equal to N_id ---------------------------------------\n    ## Covariates for SR\n    sr_terms, focal_effects, target_effects = m.net.nodes_terms(focal_individual_predictors, target_individual_predictors) \n\n    ## Varying intercept and slope for SR\n    sr_rf, sr_raw, sr_sigma, sr_L = m.net.nodes_random_effects(N_id, cholesky_density = 2) \n\n    sender_receiver = sr_terms + sr_rf\n\n    # Dyadic effect (D), its shape is equal to n dyads -----------------------------------------\n    ## Covariates for D\n    dr_terms, dyad_effects = m.net.dyadic_terms(dyad_effects)\n\n    ## Varying intercept and slope for D\n    rf, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(sender_receiver.shape[0], cholesky_density = 2)\n    dr = dr_terms + rf\n\n    lk('Y', Poisson(jnp.exp( sender_receiver + dr ), is_sparse = False), obs=result_outcomes) # is_sparse = True; if the matrix has many zeros, it can help speed up computation.\n\nm.data_on_model = dict(\n    idx = idx,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    dyad_effects = m.net.prepare_dyadic_effect(kinship), # Can be a jax array of multiple dimensions\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n)\n\nm.run(model2) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\nWe can apply multiple variables as in chapter 2: Multiple Continuous Variables.\nWe can apply interaction terms as in chapter 3: Interaction Between Continuous Variables.\nNetwork links can be modeled using Bernoulli, Binomial, Poisson, or zero-inflated Poisson distributions. So, by replacing the Poisson distribution with a binomial distribution, we can model the existence or absence of a link ‚Äî i.e., model binary networks.\nIf the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms \\(\\lambda_i\\), \\(\\pi_j\\), and \\(\\delta_{ij}\\) are no longer required. (Is it correct?)\nIn the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).",
    "crumbs": [
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Modeling Network</span>"
    ]
  },
  {
    "objectID": "23. Network with block model.html",
    "href": "23. Network with block model.html",
    "title": "25¬† Network with block model",
    "section": "",
    "text": "25.1 Considerations\nWithin networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "23. Network with block model.html#considerations",
    "href": "23. Network with block model.html#considerations",
    "title": "25¬† Network with block model",
    "section": "",
    "text": "Caution\n\n\n\n\nWe consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\nIn addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (Any in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "23. Network with block model.html#example",
    "href": "23. Network with block model.html#example",
    "title": "25¬† Network with block model",
    "section": "25.2 Example",
    "text": "25.2 Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the Network model example, with the addition of covariates Any, Merica, and Quantum, representing the block membership of each node.\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors']\n\n\ndef model(idx, result_outcomes, \n    exposure,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]\n\n    m.poisson(jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs=result_outcomes)\n\nm.data_on_model = dict(\n    idx=idx,\n    Any=Any-1, \n    Merica=Merica-1, \n    Quantum=Quantum-1,\n    result_outcomes=m.net.mat_to_edgl(data['outcomes']), \n    kinship=m.net.mat_to_edgl(kinship),\n    focal_individual_predictors=data['individual_predictors'],\n    target_individual_predictors=data['individual_predictors']\n)\n\nm.run(model3) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "23. Network with block model.html#mathematical-details",
    "href": "23. Network with block model.html#mathematical-details",
    "title": "25¬† Network with block model",
    "section": "25.3 Mathematical Details",
    "text": "25.3 Mathematical Details\n\n25.3.1 Main Formula\nThe model‚Äôs block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the Network model chapter:\n\\[\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n\\]\n\\[\n\\log(Y_{ij}) = B_{ij} + B_{ji}\n\\]\nwhere:\n\n\\(B_{ij}\\) is the link probability between category \\(i\\) and \\(j\\).\n\\(B_{ji}\\) is the link probability between category \\(j\\) to \\(i\\).\n\n\n\n25.3.2 Defining formula sub-equations and prior distributions\nTo account for all link probabilities between categories, we can define a square matrix \\(B\\) as follows: the off-diagonal elements represent the link probabilities between categories \\(i\\) and \\(j\\), while the diagonal elements represent the link probabilities within category \\(i\\).\n\\[\nB_{i,j} =\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j}\n\\end{bmatrix}\n\\]\nWhere:\n\n\\(B[i,j]\\) is the link probability between category \\(i\\) and \\(j\\) when \\(i \\neq j\\).\n\\(B[i,j]\\) is the link probability within category \\(i\\) when \\(i = j\\).\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks. Basic priors could be:\n\\[\n\\beta_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n\\]\n\\[\n\\beta_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n\\]\nwhere:\n\n\\(k \\rightarrow k\\) indicates a diagonal element.\n\\(k \\rightarrow \\tilde{k}\\) indicates an off-diagonal element.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "23. Network with block model.html#notes",
    "href": "23. Network with block model.html#notes",
    "title": "25¬† Network with block model",
    "section": "25.4 Note(s)",
    "text": "25.4 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nBy defining this block model within our network model, we are estimating assortativity üõà and disassortativity üõà for categorical variables.\nSimilarly, for continuous variables, we can generate a block model that includes all continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Network with block model</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html",
    "href": "24. Network control for data collection biases (wip).html",
    "title": "26¬† Network with data collection biases",
    "section": "",
    "text": "26.1 Considerations\nData collection biases are a persistent issue in studies of social networks. Two main types of biases can be considered: exposure biases üõà and censoring biases üõà.\nTo account for exposure biases, we can switch the network link probability model from a Poisson distribution to a Binomial distribution, as the binomial distribution allows us to account for the number of trials for each data estimation.\nTo address censoring biases, we need to add an additional equation to account for the probability of missing an interaction during observation when modeling the interaction between individuals i and j.",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#considerations",
    "href": "24. Network control for data collection biases (wip).html#considerations",
    "title": "26¬† Network with data collection biases",
    "section": "",
    "text": "Caution",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-1",
    "href": "24. Network control for data collection biases (wip).html#example-1",
    "title": "26¬† Network with data collection biases",
    "section": "26.2 Example 1",
    "text": "26.2 Example 1\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases:\nfrom BI import bi\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\nm.data_on_model = dict(\n    idx = idx,\n    Any = Any-1, \n    Merica = Merica-1, \n    Quantum = Quantum-1,\n    result_outcomes = m.net.mat_to_edgl(data['outcomes']), \n    kinship = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = data['individual_predictors'],\n    target_individual_predictors = data['individual_predictors'],\n    exposure_mat = data['exposure']\n)\n\n\ndef model(idx, result_outcomes, \n    exposure_mat,\n    kinship, \n    focal_individual_predictors, target_individual_predictors, \n    Any, Merica, Quantum):\n      # Block ---------------------------------------\n      B_any = m.net.block_model(Any,1)\n      B_Merica = m.net.block_model(Merica,3)\n      B_Quantum = m.net.block_model(Quantum,2)\n\n      ## SR shape =  N individuals---------------------------------------\n      sr =  m.net.sender_receiver(focal_individual_predictors,target_individual_predictors)\n\n      # Dyadic shape = N dyads--------------------------------------  \n      dr = m.net.dyadic_effect(dyadic_predictors)\n\n      m.dist.binomial(total_count = m.net.mat_to_edgl(exposure_mat), logits = jnp.exp(B_any + B_Merica + B_Quantum + sender_receiver + dr), obs = result_outcomes, name= 'latent network' )\n\n\n\nm.run(model) \nsummary = m.summary()\nsummary.loc[['focal_effects[0]', 'target_effects[0]', 'dyad_effects[0]']]",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#example-2",
    "href": "24. Network control for data collection biases (wip).html#example-2",
    "title": "26¬† Network with data collection biases",
    "section": "26.3 Example 2",
    "text": "26.3 Example 2\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect, a dyadic effect, and a block model effect while accounting for exposure biases and censoring biases:",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#mathematical-details",
    "href": "24. Network control for data collection biases (wip).html#mathematical-details",
    "title": "26¬† Network with data collection biases",
    "section": "26.4 Mathematical Details",
    "text": "26.4 Mathematical Details\n\n26.4.1 Main Formula\n\\[\nY_{[i,j]} \\sim \\text{Binomial}\\Big(E_{[i,j]}, Q_{[i,j]}  \\Big)\n\\]\n\\[\nQ_{[i,j]} = \\phi_{[i,j]}\\eta_{[i]}\\eta_{[j]}\n\\]\nWhere:\n\n\\(E_{[i,j]}\\) is the number of trials for each observation (i.e., the sampling effort).\n\\(Q_{[i,j]}\\) is the indicator of a true tie between \\(i\\) and \\(j\\), defined as: \\[\nQ_{[i,j]} \\sim \\begin{cases}\n0 & \\text{if no interaction occurs or if } i \\text{ or } j \\text{ is not detectable} \\\\\n1 & \\text{if } i \\text{ and } j \\text{ are both detectable}\n\\end{cases}\n\\]\n\\(\\phi_{[i,j]}\\) is the probability of a true tie between \\(i\\) and \\(j\\).\n\\(\\eta_{[i]}\\) is the probability of individual \\(i\\) being detectable.\n\\(\\eta_{[j]}\\) is the probability of individual \\(j\\) being detectable.\n\n\n\n26.4.2 Defining formula sub-equations and prior distributions\nWe can let \\(\\eta_{[i]}\\) depend on individual-specific covariates. To model the probability of censoring, we can model \\(1-\\eta_{[i]}\\): \\[\n\\text{logit}(1-\\eta_{[i]}) = \\mu_\\psi + \\hat\\psi_{[i]}  \\sigma_\\psi + \\dots\n\\]\nWhere:\n\n\\(\\mu_\\psi\\) is the intercept.\n\\(\\sigma_\\psi\\) is a scalar for the variance of random effects.\n\\(\\hat\\psi_{[i]}\\sim \\text{Normal}(0,1)\\), and the ellipsis signifies any linear model of coefficients and individual-level covariates. For example, if \\(C\\) is an animal-specific measure, like a binary variable for cryptic coloration, then the ellipsis may be replaced with \\(\\kappa_{[5]}C_{[i]}\\) to give the effects of coloration on censoring probability.",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "24. Network control for data collection biases (wip).html#notes",
    "href": "24. Network control for data collection biases (wip).html#notes",
    "title": "26¬† Network with data collection biases",
    "section": "26.5 Note(s)",
    "text": "26.5 Note(s)\n\n\n\n\n\n\nNote\n\n\n\n\nOne major limitation of this model is the necessity of having an estimation of the censoring bias for each individual.",
    "crumbs": [
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Network with data collection biases</span>"
    ]
  },
  {
    "objectID": "25. Network Metrics.html",
    "href": "25. Network Metrics.html",
    "title": "27¬† Network metrics",
    "section": "",
    "text": "27.1 General Principles\nNetwork metrics are mathematical calculations to quantify specific features of a network, including global, nodal, and polyadic measures. Unlike other chapters, here we will present a suite of the most commonly used network metrics and the corresponding BI functions under the class m.net., implemented with JAX and usable within any bi model. This section is inspired by XXX, and users willing to dig further are invited to read and cite this paper.",
    "crumbs": [
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#nodal-metrics",
    "href": "25. Network Metrics.html#nodal-metrics",
    "title": "27¬† Network metrics",
    "section": "27.2 Nodal metrics",
    "text": "27.2 Nodal metrics\nNodal metrics* enable the assessment of nodes‚Äô social heterogeneity and the understanding of underlying mechanisms such as individual characteristics (e.g., the ageing process), ecological factors (e.g., demographic variation), and evolutionary processes (e.g., differences in social styles). Node measures are calculated at a nodal level and assess, in different ways and with different meanings, how an individual is connected. Connections can be ego‚Äôs* direct links only (e.g., degree, strength), its alters‚Äô* links as well (e.g., eigenvector, clustering coefficient), or even all the links in the network (e.g., betweenness). Node measures can also be used to describe the overall network structure through distributions, means, and coefficients of variation.\n\n27.2.1 Degree and strength\nThe degree m.net.degree measures the number of links of a node. When computed on an undirected network, the degree represents the number of alters of an ego. When the network is directed, it represents the number of either incoming or outgoing* links of an ego, and it is then called in-degree m.net.indegree or out-degree m.net.outdegree, respectively. Note that degree can also be computed in directed networks; in this case, it represents the sum of incoming and outgoing links and not the number of alters.\n\\[\nD_i = \\sum_{j=1}^N a_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\nStrength (or weighted degree) m.net.strength is the sum of the links‚Äô weights in a weighted network*. When the network comprises directed links, then it is also possible to differentiate between in-strength m.net.instrength (the sum of weights of incoming links) and out-strength m.net.outstrength (the sum of weights of outgoing links). While degree and strength can be considered correlated, it may not always be the case, as individuals can interact frequently with a few social partners or vice versa (Liao, Sosa, Wu, & Zhang, 2018). Therefore, it is necessary to test their correlation prior to the analysis.\n\\[\nS_i = \\sum_{j=1}^N a_{ij} w_{ij}\n\\]\nWhere \\(a_{ij}\\) is the value of the link between nodes \\(i\\) and \\(j\\). Isolated node(s) can be considered as zero(s).\n\n\n27.2.2 Eigenvector centrality\nEigenvector centrality m.net.eigenvector is the first non-negative eigenvector value obtained by transforming an adjacency matrix linearly. It can be computed on weighted, binary, directed, or undirected networks. It measures centrality by examining the connectedness of an ego as well as that of its alters. Thus, a node‚Äôs eigenvector value can be linked either to its own degree or strength or to the degrees or strengths of the nodes to which it is connected. Eigenvector may be interpreted as the social support or social capital of an individual (Brent, Semple, Dubuc, Heistermann, & MacLarnon, 2011), that is, the real or perceived availability of social resources.\n\\[\n\\lambda c = W c\n\\]\nWhere \\(\\lambda\\) is the largest eigenvalue of the adjacency matrix \\(W\\). Isolated node(s) can be considered as zero(s).\n\n\n27.2.3 Local clustering coefficient\nThe local clustering coefficient m.net.cc measures the number of closed triplets* over the total theoretical number of triplets (i.e., open and closed), where a triplet is a set of three nodes that are connected by either two (open triplet) or three (closed triplet) edges. This measure aims to examine the links that may exist between the alters of an ego and measures the cohesion of the network. The main topological effect of closed triplets is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity (see corresponding section). The local clustering coefficient can be computed in a binary network by measuring the proportion of links between the nodes of an ego-network* divided by the number of potential links between them. In weighted networks, several versions exist, such as those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or Opsahl and Panzarasa (2009).\n\n27.2.3.1 Binary Local Clustering Coefficient\n\\[\nC_i^b = \\frac{2L}{N_i (N_i - 1)}\n\\] Where \\(L\\) is the number of links in the ego-network of node \\(i\\).\n\n\n27.2.3.2 Barrat‚Äôs Local Clustering Coefficient\n\\[\nC_i^W = \\frac{1}{S_i (D_i - 1)} \\sum_{j \\neq h \\in N} \\frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}\n\\]\nWhere \\(S_i\\) and \\(D_i\\) are the strength and the degree of node \\(i\\), respectively. \\(w_{ij}\\) and \\(w_{ih}\\) are the weights of the links, and \\(a_{ij}\\), \\(a_{ih}\\), \\(a_{jh}\\) are the links between the nodes.\n\n\n27.2.3.3 Opsahl‚Äôs Local Clustering Coefficient\n\\[\nC^W(G) = \\frac{\\sum_{\\tau_\\Delta} w}{\\sum_\\tau w}\n\\] Where \\(\\tau_\\Delta\\) represents closed triplets, and \\(w\\) is the chosen weighting scheme (maximum, minimum, arithmetic, or geometric mean).\n\n\n\n27.2.4 Betweenness\nBetweenness (WIP) is the number of times a node is included in the shortest paths (geodesic distances) generated by every combination of two nodes. The value of the betweenness indicates the theoretical role of a node in social transmission (information, disease, etc., see Figure 1), as it indicates to what extent a node connects subgroups, as a bridge, and thus is likely to spread an entity across the whole network (Newman, 2005).\n\\[\nb = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\\]\nWhere \\(\\sigma_{st}\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\), and \\(\\sigma_{st}(v)\\) is the number of those paths that pass through \\(v\\). As no paths go through isolated nodes, their betweenness value can be considered zero.",
    "crumbs": [
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#polyadic-metrics",
    "href": "25. Network Metrics.html#polyadic-metrics",
    "title": "27¬† Network metrics",
    "section": "27.3 Polyadic metrics",
    "text": "27.3 Polyadic metrics\nPatterns of interactions (how and with whom individuals interact) can be examined using specific network measures* that analyse local-scale interactions within a network and make it possible to test hypotheses about the mechanisms underlying network connectivity. These types of measures are generally used to test mechanistic biological questions, such as what factors (e.g., ecological as well as sociodemographic) affect individuals‚Äô interactions/associations.\n\n27.3.1 Assortativity\nAssortativity (Newman, 2003) (WIP) is probably the most used measure to study homophily (preferential associations or interactions among individuals sharing the same characteristics; Lazarsfeld & Merton, 1954). Assortativity values range from ‚àí1 (total disassortativity, i.e., all the nodes associate or interact with those with the opposite characteristic, such as males interacting exclusively with females) to 1 (total assortativity, i.e., all the nodes associate or interact with those with the same characteristic, such as males interacting only with males). The assortativity coefficient measures the proportion of links between and within clusters of nodes with the same characteristics. Individuals‚Äô characteristics can be continuous (e.g., age, individual network measure, personality) or categorical features (e.g., sex, matriline belonging; Figure 2). Assortativity does not consider directionality* and can be measured in weighted (Leung & Chau, 2007) or binary (Newman, 2003) networks using categorical or continuous characteristics (Figure 2). The use of one or the other assortativity variant depends on the type of characteristics being examined and, whenever possible, the weighted version should be preferred since it is more reliable than the binary version (Farine, 2014).\n\n27.3.1.1 Binary Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1 - \\sum_i a_i b_i}\n\\]\nWhere \\(e_{ii}\\) is the proportion of specific links, \\(a_i\\) is the proportion of outgoing links, and \\(b_i\\) is the proportion of incoming links.\n\n\n27.3.1.2 Weighted Continuous Assortativity\n\\[\nr = \\frac{\\sum_i e_{ii}^w - \\sum_i a_i^w b_i^w}{1 - \\sum_i a_i^w b_i^w}\n\\] Where \\(e_{ii}^w\\) is the proportion of weighted links, and \\(a_i^w\\), \\(b_i^w\\) are the proportions of weighted outgoing and incoming links.\n\n\n\n27.3.2 Transitive triplets\nTransitive triplets (WIP) are closed triplets where the links among the nodes follow a specific temporal pattern of creation, that is, when the establishment of links between nodes A and B and between nodes A and C is followed by the establishment of a link between nodes B and C. This network measure can be computed in directed, binary, or weighted networks. These types of connections can be studied over time based on the creation of links. From a static perspective, directionality can be considered by calculating the number of transitive triplets divided by the number of potential transitive triplets, and weights can also be considered by using Opsahl‚Äôs variants, which are discussed in the section on local clustering coefficient (Opsahl & Panzarasa, 2009). While transitivity is importantly related to the clustering coefficient (the clustering coefficient includes transitive triplets), not all closed triplets are transitive. Transitive triplets are one of the 16 possible configurations of a triplet considering open and closed triplets as well as link directionality (i.e., triad census).",
    "crumbs": [
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#global-metrics",
    "href": "25. Network Metrics.html#global-metrics",
    "title": "27¬† Network metrics",
    "section": "27.4 Global metrics",
    "text": "27.4 Global metrics\nThe structure of this section is based on the distinction between network connectivity and social diffusion (information or disease spread). However, the social diffusion section contains measures specifically designed to study theoretical (i.e., considering the diffusion is perfectly related to network links and link weights) social diffusion features based on geodesic distances (see corresponding section). Aspects of the structure and properties of a group (e.g., cohesion, sub-grouping) can be quantified using global network measures. For instance, one may quantify properties such as network resilience (see Diameter), network clustering* (see Modularity) through network connectivity analysis, or network transmission efficiency* (see Global efficiency) through network theoretical social diffusion analysis.\n\n27.4.1 Density\nThe density m.net.density is the ratio of existing links to all potential links in a network. This measure is easy to interpret; it assesses how fully connected a network is. Density considers neither directionality nor link weights.\n\\[\nD = \\frac{2|L|}{|N|(|N| - 1)}\n\\]\nWhere \\(L\\) is the number of links and \\(N\\) is the number of nodes. Isolated node(s) can be considered as zero(s).\n\n\n27.4.2 Geodesic Distance\nGeodesic distance m.net.geodesic_distance is the shortest path considering all potential dyads in a network. This measure thereby indicates the fastest path of diffusion. Geodesic distance can be calculated in binary, weighted*, directed, or undirected networks. In weighted networks, it can be normalized (by dividing all links by the network‚Äôs mean weight), and the strongest or the weakest links can be considered as the fastest route between two nodes. This great number of variants of geodesic distance can greatly affect the results and interpretations. Researchers must thus have knowledge of the variants and know which one is the most appropriate according to their research question (Opsahl, Agneessens, & Skvoretz, 2010).\nThe computation uses algorithms like breadth-first search, depth-first search, or Dijkstra‚Äôs algorithm. None handle isolated nodes.\n\n\n27.4.3 Diameter\nThe diameter m.net.diameter of a network represents the longest of the shortest paths in the network. The diameter is used in ASNA to examine aspects such as network cohesion and the rapidness of information or disease transmission. While global efficiency measures the theoretical social diffusion spread, diameter informs on the maximum path length of diffusion required to reach all nodes.\n\n\n27.4.4 Global efficiency\nGlobal efficiency (WIP) is the ratio between the number of individuals and the number of connections multiplied by the network diameter. It provides a quantitative measure of how efficiently information is exchanged among the nodes of the network. As global efficiency gives a probability of social diffusion, it may help to better understand social transmission phenomena in the short and long term (Migliano et al., 2017). Pasquaretta et al.¬†(2014) found a positive correlation between the neocortex ratio and global efficiency in primate species with a higher neocortex ratio. By drawing a parallel between cognitive capacities and social network efficiency, this study showed that in species with a higher neocortex ratio, individuals may adjust their social relationships to gain better access to social information and thus optimize network efficiency. Alternatively, studies on epidemiology in ant colonies showed that ants adapt their interaction rate to decrease network efficiency when infected by a pathogen (Stroeymeyt et al., 2018).\n\n\n27.4.5 Modularity\nModularity (WIP) is a measure designed to quantify the degree to which a network can be divided into different groups or clusters, and its value ranges from 0 to 1. Networks with high modularity have dense connections within the modules but sparse connections between them. Modularity can be computed in weighted, binary, directed, or undirected networks.\n\\[\nQ = \\sum_{s=1}^m \\left[ \\frac{l_s}{|E|} - \\left(\\frac{d_s}{2|E|}\\right)^2 \\right]\n\\]\nWhere \\(l_s\\) is the number of edges in the \\(s\\)-th community, and \\(d_s\\) is the sum of the degrees of the nodes in the community.\n\n\n27.4.6 Global Clustering Coefficient\nThe global clustering coefficient (WIP), like the local clustering coefficient, evaluates how well the alters of an ego are interconnected and measures the cohesion of the network. Its main topological effect is the clustering of the network, generating cohesive clusters, and is thus strongly related to modularity. However, it becomes highly correlated with density and less so with modularity as density grows. Several variants of the global clustering coefficient can be found: (a) the ratio of closed triplets to all triplets (open and closed), and (b) the binary local mean clustering coefficient derived from the node level (see Local clustering coefficient). The binary local mean clustering coefficient allows us to consider node heterogeneity and thus should be preferred over the first variant. Weighted versions also exist and are based on the same variants described in the section on the local clustering coefficient and require the same considerations.\n\\[\nC^b(G) = \\frac{\\sum \\tau_\\Delta}{\\sum \\tau}\n\\]\nWhere \\(\\tau\\) is the total number of triplets and \\(\\tau_\\Delta\\) represents closed triplets.",
    "crumbs": [
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "25. Network Metrics.html#references",
    "href": "25. Network Metrics.html#references",
    "title": "27¬† Network metrics",
    "section": "27.5 Reference(s)",
    "text": "27.5 Reference(s)\nSosa, Sueur, and Puga-Gonzalez (2021)\n\n\n\n\nSosa, Sebastian, C√©dric Sueur, and Ivan Puga-Gonzalez. 2021. ‚ÄúNetwork Measures in Animal Social Network Analysis: Their Strengths, Limits, Interpretations and Uses.‚Äù Methods in Ecology and Evolution 12 (1): 10‚Äì21. https://doi.org/https://doi.org/10.1111/2041-210X.13366.",
    "crumbs": [
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Network metrics</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html",
    "href": "26. Network Based Diffusion analysis (wip).html",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "",
    "text": "28.1 General Principles\nThe principle idea behind Network Based Diffusion analysis (NBDA) is that if social transmission is involved in the spread of a novel behavior through a group, then that spread is expected to follow a social network links. The basic model underlying NBDA states that at time \\(t\\) an individual, \\(i\\), learns the behavior of interest with a specific rate formula.\nIn principle NBDA can be consider as a survival analysis, so we have the same concepts as in chapter 12:",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "href": "26. Network Based Diffusion analysis (wip).html#general-principles",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "",
    "text": "Where the baseline hazard (e.g.¬†the hazard when all covariates are zero) is the asocial hazard.\nWhere the covariate is the sum of links toward informed individuals (i.e.¬†individuals that acquired the behavior of interest at time \\(t-1\\)).\nThus the Hazard Function which account for the network links weights covariate can thus be consider as the social rate of learning the behavior.",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#considerations",
    "href": "26. Network Based Diffusion analysis (wip).html#considerations",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "28.2 Considerations",
    "text": "28.2 Considerations\n\n\n\n\n\n\nCaution\n\n\n\n\nThere are two main NBDA variants: order-of-acquisition diffusion analysis (OADA), which takes as data the order in which individuals acquired the target behaviour, and time-of-acquisition diffusion analysis (TADA), which uses the times of acquisition of the target behaviour.",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#example",
    "href": "26. Network Based Diffusion analysis (wip).html#example",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "28.3 Example",
    "text": "28.3 Example\nBelow is an example code snippet demonstrating Bayesian Multiplex network model using the Bayesian Inference (BI) package:\n\nPython",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "href": "26. Network Based Diffusion analysis (wip).html#mathematical-details",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "28.4 Mathematical Details",
    "text": "28.4 Mathematical Details\n\n28.4.1 Formulation\nThere are two parameters of interest in the basic time of acquisition diffusion analysis model: the rate of social transmission be-tween individuals per unit of network connection,s, and the baseline rate of trait performance in the absence of social transmission, \\(Œª_0\\).\n\\[\n\\lambda_i(t) = \\lambda_0(t) (1- z_i(t))  \\left[ s \\sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \\right]\n\\]\nWhere:\n\n\\(\\lambda_i(t)\\) is the rate at which individuals i acquire the task solution at time t.\n\\(\\lambda_0(t)\\) is a baseline acquisition function determining the distribution of latencies to acquisition in the absence of social transmission (that is, through asocial learning). It can be specify by an exponential or Weibull distrbution.\n\\(z_i(t)\\) gives the status (1 = informed, 0 = na√Øve) of individual i at time t.\n\\(s\\) is the regression coefficients capturing the effect of \\(x\\) on the hazard have an assigned a normal prior.\n\\((1- z_i(t))\\) and \\(z_j (-1)\\) terms ensure that the task solution is only transmitted from informed to uninformed individuals:\n\n\\[\nz_j(t) =  Y_i \\sim \\begin{cases}\n0, & \\text{if j is naive} \\\\\n1, & \\text{if j is informed}\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#notes",
    "href": "26. Network Based Diffusion analysis (wip).html#notes",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "28.5 Notes",
    "text": "28.5 Notes\n\n\n\n\n\n\nNote",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  },
  {
    "objectID": "26. Network Based Diffusion analysis (wip).html#references",
    "href": "26. Network Based Diffusion analysis (wip).html#references",
    "title": "28¬† Network Based Diffusion analysis",
    "section": "28.6 Reference(s)",
    "text": "28.6 Reference(s)\nhttps://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13307\n\n(PDF) Quantifying diffusion in social networks: a Bayesian approach. Available from: https://www.researchgate.net/publication/270048687_Quantifying_diffusion_in_social_networks_a_Bayesian_approach [accessed Oct 24 2024].",
    "crumbs": [
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Network Based Diffusion analysis</span>"
    ]
  }
]