---
title: "üöß Dirichlet Model üöß"
description: "Modeling uncertainty about the probabilities of categories themselves."
categories: [Regression, GLM, Classification]
image: "Figures/10.png"
order: 13
---

## General Principles
To model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Dirichlet_ model.

## Considerations
::: callout-note
- We have the same considerations as for the [Multinomial model](9.&#32;Multinomial&#32;model.qmd).

:::

## Example
::: {.panel-tabset group="language"}
## Python
![](travaux-routiers.png){fig-align="center"}

<!--
``` python
from BI import bi, jnp
import jax
# Setup device ------------------------------------------------
m = bi('cpu')

# Import Data & Data Manipulation ------------------------------------------------
# Import
data_path = m.load.sim_multinomial(only_path=True)
m.data(data_path, sep=',') 

# Define model ------------------------------------------------
def model(income, career):
    # Parameter prior distributions
    alpha = m.dist.normal(0, 1, shape=(2,), name='a')
    beta = m.dist.half_normal(0.5, shape=(1,), name='b')
    s_1 = alpha[0] + beta * income[0]
    s_2 = alpha[1] + beta * income[1]
    s_3 = [0]
    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))
    # Likelihood
    m.dist.dirichlet(p[career], obs=career)

# Run sampler ------------------------------------------------ 
m.fit(model)  

# Summary ------------------------------------------------
m.summary()
```

## R
![](travaux-routiers.png){fig-align="center"}
<!---
```R
library(BayesianInference)
m=importBI(platform='cpu')
```
--->
:::

## Mathematical Details
We can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $ùêæ$ categories, the *Dirichlet* likelihood function is:

$$
Y_i \sim \text{Dirichlet}(\theta_i  \kappa) \\
\theta_i = \text{Softmax}(\phi_i) \\
\phi_{[i,1]} = \alpha_1 + \beta_1 X_i \\
\phi_{[i,2]} = \alpha_2 + \beta_2 X_i \\
... \\
\phi_{[i,k]} = 0 \\
\kappa \sim \text{Exponential}(1) \\
\alpha_{k} \sim \text{Normal}(0,1) \\
\beta_{k} \sim \text{Normal}(0.1)
$$

Where:

- $Y_i$ is the outcome [<span style="color:#0D6EFD">simplex üõà</span>]{#simplex} for observation *i*.

- $\kappa$ is the concentration parameter, it controls the prior weight on each category.
  
- $\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. 
  
- $\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\theta_i$ form a [<span style="color:#0D6EFD">simplex üõà</span>]{#simplex}.
  
- Each element of $\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\alpha_k$ and slope coefficient $\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.


## Reference(s)
