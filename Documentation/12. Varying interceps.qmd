# Varying interceps
## General Principles
To model the relationship between predictor variables and an outcome variable while allowing for different intercepts across groups or clusters, we can use a _Varying Intercepts_ model. This approach is particularly useful when data is grouped (e.g., by subject, location, or time period) and we expect the baseline level of the outcome to vary across these groups.

## Formula
We model the relationship between the predictor variables (_X1, X2, ..., Xn_) and the outcome variable (_Y_) with varying intercepts (_b_j_) for each group (_j_) using the following equation:

$$
Y_{ij} = X_{ij1} * W_1 + X_{ij2} * W_2 + ... + X_{ijn} * W_n + b_j + \epsilon_{ij}
$$

Where:
- $Y_{ij}$ is the outcome variable for observation _i_ in group _j_.
- $X_{ij1}, X_{ij2}, ..., X_{ijn}$ are the predictor variables for observation _i_ in group _j_.
- $W_1, W_2, ..., W_n$ are the regression coefficients.
- $b_j$ is the varying intercept for group _j_.
- $\epsilon_{ij}$ is the error term, typically assumed to be normally distributed with mean 0 and variance $\sigma^2$.

The varying intercepts _b_j_ are typically modeled as being drawn from a common distribution:

$$
b_j \sim \text{Normal}(\mu_b, \sigma_b^2)
$$

Where:
- _\mu_b_ is the overall mean intercept.
- _\sigma_b^2_ is the variance of the intercepts across groups.

## Considerations
In Bayesian regression with varying intercepts, we consider uncertainty in both the regression coefficients and the varying intercepts. We need to declare prior distributions for _W_1, W_2, ..., W_n_, _\mu_b_, and _\sigma_b_.

Typically, we use a _Normal_ distribution for _W_1, W_2, ..., W_n_ and _\mu_b_, and a _Half-Normal_ or _Exponential_ distribution for _\sigma_b_.


<span style="font-size:1em; color : red">
<i>
Additional conciderations : 
</i>
</span>

- Basically, the idea of varying intercepts is to generate an intercept for each group. So the intercept \( b \) is defined based on the declared groups.
- Therefore, there are as many \( b \) values as there are groups.
* These intercepts are defined using a Normal distribution with a mean specified by another prior (called a _hyper-prior_):
$$
b_j \sim \text{Normal}(\mu_b, \sigma_b^2)
$$

   Where:
    - $\mu_b$is the overall mean intercept.
    - $\sigma_b^2$ is the variance of the intercepts across groups.
  
- In the code below, the _hyper-prior_ is _a_bar_.

## Example

Below is an example code snippet demonstrating Bayesian regression with varying intercepts:

```python
from BI import bi.hard
d = pd.read_csv('/home/sosa/BI/data/reedfrogs.csv', sep = ';')
d["tank"] = np.arange(d.shape[0])
tank = jnp.array(d["tank"].astype('int32').values)
density = jnp.array(d["density"].astype('float32').values)
def model():
    sigma = yield exponential(1, 1)
    a_bar = yield normal(1, 0, 1.5)
    alpha = yield normal(48, a_bar, sigma)
    p = jnp.squeeze(alpha[tank])[0]
    y = yield Independent(Binomial(total_count = density, logits = p), reinterpreted_batch_ndims=1)

posterior, sample_stats = NUTS(model, obs = jnp.array(d.surv.astype('float32').values))
```

## Mathematical Details
We can express the Bayesian regression model with varying intercepts using probability distributions as follows:

$$
p(Y_{ij} | X_{ij}, W, b_j, \sigma) = \text{Normal}(X_{ij} * W + b_j, \sigma^2) \\
b_j \sim \text{Normal}(\mu_b, \sigma_b^2) \\
p(W_i) = \text{Normal}(0, \alpha^2) \\
p(\mu_b) = \text{Normal}(0, \beta^2) \\
p(\sigma_b) = \text{Exponential}(\lambda)
$$

Where:
- $p(Y_{ij} | X_{ij}, W, b_j, \sigma)$ is the likelihood function for the outcome variable.
- $b_j \sim \text{Normal}(\mu_b, \sigma_b^2)$ models the varying intercepts across groups.
- $p(W_i)$, $p(\mu_b)$, and $p(\sigma_b)$ are the prior distributions for the regression coefficients, overall mean intercept, and standard deviation of the intercepts, respectively.
