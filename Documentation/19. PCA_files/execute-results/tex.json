{
  "hash": "ceda732b5978bd68fe5251530af15d8b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Principal Component Analysis\"\ndescription: \"A dimensionality reduction technique to transform a large set of variables into a smaller one.\"\ncategories: [Dimensionality Reduction, Unsupervised Learning]\nimage: \"Figures/18.png\"\norder: 22\n---\n\n## General Principles\n\n**Principal Component Analysis (PCA)** [@tipping1999probabilistic] is a technique used to reduce the dimensionality of a dataset by transforming it into a new coordinate system where the greatest variance of the data is projected onto coordinates (called principal components). This method helps capture the underlying structure of high-dimensional data by identifying patterns based on variance.\n\nIn **Bayesian PCA**, uncertainty in the model parameters is explicitly taken into account by using a probabilistic framework. This allows us to not only estimate the principal components but also quantify the uncertainty around them and avoid overfitting by incorporating prior knowledge.\n\nPCA is  employed for *dimensionality reduction*, particularly in scenarios involving high-dimensional datasets, as it effectively reduces complexity while explicitly accounting for uncertainty in the underlying latent structure. This approach also plays a crucial role in *data visualization*, enabling the projection of intricate, high-dimensional data into more interpretable 2D or 3D representations. Additionally, PCA excels in *feature extraction*, where the latent variables it identifies can be repurposed as informative features for subsequent tasks, such as classification or clustering. By modeling latent variables, it further enhances the interpretability and utility of the data for a variety of analytical applications.\n\n\n## Considerations\n::: {.callout-note}\n*   In **Bayesian PCA**, we assume prior distributions for the latent variables _Z_ and the principal component loadings _W_. We place Gaussian priors on both _Z_ and _W_ and learn their posterior distributions using the observed data _X_. \n\n*   **Robustness to Outliers:** Standard PCA are sensitive to outliers due to the assumption of Gaussian noise. Robust variants of **Robust Bayesian PCA** address this by employing heavy-tailed distributions for the noise model, such as the Student's t-distribution, which reduces the influence of outliers [@archambeau2006robust; @BOUWMANS201422].\n\n*   **Automatic Dimensionality Selection:** Through techniques like Automatic Relevance Determination (ARD), Bayesian PCA can automatically determine the effective dimensionality of the latent space. Priors are placed on the relevance of each principal component, and components that are not supported by the data are effectively \"switched off' [@bishop1998bayesian; @bishop2006pattern]. \n\n* **Sparsity for High-Dimensional Data:** In high-dimensional settings, it is often desirable for the principal components to be influenced by only a subset of the original features, leading to more interpretable results. Sparse Bayesian PCA achieves this by placing sparsity-inducing priors (e.g., Laplacian or spike-and-slab priors) on the loading [@sigg2008expectation; @zou2006sparse].\n:::\n\n## Example\n\nHere is an example code snippet demonstrating Bayesian PCA using BI:\n\n::: {.panel-tabset group=\"language\"}\n\n## Build in functions\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi,jnp\nm=bi()\nm.data('iris.csv', sep=',') # Data is already scaled\nm.data_on_model = dict(\n    X=jnp.array(m.df.iloc[:,0:-2].values)\n)\nm.fit(m.models.pca(type=\"classic\"), progress_bar=False) # or robust, sparse, classic, sparse_robust_ard\nm.models.pca.plot(\n    X=m.df.iloc[:,0:-2].values,\n    y=m.df.iloc[:,-2].values, \n    feature_names=m.df.columns[0:-2], \n    target_names=m.df.iloc[:,-1].unique(),\n    color_var=m.df.iloc[:,0].values,\n    shape_var=m.df.iloc[:,-2].values\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](19. PCA_files/figure-pdf/cell-2-output-2.png){fig-pos='H'}\n:::\n:::\n\n\n## Standard  \n```python\ndef model(x_train, data_dim, latent_dim, num_datapoints): \n    # Gaussian prior for the principal component 'W'.\n    w = m.dist.normal(0, 1, shape=(data_dim, latent_dim), name='w')\n\n    # Gaussian prior on the latent variables 'Z'\n    z = m.dist.normal(0, 1, shape=(latent_dim, num_datapoints), name='z')\n\n    # Exponential prior on the noise variance 'epsilon'\n    epsilon = m.dist.exponential(1, name='epsilon')\n\n    # Likelihood\n    m.dist.normal(w @ z , epsilon, obs = x_train)  \n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model)\n```\n\n<!--\n## With ARD\n```python\n\nfrom BI import bi\nimport jax.numpy as jnp\n\nm = bi(platform='cpu')\n\ndef model_ARD(X, data_dim, latent_dim, num_datapoints):\n    # --- Automatic Dimensionality Selection using an ARD Prior ---\n    # There is one 'alpha' for each latent dimension.\n    # This 'alpha' acts as a prior on the \"relevance\" of each principal component. A large alpha will signal low relevance.\n    alpha = m.dist.gamma(.05, 1e-3, shape=(latent_dim,), name='alpha')\n\n    # Gaussian prior for the principal component 'W'.\n    # The precision (1 / variance) of the weights for each component is controlled by its corresponding 'alpha'.\n    w = m.dist.normal(0, 1. / jnp.sqrt(alpha)[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # Gaussian prior on the latent variables 'Z'\n    z = m.dist.normal(0, 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # Noise model\n    # We assume the noise is Gaussian, but we don't know its variance. So, we place a Gamma prior on the precision (1 / variance) to learn it from the data.\n    precision = m.dist.gamma(1.0, 1.0, name='precision')\n    # We convert the learned precision into a standard deviation for use in the likelihood.\n    stddv = 1. / jnp.sqrt(precision)\n\n    # Likelihood\n    # 'X' is modeled as a linear projection of the latents (w @ z) plus Gaussian noise.\n    m.dist.normal(w @ z, stddv, obs=X)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_ARD)\nm.summary()\n```\n\n## Robust \n```python\ndef model_robust(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    Robust Bayesian PCA model using a Student's t-distribution for the likelihood.\n    \"\"\"\n    # --- Standard Priors for W and Z ---\n    w = m.dist.normal(0, 1., shape=(data_dim, latent_dim), name='w')\n    z = m.dist.normal(0, 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # --- Robustness to Outliers via a Heavy-Tailed Noise Model ---\n    # This defines the prior on the scale (similar to standard deviation) of the noise.\n    sigma = m.dist.halfcauchy(1.0, name='sigma')\n\n    # This is a prior on the \"degrees of freedom\" ('nu') of the Student's t-distribution.\n    # This parameter controls the \"heaviness\" of the tails. A small 'nu' means\n    # heavier tails, making the model more robust to outliers. By learning this\n    # parameter, the model can adapt its robustness to the data.\n    nu = m.dist.gamma(2.0, 0.1, name='nu')\n\n    # This is the key line for this model. The likelihood is a Student's t-distribution.\n    # As your description states, this \"heavy-tailed distribution... reduces the\n    # influence of outliers\" by treating them as more plausible events than a\n    # Gaussian distribution would, thus preventing them from skewing the results.\n    m.dist.studentt(df=nu, loc=w @ z, scale=sigma, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_robust)\nm.summary()\n```\n\n## Sparse \n```python\ndef model_sparse(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    Sparse Bayesian PCA model using a Laplace prior on the weights (w).\n    \"\"\"\n    # --- Sparsity for High-Dimensional Data via a Sparsity-Inducing Prior ---\n\n    # This is the first part of a hierarchical prior (known as the Bayesian Lasso).\n    # We place a prior on 'lambda_', which will control the scale of our Laplace prior.\n    # This allows the model to learn the appropriate level of sparsity from the data.\n    lambda_ = m.dist.gamma(1.0, 1.0, shape=(latent_dim,), name='lambda')\n\n    # This is the key line for this model. We place a Laplace prior on the loadings 'W'.\n    # As your description states, this is a \"sparsity-inducing prior\". The Laplace\n    # distribution is sharply peaked at zero, which encourages many of the weight\n    # values in 'W' to be exactly zero, leading to \"more interpretable results\".\n    w = m.dist.laplace(0., 1. / lambda_[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # --- Standard Model Components ---\n    \n    # We place a standard Gaussian prior on the latent variables 'Z', as described\n    # in the note: \"We place Gaussian priors on both Z and W...\".\n    z = m.dist.normal(0., 1., shape=(latent_dim, num_datapoints), name='z')\n    \n    # This section defines the standard Gaussian noise model, which is separate\n    # from the sparsity-inducing prior on the weights.\n    precision = m.dist.gamma(1.0, 1.0, name='precision')\n    stddv = 1. / jnp.sqrt(precision)\n\n    # This is the standard likelihood, same as in the ARD model. The generative story\n    # is unchanged, but the prior on 'W' now enforces the desired sparsity property.\n    m.dist.normal(w @ z, stddv, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_sparse)\nm.summary()\n```\n\n## Sparse robust with ARD\n```python\n\ndef model_sparse_robust_ard(x_train, data_dim, latent_dim, num_datapoints):\n    \"\"\"\n    A combined Sparse, Robust Bayesian PCA model with Automatic Relevance Determination (ARD).\n\n    - Sparsity: Achieved with a Laplace prior on the weights 'w'.\n    - Robustness: Achieved with a Student's t-distribution for the likelihood.\n    - ARD: Achieved by placing a hierarchical prior on the scale of the Laplace\n      distribution, allowing entire latent dimensions to be pruned.\n    \"\"\"\n    # --- ARD and Sparsity-Inducing Prior on w ---\n    \n    # This is the ARD component. We define a relevance parameter 'lambda_' for each\n    # latent dimension. A large lambda will signal that the corresponding\n    # component is not relevant and should be shrunk away.\n    # The Gamma prior ensures lambda_ is positive.\n    lambda_ = m.dist.gamma(1.0, 1.0, shape=(latent_dim,), name='lambda')\n\n    # This is the Sparsity component. We use a Laplace prior for the weights 'w'.\n    # The COMBINED effect happens here: the scale of the Laplace distribution is\n    # controlled by the ARD parameter 'lambda_'. If a component is irrelevant\n    # (large lambda_), the scale becomes small, and the Laplace prior aggressively\n    # forces the weights in that column of 'w' to zero.\n    # This gives us both sparsity and automatic dimensionality selection.\n    w = m.dist.laplace(0., 1. / lambda_[None, :], shape=(data_dim, latent_dim), name='w')\n\n    # --- Standard Prior for Z ---\n    \n    # The prior on the latent variables 'Z' remains a standard Gaussian.\n    z = m.dist.normal(0., 1., shape=(latent_dim, num_datapoints), name='z')\n\n    # --- Robustness to Outliers via a Heavy-Tailed Noise Model ---\n    \n    # Prior on the scale of the Student's t-distribution.\n    sigma = m.dist.halfcauchy(1.0, name='sigma')\n\n    # Prior on the degrees of freedom 'nu', which controls the robustness.\n    nu = m.dist.gamma(2.0, 0.1, name='nu')\n\n    # The likelihood is the Student's t-distribution, which makes the entire\n    # model robust to outliers in the observed data 'x_train'.\n    m.dist.studentt(df=nu, loc=w @ z, scale=sigma, obs=x_train)\n\nm.data_on_model = dict(\n    x_train=x_train_h,\n    data_dim=data_dim,\n    latent_dim=4,\n    num_datapoints=num_datapoints\n)\nm.fit(model_sparse_robust_ard)\nm.summary()\n\n```\n-->\n:::\n\n\n\n\n## Mathematical Details\n\nWe assume the observed data matrix $X$ is centered and arranged with features as rows and samples as columns, $X \\in \\mathbb{R}^{N \\times V}$ where $N$ is the number of observations and $V$ the number of variables. The generative model projects the data into a lower-dimensional space with $K$ latent variables, $K \\leq V$, using the following equation:\n$$\nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n$$\n\nwhere :\n\n- $X$ is the observed data matrix.\n\n- $Z \\in \\mathbb{R}^{N \\times K}$ is the latent variable matrix (latent features) with $K \\ll D$. $Z$ is defined by a Normal distribution with mean 0 and variance 1.\n\n- $W \\in \\mathbb{R}^{K \\times V}$ is the matrix of principal components (*projection matrix*). $W$ is defined by a Normal distribution with mean 0 and variance 1.\n\n- $\\sigma$ is the standard deviation of the normal distribution.\n\nThe likelihood and priors are defined element-wise for $v=1...V$, $n=1...N$, and $k=1...K$. for the following models:\n\n### Standard PCA\n\n\n$$\nX \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n$$\n$$\nZ \\sim \\text{Normal}(0, 1)\n$$\n$$\nW \\sim \\text{Normal}(0, 1)\n$$\n$$\n\\sigma \\sim \\text{Exponential}(1)\n$$\n\n<!---\n### PCA with ARD\nIn PCA with Automatic Relevance Determination (ARD), the variance of the prior on each column of $W$ is controlled by a unique relevance parameter $\\alpha_k$.\n\n$$\nX_{dn} \\sim \\text{Normal}(Z \\cdot W, \\sigma)\n$$\n$$\nZ_{kn} \\sim \\text{Normal}(0, 1)\n$$\n$$\nW_{dk} \\sim \\text{Normal}(0, \\alpha_k^{-1})\n$$\n$$\n\\alpha_k \\sim \\text{Gamma}(0.05, 10^{-3})\n$$\n\n$$\n\\sigma \\sim \\text{Normal}(0, \\tau^{-1})\n$$\n\n$$\n\\tau \\sim \\text{Gamma}(1, 1)\n$$\n\nWhere:\n\n- $\\alpha \\in \\mathbb{R}^{K}$ is the **relevance parameter vector**. Each element $\\alpha_k$ controls the precision (inverse variance) of the weights for the *k*-th principal component. If a component is not supported by the data, its corresponding $\\alpha_k$ will become large, forcing the weights in the *k*-th column of $W$ towards zero and effectively \"switching off\" that component.\n\n- $\\tau$ is the **noise precision** (inverse variance) of the data.\n\n\n\n### Robust PCA\nIn Robust PCA, the model is modified to handle outliers by using a heavy-tailed distribution for the likelihood:\n\n$$\nX_{dn} \\sim \\text{StudentT}(\\nu, Z \\cdot W, \\sigma)\n$$\n$$\nZ_{kn} \\sim \\text{Normal}(0, 1)\n$$\n$$\nW_{dk} \\sim \\text{Normal}(0, 1)\n$$\n$$\n\\nu \\sim \\text{Gamma}(2, 0.1) \n$$\n$$\n\\sigma \\sim \\text{HalfCauchy}(1) \n$$\n\nWhere:\n\n- $\\nu$ is the **degrees of freedom** of the Student's t-distribution. It controls the \"heaviness\" of the tails. Small values of $\\nu$ lead to greater robustness. By placing a prior on it, we allow the model to learn the appropriate level of robustness from the data.\n\n- $\\sigma$ is the **scale parameter** of the Student's t-distribution, which is analogous to the standard deviation in a Normal distribution.\n\n\n### Sparse PCA\nIn Sparse PCA, a Laplace prior is placed on the weights $W$ to induce sparsity.\n\n$$\nX_{dn} \\sim \\text{Normal}(Z \\cdot W, sigma)\n$$\n$$\nZ_{kn} \\sim \\text{Normal}(0, 1)\n$$\n$$\nW_{dk} \\sim \\text{Laplace}(0, \\lambda_k^{-1})\n$$\n$$\n\\lambda_k \\sim \\text{Gamma}(1, 1) \n$$\n\n$$\n\\sigma \\sim \\text{Normal}(0, \\tau^{-1})\n$$\n$$\n\\tau \\sim \\text{Gamma}(1, 1) \n$$\n\nwhere:\n\n- $\\lambda \\in \\mathbb{R}^{K}$ is the **sparsity control vector**. Each element $\\lambda_k$ is the rate parameter (inverse scale) for the Laplace prior on the *k*-th column of $W$. A larger $\\lambda_k$ enforces stronger sparsity on that component.\n\n- $\\tau$ is the **noise precision** (inverse variance) of the data.\n\n\n\n### Sparse Robust PCA with ARD\nThis model combines all three features: a sparsity-inducing prior, a robust likelihood, and automatic relevance determination.\n\n$$\nX_{dn} \\sim \\text{StudentT}(\\nu, Z \\cdot W, \\sigma)\n$$\n$$\nZ_{kn} \\sim \\text{Normal}(0, 1)\n$$\n$$\nW_{dk} \\sim \\text{Laplace}(0, \\lambda_k^{-1}) \n$$\n$$\n\\lambda_k \\sim \\text{Gamma}(1, 1) \n$$\n$$\n\\nu \\sim \\text{Gamma}(2, 0.1) \n$$\n$$\n\\sigma \\sim \\text{HalfCauchy}(1) \n$$\n-->\n## Note \n- To account for **[<span style=\"color:#0D6EFD\">sign ambiguity ðŸ›ˆ</span>]{#signAmbiguity}** in PCA, we can set the number of latent dimensions $K$ to be equal to the number of variables $V$. Then, we can calculate the dot product between the estimated parameters and the data. If it is negative, we multiply the estimated parameters by -1 to align them with the data. Below, a code snippet highlights how to do this:\n\n\n```python\n\ntrue_params = jnp.array(real_data)      \nestimated_params = jnp.array(m.posteriors) \n\n# Compute dot product\ndot_product = jnp.dot(true_params, estimated_params)\n\n# Align signs if necessary\nif dot_product < 0:\n    estimated_params = -estimated_params\n\n# Plot the aligned parameters\nplt.scatter(true_params, estimated_params, alpha=0.7)\nplt.plot([min(true_params), max(true_params)], [min(true_params), max(true_params)], 'r--')\nplt.xlabel('True Parameters')\nplt.ylabel('Estimated Parameters')\nplt.title('True vs. Estimated Parameters After Sign Alignment')\nplt.show()\n```\n![](pca2.png)\n\n\n\n\n\n\n## Reference(s)\n\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "19. PCA_files/figure-pdf"
    ],
    "filters": []
  }
}