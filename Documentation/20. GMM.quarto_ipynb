{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Gaussian Mixture Models\"\n",
        "description: \"A probabilistic model for representing normally distributed subpopulations, often used for clustering.\"\n",
        "categories: [Clustering, Unsupervised Learning]\n",
        "image: \"Figures/19.png\"\n",
        "order: 23\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "\n",
        "To discover group structures or clusters in data, we can use a **Gaussian Mixture Model (GMM)**. This is a parametric clustering method. A GMM assumes that the data is generated from a mixture of a **pre-specified number (`K`)** of different Gaussian distributions. The model's goal is to figure out:\n",
        "\n",
        "1.  **The properties of each of the `K` clusters**: For each of the `K` clusters, it estimates its center (mean $\\mu$) and its shape/spread (covariance $\\Sigma$).\n",
        "2.  **The mixture weights**: It estimates the proportion of the data that belongs to each cluster.\n",
        "3.  **The assignment of each data point**: It determines the probability of each data point belonging to each of the $K$ clusters.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "::: callout-caution\n",
        "-   A GMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters, *except for the number of clusters, $K$*, which must be fixed in advance.\n",
        "\n",
        "-   The key parameters and their priors are:\n",
        "\n",
        "    -   **Number of Clusters $K$**: This is a **fixed hyperparameter** that you must choose before running the model. Choosing the right `K` often involves running the model multiple times and using model comparison criteria (like cross-validation, AIC, or BIC).\n",
        "    -   **Cluster Weights `w`**: These are the probabilities of drawing a data point from any given cluster. Since there are a fixed number `K` of them and they must sum to 1, they are typically given a `Dirichlet` prior. A symmetric `Dirichlet` prior (e.g., `Dirichlet(1, 1, ..., 1)`) represents an initial belief that all clusters are equally likely.\n",
        "    -   **Cluster Parameters ($\\mu$, $\\Sigma$): Each of the `K` clusters has a mean $\\mu$ and a covariance matrix $\\Sigma$. We place priors on these to define our beliefs about their plausible values.\n",
        "\n",
        "-   Like the DPMM, the model is often implemented in its marginalized form . Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n",
        "  \n",
        "- To increase accuracy we run a k-means algorithm to initialize the cluster mean priors.\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "Below is an example of a GMM implemented in BI. The goal is to cluster a synthetic dataset into a pre-specified K=4 groups.\n",
        "\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "## Python"
      ],
      "id": "a653658e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi, jnp\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "m = bi()\n",
        "# Generate synthetic data\n",
        "data, true_labels = make_blobs(\n",
        "    n_samples=500, centers=8, cluster_std=0.8,\n",
        "    center_box=(-10,10), random_state=101\n",
        ")\n",
        "\n",
        "\n",
        "#  The model\n",
        "def gmm(data, K, initial_means): # Here K is the *exact* number of clusters\n",
        "    D = data.shape[1]  # Number of features\n",
        "    alpha_prior = 0.5 * jnp.ones(K)\n",
        "    w = m.dist.dirichlet(concentration=alpha_prior, name='weights') \n",
        "\n",
        "    with m.dist.plate(\"components\", K): # Use fixed K\n",
        "        mu = m.dist.multivariate_normal(loc=initial_means, covariance_matrix=0.1*jnp.eye(D), name='mu')        \n",
        "        sigma = m.dist.half_cauchy(1, shape=(D,), event=1, name='sigma')\n",
        "        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name='Lcorr')\n",
        "\n",
        "        scale_tril = sigma[..., None] * Lcorr\n",
        "\n",
        "    m.dist.mixture_same_family(\n",
        "        mixing_distribution=m.dist.categorical(probs=w, create_obj=True),\n",
        "        component_distribution=m.dist.multivariate_normal(loc=mu, scale_tril=scale_tril, create_obj=True),\n",
        "        name=\"obs\",\n",
        "        obs=data\n",
        "    )\n",
        "\n",
        "# Kmeans clustering do initiate the means\n",
        "m.ml.KMEANS(data, n_clusters=8)\n",
        "m.data_on_model = {\"data\": data,\"K\": 8 }\n",
        "m.data_on_model['initial_means'] = m.ml.results['centroids']\n",
        "\n",
        "\n",
        "m.fit(gmm) # Optimize model parameters through MCMC sampling\n",
        "m.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM"
      ],
      "id": "b0121e33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R\n",
        "\n",
        "![](travaux-routiers.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## Julia\n",
        "```julia\n",
        "@BI function gmm(data, K, initial_means)\n",
        "    D = data.shape[1] \n",
        "    alpha_prior = 0.5 * jnp.ones(K)\n",
        "    w = m.dist.dirichlet(concentration=alpha_prior, name=\"weights\") \n",
        "\n",
        "    # We capture the output of the pywith block\n",
        "    # The block returns a tuple (mu, scale_tril)\n",
        "    mu, scale_tril = pywith(m.dist.plate(\"components\", K)) do _\n",
        "        mu_inner = m.dist.multivariate_normal(\n",
        "            loc=initial_means, \n",
        "            covariance_matrix=0.1*jnp.eye(D), \n",
        "            name=\"mu\"\n",
        "        )        \n",
        "        \n",
        "        sigma = m.dist.half_cauchy(1, shape=(D,), event=1, name=\"sigma\")\n",
        "        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name=\"Lcorr\")\n",
        "\n",
        "        # FIX: Use expand_dims instead of slicing\n",
        "        scale_tril_inner = jnp.expand_dims(sigma, -1) * Lcorr\n",
        "        \n",
        "        # Return them so they are available outside\n",
        "        (mu_inner, scale_tril_inner)\n",
        "    end\n",
        "\n",
        "    m.dist.mixture_same_family(\n",
        "        mixing_distribution=m.dist.categorical(probs=w, create_obj=true),\n",
        "        component_distribution=m.dist.multivariate_normal(loc=mu, scale_tril=scale_tril, create_obj=true),\n",
        "        name=\"obs\",\n",
        "        obs=data\n",
        "    )\n",
        "end\n",
        "\n",
        "# Run\n",
        "m.fit(gmm)\n",
        "m.summary()\n",
        "```\n",
        "\n",
        ":::\n",
        "## Mathematical Details\n",
        "\n",
        "\n",
        "This section describes the generative process for a GMM. \n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "Y_{i,1} \\\\\n",
        "\\vdots \\\\\n",
        "Y_{i,D}\n",
        "\\end{pmatrix}\n",
        "\\sim \n",
        "\\text{MVN}\\left(\n",
        "\\begin{pmatrix}\n",
        "\\mu_{z_i,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{z_i,D}\n",
        "\\end{pmatrix},\n",
        "\\Sigma_{z_i}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\mu_{k,1} \\\\\n",
        "\\vdots \\\\\n",
        "\\mu_{k,D}\n",
        "\\end{pmatrix}\n",
        "\\sim \n",
        "\\text{MVN}\\left(\n",
        "\\begin{pmatrix}\n",
        "A_{k,1} \\\\\n",
        "\\vdots \\\\\n",
        "A_{k,D}\n",
        "\\end{pmatrix},\n",
        "B\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Sigma_k = \\text{Diag}(\\sigma_k) \\Omega_k  \\text{Diag}(\\sigma_k)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{[k,d]} \\sim \\text{HalfCauchy}(1) \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Omega_k \\sim \\text{LKJ}(2) \n",
        "$$\n",
        "\n",
        "$$\n",
        "z_{i} \\sim \\text{Categorical}(\\pi) \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\pi \\sim \\text{Dirichlet}(0.5, \\dots, 0.5)\n",
        "$$\n",
        "\n",
        "\n",
        "Where : \n",
        "\n",
        "*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n",
        "*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n",
        "*   $\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix}$ is a prior for the $k$-th mean vector as derived by a *KMEANS* clustering algorithm. \n",
        "*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n",
        "*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n",
        "*  $\\text{Diag}(\\sigma_k)$ is a diagonal matrix whose diagonal entries are the standard deviations:\n",
        "  $$\n",
        "  \\text{Diag}(\\sigma_k) =\n",
        "  \\begin{pmatrix}\n",
        "  \\sigma_{[k,1]} & 0 & \\cdots & 0 \\\\\n",
        "  0 & \\sigma_{[k,2]} &        & \\vdots \\\\\n",
        "  \\vdots &        & \\ddots & 0 \\\\\n",
        "  0 & \\cdots & 0 & \\sigma_{[k,D]}\n",
        "  \\end{pmatrix}.\n",
        "  $$\n",
        "\n",
        "*   $\\sigma_{k}$ is a $D$-vector of standard deviations for the $k$-th cluster where each element, $d$, has a half-cauchy prior.\n",
        "*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n",
        "*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n",
        "*   $\\pi$ is a vector of $K$ cluster weights.\n",
        "\n",
        "\n",
        "Where : \n",
        "\n",
        "*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n",
        "\n",
        "*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n",
        "\n",
        "*   $\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix}$ is a prior for the $k$-th mean vector as derived by a *KMEANS* clustering algorithm. \n",
        "\n",
        "*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n",
        "\n",
        "*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n",
        "\n",
        "*   $\\sigma_k$ is a diagonal matrix of standard deviations for the $k$-th cluster.\n",
        "\n",
        "*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n",
        "\n",
        "*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n",
        "\n",
        "*   $\\pi$ is a vector of $K$ cluster weights.\n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        "::: callout-note\n",
        "The primary challenge of the GMM compared to the DPMM is the need to **manually specify the number of clusters `K`**. If the chosen `K` is too small, the model may merge distinct clusters. If `K` is too large, it may split natural clusters into meaningless sub-groups. Therefore, applying a GMM often involves an outer loop of model selection where one fits the model for a range of `K` values and uses a scoring metric to select the best one.\n",
        ":::\n",
        "\n",
        "## Reference(s)\n",
        "C. M. Bishop (2006). *Pattern Recognition and Machine Learning*. Springer. (Chapter 9)"
      ],
      "id": "163f7a44"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}