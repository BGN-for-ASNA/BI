% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={BI documentation},
  pdfauthor={Sebastian Sosa},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{BI documentation}
\author{Sebastian Sosa}
\date{2024-09-09}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{}\label{section}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

\section{1.1 Model set-up}\label{model-set-up}

We define a likelihood (e.g., a mathematical formula that specifies the
plausibility of the data). The likelihood has parameters (e.g.,
adjustable inputs) for which we define priors (e.g., initial
plausibility assignment for each possible value of the parameter).
Considering a linear regression with an intercept (e.g., \(Œº\) value
when \(x\) is at zero, or at the mean if the data is centered), a slope
(e.g., \(Œº\) change value when \(x\) is incremented by one unit), and
assuming the data is centered ({ as we will always consider in the next
sections}):

{ * Toolpit available for each lines of equation }

\href{bi/doc/0.\%20\%Introduction.md}{\[y \sim  Normal(Œº,œÉ)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œº \sim Œ± + Œ≤x
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œ± \sim Normal(0,1)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œ≤ \sim Normal(0,1)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ œÉ \sim Uniform(0,1)
\]}

\section{1.2 Model fitting}\label{model-fitting}

By using probability distributions for parameters, we can better tune
the model by describing parameters with `\emph{subequations}' and
accounting for \emph{correlated varying effects}, \emph{Gaussian
processes}, \emph{measurement error}, and \emph{missing data}.

In addition, we can use \emph{Bayesian updating} using the
\emph{Bayesian theorem} to `reshape' the prior distributions by
considering every possible combination of values for ¬µ and œÉ and scoring
each combination by its relative plausibility in light of the data.
These relative plausibilities are the posterior probabilities of each
combination of values ¬µ and œÉ: the \emph{posterior distributions}.
Various techniques can be used to approximate the mathematical
definition of Bayes' theorem: grid approximation, quadratic
approximation, and Markov chain Monte Carlo (\emph{MCMC}).

\hyperref[]{\[\frac{likelihood*Priors}{average likelihood}\]}

\section{1.3 Model `diagnostic'}\label{model-diagnostic}

The posterior distribution can be described using percentile intervals
(\emph{PI}), the highest posterior density interval (\emph{HPDI}), and
point estimates. We can also sample the posterior distribution and
generate \emph{dummy data}, which can help us to check and validate the
model through \emph{observations and p uncertainty propagation on the
samples}. In some aspects, it is the opposite of a null model as it
represents an expected model.

\section{1.4 Link functions}\label{link-functions}

We will see different families of regressions that have different
distribtions. For the moment, we just need to know that those different
distribtions required \_link function (for each specific family we will
discuss the corresponding link function):

\section{Vocabulary}\label{vocabulary}

This method evaluates if the variable that we want to predict -the
dependent variable (\emph{Y})- and the variable(s) that may
affect-independent variables (\emph{Xs})- this dependent variable is

\section{Considerations}\label{considerations}

When implementing Bayesian linear regression with TensorFlow
Probability, it's important to consider the following: - Specifying
appropriate prior distributions for the model parameters. - Choosing an
appropriate likelihood function that captures the relationship between
the inputs and outputs. - Selecting an inference method to approximate
the posterior distribution over parameters, such as Markov chain Monte
Carlo (MCMC) or variational inference.

\bookmarksetup{startatroot}

\chapter{Linear Regression for continuous
variable}\label{linear-regression-for-continuous-variable}

\section{General Principles}\label{general-principles}

To study relationships between a continuous independent variable and a
continuous dependent variable (e.g., height and weight), we can use
linear regression. Essentially, we draw a line that passes through the
point cloud of the two variables being tested. For this, we need to
have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  An intercept \(\alpha\), which represents the origin of the line---the
  expected value of the dependent variable (height) when the independent
  variable (weight) is equal to zero.
\item
  A coefficient \(\beta\), which informs us about the slope of the line.
  In other words, it tells us how much Y (height) increases for each
  increment of the independent variable (weight).
\item
  A variance term \(\sigma\), which informs us about the spread of
  points around the line, i.e., the variance around the prediction.
\end{enumerate}

\includegraphics{index_files/mediabag/1-WCcaObzvvVzcrg8CBi.webp}

\section{Considerations}\label{considerations-1}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  Bayesian models consider \phantomsection\label{uncertainty}{{model
  parameter uncertainty üõà}}, allowing for the quantification of
  confidence or uncertainty through the parameters'
  \phantomsection\label{posterior}{{posterior distribution üõà}}.
  Therefore, we need to declare \phantomsection\label{prior}{{prior
  distributions üõà}} for each model parameter, in your case for:
  \(\alpha\), \(\beta\), and \(\sigma^2\).
\item
  Prior distributions are built following these considerations:

  \begin{itemize}
  \item
    As the data is \phantomsection\label{scaled}{{scaled üõà}} (see
    introduction), we can use a Normal distribution for \(\alpha\) and
    \(\beta\), with a mean of 0 and a standard deviation of 1.
  \item
    Since \(\sigma\) is strictly positive, we can use any distribution
    that is positively defined, such as the Exponential or Gamma
    distribution.
  \end{itemize}
\item
  Gaussian regression deals directly with continuous outcomes,
  estimating a linear relationship between predictors and the outcome
  variable without needing a \phantomsection\label{linkF}{{link function
  üõà}}. This simplifies interpretation, as coefficients represent direct
  changes in the outcome variable.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example}

Below is an example code snippet demonstrating Bayesian linear
regression using the Bayesian Inference (BI) package. Data consist of
two continuous variables (height and weight), and the goal is to
estimate the effect of weight on height.

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ .bi.main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Howell1.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.df }\OperatorTok{=}\NormalTok{ m.df[m.df.age }\OperatorTok{\textgreater{}} \DecValTok{18}\NormalTok{] }\CommentTok{\# Manipulate}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Scale}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(height, weight): }
    \CommentTok{\# Parameters priors distributions   }
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{178}\NormalTok{, }\DecValTok{20}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)   }
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.uniform(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"height"}\NormalTok{, Normal(alpha }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ weight, sigma), obs }\OperatorTok{=}\NormalTok{ height)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model)  }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/Howell1.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}

\CommentTok{\# fileter data frame}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{df }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{df[m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textgreater{}} \DecValTok{18}\NormalTok{,]}

\CommentTok{\# Scale}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{scale}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{)) }

\CommentTok{\# convert data to jax arrays}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{))}

\FunctionTok{bi.dist.uniform}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}\AttributeTok{sample=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(height, weight)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  s }\OtherTok{=} \FunctionTok{bi.dist.uniform}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  a }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{178}\NormalTok{, }\DecValTok{20}\NormalTok{,  }\AttributeTok{name =} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  b }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
  
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{,  }\FunctionTok{bi.dist.normal}\NormalTok{(a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{*}\NormalTok{ weight, s), }\AttributeTok{obs =}\NormalTok{ height)}
\NormalTok{\}}


\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation}

The following equation allows us to draw a line: \[
Y_i = \alpha + \beta  X_i + \sigma_i
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta\) is the regression coefficient.
\item
  \(X_i\) is the input variable for observation \emph{i}.
\item
  \(\sigma_i\) is a vector of error terms for observation \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express a Bayesian
version of this regression model using the following model:

\[
Y_i \sim Normal(\alpha + \beta   X_i, \sigma)
\]

\[
\alpha \sim Normal(0, 1)
\]

\[
\beta \sim Normal(0, 1)
\]

\[
\sigma \sim Uniform(0, 50)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is dependent variable for observation \emph{i}.
\item
  \(\alpha\) and \(\beta\) are the regression coefficients and intercept
  parameters, respectively.
\item
  \(X_i\) is the input variable for observation \emph{i}.
\item
  \(\sigma\) is a prior for the variance term standard deviation of the
  normal distribution that describes the variance in the relationship
  between the dependent variable \(Y\) and the independent variable
  \(X\).
\end{itemize}

\section{Notes}\label{notes}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

We can observe a difference between the \emph{Frequentist} and the
\emph{Bayesian} formulation regarding the error term. Indeed, in the
\emph{Frequentist} formulation, the error term \(\sigma_i\) represents
random fluctuations around the predicted values. This assumption leads
to point estimates for \(\alpha\) and \(\beta\), without accounting for
uncertainty in these estimates. In contrast, the \emph{Bayesian}
formulation treats \(\sigma\) as a parameter with its own prior
distribution. This allows us to incorporate our uncertainty about the
error term into the model.

\end{tcolorbox}

\section{Reference(s)}\label{references}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Multiple continuous variables
model}\label{multiple-continuous-variables-model}

\section{General Principles}\label{general-principles-1}

To study relationships between multiple continuous variables (e.g., the
effect of weight and age on height), we can use a multiple regression
approach. Essentially, we extend
\href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Linear
Regression for continuous variable} by adding a regression coefficient
\(\beta_x\) for each continuous variable (e.g., \(\beta_{weight}\) and
\(\beta_{age}\)).

\includegraphics{index_files/mediabag/0-dJqdzk1aMo2OQR7O.pdf}

\section{Considerations}\label{considerations-2}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for the
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  The model interpretation of the regression coefficients \(\beta_x\) is
  considered for fixed values of the other independent variable(s)'
  regression coefficients ---i.e., for a given age, \(\beta_{weight}\)
  represents the expected change in the dependent variable variation
  (height) for each one-unit increase in weight, holding all other
  variable(s) constant (age).
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-1}

Below is example code demonstrating Bayesian multiple linear regression
using the Bayesian Inference (BI) package. Data consist of three
continuous variables (\emph{height}, \emph{weight}, \emph{age}), and the
goal is to estimate the effect of \emph{weight} and \emph{age} on
\emph{height}.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Howell1.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Import}
\NormalTok{m.df }\OperatorTok{=}\NormalTok{ m.df[m.df.age }\OperatorTok{\textgreater{}} \DecValTok{18}\NormalTok{] }\CommentTok{\# Manipulate}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Scale}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(height, weight, age):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)    }
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta1\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta2\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ bi.dist.uniform(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(alpha }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ weight }\OperatorTok{+}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ age, sigma), obs }\OperatorTok{=}\NormalTok{ height)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}../data/Howell1.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{df }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{df[m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textgreater{}} \DecValTok{18}\NormalTok{,] }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{scale}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Scale}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(height, weight, age)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta1 }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta2 }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(  }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b2\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))   }
\NormalTok{  sigma }\OtherTok{=} \FunctionTok{bi.dist.uniform}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\FunctionTok{bi.dist.normal}\NormalTok{(alpha }\SpecialCharTok{+}\NormalTok{ beta1 }\SpecialCharTok{*}\NormalTok{ weight }\SpecialCharTok{+}\NormalTok{ beta2 }\SpecialCharTok{*}\NormalTok{ age, sigma), }\AttributeTok{obs=}\NormalTok{height)}
\NormalTok{\}}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-1}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-1}

We model the relationship between the independent variables
\((X_{1i}, X_{2i}, ..., X_{ni})\) and the dependent variable \emph{Y}
using the following equation:

\[
ùëå_i = \alpha +\beta_1  ùëã_{1i} + \beta_2  ùëã_{2i} + ... + \beta_n  ùëã_{ni} + \sigma_i
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(X_{1i}\), \(X_{2i}\), \ldots, \(X_{ni}\) are the values of the
  independent variables for observation \emph{i}.
\item
  \(\beta_1\), \(\beta_2\), \ldots, \(\beta_n\) are the regression
  coefficients.
\item
  \(\sigma_i\) is a vector of error terms for observation \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-1}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
model as follows:

\[
ùëå \sim Normal(\alpha + \sum_k^n  \beta_k  X, œÉ¬≤)
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\beta_k \sim Normal(0,1)
\]

\[
œÉ \sim Uniform(0, 50)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the prior distribution for the intercept.
\item
  \(\beta_k\) are the prior distributions for the regression
  coefficients \emph{k} distinct regression coefficients.
\item
  \(X_{1i}\), \(X_{2i}\), \ldots, \(X_{ni}\) are the values of the
  independent variables for observation \emph{i}.
\item
  \(\sigma\) is the prior distribution for the standard deviation,
  ensuring that it is positive.
\end{itemize}

\section{Reference(s)}\label{references-1}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Interaction terms}\label{interaction-terms}

\section{General Principles}\label{general-principles-2}

To study relationships between two independent continuous variables and
their interaction effect on a dependent variable (e.g., temperature and
humidity affecting energy consumption), we can use Regression Analysis
with Interaction Terms. In this approach, we extend the simple linear
regression model to include an interaction term (a multiplication)
between the two continuous variables.

Parallel lines indicate that there is no interaction effect, while
different slopes suggest that one might be present. Below is the plot
for temperature x humidity. The crossed lines on the graph suggest that
there is an interaction effect. The graph shows that energy consumption
levels are higher for humidity when the temperature is high. Conversely,
energy consumption levels are higher for temperature when humidity is
low.

\includegraphics{index_files/mediabag/interactions_plot_ca.png}

\section{Considerations}\label{considerations-3}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same assumptions as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We wish to model the relationship between independent variable
  \emph{Y} and dependent variable \(X_1\) to vary as a function of
  dependent variable \(X_2\). To do this, we explicitly model the
  hypothesis that the slope between \emph{Y} and \(X_1\) depends---is
  conditional---upon \(X_2\).
\item
  For continuous interactions with scaled data, the intercept becomes
  the \phantomsection\label{grandMean}{{grand mean üõà}} of the outcome
  variable.
\item
  The interpretation of estimates is more complex. The estimate of
  non-interaction terms reflects the expected change in \emph{Y} when
  \(X_1\) increases by one unit, holding \(X_2\) constant at its average
  value. The estimate of interaction terms represents how the effect of
  \(X_1\) on \emph{Y} changes depending on the value of \(X_2\), and
  vice versa, showing how the relationship between the two variables
  influences the outcome \emph{Y}.
\item
  \phantomsection\label{triptych}{{Triptych üõà}} plots are very handy for
  understanding the impact of interactions, especially when more than
  two interactions are present.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-2}

Below is example code demonstrating Bayesian regression with an
interaction term between two continuous variables using the Bayesian
Inference (BI) package. Data consist of three continuous variables
(temperature, humidity, energy consumption), and the goal is to estimate
the effect of the interaction between temperature and humidity on energy
consumption.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/tulips.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Scale}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(blooms, shade, water):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}beta1\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}beta2\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta\_interaction\_ }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}beta\_interaction\_\textquotesingle{}}\NormalTok{)    }
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(alpha }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ water }\OperatorTok{+}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ shade }\OperatorTok{+}\NormalTok{ beta\_interaction\_ }\OperatorTok{*}\NormalTok{ water }\OperatorTok{*}\NormalTok{ shade, sigma), obs}\OperatorTok{=}\NormalTok{blooms)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/tulips.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{scale}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Scale}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(blooms, water,shade)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta1 }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\DecValTok{0}\NormalTok{,  }\FloatTok{0.25}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta2 }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(  }\DecValTok{0}\NormalTok{,  }\FloatTok{0.25}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b2\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))   }
\NormalTok{  beta\_interaction\_ }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(  }\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}bint\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{)) }
\NormalTok{  sigma }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{,}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\FunctionTok{bi.dist.normal}\NormalTok{(alpha }\SpecialCharTok{+}\NormalTok{ beta1}\SpecialCharTok{*}\NormalTok{water }\SpecialCharTok{+}\NormalTok{ beta2}\SpecialCharTok{*}\NormalTok{shade }\SpecialCharTok{+}\NormalTok{ beta\_interaction\_}\SpecialCharTok{*}\NormalTok{water}\SpecialCharTok{*}\NormalTok{shade, sigma), }\AttributeTok{obs=}\NormalTok{blooms)}
\NormalTok{\}}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-2}

\section{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-2}

We model the relationship between the input features (\(X_1\) and
\(X_2\)) and the target variable (\(Y\)) using the following equation:
\[
ùëå_i = \alpha + \beta_1 ùëã_{1i} + \beta_2 ùëã_{2i} + \beta_{interaction} ùëã_{1i} ùëã_{2i} + \sigma
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(X_{1i}\) and \(X_{2i}\) are the two values of the independent
  continuous variables for observation \emph{i}.
\item
  \(\beta_1\) and \(\beta_2\) are the regression coefficients for
  \(X_{1}\) and \(X_{2}\), respectively.
\item
  \(\beta_{interaction}\) is the regression coefficient for the
  interaction term \((X_{1}  X_{2})\).
\item
  \(\sigma\) is the error term, assumed to be normally distributed.
\end{itemize}

In this context, the interaction term \(X_{1i} * X_{2i}\) captures the
joint effect of \(X_{1i}\) and \(X_{2i}\) on the target variable
\(Y_i\).

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-2}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
Y \sim Normal(\alpha +  \beta_1  X_{1i}‚Äã + \beta_2  X_{2i}‚Äã‚Äã + \beta_{interaction}  X_1{1i} X_{2i}‚Äã ,  \sigma)
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\beta_1 \sim Normal(0,1)
\]

\[
\beta_2 \sim Normal(0,1)
\]

\[
\beta_{interaction} \sim Normal(0,1)
\]

\[
œÉ \sim Exponential(1)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the prior distribution for the intercept.
\item
  \(\beta_1\), \(\beta_2\), and \(\beta_{interaction}\) are the prior
  distributions for the regression coefficients.
\item
  \(X_{1i}\) and \(X_{2i}\) are the two values of the independent
  continuous variables for observation \emph{i}.
\item
  \(\sigma\) is the prior distribution for the standard deviation,
  ensuring it is positive.
\end{itemize}

\section{Reference(s)}\label{references-2}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Regression for Categorical
Variables}\label{regression-for-categorical-variables}

\section{General Principles}\label{general-principles-3}

To study the relationship between a categorical independent variable and
a continuous dependent variable, we use a \emph{Categorical model} which
applies \emph{stratification}.

\emph{Stratification} involves modeling how the \emph{k} different
categories of the independent variable affect the target continuous
variable by performing a regression for each \emph{k} category and
assigning a regression coefficient for each category. To implement
stratification, categorical variables are often encoded using
\phantomsection\label{ohe}{{one-hot encoding üõà}} or by converting
categories to \phantomsection\label{indices}{{indices üõà}}.

\includegraphics{index_files/mediabag/tumblr_inline_o8j406.png}

\section{Considerations}\label{considerations-4}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  As we generate regression coefficients for each \emph{k} category, we
  need to specify a prior with a shape equal to the number of categories
  \emph{k} in the code (see comments in the code).
\item
  To compare differences between categories, we need to compute the
  distribution of the differences between categories, known as the
  contrast distribution. \textbf{Never compare confidence intervals or
  p-values directly}.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-3}

Below is example code that demonstrates Bayesian regression with an
independent categorical variable using the Bayesian Inference (BI)
package. The data consist of one continuous dependent variable
(\emph{kcal\_per\_g}), representing the caloric value of milk per gram,
and a categorical independent variable, representing species clade
membership. The goal is to estimate the differences in milk calories
between clades.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/milk.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.index([}\StringTok{"clade"}\NormalTok{]) }\CommentTok{\# Manipulate}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Scale}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{, }\StringTok{"index\_clade"}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(kcal\_per\_g, index\_clade):    }
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{,), name}\OperatorTok{=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)  }\CommentTok{\# we specify a vector of length 4 as we have 4 categories}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(beta[index\_clade], sigma), obs}\OperatorTok{=}\NormalTok{kcal\_per\_g)}


\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/milk.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{scale}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{index}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}clade\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Scale}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}index\_clade\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(kcal\_per\_g, index\_clade)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  beta }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  sigma }\OtherTok{=}\FunctionTok{bi.dist.exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{,}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"Y"}\NormalTok{, }\FunctionTok{bi.dist.normal}\NormalTok{(beta[index\_clade], sigma), }\AttributeTok{obs=}\NormalTok{kcal\_per\_g)}
\NormalTok{\}}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{() }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-3}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-3}

We model the relationship between the categorical input feature (X) and
the target variable (Y) using the following equation:

\[
Y_i = \alpha + \beta_k X_i + \sigma
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta_k\) are the regression coefficients for each \emph{k}
  category.
\item
  \(X_i\) is the encoded categorical input variable for observation
  \emph{i}.
\item
  \(\sigma\) is the error term.
\end{itemize}

We can interpret \(\beta_i\) as the effect of each category on \(Y\)
relative to the baseline (usually one of the categories or the
intercept).

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-3}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
Y \sim Normal(\alpha +  \beta_k X, \sigma)
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\beta_k \sim Normal(0,1)
\]

\[
\sigma \sim Exponential(1)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\alpha\) is the prior distribution for the intercept.
\item
  \(\beta_k\) are \emph{k} prior distributions for \emph{k} regression
  coefficients.
\item
  \(X_i\) is the encoded categorical input variable for observation
  \emph{i}.
\item
  \(\sigma\) is the prior distribution for the standard deviation,
  ensuring it is positive.
\end{itemize}

\section{Notes}\label{notes-1}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply multiple variables similarly as in
  \href{2.\%20Multiple\%20continuous\%20Variables.qmd}{Chapter 2:
  Multiple Continuous Variables}.
\item
  We can apply interaction terms similarly as in
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{Chapter
  3: Interaction between Continuous Variables}.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-3}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Binomial Model}\label{binomial-model}

\section{General Principles}\label{general-principles-4}

To model the relationship between a binary dependent variable ---e.g.,
success/failure, yes/no, or 1/0--- and one or more independent
variables, we can use a \emph{Binomial model}.

\includegraphics{index_files/mediabag/xHlvv.png}

\section{Considerations}\label{considerations-5}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We have the first \phantomsection\label{linkF}{{link function üõà}}
  \emph{logit}. The \emph{logit} link function in the Bayesian binomial
  model converts the linear combination of predictor variables into
  probabilities, making it suitable for modeling binary outcomes. It
  helps to estimate the relationship between predictors and the
  probability of success, ensuring results fall within the bounds of the
  binomial distribution.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-4}

Below is an example code snippet that demonstrates Bayesian binomial
regression using the Bayesian Inference (BI) package. The data consist
of one binary dependent variable (\emph{pulled\_left}), which represents
the side individuals pulled. The goal is to evaluate the probability of
pulling the left side.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/chimpanzees.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}actor\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}side\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cond\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(pulled\_left, actor, side, cond):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(logits}\OperatorTok{=}\NormalTok{alpha), obs}\OperatorTok{=}\NormalTok{pulled\_left)}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, init\_strategy}\OperatorTok{=}\NormalTok{numpyro.infer.initialization.init\_to\_mean()) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/chimpanzees.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pulled\_left)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{( }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"Y"}\NormalTok{, }\FunctionTok{bi.dist.binomial}\NormalTok{(}\AttributeTok{logits =}\NormalTok{ alpha), }\AttributeTok{obs=}\NormalTok{pulled\_left)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{() }\CommentTok{\# Get posterior distributions}

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-4}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-4}

We model the relationship between the independent variable (\(X_i\)) and
the binary dependent variable (\(Y_i\)) using the following equation: \[
logit(Y_i) = \alpha + \beta X_i 
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the probability of success (i.e., the probability of the
  binary outcome being 1) for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta\) is the regression coefficient.
\item
  \(X_i\) is the value of the independent variable for observation
  \emph{i}.
\item
  \(logit(Y_i)\) is the log-odds of success, calculated as the log of
  the odds ratio of success. Through this link function, the
  relationship between the independent variables and the log-odds of
  success is modeled linearly, allowing us to interpret the effect of
  each independent variable on the log-odds of success for observation
  \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-4}

\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[ 
Y_i \sim Binomial(n = 1, p)
\]

\[
logit(p) \sim \alpha + \beta X_i
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\beta \sim Normal(0,1)
\]

Where:

\begin{itemize}
\item
  \(Y\) is the probability of success (i.e., the probability of the
  binary outcome being 1) for observation \emph{i}.
\item
  \(n = 1\) represents the number of trials in the binomial distribution
  (binary outcome).
\item
  \(\beta\) and \(\alpha\) are the prior distributions for the
  regression coefficients and intercept, respectively.
\item
  \(logit\) is the log-odds of success, calculated as the log of the
  odds ratio of success. Through this link function, the relationship
  between the independent variables and the log-odds of success is
  modeled linearly, allowing us to interpret the effect of each
  independent variable on the log-odds of success for observation
  \emph{i}.
\end{itemize}

\section{Notes}\label{notes-2}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply multiple variables similarly as in
  \href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as in
  \href{/3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply categorical variables similarly as in
  \href{4.\%20Categorical\%20variable.qmd}{chapter 4}.
\item
  Below is an example code snippet demonstrating a Bayesian binomial
  model for multiple categorical variables using the Bayesian Inference
  (BI) package. The data consist of one binary dependent variable
  (\emph{pulled\_left}), which represents the side individuals pulled,
  and three independent variables (\emph{actor}, \emph{side},
  \emph{cond}). The goal is to evaluate, for each individual, the
  probability of pulling the left side, accounting for whether the
  individual is left-handed or right-handed, as well as the different
  conditions.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/chimpanzees.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df[}\StringTok{"side"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.prosoc\_left  }\CommentTok{\# right 0, left 1}
\NormalTok{m.df[}\StringTok{"cond"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.condition  }\CommentTok{\# no partner 0, partner 1}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{, }\StringTok{"actor"}\NormalTok{, }\StringTok{"side"}\NormalTok{, }\StringTok{"cond"}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(pulled\_left):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,), name}\OperatorTok{=}\StringTok{"alpha"}\NormalTok{)  }\CommentTok{\# generating k intercepts (one for each actor)}
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{,), name}\OperatorTok{=}\StringTok{"beta"}\NormalTok{)  }\CommentTok{\# generating k regression coefficients for each k prosoc\_left}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{,), name}\OperatorTok{=}\StringTok{"beta"}\NormalTok{)  }\CommentTok{\# generating k regression coefficients for each k condition}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(logits}\OperatorTok{=}\NormalTok{alpha[actor] }\OperatorTok{+}\NormalTok{ beta1[side] }\OperatorTok{+}\NormalTok{ beta2[cond]), obs}\OperatorTok{=}\NormalTok{pulled\_left)}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, init\_strategy}\OperatorTok{=}\NormalTok{numpyro.infer.initialization.init\_to\_mean()) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}

\NormalTok{:::}

\CommentTok{\#\# Reference(s)}
\AttributeTok{@mcelreath2018statistical}



\NormalTok{\textasciigrave{}}\OperatorTok{\textless{}!{-}{-}}\NormalTok{ quarto}\OperatorTok{{-}}\BuiltInTok{file}\OperatorTok{{-}}\NormalTok{metadata: eyJyZXNvdXJjZURpciI6Ii4ifQ}\OperatorTok{==} \OperatorTok{{-}{-}\textgreater{}}\NormalTok{\textasciigrave{}\{}\OperatorTok{=}\NormalTok{html\}}

\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{}\OperatorTok{=}\NormalTok{html\}}
\OperatorTok{\textless{}!{-}{-}}\NormalTok{ quarto}\OperatorTok{{-}}\BuiltInTok{file}\OperatorTok{{-}}\NormalTok{metadata: eyJyZXNvdXJjZURpciI6Ii4iLCJib29rSXRlbVR5cGUiOiJjaGFwdGVyIiwiYm9va0l0ZW1OdW1iZXIiOjgsImJvb2tJdGVtRmlsZSI6IjYuIEJldGEgYmlub21pYWwgbW9kZWwucW1kIiwiYm9va0l0ZW1EZXB0aCI6MH0}\OperatorTok{=} \OperatorTok{{-}{-}\textgreater{}}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\chapter{Beta-Binomial Model}\label{beta-binomial-model}

\section{General Principles}\label{general-principles-5}

To model the relationship between a binary outcome variable representing
success counts and one or more independent variables with
\phantomsection\label{overdispersion}{{overdispersion üõà}}, we can use
the \emph{Beta-Binomial model}.

\section{Considerations}\label{considerations-6}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{5.\%20Binomial\%20model.qmd}{Binomial regression}.
\item
  A Beta-Binomial model assumes that each binomial count observation has
  its own probability of success. The model estimates the distribution
  of probabilities of success across cases, instead of a single
  probability of success.
\item
  A Beta distribution has two parameters: the rates for each
  probabilities and a shape parameter Œ∏. Œ∏ influence how probabilities
  are distributed between 0 and 1. Specifically, it consists of two
  parameters, \(\gamma\) and \(\eta\) , which determine the
  concentration of probability around 0 and 1. The values of ùõº and ùõΩ
  shape the distribution:

  \begin{itemize}
  \item
    If both are greater than 1, the distribution is bell-shaped and
    centered around the mean ùëù.
  \item
    If \(\gamma\) \textless{} 1 and \(\eta\) \textless{} 1, the
    distribution is U-shaped, indicating that outcomes are more likely
    to be near 0 or 1.
  \item
    If ùõº = \(\gamma\) and \(\eta\) = 1, the Beta distribution is
    uniform.
  \end{itemize}
\end{itemize}

Thus, the shape parameters ùõº \(\gamma\) \(\eta\) provide flexibility in
modeling various types of prior beliefs about probabilities.

\end{tcolorbox}

\section{Example}\label{example-5}

Below is an example code snippet demonstrating Bayesian Beta-Binomial
regression using the Bayesian Inference (BI) package. The data consist
of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  One binary dependent variable (admit), which represents candidates'
  admission status.
\item
  One independent categorical variable representing individuals' gender
  (gid).
\item
  Additionally, we have the number of applications (applications) per
  individual, which will be used to account for independent rates.
\end{enumerate}

The goal is to evaluate whether the probability of admission is
different between genders, while accounting for differences in the
number of applications between genders.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}\CommentTok{\#}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/UCBadmit.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\CommentTok{\# Data type (int, float is important, specially for indices)}
\NormalTok{m.df[}\StringTok{"gid"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (m.df[}\StringTok{"applicant.gender"}\NormalTok{] }\OperatorTok{!=} \StringTok{"male"}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{) }\CommentTok{\# Manipulate}
\NormalTok{gid }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"gid"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}
\NormalTok{applications }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"applications"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}
\NormalTok{admit }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"admit"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}

\CommentTok{\# Send to model (convert to jax array)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    gid}\OperatorTok{=}\NormalTok{gid,}
\NormalTok{    applications}\OperatorTok{=}\NormalTok{applications,}
\NormalTok{    admit}\OperatorTok{=}\NormalTok{admit}
\NormalTok{) }

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(gid, applications, admit):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    phi }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{)}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\FloatTok{0.}\NormalTok{, }\FloatTok{1.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    t }\OperatorTok{=}\NormalTok{ phi }\OperatorTok{+} \DecValTok{2}
\NormalTok{    pbar }\OperatorTok{=}\NormalTok{ jax.nn.sigmoid(alpha[gid])}
\NormalTok{    gamma }\OperatorTok{=}\NormalTok{ pbar }\OperatorTok{*}\NormalTok{ t}
\NormalTok{    eta }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ pbar) }\OperatorTok{*}\NormalTok{ t}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, BetaBinomial(total\_count}\OperatorTok{=}\NormalTok{applications, concentration1}\OperatorTok{=}\NormalTok{gamma, concentration0}\OperatorTok{=}\NormalTok{eta), obs}\OperatorTok{=}\NormalTok{admit)}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{jax }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"jax"}\NormalTok{)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/UCBadmit.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{df[}\StringTok{"gid"}\NormalTok{] }\OtherTok{=} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df[}\StringTok{"applicant.gender"}\NormalTok{] }\SpecialCharTok{==} \StringTok{"male"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}gid\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}applications\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}admit\textquotesingle{}}\NormalTok{ )) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(gid, applications, admit)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  phi }\OtherTok{=} \FunctionTok{bi.dist.exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{,}\AttributeTok{shape=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\FloatTok{0.}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\AttributeTok{shape=} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{), }\AttributeTok{name=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{  t }\OtherTok{=}\NormalTok{ phi }\SpecialCharTok{+} \DecValTok{2}
\NormalTok{  pbar }\OtherTok{=}\NormalTok{ jax}\SpecialCharTok{$}\NormalTok{nn}\SpecialCharTok{$}\FunctionTok{sigmoid}\NormalTok{(alpha[gid])}
\NormalTok{  gamma }\OtherTok{=}\NormalTok{ pbar }\SpecialCharTok{*}\NormalTok{ t}
\NormalTok{  eta }\OtherTok{=}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pbar) }\SpecialCharTok{*}\NormalTok{ t}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\FunctionTok{bi.dist.betabinomial}\NormalTok{(}\AttributeTok{total\_count=}\NormalTok{applications, }\AttributeTok{concentration1=}\NormalTok{gamma, }\AttributeTok{concentration0=}\NormalTok{eta), }\AttributeTok{obs=}\NormalTok{admit)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{() }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-5}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-5}

???

\subsection{\texorpdfstring{\emph{Bayesian
Model}}{Bayesian Model}}\label{bayesian-model}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
Y_i \sim BetaBinomial(n_i, \gamma_i, \eta_i)
\]

\[
\gamma_i = \overline{\rho}   \tau 
\]

\[
\eta_i = (1 - \overline{\rho} ) \tau 
\]

\[
\overline{\rho} = logit(\alpha_i)
\]

\[
\tau = \phi + 2
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\phi \sim Exponential(1)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the ount of successes for the \emph{i}-th observation,
  which follows a beta-binomial distribution with \(n_i\) trials.
\item
  \(\gamma_i\) represents the concentration parameter for the number of
  successes, derived from the probability of success and scaled by
  \(\tau\).
\item
  \(\eta_i\) represents the concentration parameter for failures,
  derived from the probability of failure \((1 - \overline{\rho} )\) and
  also scaled by \(\tau\).
\item
  \(\overline{\rho}\) is the probability of success for the \emph{i}-th
  observation. The logit function transforms the linear predictor Œ±
  (which can take any real value) into a probability value between 0 and
  1.
\item
  \(\tau\) is derived from ùúô and is used as a scaling factor for the
  shape parameters ùõæ and ùúÇ.
\item
  Œ± is a vector of parameters, each representing the effect of the group
  variable *i on the success probability.
\item
  œï is a random variable following an exponential distribution with a
  rate of 1.
\end{itemize}

\section{Reference(s)}\label{references-4}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Poisson model}\label{poisson-model}

\section{General Principles}\label{general-principles-6}

To model the relationship between a count outcome variable---e.g.,
counts of events occurring in a fixed interval of time or space---and
one or more independent variables, we can use the \emph{Poisson model}.

This is a special shape of the binomial distribution; it is useful
because it models binomial events for which the number of trials \(n\)
is unknown or uncountably large.

\includegraphics{index_files/mediabag/Comparison-of-linear.png}

\section{Considerations}\label{considerations-7}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We have the second \phantomsection\label{linkF}{{link function üõà}}.
  The conventional link function for a Poisson model is the \emph{log}
  link (it ensures that \emph{Œª} is always positive).
\item
  To invert the log link function and model linearly the relationship
  between the predictor variables and the log of the mean rate
  parameter, we can apply the exponential function (see comment in
  code).
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-6}

Below is an example code snippet demonstrating a Bayesian Poisson model
using the Bayesian Inference (BI) package. Data consist of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  A continuous dependent variable \emph{total\_tools}, which represents
  the number of tools produced by a civilization.
\item
  A continuous independent variable \emph{population} representing
  population size.
\item
  A categorical independent variable \emph{cid} representing different
  civilizations.
\end{enumerate}

The goal is to estimate the production of tools based on population
size, accounting for each civilization.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Kline.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.sale([}\StringTok{"population"}\NormalTok{])  }\CommentTok{\# Scale}
\NormalTok{m.df[}\StringTok{"cid"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (m.df.contact }\OperatorTok{==} \StringTok{"high"}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{) }\CommentTok{\# Manipulate}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}total\_tools\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cid\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model }


\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(cid, P, total\_tools):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Poisson(jnp.exp(alpha[cid] }\OperatorTok{+}\NormalTok{ beta[cid]}\OperatorTok{*}\NormalTok{P)), obs}\OperatorTok{=}\NormalTok{total\_tools)  }\CommentTok{\# Exponential ensures non{-}negative values and inverts the log link function}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{jnp }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"jax.numpy"}\NormalTok{)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/Kline.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{scale}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{))}\CommentTok{\# Scale}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{df[}\StringTok{"cid"}\NormalTok{] }\OtherTok{=}  \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{contact }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}total\_tools\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cid\textquotesingle{}}\NormalTok{ )) }\CommentTok{\# Send to model (convert to jax array)}


\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(total\_tools, population, cid)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  l }\OtherTok{=}\NormalTok{ jnp}\SpecialCharTok{$}\FunctionTok{exp}\NormalTok{(alpha[cid] }\SpecialCharTok{+}\NormalTok{ beta[cid]}\SpecialCharTok{*}\NormalTok{population)}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\FunctionTok{bi.dist.poisson}\NormalTok{(l), }\AttributeTok{obs=}\NormalTok{total\_tools)}
\NormalTok{\}}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{() }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-6}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-6}

We model the relationship between the predictor variable (\(X_i\)) and
the count outcome variable (\(Y_i\)) using the following equation:

\[
\log(\lambda_i) = \alpha + \beta  X_i 
\]

Where:

\begin{itemize}
\item
  \(\lambda_i\) is the mean rate parameter of the Poisson distribution
  (expected count) for observation \emph{i}, modeled as the exponential
  function of the linear combination of predictors.
\item
  \(\log(\lambda_i)\) is the log of the mean rate parameter for
  observation \emph{i}, ensuring it is positive.
\item
  \(\beta\) is the regression coefficient.
\item
  \(\alpha\) is the intercept term.
\item
  \(X_i\) is the value for the independent variables for observation
  \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-5}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
Y \sim Poisson(\lambda_i)
\]

\[
\log(\lambda_i) \sim \alpha + \beta X_i
\]

\[
\alpha \sim Normal(0, 1)
\]

\[
\beta \sim Normal(0, 1)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is the dependent variable for observation \emph{i}.
\item
  \(\lambda_i\) is the mean rate parameter of the Poisson distribution
  for observation \emph{i}, modeled as the exponential function of the
  linear combination of predictors.
\item
  \(\log(\lambda_i)\) is the log of the mean rate parameter for
  observation \emph{i}.
\item
  \(\alpha\) and \(\beta\) are the prior distributions for the intercept
  and the regression coefficients, respectivelly .
\item
  \(\lambda\) is the mean rate parameter of the Poisson distribution,
  modeled as the exponential function of the linear combination of
  predictors.
\item
  \(X_i\) is the value for the independent variables for observation
  \emph{i}.
\end{itemize}

\section{Notes}\label{notes-3}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply multiple variables similarly as in
  \href{2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as in
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply categorical variables similarly as in
  \href{4.\%20Categorical\%20variable.md}{chapter 4}.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-5}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Gamma-Poisson model}\label{gamma-poisson-model}

\section{General Principles}\label{general-principles-7}

To model the relationship between a count outcome variable and one or
more independent variables with
\phantomsection\label{overdispersion}{{overdispersion üõà}}, we can use
the \emph{Negative Binomial model}.

\section{Considerations}\label{considerations-8}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for the
  \href{7.\%20Poisson\%20model.qmd}{Poisson model}.
\item
  Overdispersion is handled because the Negative Binomial model assumes
  that each Poisson count observation has its own rate. This is an
  additional parameter specified in the model (in the code, it is
  \texttt{log\_days}).
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-7}

Below is an example code snippet demonstrating Bayesian Gamma-Poisson
model using the Bayesian Inference (BI) package:

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import} \OperatorTok{*}
\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu)\# Import}

\ErrorTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Sim dat Gamma poisson.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Import}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}log\_days\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monastery\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{]) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(log\_days, monastery, output):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    l }\OperatorTok{=}\NormalTok{ log\_days }\OperatorTok{+}\NormalTok{ a }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ monastery}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Poisson(rate}\OperatorTok{=}\NormalTok{l), obs}\OperatorTok{=}\NormalTok{output)}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(BI)}
\NormalTok{m}\OtherTok{=}\FunctionTok{importBI}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load csv file}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\AttributeTok{package =} \StringTok{"BI"}\NormalTok{),}\StringTok{"/data/Sim dat Gamma poisson.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}log\_days\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monastery\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{ )) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(log\_days, monastery, y)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  beta }\OtherTok{=} \FunctionTok{bi.dist.normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{  l }\OtherTok{=}\NormalTok{ log\_days }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ monastery}
  \CommentTok{\# Likelihood}
\NormalTok{  m}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\FunctionTok{bi.dist.poisson}\NormalTok{(}\AttributeTok{rate=}\NormalTok{l), }\AttributeTok{obs=}\NormalTok{y)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{() }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-7}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-7}

We model the relationship between the independent variable \(X\) and the
count outcome variable \(Y\) using the following equation:

\[
\log(\lambda_i) = \exp(\text{rates}_i + \alpha + \beta X_i)
\]

Where:

\begin{itemize}
\item
  \(\lambda_i\) is the mean rate parameter of the negative binomial
  distribution (expected count) for observation \emph{i}.
\item
  \(\log(\lambda_i)\) is the log of the mean rate parameter, ensuring it
  is positive for observation \emph{i}.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta\) is the regression coefficients.
\item
  \(X_i\) is the value of the predictor variable for observation
  \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-1}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
Y_i \sim \text{Poisson}(\lambda)
\]

\[
\log(\lambda_i) \sim \text{rates}_i + \alpha + \beta X_i
\]

\[
\alpha \sim \text{Normal}(0,1)
\]

\[
\beta \sim \text{Normal}(0,1)
\]

Where:

\begin{itemize}
\item
  \(Y_i\) is dependent variable for observation \emph{i}.
\item
  \(\lambda_i\) is the mean rate parameter of the Poisson distribution
  for observation \emph{i}, assuming that each Poisson count observation
  has its own \(rate_i\).
\item
  \(\log(\lambda_i)\) is the log of the mean rate parameterfor
  observation \emph{i}, ensuring it is positive.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta\) is the regression coefficients.
\item
  \(X_i\) is the value of the predictor variable for observation
  \emph{i}.
\end{itemize}

\section{Notes}\label{notes-4}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply multiple variables similarly as in
  \href{2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as in
  \href{3.\%20Interaction\%20between\%20Continuous\%20Variables.qmd}{chapter
  3}.
\item
  We can apply categorical variables similarly as in
  \href{4.\%20Categorical\%20variable.md}{chapter 4}.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-6}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Multinomial model}\label{multinomial-model}

\section{General Principles}\label{general-principles-8}

To model the relationship between a categorical outcome variable with
more than two categories and one or more independent variables, we can
use a \emph{Multinomial} model.

\includegraphics{index_files/mediabag/Multinomial-Logistic.jpg}

\section{Considerations}\label{considerations-9}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  One way to interpret a multinomial model is to consider that we need
  to build \(K - 1\) linear models, where \(K\) is the number of
  categories. Once we get the linear prediction for each category, we
  can convert these predictions to probabilities by building a
  \phantomsection\label{simplex}{{simplex üõà}}. To do this, we convert
  the regression outputs using the
  \phantomsection\label{softmax}{{softmax function üõà}} (see the
  ``nn.softmax'' line in the code).
\item
  The intercept \(\alpha\) captures the difference in the log-odds of
  the outcome categories; thus, different categories need different
  intercepts.
\item
  On the other hand, as we assume that the effect of each predictor on
  the outcome is consistent across all categories, the regression
  coefficients \(\beta\) are shared across categories.
\item
  The relationship between the predictor variables and the log-odds of
  each category is modeled linearly, allowing us to interpret the effect
  of each predictor on the log-odds of each category.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-8}

Below is an example code snippet demonstrating Bayesian multinomial
model using the Bayesian Inference (BI) package:

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Simulated data{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# simulate career choices among 500 individuals}
\NormalTok{N }\OperatorTok{=} \DecValTok{500}  \CommentTok{\# number of individuals}
\NormalTok{income }\OperatorTok{=}\NormalTok{ jnp.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{])  }\CommentTok{\# expected income of each career}
\NormalTok{score }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ income  }\CommentTok{\# scores for each career, based on income}

\CommentTok{\# next line converts scores to probabilities}
\NormalTok{p }\OperatorTok{=}\NormalTok{ jnp.array(jax.nn.softmax(score))}

\CommentTok{\# now simulate choice}
\CommentTok{\# outcome career holds event type values, not counts}
\NormalTok{career }\OperatorTok{=}\NormalTok{ bi.dist.categorical(p, shape}\OperatorTok{=}\NormalTok{N, sample}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    income}\OperatorTok{=}\NormalTok{income,}
\NormalTok{    career}\OperatorTok{=}\NormalTok{career}
\NormalTok{)}
\NormalTok{d.to\_csv(}\StringTok{\textquotesingle{}Sim data multinomial.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(income, career):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.halfnormal(}\FloatTok{0.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    s\_1 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    s\_2 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{1}\NormalTok{]}
\NormalTok{    s\_3 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ jax.nn.softmax(jnp.stack([s\_1[}\DecValTok{0}\NormalTok{], s\_2[}\DecValTok{0}\NormalTok{], s\_3[}\DecValTok{0}\NormalTok{]]))}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Categorical(probs}\OperatorTok{=}\NormalTok{p[career]), obs}\OperatorTok{=}\NormalTok{career)}

\CommentTok{\# Run sampler {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\NormalTok{m.run(model)  }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}

\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}Sim data multinomial.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{sep=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}

\NormalTok{keys }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{, }\StringTok{"career"}\NormalTok{)}
\NormalTok{income }\OtherTok{=} \FunctionTok{unique}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{income)}
\NormalTok{income }\OtherTok{=}\NormalTok{ income[}\FunctionTok{order}\NormalTok{(income)]}
\NormalTok{values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(income), }\FunctionTok{as.integer}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{career))}
\NormalTok{m.data\_on\_model }\OtherTok{=} \FunctionTok{py\_dict}\NormalTok{(keys, values, }\AttributeTok{convert =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{m.data\_on\_model}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(income, career)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  alpha }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2}\NormalTok{)))}
\NormalTok{  beta }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{halfnormal}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{)))}
\NormalTok{  s\_1 }\OtherTok{=}\NormalTok{ alpha[}\DecValTok{0}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ income[}\DecValTok{1}\NormalTok{]}
\NormalTok{  s\_2 }\OtherTok{=}\NormalTok{ alpha[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ income[}\DecValTok{1}\NormalTok{]}
\NormalTok{  s\_3 }\OtherTok{=} \DecValTok{0} \CommentTok{\# reference category}
\NormalTok{  p }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jax}\SpecialCharTok{$}\NormalTok{nn}\SpecialCharTok{$}\FunctionTok{softmax}\NormalTok{(bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{stack}\NormalTok{(}\FunctionTok{list}\NormalTok{(s\_1[}\DecValTok{0}\NormalTok{], s\_2[}\DecValTok{0}\NormalTok{], s\_3[}\DecValTok{0}\NormalTok{])))}
  \CommentTok{\# Likelihood}
\NormalTok{  bi}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, bi}\SpecialCharTok{$}\FunctionTok{Categorical}\NormalTok{(}\AttributeTok{probs=}\NormalTok{p[career]), }\AttributeTok{obs=}\NormalTok{career)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{)}\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-8}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-8}

We model the relationship between the predictor variables (X1, X2,
\ldots, Xn) and the categorical outcome variable (\(Y_i\)) using the
following equation:

\[
logit(p_ik) = Œ±_k + Œ≤ X_i 
\]

Where:

\begin{itemize}
\item
  \(p_ik\) is the probability of the ùëñ-th observation being in category
  ùëò.
\item
  \(Œ±_k\) is the intercept for category ùëò.
\item
  \(Œ≤\) is the regression coefficients common to all categories.
\item
  \(X_i\) is the vector of independent variables for the ùëñ-th
  observation.
\item
  A reference category is often chosen to simplify the model.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-2}

In Bayesian multinomial modeling, the likelihood function of the data is
specified using a multinomial distribution. The multinomial distribution
models the counts of outcomes falling into different categories. For an
outcome variable ùë¶ with ùêæ categories, the multinomial likelihood
function is: \[
Multinomial(y|Œ∏)=\frac{N!}{‚àè^K_{k=1}yk!} ‚àè_{k=1}^{K} Œ∏_{k}^{y_k}
\]

Where:

\begin{itemize}
\item
  \(y=(y_1, y_2,‚Ä¶,y_K)\) represents the counts of observations in each
  of the ùêæ categories.
\item
  \(N\) is the total number of observations or trials.
\item
  \(Œ∏=(Œ∏_1,Œ∏_2,‚Ä¶,Œ∏_K)\) is a simplex of category probabilities, with
  \(Œ∏_k\) representing the probability of category ùëò.
\item
  \(\frac{N!}{‚àè^K_{k=1}yk!}\) is the multinomial coefficient that
  accounts for the number of ways to arrange the observations into the
  categories. This coefficient ensures that the likelihood function
  properly accounts for the permutations of the counts across different
  categories.
\item ~
  \section{Reference(s)}\label{references-7}

  McElreath (2018)
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Dirichlet Model}\label{dirichlet-model}

\section{General Principles}\label{general-principles-9}

To model the relationship between a categorical outcome variable with
more than two categories and one or more independent variables with
\phantomsection\label{overdispersion}{{overdispersion üõà}}, we can use a
\emph{Dirichlet} model.

\includegraphics{index_files/mediabag/Multinomial-Logistic.jpg}

\section{Considerations}\label{considerations-10}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for the
  \href{7.\%20Multinomial\%20model.qmd}{Multinomial model}.
\item
  One major difference from the multinomial model is that the Dirichlet
  model doesn't require a simplex but rather strictly positive values.
  We can thus exponentiate the outputs from the categorical regressions
  instead of using the softmax function.
\item
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulated data{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# simulate career choices among 500 individuals}
\NormalTok{N }\OperatorTok{=} \DecValTok{500}  \CommentTok{\# number of individuals}
\NormalTok{income }\OperatorTok{=}\NormalTok{ jnp.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{])  }\CommentTok{\# expected income of each career}
\NormalTok{score }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ income  }\CommentTok{\# scores for each career, based on income}

\CommentTok{\# next line converts scores to probabilities}
\NormalTok{p }\OperatorTok{=}\NormalTok{ jnp.array(jax.nn.softmax(score))}

\CommentTok{\# now simulate choice}
\CommentTok{\# outcome career holds event type values, not counts}
\NormalTok{career }\OperatorTok{=}\NormalTok{ bi.dist.categorical(p, shape}\OperatorTok{=}\NormalTok{N, sample}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    income}\OperatorTok{=}\NormalTok{income,}
\NormalTok{    career}\OperatorTok{=}\NormalTok{career}
\NormalTok{)}
\NormalTok{d.to\_csv(}\StringTok{\textquotesingle{}Sim data multinomial.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(income, career):}
    \CommentTok{\# Parameters priors distributions}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.halfnormal(}\FloatTok{0.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    s\_1 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    s\_2 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{1}\NormalTok{]}
\NormalTok{    s\_3 }\OperatorTok{=}\NormalTok{ alpha[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ jax.nn.exp(jnp.stack([s\_1[}\DecValTok{0}\NormalTok{], s\_2[}\DecValTok{0}\NormalTok{], s\_3[}\DecValTok{0}\NormalTok{]]))}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, DirichletMultinomial(probs}\OperatorTok{=}\NormalTok{p[career]), obs}\OperatorTok{=}\NormalTok{career)}
\CommentTok{\# Run sampler {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\NormalTok{m.run(model)  }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-9}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula}

\subsection{\texorpdfstring{\emph{Bayesian
Model}}{Bayesian Model}}\label{bayesian-model-3}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\section{Reference(s)}\label{references-8}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Zero inflated}\label{zero-inflated}

\section{General Principles}\label{general-principles-10}

Zero-Inflated Regression models are used when the outcome variable is a
count variable with an excess of zero counts. These models combine a
count model (e.g., Poisson or Negative Binomial) with a separate model
for predicting the probability of excess zeros.

\section{Considerations}\label{considerations-11}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  In Bayesian Zero-Inflated regression, we consider uncertainty in the
  model parameters and provide a full posterior distribution over them.
  We need to declare prior distributions for
  \(W_{1\pi}, W_{2\pi}, ..., W_{n\pi}\),
  \(W_{1\lambda}, W_{2\lambda}, ..., W_{n\lambda}\), \(b_\pi\), and
  \(b_\lambda\).
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-10}

Below is an example code snippet demonstrating Bayesian Zero-Inflated
Poisson regression using the Bayesian Inference (BI) package. The data
represent the production of books in a monastery (y), which is affected
by the number of days that individuals work, as well as the number of
days individuals drink.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ jax.scipy.special }\ImportTok{import}\NormalTok{ expit}
\NormalTok{r.seed(}\DecValTok{42}\NormalTok{)}
\ImportTok{from}\NormalTok{ main }\ImportTok{import} \OperatorTok{*}

\CommentTok{\# Simulated data{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{prob\_drink }\OperatorTok{=} \FloatTok{0.2}  \CommentTok{\# 20\% of days}
\NormalTok{rate\_work }\OperatorTok{=} \DecValTok{1}     \CommentTok{\# average 1 manuscript per day}

\CommentTok{\# Sample one year of production}
\NormalTok{N }\OperatorTok{=} \DecValTok{365}

\NormalTok{np.random.seed(}\DecValTok{365}\NormalTok{)}
\NormalTok{drink }\OperatorTok{=}\NormalTok{ np.random.binomial(}\DecValTok{1}\NormalTok{, prob\_drink, N)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ drink) }\OperatorTok{*}\NormalTok{ np.random.poisson(rate\_work, N)}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    y}\OperatorTok{=}\NormalTok{jnp.array(y)}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(y):}
\NormalTok{    al }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}al\textquotesingle{}}\NormalTok{)}
\NormalTok{    ap }\OperatorTok{=}\NormalTok{ dist.normal(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}ap\textquotesingle{}}\NormalTok{)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ expit(ap)}
\NormalTok{    lambda\_ }\OperatorTok{=}\NormalTok{ jnp.exp(al)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, ZeroInflatedPoisson(p, lambda\_), obs}\OperatorTok{=}\NormalTok{y)}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}

\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Define parameters}
\NormalTok{prob\_drink }\OtherTok{=} \FloatTok{0.2}  \CommentTok{\# 20\% of days}
\NormalTok{rate\_work }\OtherTok{=} \DecValTok{1}     \CommentTok{\# average 1 manuscript per day}
\CommentTok{\# sample one year of production}
\NormalTok{N }\OtherTok{=} \FunctionTok{as.integer}\NormalTok{(}\DecValTok{365}\NormalTok{)}
\NormalTok{drink }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{total\_count =} \FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{), }\AttributeTok{probs =}\NormalTok{ prob\_drink, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(N), }\AttributeTok{sample =}\NormalTok{ T ) }\CommentTok{\# An example of sampling a distribution with BI}
\NormalTok{y }\OtherTok{=}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ drink) }\SpecialCharTok{*}\NormalTok{  bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{poisson}\NormalTok{(rate\_work, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(N), }\AttributeTok{sample =}\NormalTok{ T)}
\NormalTok{data }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{y }\OtherTok{=}\NormalTok{ y }
\NormalTok{m}\SpecialCharTok{$}\NormalTok{data\_on\_model }\OtherTok{=}\NormalTok{ data}
\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y)\{}
\NormalTok{  al }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}al\textquotesingle{}}\NormalTok{)}
\NormalTok{  ap }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}ap\textquotesingle{}}\NormalTok{)}
\NormalTok{  p }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jax}\SpecialCharTok{$}\NormalTok{scipy}\SpecialCharTok{$}\NormalTok{special}\SpecialCharTok{$}\FunctionTok{expit}\NormalTok{(al)}
\NormalTok{  lambda\_ }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{exp}\NormalTok{(al)}
\NormalTok{  bi}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, bi}\SpecialCharTok{$}\FunctionTok{ZeroInflatedPoisson}\NormalTok{(p, lambda\_), }\AttributeTok{obs=}\NormalTok{y)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\chapter{Mathematical Details}\label{mathematical-details-10}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-9}

We model the relationship between the independent variable \(X\) and the
count outcome variable \(Y\) using two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  A logistic regression model to predict the probability of an excess
  zero.
\item
  A count model (e.g., Poisson or Negative Binomial) to predict the
  count outcome.
\end{enumerate}

The overall model can be represented as follows:

\[
\begin{aligned}
& \text{logit}(\pi) = \alpha_\pi + \beta_\pi X_i \\
& \text{log}(\lambda) = \alpha_\lambda + \beta_\lambda X_i\\
& Y_i \sim \begin{cases} 
0 & \text{with probability } \pi \\
\text{CountModel}(\lambda) & \text{with probability } (1 - \pi) 
\end{cases}
\end{aligned}
\]

Where:

\begin{itemize}
\item
  \(\pi\) is the probability of an excess zero.
\item
  \(\lambda\) is the mean rate parameter of the count model.
\item
  \(\alpha_\pi\) and \(\beta_\pi\) are respectively the intercept and
  the regression coefficient for the logistic model.
\item
  \(\alpha_\lambda\) and \(\beta_\lambda\) are respectively the
  intercept and the regression coefficient for the count model.
\item
  \(X_i\) is the independent variables value for observation \emph{i}.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-6}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express the Bayesian
regression model accounting for prior distribution as follows:

\[
ùëå\sim ZIPoisson(\pi,\lambda)
\]

\[
\text{logit}(\pi) = \alpha_\pi + \beta_\pi X
\]

\[
\text{log}(\lambda) = \alpha_\lambda + \beta_\lambda X
\]

\[
\alpha_\pi \sim \text{Normal}(0,1)
\]

\[
\beta_\pi \sim \text{Normal}(0,1)
\]

\[
\alpha_\lambda \sim \text{Normal}(0,1)
\]

\[
\beta_\lambda \sim \text{Normal}(0,1)
\]

Where:

\begin{itemize}
\item
  \(\pi\) is the probability of an excess zero.
\item
  \(\lambda\) is the mean rate parameter of the count model.
\item
  \(\alpha_\pi\) and \(\beta_\pi\) are respectively the intercept and
  the regression coefficient for the logistic model.
\item
  \(\alpha_\lambda\) and \(\beta_\lambda\) are respectively the
  intercept and the regression coefficient for the count model.
\item
  \(X_i\) is the independent variables value for observation \emph{i}.
\end{itemize}

\section{Reference(s)}\label{references-9}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Survival Analysis}\label{survival-analysis}

\section{General Principles}\label{general-principles-11}

Survival analysis studies the time until an event of interest (e.g.,
death, recovery, information acquisition) occurs. When analyzing binary
survival outcomes (e.g., alive or dead), we can use models such as Cox
proportional hazards to evaluate the effect of predictors on survival
probabilities.

Key concepts include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hazard Function}: The instantaneous risk of the event
  occurring at a given time.
\item
  \textbf{Survival Function}: The probability of surviving beyond a
  given time.
\item
  \textbf{Covariates}: Variables (e.g., age, treatment) that may affect
  survival probabilities.
\item
  \textbf{Baseline Hazard}: The hazard when all covariates are zero,
  which forms the reference for comparing different conditions.
\end{enumerate}

\section{Considerations}\label{considerations-12}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  Bayesian models provide a framework to account for
  \phantomsection\label{uncertainty}{{uncertainty üõà}} in parameter
  estimates through posterior distributions. You will need to define
  \phantomsection\label{prior}{{prior distributions üõà}} for all model
  parameters, such as baseline hazard, covariate effects, and variance
  terms.
\item
  In survival analysis:

  \begin{itemize}
  \item
    The \textbf{baseline hazard} can follow distributions like
    Exponential, Weibull, or Gompertz, depending on the data.
  \item
    Censoring (when the event is not observed for some subjects) must be
    accounted for in the likelihood function. Proper handling is
    essential for unbiased results.
  \end{itemize}
\item
  Bayesian survival models allow flexible handling of time-dependent
  covariate, random effects, and incorporate uncertainty more naturally
  than Frequentist methods.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-11}

Here's an example Bayesian survival analysis using the \textbf{Bayesian
Inference (BI)} package. The data come from a clinical trial of
mastectomy for breast cancer. The goal is to estimate the effect of
treatment \texttt{metastasized} coded as 0 (no metastasis) and 1
(metastasis) on the survival probability \texttt{event} for each
patient. Time is continuous and censoring is indicated by the event
variable.

\subsection{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import} \OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}data/mastectomy.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}

\NormalTok{m.df.metastasized }\OperatorTok{=}\NormalTok{ (m.df.metastasized }\OperatorTok{==} \StringTok{"yes"}\NormalTok{).astype(np.int64)}
\NormalTok{m.df.event }\OperatorTok{=}\NormalTok{ jnp.array(m.df.event.values, dtype}\OperatorTok{=}\NormalTok{jnp.int32)}

\CommentTok{\#\# Create survival object}
\NormalTok{m.surv\_object(time}\OperatorTok{=}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{, event}\OperatorTok{=}\StringTok{\textquotesingle{}event\textquotesingle{}}\NormalTok{, cov}\OperatorTok{=}\StringTok{\textquotesingle{}metastasized\textquotesingle{}}\NormalTok{, interval\_length}\OperatorTok{=}\DecValTok{3}\NormalTok{)}

\CommentTok{\# Plot censoring {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.plot\_censoring(cov}\OperatorTok{=}\StringTok{\textquotesingle{}metastasized\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(intervals, death, metastasized, exposure):}
    \CommentTok{\# Parameters priors distributions{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
    \CommentTok{\#\# Base hazard distribution}
\NormalTok{    lambda0 }\OperatorTok{=}\NormalTok{ bi.dist.gamma(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{, shape}\OperatorTok{=}\NormalTok{ intervals.shape, name }\OperatorTok{=} \StringTok{\textquotesingle{}lambda0\textquotesingle{}}\NormalTok{)}
    \CommentTok{\#\# Covariate effect distribution}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1000}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{1}\NormalTok{,),  name}\OperatorTok{=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)}
    \CommentTok{\#\#\# Likelihood}
    \CommentTok{\#\#\#\# Compute hazard rate based on covariate effect}
\NormalTok{    lambda\_ }\OperatorTok{=}\NormalTok{ m.hazard\_rate(cov }\OperatorTok{=}\NormalTok{ metastasized, beta }\OperatorTok{=}\NormalTok{ beta, lambda0 }\OperatorTok{=}\NormalTok{ lambda0)}
    \CommentTok{\#\#\#\# Compute exposure rates}
\NormalTok{    mu }\OperatorTok{=}\NormalTok{ exposure }\OperatorTok{*}\NormalTok{ lambda\_}

    \CommentTok{\# Likelihood calculation}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ lk(}\StringTok{\textquotesingle{}obs\textquotesingle{}}\NormalTok{, Poisson(mu }\OperatorTok{+}\NormalTok{ jnp.finfo(mu.dtype).tiny), obs }\OperatorTok{=}\NormalTok{ death)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, num\_samples}\OperatorTok{=}\DecValTok{500}\NormalTok{) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\BuiltInTok{print}\NormalTok{(m.summary())}

\CommentTok{\# PLot hazards and survival function {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.plot\_surv()}
\end{Highlighting}
\end{Shaded}

\subsection{R}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-11}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-10}

The Cox proportional hazards model can be expressed as: \[
h(t | X) = h_0(t) \exp(\beta^T X)
\]

\begin{itemize}
\item
  Where:

  \begin{itemize}
  \item
    \(h(t | X)\) is the hazard at time \(t\) for covariates \(X\).
  \item
    \(h_0(t)\) is the baseline hazard function (e.g., exponential,
    Weibull).
  \item
    \(X\) represents the covariates (such as age, treatment).
  \item
    \(\beta\) are the regression coefficients to be estimated.
  \end{itemize}
\item
  Censoring is accounted for by multiplying the hazard function by a
  factor that depends on the censoring distribution, usually modeled as
  independent censoring with a rate Œ¥(t):

  \begin{itemize}
  \tightlist
  \item
    \(Y_i(t) = Poisson(h(t | X) * Œ¥(t))\)
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-7}

In Bayesian survival analysis, we define priors for each parameter:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hazard Function}: The hazard rate at time ( \(t\) ) for an
  individual is given by:
  \[Y_i(t) = Poisson(\lambda(t) * censoring(t))\]
  \[\lambda(t) = \lambda_0(t)\exp(x\beta)\]

  \[ \beta \sim N(\mu_\beta, \sigma^2_\beta) \]

  \[\mu_\beta \sim N(0, 10^2)\]

  \[\sigma^2_\beta \sim U(0, 10)\]

  Where:

  \begin{itemize}
  \item
    \(Y_i(t)\) is the status of the \(i\)-th subject at time \(t\) coded
    as a binary variable: \[
    Y_i(t) = 
    \begin{cases} 
    1 & \text{if subject } i \text{ died at time $t$  }, \\ 
    0 & \text{otherwise.}
    \end{cases}
    \]
  \item
    \(\lambda(t)\): Hazard function at time \(t\).
  \item
    \(\lambda_0(t)\): Baseline hazard function (e.g., exponential or
    Weibull).
  \item
    \(x\): Covariates (e.g., age, treatment).
  \item
    \(\beta\): Regression coefficients capturing the effect of \(x\) on
    the hazard have an assigned a normal prior.
  \item
    \(\mu_\beta\): Mean of the normal distribution.
  \item
    \(\sigma^2_\beta\): Variance of the normal distribution.
  \end{itemize}
\end{enumerate}

\section{Reference(s)}\label{references-10}

\bookmarksetup{startatroot}

\chapter{Varying Intercepts}\label{varying-intercepts}

\section{General Principles}\label{general-principles-12}

To model the relationship between dependent variables and an independent
variable while allowing for different intercepts across groups or
clusters, we can use a \emph{Varying Intercepts} model. This approach is
particularly useful when data is grouped (e.g., by subject, location, or
time period) and we expect the baseline level of the outcome to vary
across these groups.

\section{Considerations}\label{considerations-13}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  The main idea of varying intercepts is to generate an intercept for
  each group, allowing each group to start at different levels. Thus,
  the intercept \(\beta\) is defined based on the \(k\) declared groups.
\item
  Each intercept has its own prior -i.e., a
  \phantomsection\label{hyperP}{{hyper-prior üõà}}
\item
  In the code below, the \emph{hyper-prior} is \texttt{a\_bar}.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-12}

Below is an example code snippet demonstrating Bayesian regression with
varying intercepts using the Bayesian Inference (BI) package. The data
consist of a dependent variable representing individuals' survival
(\emph{surv}) and an independent categorical variable (\emph{tank}),
which indicates the tank where the individual was born, with a total of
48 tanks.

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/reedfrogs.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }\CommentTok{\# Import}
\NormalTok{m.df[}\StringTok{"tank"}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.arange(m.df.shape[}\DecValTok{0}\NormalTok{]) }\CommentTok{\# Manipulate}
\NormalTok{tank }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"tank"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}
\NormalTok{density }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"density"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}
\NormalTok{surv }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"surv"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values) }\CommentTok{\# Manipulate}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    tank }\OperatorTok{=}\NormalTok{ tank,}
\NormalTok{    surv }\OperatorTok{=}\NormalTok{ surv}
\NormalTok{) }\CommentTok{\# Send to model (convert to jax array)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(tank, surv):}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential( }\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    a\_bar }\OperatorTok{=}\NormalTok{ dist.normal( }\FloatTok{0.}\NormalTok{, }\FloatTok{1.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}a\_bar\textquotesingle{}}\NormalTok{)}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal( a\_bar, sigma, shape}\OperatorTok{=}\NormalTok{ [}\DecValTok{48}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ jnp.squeeze(alpha[tank])}
    \CommentTok{\# Likelihood}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(total\_count }\OperatorTok{=}\NormalTok{ density, logits }\OperatorTok{=}\NormalTok{ p), obs}\OperatorTok{=}\NormalTok{surv)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}

\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import Data \& Data Manipulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}../data/reedfrogs.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{sep=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Import}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{tank }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}tank\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}surv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{)) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{tank }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{tank}\SpecialCharTok{$}\FunctionTok{astype}\NormalTok{(bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\NormalTok{int32) }\CommentTok{\# Manipulate}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{surv }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{surv}\SpecialCharTok{$}\FunctionTok{astype}\NormalTok{(bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\NormalTok{int32) }\CommentTok{\# Manipulate}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(tank, surv, density)\{}
  \CommentTok{\# Parameters priors distributions}
\NormalTok{  sigma }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{( }\DecValTok{1}\NormalTok{,  }\AttributeTok{name =} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{  a\_bar }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\AttributeTok{name=}\StringTok{\textquotesingle{}a\_bar\textquotesingle{}}\NormalTok{)}
\NormalTok{  alpha }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(a\_bar, sigma, }\AttributeTok{name=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{48}\NormalTok{)))}
\NormalTok{  p }\OtherTok{=}\NormalTok{ alpha[tank]}
  \CommentTok{\# Likelihood}
\NormalTok{  bi}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, bi}\SpecialCharTok{$}\FunctionTok{Binomial}\NormalTok{(}\AttributeTok{total\_count =}\NormalTok{ density, }\AttributeTok{logits =}\NormalTok{ p), }\AttributeTok{obs=}\NormalTok{surv)}
\NormalTok{\} }

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }\CommentTok{\# Optimize model parameters through MCMC sampling}

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{) }\CommentTok{\# Get posterior distributions}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-12}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-11}

We model the relationship between the independent variables \(X\) and
the outcome variable \(Y\) with varying intercepts \(\alpha\) for each
group \(k\) using the following equation:

\[
Y_{ik} = \alpha_k + \beta X_{ik} + \sigma
\]

Where:

\begin{itemize}
\item
  \(Y_{ik}\) is the outcome variable for observation \(i\) in group
  \(k\).
\item
  \(\alpha_k\) is the varying intercept for group \(k\).
\item
  \(X_{ik}\) is the independent variable for observation \(i\) in group
  \(k\).
\item
  \(\beta\) is the regression coefficient term.
\item
  \(\sigma\) is the error term, typically assumed to be normally
  distributed and positive.
\end{itemize}

\subsection{Bayesian Model}\label{bayesian-model-4}

We can express the Bayesian regression model accounting for prior
distributions as follows:

\[
Y_{ik} = Normal(\mu_{ik}, \sigma)
\] \[
\mu_{ik} = \alpha_j + \beta X_{ik} + \sigma 
\] \[
\alpha_k \sim \text{Normal}(\mu_{\alpha_k}, \sigma_{\alpha_k}) 
\] \[
\beta \sim \text{Normal}(0, 1)
\] \[
\sigma \sim \text{Exponential}(1)
\] \[
\mu_{\alpha_k} \sim \text{Normal}(0, 1)
\] \[
\sigma_{\alpha_k} \sim \text{Exponential}(1)
\]

Where:

Where:

\begin{itemize}
\item
  \(Y_{ij}\) is the likelihood function for the outcome variable.
\item
  \(\alpha_k\) is the varying intercept for group \(k\).
\item
  \(\mu_{\alpha_k}\) is the overall mean intercept.
\item
  \(\sigma_{\alpha_k}\) is the variance of the intercepts across groups.
\item
  \(\beta\) is the prior distribution for the regression coefficients.
\item
  \(\sigma\) is the prior distribution for the error term.
\end{itemize}

\section{Notes}\label{notes-5}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply multiple variables similarly to
  \href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{Chapter
  2}.
\item
  We can apply interaction terms similarly to
  \href{./3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{Chapter
  3}.
\item
  We can apply categorical variables similarly to
  \href{./4.\%20Categorical\%20variable.qmd}{Chapter 4}.
\item
  We can apply varying intercepts with any distribution developed in
  previous chapters.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-11}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Varying slopes}\label{varying-slopes}

\section{General Principles}\label{general-principles-13}

To model the relationship between predictor variables and a dependent
variable while allowing for varying effects across groups or clusters,
we use a \emph{varying slopes} model.

This approach is useful when we expect the relationship between
predictors and the dependent variable to differ across groups (e.g.,
different slopes for different subjects, locations, or time periods).
This allows every unit in the data to have its own unique response to
any treatment, exposure, or event, while also improving estimates via
pooling.

\section{Considerations}\label{considerations-14}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We have the same considerations as for
  \href{12.\%20Varying\%20intercepts.qmd}{12. Varying intercepts}.
\item
  The idea is pretty similar to categorical models, where a slope is
  specified for each category. However, here, we also estimate
  relationships between different groups. This leads to a different
  mathematical approach, as to model these relationships between groups,
  we model a {matrix of covariance üõà}.
\item
  The covariance matrix requires a correlation matrix distribution which
  is modeled using a \(LKJcorr\) distribution that holds a parameter
  \(Œ∑\). \(Œ∑\) is usually set to 2 to define a weakly informative prior
  that is skeptical of extreme correlations near ‚àí1 or 1. When we use
  \(LKJcorr(1)\), the prior is flat over all valid correlation matrices.
  When the value is greater than 1, then extreme correlations are less
  likely.
\item
  The Half-Cauchy distribution is used when modeling the covariance
  matrix to specify strictly positive values for the diagonal of the
  covariance matrix, ensuring positive variances.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-13}

Below is an example code snippet demonstrating Bayesian regression with
varying effects:

\subsection{Simulated data}\label{simulated-data}

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\NormalTok{m.data(}\StringTok{\textquotesingle{}Sim data multivariatenormal.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }
\NormalTok{m.data.to\_model([}\StringTok{\textquotesingle{}cafe\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}wait\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}N\_cafes\textquotesingle{}}\NormalTok{])}

\KeywordTok{def}\NormalTok{ model(cafe, wait, N\_cafes):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{) }\CommentTok{\#Standard Normal Vector }
\NormalTok{    b }\OperatorTok{=}\NormalTok{ dist.normal(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{) }\CommentTok{\# Standard Normal Vector }
\NormalTok{    sigma\_cafe }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\_cafe\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential( }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    Rho }\OperatorTok{=}\NormalTok{ dist.lkj(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}Rho\textquotesingle{}}\NormalTok{) }\CommentTok{\# Cholesky Factor}
    \CommentTok{\# Applies the correlation structure between the variables.}
    \CommentTok{\# Scaling the Cholesky Factor by the standard deviations.}

\NormalTok{    cov }\OperatorTok{=}\NormalTok{ jnp.outer(sigma\_cafe, sigma\_cafe) }\OperatorTok{*}\NormalTok{ Rho }\CommentTok{\# In a multivariate normal distribution, we not only have correlations (from sr\_L), but also individual standard deviations for each variable (from sr\_sigma). The diag\_pre\_multiply operation ensures that each variable\textquotesingle{}s correlation structure is properly scaled by its standard deviation. This makes it possible to account for both correlations and individual variability when generating samples.}
    \CommentTok{\# This operation applies the correlation and standard deviation structure (encoded in scaled\_sr\_L) to the standard normal variables sr\_raw. The result is a sample from a multivariate normal distribution that respects both the correlations between variables and the individual standard deviations.}

\NormalTok{    a\_cafe\_b\_cafe }\OperatorTok{=}\NormalTok{ dist.multivariatenormal(jnp.stack([a, b]), cov, shape }\OperatorTok{=}\NormalTok{ [N\_cafes], name }\OperatorTok{=} \StringTok{\textquotesingle{}a\_cafe\textquotesingle{}}\NormalTok{)    }

\NormalTok{    a\_cafe, b\_cafe }\OperatorTok{=}\NormalTok{ a\_cafe\_b\_cafe[:, }\DecValTok{0}\NormalTok{], a\_cafe\_b\_cafe[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{    mu }\OperatorTok{=}\NormalTok{ a\_cafe[cafe] }\OperatorTok{+}\NormalTok{ b\_cafe[cafe] }\OperatorTok{*}\NormalTok{ afternoon}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(mu, sigma), obs}\OperatorTok{=}\NormalTok{wait)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}

\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}resources/data/Sim data multivariatenormal.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{sep=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}cafe\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}wait\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}afternoon\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(cafe, afternoon, wait, }\AttributeTok{N\_cafes =} \FunctionTok{as.integer}\NormalTok{(}\DecValTok{20}\NormalTok{) )\{}
\NormalTok{  a }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{)))}
\NormalTok{  b }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{)))}
\NormalTok{  sigma\_cafe }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{shape=} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2}\NormalTok{)), }\AttributeTok{name =} \StringTok{\textquotesingle{}sigma\_cafe\textquotesingle{}}\NormalTok{)}
\NormalTok{  sigma }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{( }\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{  Rho }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{lkj}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2}\NormalTok{), }\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2}\NormalTok{), }\AttributeTok{name =} \StringTok{\textquotesingle{}Rho\textquotesingle{}}\NormalTok{)}
\NormalTok{  cov }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{outer}\NormalTok{(sigma\_cafe, sigma\_cafe) }\SpecialCharTok{*}\NormalTok{ Rho}

\NormalTok{  a\_cafe\_b\_cafe }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{multivariatenormal}\NormalTok{(bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{squeeze}\NormalTok{(bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{stack}\NormalTok{(}\FunctionTok{list}\NormalTok{(a, b))), cov, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(N\_cafes), }\AttributeTok{name =} \StringTok{\textquotesingle{}a\_cafe\textquotesingle{}}\NormalTok{)  }

\NormalTok{  a\_cafe }\OtherTok{=}\NormalTok{ a\_cafe\_b\_cafe[, }\DecValTok{0}\NormalTok{]}
\NormalTok{  b\_cafe }\OtherTok{=}\NormalTok{ a\_cafe\_b\_cafe[, }\DecValTok{1}\NormalTok{]}

\NormalTok{  mu }\OtherTok{=}\NormalTok{ a\_cafe[cafe] }\SpecialCharTok{+}\NormalTok{ b\_cafe[cafe] }\SpecialCharTok{*}\NormalTok{ afternoon}

\NormalTok{  bi}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"y"}\NormalTok{, bi}\SpecialCharTok{$}\FunctionTok{Normal}\NormalTok{(mu, sigma), }\AttributeTok{obs=}\NormalTok{wait)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-13}

\section{Mathematical Details}\label{mathematical-details-14}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-1}

We model the relationship between the independent variable \(X\) and the
outcome variable \(Y\) with varying intercepts (\(\alpha\)) and varying
slopes (\(\beta\)) for each group (\(k\)) using the following equation:

\[
Y_{ik} = \alpha_k + \beta_k X_{ik} + \sigma
\]

Where:

\begin{itemize}
\item
  \(Y_{ik}\) is the outcome variable for observation \(i\) in group
  \(k\).
\item
  \(X_{ik}\) is the independent variable for observation \(i\) in group
  \(k\).
\item
  \(\alpha_k\) is the varying intercept for group \(k\).
\item
  \(\beta_k\) is the varying regression coefficient for group \(k\).
\item
  \(\sigma\) is the error term, assumed to be strictly positive.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
Model}}{Bayesian Model}}\label{bayesian-model-5}

We can express the Bayesian regression model accounting for prior
distributions as follows:

\[
Y_{ik} \sim \text{Normal}(\mu_{ik} , \sigma)
\] \mu\_\{ik\} = \alpha\_k + \beta\emph{k X}\{ik\} + \sigma  \[ 
\alpha_k \sim Normal(0,1) 
\] \beta\_k \sim Normal(0,1) \[ 
\sigma \sim Exponential(0,1)
\]

The varying intercepts (\(\alpha_k\)) and slopes (\(\beta_k\)) are
modeled using a \emph{Multivariate Normal distribution}:

\[ 
\begin{pmatrix} 
\alpha_k \\ 
\beta_k 
\end{pmatrix} \sim \text{MultivariateNormal}\left( 
\begin{pmatrix} 
0 \\ 
0 
\end{pmatrix}, 
\begin{pmatrix} 
\sigma_\alpha^2 & \sigma_\pi \sigma_{\alpha\rho} \\ 
\sigma_\alpha \sigma_{\pi\rho} & \sigma_\pi 
\end{pmatrix} 
\right) 
\]

Where:

\begin{itemize}
\item
  \(\left(\begin{array}{cc} 0 \\ 0 \end{array}\right)\) is the prior for
  the average intercept.
\item
  \(\left(\begin{array}{cc} \sigma_\alpha^2 & \sigma_\alpha \sigma_{\pi \rho} \\ \sigma_\alpha \sigma_{\pi \rho} & \sigma_\pi^2 \end{array}\right)\)
  is the covariance matrix which specifies the variance and covariance
  of \(\alpha_k\) and \(\beta_k\),
\item
  where:

  \begin{itemize}
  \tightlist
  \item
    \(\sigma_\alpha^2\) is the variance of \(\alpha_k\).
  \item
    \(\sigma_\pi^2\) is the variance of \(\beta_k\).
  \item
    \(\sigma_\alpha \sigma_{\pi \rho}\) is the covariance between
    \(\alpha_k\) and \(\beta_k\).
  \end{itemize}
\end{itemize}

For computational reasons, it is often better to implement a {centered
version of the varying intercept üõà} that is equivalent to the
\emph{Multivariate Normal distribution} approach:

\[
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
\sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
\]

\begin{itemize}
\item
  Where:

  \begin{itemize}
  \item
    \(\sigma_\alpha \sim \text{Exponential}(1)\) is the prior standard
    deviation among intercepts.
  \item
    \(\sigma_\beta \sim \text{Exponential}(1)\) is the prior standard
    deviation among slopes.
  \item
    \(L \sim \text{LKJcorr}(\eta)\) is the prior for the correlation
    matrix using the {Cholesky Factor üõà}.
  \end{itemize}
\end{itemize}

The full centered version of the model is thus:

\[
Y_{i} \sim \text{Normal}(\mu_k , \sigma) \\
\]

\[
\mu_k =   \alpha_k + \beta_i X_i \\
\]

\[
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
\sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
\]

\[
\alpha \sim Normal(0,1)
\] \[ 
\beta \sim Normal(0,1)
\] \[ 
\sigma_\alpha \sim Exponential(1)
\]

\[ 
\sigma_\pi \sim Exponential(1)
\]

\[ 
L \sim LKJcorr(2)
\]

\section{\texorpdfstring{\emph{Multivariate Model with One Random Slope
for Each
Variable}}{Multivariate Model with One Random Slope for Each Variable}}\label{multivariate-model-with-one-random-slope-for-each-variable}

We can apply a multivariate model similarly to
\href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{Chapter
2}. In this case, we apply the same principle, but with a covariance
matrix of a dimension equal to the number of varying slopes we define.
For example, if we want to generate random slopes for \(i\) actors in a
model with two independent variables \(X_1\) and \(X_2\), we can define
the formula as follows:

\[
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) 
\]

\[
\mu_i =   \alpha_i + \beta_{1i} X_{1i}  + \beta_{1i} X_{2i} 
\]

\[ 
\begin{pmatrix} 
\alpha_{i}\\ 
\beta_{1i}\\ 
\beta_{2i} 
\end{pmatrix} 
\sim \begin{pmatrix} 
\sigma_{\alpha}\\ 
\sigma_{\pi}\\ 
\sigma_{\gamma} 
\end{pmatrix} \circ L \cdot \begin{pmatrix} 
\widehat{\alpha}_{k} \\ 
\widehat{\pi}_{k} \\ 
\widehat{\gamma}_{k} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha} \sim Exponential(1)
\] \sigma\_\{\pi\} \sim Exponential(1) \[ 
\sigma_{\gamma} \sim Exponential(1) 
\]

\[ 
L \sim LKJcorr(2)
\]

\section{\texorpdfstring{\emph{Multivariate Random Slopes on a Single
Variable}}{Multivariate Random Slopes on a Single Variable}}\label{multivariate-random-slopes-on-a-single-variable}

For more than two varying effects, we apply the same principle but with
a covariance matrix for each varying effect that are summed to generate
the varying intercept and slope. For example, if we want to generate
random slopes for \(i\) actors and \(k\) groups, we can define the
formula as follows:

\[
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) \\
\]

\[
\mu_i =   \alpha_i + \beta_{i} X_i 
\] \[ 
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
\] \[
\beta_{i} = \beta + \beta_{actor[i]} + \beta_{group[i]} 
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\beta \sim Normal(0,1) 
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{\text{actor}} 
\end{pmatrix} 
\sim 
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} 
\end{pmatrix} \circ L_a \cdot \begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha a} \sim Exponential(1)
\] \[
\sigma_{\pi a} \sim Exponential(1)
\] \[
L_{a} \sim LKJcorr(2)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{\text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha g} \sim Exponential(1)
\] \[
\sigma_{\pi g} \sim Exponential(1) 
\] \[
L_{g} \sim LKJcorr(2)
\]

\section{Notes}\label{notes-6}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  We can apply interaction terms similarly to
  \href{./3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{Chapter
  3}.
\item
  We can apply categorical variables similarly to
  \href{./4.\%20Categorical\%20variable.qmd}{Chapter 4}.
\item
  We can apply varying slopes with any distribution presented in
  previous chapters. Below is the formula and the code snippet for a
  Binomial multivariate model with interaction between two independent
  variables \(X_1\) and \(X_2\) and multiple varying effects for each
  actor and each group
\end{itemize}

\[
p(Y_{i} |n , p_i) \sim \text{Binomial}(n = 1, p_i) \\
\]

\[
logit{p_i}=   \alpha_i + (\beta_{1i}  + \beta_{2i} X_{2i})  X_{1i}
\] \[
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
\] \[
\beta_{1i} = \beta + \beta_{1 actor[i]} + \beta_{ group[i]}
\] \[
\beta_{2i} = \beta + \beta_{2 actor[i]} + \beta_{2 group[i]}
\]

\[
\alpha \sim Normal(0,1)
\] \[
\beta \sim Normal(0,1)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{1 \, \text{actor}} \\ 
\beta_{2 \, \text{actor}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} \\ 
\sigma_{\gamma a} 
\end{pmatrix} \circ L_a \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} \\ 
\widehat{\gamma}_{ka} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha a} \sim Exponential(1) 
\]

\[
\sigma_{\pi a} \sim Exponential(1) 
\] \[
\sigma_{\gamma a} \sim Exponential(1) 
\] \[
L_{a} \sim LKJcorr(2)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{1 \, \text{group}} \\ 
\beta_{2 \, \text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} \\ 
\sigma_{\gamma g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} \\ 
\widehat{\gamma}_{kg} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha g} \sim Exponential(1)
\] \[
\sigma_{\pi g} \sim Exponential(1) 
\] \[
\sigma_{\gamma g} \sim Exponential(1)
\] \[
L_{g} \sim LKJcorr(2)
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Import data}
\NormalTok{m.read\_csv(}\StringTok{"../data/chimpanzees.csv"}\NormalTok{, sep}\OperatorTok{=}\StringTok{";"}\NormalTok{)}
\NormalTok{m.df[}\StringTok{"block\_id"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.block}
\NormalTok{m.df[}\StringTok{"treatment"}\NormalTok{] }\OperatorTok{=} \DecValTok{1} \OperatorTok{+}\NormalTok{ m.df.prosoc\_left }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ m.df.condition}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}treatment\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}actor\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}block\_id\textquotesingle{}}\NormalTok{])}


\KeywordTok{def}\NormalTok{ model(tid, actor, block\_id, L}\OperatorTok{=}\VariableTok{None}\NormalTok{, link}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
    \CommentTok{\# fixed priors}
\NormalTok{    g }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}g\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    sigma\_actor }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\_actor\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    L\_Rho\_actor }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{"L\_Rho\_actor"}\NormalTok{)}
\NormalTok{    sigma\_block }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"sigma\_block"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    L\_Rho\_block }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{"L\_Rho\_block"}\NormalTok{)}

    \CommentTok{\# adaptive priors {-} non{-}centered}
\NormalTok{    z\_actor }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"z\_actor"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{))}
\NormalTok{    z\_block }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"z\_block"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ deterministic(}
        \StringTok{"alpha"}\NormalTok{, ((sigma\_actor[..., }\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ L\_Rho\_actor) }\OperatorTok{@}\NormalTok{ z\_actor).T}
\NormalTok{    )}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ deterministic(}
        \StringTok{"beta"}\NormalTok{, ((sigma\_block[..., }\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ L\_Rho\_block) }\OperatorTok{@}\NormalTok{ z\_block).T}
\NormalTok{    )}

\NormalTok{    logit\_p }\OperatorTok{=}\NormalTok{ g[tid] }\OperatorTok{+}\NormalTok{ alpha[actor, tid] }\OperatorTok{+}\NormalTok{ beta[block\_id, tid]}
\NormalTok{    dist(}\StringTok{"L"}\NormalTok{, dist.Binomial(logits}\OperatorTok{=}\NormalTok{logit\_p), obs}\OperatorTok{=}\NormalTok{L)}

    \CommentTok{\# compute ordinary correlation matrixes from Cholesky factors}
    \ControlFlowTok{if}\NormalTok{ link:}
\NormalTok{        deterministic(}\StringTok{"Rho\_actor"}\NormalTok{, L\_Rho\_actor }\OperatorTok{@}\NormalTok{ L\_Rho\_actor.T)}
\NormalTok{        deterministic(}\StringTok{"Rho\_block"}\NormalTok{, L\_Rho\_block }\OperatorTok{@}\NormalTok{ L\_Rho\_block.T)}
\NormalTok{        deterministic(}\StringTok{"p"}\NormalTok{, expit(logit\_p))}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Reference(s)}\label{references-12}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Gaussian Processes}\label{gaussian-processes}

\section{General Principles}\label{general-principles-14}

Through varying intercepts and slopes we have seen how quantify some of
the unique features that generate variation across clusters and
covariance among the observations within each cluster. But through the
covariance matrix that is used to account for correlation between
clusters, we are assuming inherently linear relationships between
clusters. What if we want to model the relationship between two
variables that are not linearly related? In this case, we can use a
Gaussian Process (GP) to model the relationship between two variables.
Basically, a GP is a a varying slope model with a covariance matrix
where each element of the matrix is a
\phantomsection\label{kernel}{{kernel function üõà}}.

\section{Considerations}\label{considerations-15}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  To capture complex, non-linear relationships in data where the
  underlying function is smooth but has an unknown functional form, GPs
  use a \phantomsection\label{kernel}{{kernel üõà}}
\item
  GPs assume normally distributed errors and may not be appropriate for
  all types of noise
\item
  The choice of kernel hyperparameters can significantly impact results,
  thus GPs require choosing an appropriate kernel function that captures
  the expected behavior of your data.
\item
  Though kernel definition we can incorporate domain knowledge.
\item
  They scale poorly with dataset size (O(n¬≥) complexity) due to matrix
  operations, thus memory requirements can be substantial for large
  datasets which as lead neural networks to be used instead to resolve
  large non linear problems.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-14}

Below is an example code snippet demonstrating Gaussian Process
regression using the Bayesian Inference (BI) package. Data consist in a
continuous dependet variable (\emph{total\_tools}) representing the
number of tools inveted in the islands, and a continuous independent
variable (\emph{population}) representing the population of the islands.
The goal is to estimate the effect of population on the total tools. We
use the distance amtrix of the islands for the kernel function in order
to capture the spatial dependence of the relationship.

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import} \OperatorTok{*}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\NormalTok{m.data(}\StringTok{\textquotesingle{}resources/data/Kline2.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{";"}\NormalTok{)}
\NormalTok{islandsDistMatrix }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}resources/data/islandsDistMatrix.csv\textquotesingle{}""}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}total\_tools\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{])}
\NormalTok{m.data\_on\_model[}\StringTok{"society"}\NormalTok{] }\OperatorTok{=}\NormalTok{ jnp.arange(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{)}\CommentTok{\# index observations}
\NormalTok{m.data\_on\_model[}\StringTok{"Dmat"}\NormalTok{] }\OperatorTok{=}\NormalTok{ islandsDistMatrix.values }\CommentTok{\# Distance matrix}


\KeywordTok{def}\NormalTok{ model(Dmat, population, society, total\_tools):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    g }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}g\textquotesingle{}}\NormalTok{)}

    \CommentTok{\# non{-}centered Gaussian Process prior}
\NormalTok{    etasq }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}etasq\textquotesingle{}}\NormalTok{)}
\NormalTok{    rhosq }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}rhosq\textquotesingle{}}\NormalTok{)}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}z\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{10}\NormalTok{,))}
\NormalTok{    SIGMA, L\_SIGMA, k }\OperatorTok{=}\NormalTok{ bi.kernel\_sq\_exp(Dmat, z, etasq, rhosq, }\FloatTok{0.01}\NormalTok{)}
 
\NormalTok{    lambda\_ }\OperatorTok{=}\NormalTok{ a }\OperatorTok{*}\NormalTok{ population}\OperatorTok{**}\NormalTok{b }\OperatorTok{/}\NormalTok{ g }\OperatorTok{*}\NormalTok{ jnp.exp(k[society])}

\NormalTok{    lk(}\StringTok{"T"}\NormalTok{, Poisson(lambda\_), obs}\OperatorTok{=}\NormalTok{total\_tools)}

\CommentTok{\# Run sampler {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, num\_samples}\OperatorTok{=}\DecValTok{500}\NormalTok{) }
\NormalTok{m.summary()}
\end{Highlighting}
\end{Shaded}

\section{R}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\NormalTok{bi }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"main"}\NormalTok{)}

\CommentTok{\# Setup device {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\FunctionTok{bi}\NormalTok{(}\AttributeTok{platform=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}resources/data/Kline2.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{sep=}\StringTok{";"}\NormalTok{)}
\NormalTok{islandsDistMatrix }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{pd}\SpecialCharTok{$}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}resources/data/islandsDistMatrix.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{index\_col=}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{0}\NormalTok{))}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{data\_to\_model}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}total\_tools\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{))}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{society }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{arange}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{, }\AttributeTok{dtype=}\StringTok{\textquotesingle{}int64\textquotesingle{}}\NormalTok{)}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{data\_on\_model}\SpecialCharTok{$}\NormalTok{Dmat }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{array}\NormalTok{(islandsDistMatrix)}


\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(Dmat, population, society, total\_tools)\{}

\NormalTok{  a }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{  b }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{  g }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}g\textquotesingle{}}\NormalTok{)}
  
  \CommentTok{\# non{-}centered Gaussian Process prior}
\NormalTok{  etasq }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}etasq\textquotesingle{}}\NormalTok{)}
\NormalTok{  rhosq }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{exponential}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}rhosq\textquotesingle{}}\NormalTok{)}
\NormalTok{  z }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{dist}\SpecialCharTok{$}\FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}z\textquotesingle{}}\NormalTok{, }\AttributeTok{shape =} \FunctionTok{tuple}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(}\DecValTok{10}\NormalTok{)))}
\NormalTok{  r }\OtherTok{=}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{gaussian}\SpecialCharTok{$}\FunctionTok{kernel\_sq\_exp}\NormalTok{(Dmat, z, etasq, rhosq, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{  SIGMA }\OtherTok{=}\NormalTok{ r[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{  L\_SIGMA }\OtherTok{=}\NormalTok{ r[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{  k }\OtherTok{=}\NormalTok{ r[[}\DecValTok{3}\NormalTok{]]}
\NormalTok{  lambda\_ }\OtherTok{=}\NormalTok{ a }\SpecialCharTok{*}\NormalTok{ population}\SpecialCharTok{**}\NormalTok{b }\SpecialCharTok{/}\NormalTok{ g }\SpecialCharTok{*}\NormalTok{ bi}\SpecialCharTok{$}\NormalTok{jnp}\SpecialCharTok{$}\FunctionTok{exp}\NormalTok{(k[society])}
\NormalTok{  bi}\SpecialCharTok{$}\FunctionTok{lk}\NormalTok{(}\StringTok{"T"}\NormalTok{, bi}\SpecialCharTok{$}\FunctionTok{Poisson}\NormalTok{(lambda\_), }\AttributeTok{obs=}\NormalTok{total\_tools)}
\NormalTok{\}}

\CommentTok{\# Run MCMC {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m}\SpecialCharTok{$}\NormalTok{sampler}\SpecialCharTok{$}\FunctionTok{print\_summary}\NormalTok{(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-15}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-2}

The following equation allows us to evaluate the relationship between
dependent variable \(Y\) and independent variable \(X\) while
incorporating a GP for variable \(Z\) :

\[
Y_i = \alpha + \beta  X_i + \gamma_{Z_i}
\]

where: - \(Y_i\) is the i-th value for the dependent variable \(Y\).

\begin{itemize}
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta\) is the regression coefficient term.
\item
  \(X_i\) is the i-th value for independent variable \(X\).
\item
  \(\gamma_{Z_i}\) is the gaussian process i-th value for independent
  variable \(Z\).
\end{itemize}

The GP \(\gamma_{Z_i}\) follow a multivariate normal distribution:

\[
\begin{pmatrix}
    Z_1 \\
    \vdots \\
    Z_{n}
\end{pmatrix}
\sim MVNormal \left(
\begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix},
K
\right)
\]

where:

\begin{itemize}
\item
  \((Z_1, ..., Z_n)\) represents a collection of all values of the
  random variable \(Z\).
\item
  \((0, ..., 0)\) represents the mean vector of the multivariate normal
  distribution of the same size as the number of random variable and set
  to \phantomsection\label{kernelMean0}{{zero üõà}}.
\item
  \(K\) is the covariance matrix of the random variable \(Z\). Each
  element \(K{ij}\) of the matrix is given by the kernel function
  evaluated at the corresponding points: \(K_{ij} = k(Z_i,Z_j)\)
\end{itemize}

\[
K = \begin{pmatrix}
    k(Z_1, Z_1) & k(Z_1, Z_2) & \cdots & k(Z_1, Z_{n}) \\
    k(Z_2, Z_1) & k(Z_2, Z_2) & \cdots & k(Z_2, Z_{n}) \\
    \vdots & \vdots & \ddots & \vdots \\
    k(Z_{n}, Z_1) & k(Z_{n}, Z_2) & \cdots & k(Z_{n}, Z_{n})
\end{pmatrix}
\]

\begin{itemize}
\tightlist
\item
  Multiple kernel function exist and will be discussed in the
  \hyperref[notes]{Note(s)} section. But the most common one is the
  quadratic kernel:
\end{itemize}

\[
K_{ij} = \eta^2 exp(-p^2D_{ij}^2) + \delta_{ij} \sigma^2 
\]

Where:

\begin{itemize}
\item
  \(\eta\) is the signal variance, representing the overall variance of
  the outputs of the Gaussian process. It scales the influence of the
  kernel function. A larger \(\eta^2\) indicates a wider range of values
  the function can take.
\item
  \(p\) determines the rate of decline.
\item
  \(D_{ij}\) is the distance between the \(i\)-th and \(j\)-th points.
\item
  \(\delta_{ij}\) is the Kronecker delta taking a value of zero when
  \(i = j\), allowing to included in the calculation the
  self-covariance.
\item
  \(\sigma^2\) is the noise variance, which accounts for the observation
  noise in the data. It represents the uncertainty or variability in the
  measurements or outputs at each point. The term effectively adds this
  noise variance only when \(i = ùëó\), ensuring that the diagonal
  elements of the covariance matrix represent the total variance at each
  input point.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-6}

In the Bayesian formulation, we define each parameter with
\phantomsection\label{prior}{{priors üõà}}. We can express a Bayesian
version of this GP using the following model:

\[
Y_i = \alpha + \beta  X_i + \gamma_{Z_i}
\]

\[
\gamma \sim MVNormal \left(
\begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix},
K
\right)
\]

\[
K_{ij} = \eta^2 exp(-p^2D_{ij}^2) + \delta_{ij} \sigma^2 
\]

\[
\alpha \sim Normal(0,1)
\]

\[
\eta^2 \sim HalfCauchy(0,1)
\]

\[
p^2 \sim HalfCauchy(0,1)
\]

where:

\begin{itemize}
\item
  \(Y_i\) is the i-th value for the dependent variable \(Y\).
\item
  \(\alpha\) is the intercept term with prior \(Normal(0,1)\).
\item
  \(\beta\) is the regression coefficient term with prior
  \(Normal(0,1)\).
\item
  \(X_i\) is the i-th value for independent variable \(X\).
\item
  \(\gamma_{Z_i}\) is the gaussian process i-th value for independent
  variable \(Z\).
\item
  \(\gamma\) is the latent function modeled by the GP.
\item
  \(K_{ij}\) is the kernel function evaluated at the corresponding
  points: \(K_{ij} = k(Z_i,Z_j)\) with priors HalfCauchy(0,1) for
  \(\eta^2\) and \(p^2\) to ensure positive values.
\end{itemize}

\section{Notes}\label{notes}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

Common kernel functions include:

\begin{itemize}
\item
  \emph{Radial Basis Function} (RBF) or Squared Exponential Kernel:
  \[k(x,x') = \sigma^2 \exp\left(-\frac{||x-x'||^2}{2l^2}\right)\]
\item
  \emph{Rational Quadratic Kernel}, this kernel is equivalent to adding
  together many RBF kernels with different length scales:
  \[k(x,x') = \sigma^2 \left(1 + \frac{||x-x'||^2}{2l^2}\right)^{-\alpha}\]
\item
  \emph{Periodic kernel} allows to model functions which repeat
  themselves exactly:
  \[k(x,x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi||x-x'||/p)}{l^2}\right)\]
\item
  \emph{Locally Periodic Kernel}:
\end{itemize}

\[k(x,x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi||x-x'||/p)}{l^2}\right) \exp\left(-\frac{||x-x'||^2}{2l^2}\right)\]

\begin{itemize}
\item
  GPs can be extended to classification problems using link functions
  Multi-output problems using matrix-valued kernels Deep learning
  through Deep Kernel Learning
\item
  Computational tricks for large datasets include:

  \begin{itemize}
  \tightlist
  \item
    Sparse approximations (e.g., FITC, VFE)
  \item
    Inducing points methods
  \item
    Random Fourier features
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-13}

McElreath (2018)

https://www.cs.toronto.edu/\textasciitilde duvenaud/cookbook/

\bookmarksetup{startatroot}

\chapter{Measuring error}\label{measuring-error}

\section{General Principles}\label{general-principles-15}

Measuring error refers to the variability in the measurement of a
variable, which can be caused by various factors such as measurement
error, sampling error, or measurement bias. It is an important
consideration in many fields, including statistics, economics, and
engineering, where accurate measurements are crucial for making informed
decisions.

In order to account for measurement error, we can use a measurement
error model. This model assumes that the measurement of a variable is
subject to measurement error, which can be modeled using a probability
distribution. The model can be used to estimate the parameters of the
measurement error distribution, such as the mean and variance, and to
make predictions about the measurements based on the estimated
parameters.

Additionally, measurement error can be generated by several factors,
such as sampling bias, censoring bias, and group size heterogeneity.

Measurement error models are composed models (i.e., models with
sub-models) that evaluate different generative processes, starting with
the measurement error process, which is then used to generate the
observed data.

\section{Example}\label{example-15}

Below is an example code snippet demonstrating Bayesian Measuring error
model using the Bayesian Inference (BI) package. The data consist of
three continuous variables (marriage rate, divorce rates, age), and the
goal is to estimate the effect of age and marriage rate on divorce
rates, while considering that your divorce rate is have a measurement
error.

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-16}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-8}

\[
D_i^* \sim Normal(D_i, \sigma_i)
\]

\[
D_i \sim Normal(\mu_i, \sigma)
\]

\[
\mu_i = \alpha + \beta_A A_i + \beta_M M_i 
\]

\[
\sigma \sim Normal(1)
\]

where:

\begin{itemize}
\item
  \(D_i^*\) is the observed divorce rate.
\item
  \(D_i\) is the true divorce rate.
\item
  \(\mu_i\) is the mean of the true divorce rate.
\item
  \(\sigma\) is the standard deviation of the true divorce rate.
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta_A\) is the regression coefficient for age.
\item
  \(\beta_M\) is the regression coefficient for marriage rate.
\end{itemize}

\section{Notes}\label{notes-7}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

This is am approach that can be extended to any kind of models
previously described. For example, one could generate a bernoulli
measurement error model by generating the process of probability of
success and of failure. We can even go further by potentially having
error rate that is present only in one of the two outcomes.

\end{tcolorbox}

\section{Reference(s)}\label{references-14}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Missing data}\label{missing-data}

\section{General Principles}\label{general-principles-16}

\section{Considerations}\label{considerations-16}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Example}\label{example-16}

Below is an example code snippet demonstrating Bayesian Missing data
model using the Bayesian Inference (BI) package:

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-17}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-12}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-9}

\section{Notes}\label{notes-8}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Reference(s)}\label{references-15}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Latent Variables}\label{latent-variables}

\section{General Principles}\label{general-principles-17}

In some scenarios, the observed data does not directly reflect the
underlying structure or factors influencing the outcome. Instead,
\emph{latent variables} --- variables that are not directly observed but
inferred from the data --- can help model this hidden structure. These
latent variables capture unobserved factors that affect the relationship
between predictors (\emph{X}) and the outcome (\emph{Y}).

We model the relationship between the predictor variables (\emph{X}) and
the outcome variable (\emph{Y}) with a latent variable (\emph{Z}) as
follows:

\[
Y = f(X, Z) + \epsilon
\]

Where: - \emph{Y} is the observed outcome variable. - \emph{X} is the
observed predictor variable(s). - \emph{Z} is the latent (unobserved)
variable, which we aim to infer. - \emph{f(X, Z)} is the function that
relates \emph{X} and \emph{Z} to \emph{Y}. - \emph{\epsilon} is the
error term, typically assumed to be normally distributed with mean 0 and
variance \emph{\sigma\^{}2}.

The latent variable \emph{Z} can represent various phenomena, such as
group-level effects, time-varying trends, or individual-level factors,
that are not captured by the observed predictors alone.

\section{Considerations}\label{considerations-17}

In Bayesian regression with latent variables, we consider the
uncertainty in both the observed and latent variables. We declare prior
distributions for the latent variables, in addition to the usual priors
for regression coefficients and intercepts. These latent variables are
often modeled using Gaussian distributions (\emph{Normal} priors) or
more flexible distributions such as \emph{Multivariate Normal} for
correlations among the latent variables.

The goal is to infer the posterior distribution over both the parameters
and the latent variables, given the observed data.

\section{Example}\label{example-17}

Below is an example code snippet demonstrating Bayesian regression with
latent variables using TensorFlow Probability:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Data Simulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{NY }\OperatorTok{=} \DecValTok{4}  \CommentTok{\# Number of dependent variables or outcomes (e.g., dimensions for latent variables)}
\NormalTok{NV }\OperatorTok{=} \DecValTok{8}  \CommentTok{\# Number of observations or individual{-}level data points (e.g., subjects)}

\CommentTok{\# Initialize the matrix Y2 with shape (NV, NY) filled with NaN values, to be filled later}
\NormalTok{Y2 }\OperatorTok{=}\NormalTok{ np.full((NV, NY), np.nan)}

\CommentTok{\# Generate the means and offsets for the data}
\CommentTok{\# means: Generate random normal means for each of the NY outcomes}
\CommentTok{\# offsets: Generate random normal offsets for each of the NV observations}
\NormalTok{means }\OperatorTok{=}\NormalTok{ bi.distribution.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(NY,), sample }\OperatorTok{=} \VariableTok{True}\NormalTok{, seed }\OperatorTok{=} \DecValTok{10}\NormalTok{)}
\NormalTok{offsets }\OperatorTok{=}\NormalTok{ bi.distribution.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(NV,}\DecValTok{1}\NormalTok{), sample }\OperatorTok{=} \VariableTok{True}\NormalTok{, seed }\OperatorTok{=} \DecValTok{20}\NormalTok{)}

\CommentTok{\# Fill the matrix Y2 with simulated data based on the generated means and offsets}
\CommentTok{\# Each observation (i) is the sum of an individual{-}specific offset and an outcome{-}specific mean}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(NV):}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(NY):}
\NormalTok{        Y2[i, k] }\OperatorTok{=}\NormalTok{ means[k] }\OperatorTok{+}\NormalTok{ offsets[i]}

\CommentTok{\# Simulate individual{-}level random effects (e.g., random slopes or intercepts)}
\CommentTok{\# b\_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates}
\NormalTok{b\_individual }\OperatorTok{=}\NormalTok{ bi.distribution.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{N, K), sample }\OperatorTok{=} \VariableTok{True}\NormalTok{, seed }\OperatorTok{=} \DecValTok{0}\NormalTok{)}

\CommentTok{\# mu: Add an additional effect \textquotesingle{}a\textquotesingle{} to the individual{-}level random effects \textquotesingle{}b\_individual\textquotesingle{}}
\CommentTok{\# \textquotesingle{}a\textquotesingle{} could represent a population{-}level effect or a baseline}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ b\_individual }\OperatorTok{+}\NormalTok{ a}

\CommentTok{\# Convert Y2 to a JAX array for further computation in a JAX{-}based framework}
\NormalTok{Y2 }\OperatorTok{=}\NormalTok{ jnp.array(Y2)}


\CommentTok{\# Set data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{dat }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    NY }\OperatorTok{=}\NormalTok{ NY,}
\NormalTok{    NV }\OperatorTok{=}\NormalTok{ NV,}
\NormalTok{    Y2 }\OperatorTok{=}\NormalTok{ Y2}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(NY, NV, Y2):}
\NormalTok{    means }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(NY,), name }\OperatorTok{=} \StringTok{\textquotesingle{}means\textquotesingle{}}\NormalTok{)}
\NormalTok{    offset }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(NY,}\DecValTok{1}\NormalTok{), name }\OperatorTok{=} \StringTok{\textquotesingle{}offset\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ bi.dist.exponential(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(NY,), name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{  numpyro.sample(}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{, numpyro.distributions.Exponential(}\DecValTok{1}\NormalTok{).expand([NY])) }
\NormalTok{    tmp }\OperatorTok{=}\NormalTok{ jnp.tile(means, (NV, }\DecValTok{1}\NormalTok{)).reshape(NV,NY)  }
\NormalTok{    mu\_l }\OperatorTok{=}\NormalTok{ tmp }\OperatorTok{+}\NormalTok{ offset }
\NormalTok{    numpyro.sample(}\StringTok{\textquotesingle{}Y2\textquotesingle{}}\NormalTok{, Normal(mu\_l, jnp.tile(sigma, [NV, }\DecValTok{1}\NormalTok{])), obs}\OperatorTok{=}\NormalTok{Y2)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-18}

We can express the Bayesian latent variable model using probability
distributions as follows:

\[
\begin{aligned}
& p(Y | X, Z, W, \sigma) = \text{Normal}(X * W + Z, \sigma^2) \\
& p(Z) = \text{Normal}(0, \tau^2) \\
& p(W) = \text{Normal}(0, \alpha^2) \\
\end{aligned}
\]

Where: - \emph{p(Y \textbar{} X, Z, W, \sigma)} is the likelihood
function for the observed outcome variable, which depends on both the
observed predictor \emph{X} and the latent variable \emph{Z}. -
\emph{p(Z)} is the prior distribution for the latent variable \emph{Z},
often modeled as \emph{Normal} with a mean of 0 and variance
\emph{\tau\^{}2}. - \emph{p(W)} is the prior distribution for the
regression coefficient(s) \emph{W}, typically assumed to follow a
\emph{Normal} distribution with mean 0 and variance \emph{\alpha\^{}2}.

The latent variable \emph{Z} introduces additional flexibility to the
model, capturing unobserved influences on the outcome \emph{Y}.

\section{Interpretation of Latent
Variables}\label{interpretation-of-latent-variables}

\begin{itemize}
\item
  \textbf{Latent Variable (\emph{Z})}: Represents hidden factors not
  captured by the observed variables, allowing the model to explain more
  of the variance in the outcome. For instance, in a psychological
  model, \emph{Z} might represent a latent trait such as intelligence or
  anxiety that influences the outcome.
\item
  \textbf{Posterior Inference}: The posterior distribution of the latent
  variable \emph{Z} can give insights into how much the unobserved
  factors contribute to the outcome.
\end{itemize}

\section{Use Cases}\label{use-cases}

\begin{itemize}
\tightlist
\item
  \textbf{Latent Factors in Psychometrics}: In psychometric models,
  latent variables represent traits or abilities that are not directly
  observed, such as cognitive ability or personality traits.
\item
  \textbf{Time-Varying Effects}: Latent variables can represent
  unobserved time trends or individual-specific effects in time-series
  or longitudinal models.
\item
  \textbf{Mixed Models}: In hierarchical or mixed models, latent
  variables can represent group-specific intercepts or slopes.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Principal Component
Analysis}\label{principal-component-analysis}

\section{General Principles}\label{general-principles-18}

\textbf{Principal Component Analysis (PCA)} is a technique used to
reduce the dimensionality of a dataset by transforming it into a new
coordinate system where the greatest variance by any projection of the
data comes to lie on the first coordinates (called principal
components). This method helps capture the underlying structure of
high-dimensional data by identifying patterns based on variance.

In \textbf{Bayesian PCA}, uncertainty in the model parameters is
explicitly taken into account by using a probabilistic framework. This
allows us to not only estimate the principal components but also
quantify the uncertainty around them, as well as avoid overfitting by
incorporating prior knowledge.

\subsection{Goal:}\label{goal}

\begin{itemize}
\item
  \textbf{Reduce dimensionality} while retaining as much variance as
  possible.
\item
  \textbf{Infer posterior distributions} over the principal components,
  instead of point estimates, by incorporating prior distributions over
  the parameters.
\end{itemize}

\subsection{Use Cases}\label{use-cases-1}

\begin{itemize}
\item
  \textbf{Dimensionality Reduction}: Bayesian PCA is commonly used to
  reduce the dimensionality of high-dimensional datasets while
  incorporating uncertainty about the latent structure.
\item
  \textbf{Data Visualization}: By projecting data into a
  lower-dimensional space, PCA helps in visualizing high-dimensional
  datasets in 2D or 3D plots.
\item
  \textbf{Noise Modeling}: Bayesian PCA provides an advantage over
  classical PCA by explicitly modeling noise and accounting for
  uncertainty in the data.
\item
  \textbf{Feature Extraction}: The latent variables learned by Bayesian
  PCA can serve as new features for downstream tasks, such as
  classification or clustering.
\item
  \textbf{latent variable modeling}: The latent variables learned by
  Bayesian PCA can serve as new features for downstream tasks, such as
  classification or clustering.
\end{itemize}

\section{Considerations}\label{considerations-18}

In \textbf{Bayesian PCA}, we assume prior distributions for the latent
variables \emph{Z} and the principal component loadings \emph{W}. We
place Gaussian priors on both \emph{Z} and \emph{W}, and learn their
posterior distributions using the observed data \emph{X}.

This approach differs from traditional PCA by allowing the posterior
distributions to reflect uncertainty in the model parameters.

\section{Example}\label{example-18}

Here is an example code snippet demonstrating Bayesian PCA using
TensorFlow Probability:

\begin{Shaded}
\begin{Highlighting}[]

\ImportTok{from}\NormalTok{ main }\ImportTok{import} \OperatorTok{*}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Data simulation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{plt.style.use(}\StringTok{"ggplot"}\NormalTok{)}
\NormalTok{warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{)}

\NormalTok{num\_datapoints }\OperatorTok{=} \DecValTok{5000}
\NormalTok{data\_dim }\OperatorTok{=} \DecValTok{2}
\NormalTok{latent\_dim }\OperatorTok{=} \DecValTok{1}
\NormalTok{stddv\_datapoints }\OperatorTok{=} \FloatTok{0.5}

\CommentTok{\# Simulate data}
\KeywordTok{def}\NormalTok{ sim\_data(data\_dim, latent\_dim, num\_datapoints, stddv\_datapoints, seed }\OperatorTok{=} \DecValTok{0}\NormalTok{): }
\NormalTok{    w }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(data\_dim, latent\_dim), name}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, sample}\OperatorTok{=}\VariableTok{True}\NormalTok{, seed}\OperatorTok{=}\NormalTok{seed)}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(latent\_dim, num\_datapoints), name}\OperatorTok{=}\StringTok{\textquotesingle{}z\textquotesingle{}}\NormalTok{, sample}\OperatorTok{=}\VariableTok{True}\NormalTok{, seed}\OperatorTok{=}\NormalTok{seed)}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ bi.dist.normal(w }\OperatorTok{@}\NormalTok{ z, stddv\_datapoints, name}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, sample}\OperatorTok{=}\VariableTok{True}\NormalTok{, seed}\OperatorTok{=}\NormalTok{seed)}
    \ControlFlowTok{return}\NormalTok{ w, z, x}

\NormalTok{actual\_w, actual\_z, x\_train }\OperatorTok{=}\NormalTok{sim\_data(data\_dim, latent\_dim, num\_datapoints, stddv\_datapoints, seed }\OperatorTok{=} \DecValTok{20}\NormalTok{)}
\NormalTok{plt.scatter(x\_train[}\DecValTok{0}\NormalTok{, :], x\_train[}\DecValTok{1}\NormalTok{, :], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{plt.axis([}\OperatorTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\OperatorTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{])}
\NormalTok{plt.title(}\StringTok{"Data set"}\NormalTok{)}
\NormalTok{plt.show()}


\CommentTok{\# Model using simulated data}
\KeywordTok{def}\NormalTok{ model(x\_train, data\_dim, latent\_dim, num\_datapoints, stddv\_datapoints, seed }\OperatorTok{=} \DecValTok{0}\NormalTok{): }
\NormalTok{    w }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(data\_dim, latent\_dim), name}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{)}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{(latent\_dim, num\_datapoints), name}\OperatorTok{=}\StringTok{\textquotesingle{}z\textquotesingle{}}\NormalTok{)}
\NormalTok{    lk(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, Normal(w }\OperatorTok{@}\NormalTok{ z, stddv\_datapoints), obs }\OperatorTok{=}\NormalTok{ x\_train)  }
    
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    x\_train }\OperatorTok{=}\NormalTok{ x\_train, }
\NormalTok{    data\_dim }\OperatorTok{=}\NormalTok{ data\_dim, }
\NormalTok{    latent\_dim }\OperatorTok{=}\NormalTok{ latent\_dim, }
\NormalTok{    num\_datapoints }\OperatorTok{=}\NormalTok{ num\_datapoints, }
\NormalTok{    stddv\_datapoints }\OperatorTok{=}\NormalTok{ stddv\_datapoints}
\NormalTok{)}

\NormalTok{m.run(model) }
\NormalTok{summary }\OperatorTok{=}\NormalTok{ m.summary()}
\NormalTok{real\_data }\OperatorTok{=}\NormalTok{ jnp.concatenate([actual\_w.flatten(), actual\_z.flatten()]) }\CommentTok{\# concatenate the actual values of w and z}
\NormalTok{posteriors }\OperatorTok{=}\NormalTok{ summary.iloc[:,}\DecValTok{0}\NormalTok{]}


\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.plot(real\_data, posteriors, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Posteriors\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{pca1.png}

\section{Mathematical Details}\label{mathematical-details-19}

\section{Formulation}\label{formulation}

Given an observed data matrix \(X \in \mathbb{R}^{N \times D}\) (where
\emph{N} is the number of samples and \emph{D} is the number of
dimensions), we assume the data is generated by a lower-dimensional
latent variable model:

\[
X = ZW^T + \epsilon
\]

\[
Z = \mathcal{Normal}(0, 1)
\]

\[
W = \mathcal{Normal}(0, 1)
\]

\[
\epsilon \sim \mathcal{Exponential}(1)
\]

Where:

\begin{itemize}
\item
  \(X\) is the observed data matrix.
\item
  \(Z \in \mathbb{R}^{N \times K}\) is the latent variable matrix
  (latent features with \(K \ll D\)). \(Z\) is define by normal
  distribution with mean 0 and variance 1.
\item
  \(W \in \mathbb{R}^{D \times K}\) is the matrix of principal
  components (\emph{projection matrix}). \(W\) is define by normal
  distribution with mean 0 and variance 1.
\item
  \(\epsilon\) is Gaussian noise, assumed to be normally distributed:
  \(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\).
\end{itemize}

\section{Note}\label{note}

\begin{itemize}
\tightlist
\item
  To account for \textbf{\phantomsection\label{signAmbiguity}{{sign
  ambiguity üõà}}} in PCA occur due to we can align the signs of the
  estimated parameters with the true parameters before comparison by
  calculating the dot product between the true parameters and the
  estimated parameters and if the dot product is negative, multiply the
  estimated parameters by -1 to align them with the true parameters.
  Bellow a code snippet to highlight how to do it:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{true\_params }\OperatorTok{=}\NormalTok{ jnp.array(real\_data)      }
\NormalTok{estimated\_params }\OperatorTok{=}\NormalTok{ jnp.array(posteriors) }

\CommentTok{\# Compute dot product}
\NormalTok{dot\_product }\OperatorTok{=}\NormalTok{ jnp.dot(true\_params, estimated\_params)}

\CommentTok{\# Align signs if necessary}
\ControlFlowTok{if}\NormalTok{ dot\_product }\OperatorTok{\textless{}} \DecValTok{0}\NormalTok{:}
\NormalTok{    estimated\_params }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{estimated\_params}

\CommentTok{\# Plot the aligned parameters}
\NormalTok{plt.scatter(true\_params, estimated\_params, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{plt.plot([}\BuiltInTok{min}\NormalTok{(true\_params), }\BuiltInTok{max}\NormalTok{(true\_params)], [}\BuiltInTok{min}\NormalTok{(true\_params), }\BuiltInTok{max}\NormalTok{(true\_params)], }\StringTok{\textquotesingle{}r{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}True Parameters\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Estimated Parameters\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}True vs. Estimated Parameters After Sign Alignment\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{pca2.png}

\bookmarksetup{startatroot}

\chapter{Modeling Network}\label{modeling-network}

A network represents the relationships (links) between entities (nodes).
These links can be weighted (weighted network) or unweighted (binary
network), directed (directed network) or undirected (undirected
network). Regardless of their type, networks generate links shared by
nodes, leading to data dependency when modeling the network. One
proposed solution is to model network links with random
\href{12.\%20Varying\%20intercepts.qmd}{intercepts} and
\href{13.\%20Varying\%20slopes.qmd}{effects}. By adding such parameters
to the model, we can account for the correlations between node link
relationships.

\section{Considerations}\label{considerations-19}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  The particularity here is that varying intercepts and slopes are
  generated for both \phantomsection\label{NodeF}{{nodal effects üõà}} and
  \phantomsection\label{DyadicF}{{dyadic effects üõà}}. Those the varying
  intercepts and slopes are identical to those described in previous
  chapters and will therefore not be detailed further and only the
  random centered version of the varying slopes will be described here.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-19}

Below is an example code snippet demonstrating Bayesian network model
with send-receiver effect:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Building model and sampling it {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{ids }\OperatorTok{=}\NormalTok{ jnp.arange(}\DecValTok{0}\NormalTok{,data[}\StringTok{\textquotesingle{}N\_id\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{])}
\NormalTok{idx }\OperatorTok{=}\NormalTok{ bi.net.vec\_node\_to\_edgle(jnp.stack([ids, ids], axis }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{1}\NormalTok{))}

\AttributeTok{@jit}
\KeywordTok{def}\NormalTok{ logit(x):}
    \ControlFlowTok{return}\NormalTok{ jnp.log(x }\OperatorTok{/}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x))}

\KeywordTok{def}\NormalTok{ model2(idx, result\_outcomes, dyad\_effects, focal\_individual\_predictors, target\_individual\_predictors):}
\NormalTok{    N\_id }\OperatorTok{=}\NormalTok{ ids.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Sender Receiver effect (SR) its shape is equal to N\_id {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
    \CommentTok{\#\# Varying intercept and slope for SR}
\NormalTok{    sr\_rf, sr\_raw, sr\_sigma, sr\_L }\OperatorTok{=}\NormalTok{ bi.net.nodes\_random\_effects(N\_id, cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{) }
\NormalTok{    sender\_receiver }\OperatorTok{=}\NormalTok{ sr\_rf}

    \CommentTok{\# Dyadic effect (D) its shape is equal to n dyads {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
    \CommentTok{\#\# Varying intercept and slope for D}
\NormalTok{    rf, dr\_raw, dr\_sigma, dr\_L }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_random\_effects(sender\_receiver.shape[}\DecValTok{0}\NormalTok{], cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{)}
\NormalTok{    dr }\OperatorTok{=}\NormalTok{  rf}

\NormalTok{    lk(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, Poisson(jnp.exp( sender\_receiver }\OperatorTok{+}\NormalTok{ dr )), obs}\OperatorTok{=}\NormalTok{result\_outcomes)}

\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    idx }\OperatorTok{=}\NormalTok{ idx,}
\NormalTok{    result\_outcomes }\OperatorTok{=}\NormalTok{ bi.net.mat\_to\_edgl(data[}\StringTok{\textquotesingle{}outcomes\textquotesingle{}}\NormalTok{]), }
\NormalTok{    dyad\_effects }\OperatorTok{=}\NormalTok{ bi.net.prepare\_dyadic\_effect(kinship), }\CommentTok{\# Can be a jax array of multiple dimensions}
\NormalTok{    focal\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{],}
\NormalTok{    target\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{]}
\NormalTok{)}

\NormalTok{m.run(model2) }
\NormalTok{summary }\OperatorTok{=}\NormalTok{ m.summary()}
\NormalTok{summary}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-20}

\subsection{\texorpdfstring{\emph{Main
Formula}}{Main Formula}}\label{main-formula}

The simple model that can be built to model link weights between nodes
\emph{i} and \emph{j} can be defined using a poisson distribution:

\[
G_{ij} \sim Poisson(Y_{ij})
\]

\[
log(Y_{ij}) =  \lambda_i + \pi_j + \delta_{ij}
\]

where:

\begin{itemize}
\item
  \(Y\_{ij}\) is the weight of links between \emph{i} and \emph{j}.
\item
  \(\lambda_i\) is the \phantomsection\label{senderF}{{sender effect
  üõà}}.
\item
  \(\pi_j\) is the \phantomsection\label{receiverF}{{receiver effect
  üõà}}.
\item
  \(\delta_{ij}\) is the \phantomsection\label{DyadicF2}{{dyadic effect
  üõà}}.
\item ~
  \subsection{\texorpdfstring{\emph{Defining formula sub-equations and
  prior
  distributions}}{Defining formula sub-equations and prior distributions}}\label{defining-formula-sub-equations-and-prior-distributions}
\end{itemize}

\(\lambda_i\) and \(\pi_j\) are varying
\href{12.\%20Varying\%20intercepts.qmd}{intercepts} and
\href{13.\%20Varying\%20slopes.qmd}{slopes} identical to those described
in previous chapters and are define through the following equations:

\[
\left(\begin{array}{cc} 
\lambda_i \\
\pi_j 
\end{array}\right) 
\sim 
MultivariateNormal\left(\begin{array}{cc} 
\left(\begin{array}{cc} 
\sigma_\lambda \\
\sigma_\pi
\end{array}\right) \circ 
\left(\begin{array}{cc} 
L *
\left(\begin{array}{cc} 
\hat{\lambda}_i \\ \hat{\pi}_i
\end{array}\right)
\end{array}\right)
\end{array}\right)
\]

\[
\sigma_\lambda \sim Exponential(1)
\]

\[
\sigma_\pi \sim Exponential(1)
\]

\[
L \sim LKJ(2)
\]

Similarly, for each dyads we can define varying intercepts and slopes to
account for correlation between the propensity to emit and receive links
of a dyad :

\[
\left(\begin{array}{cc} 
\delta_{ij} \\
\delta_{ji}
\end{array}\right) 
\sim 
MultivariateNormal\left(\begin{array}{cc} 
\left(\begin{array}{cc} 
\sigma_\delta \\
\sigma_\delta
\end{array}\right) \circ 
\left(\begin{array}{cc} 
L_\delta *
\left(\begin{array}{cc} 
\hat{\delta}_{ij} \\ \hat{\delta}_{ji}
\end{array}\right)
\end{array}\right)
\end{array}\right)
\]

\[
\sigma_\delta \sim Exponential(1)
\]

\[
\sigma_\delta \sim Exponential(1)
\]

\[
L \sim LKJ(2)
\]

\section{Note(s)}\label{notes-9}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  Note that any additional covariates can be summed with a regression
  coefficient to \(\lambda_i\), \(\pi_j\) and \(\delta_{ij}\). Of course
  for \(\lambda_i\), \(\pi_j\) as they represent nodal effects those
  covariates need to be nodal characteristics (e.g., sex, age) whereas
  for \(\\delta_{ij}\) as it represents dyadic effects those covariates
  need to be dyadic characteristics (e.g., genetic distances).
  Concidering previous example given a vector of nodal characteristics
  \emph{individual\_predictors} and a a matrix of dyadic characteristics
  \emph{kinship} we can incorporate those covariates in the
  sender-receiver effect and dyadic effect respectivelly as fellow:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ model2(idx, result\_outcomes, dyad\_effects, focal\_individual\_predictors, target\_individual\_predictors):}
\NormalTok{    N\_id }\OperatorTok{=}\NormalTok{ ids.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Sender Receiver effect (SR) its shape is equal to N\_id {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
    \CommentTok{\#\# Covariates for SR}
\NormalTok{    sr\_terms, focal\_effects, target\_effects }\OperatorTok{=}\NormalTok{ bi.net.nodes\_terms(focal\_individual\_predictors, target\_individual\_predictors) }

    \CommentTok{\#\# Varying intercept and slope for SR}
\NormalTok{    sr\_rf, sr\_raw, sr\_sigma, sr\_L }\OperatorTok{=}\NormalTok{ bi.net.nodes\_random\_effects(N\_id, cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{) }

\NormalTok{    sender\_receiver }\OperatorTok{=}\NormalTok{ sr\_terms }\OperatorTok{+}\NormalTok{ sr\_rf}

    \CommentTok{\# Dyadic effect (D) its shape is equal to n dyads {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
    \CommentTok{\#\# Covariates for D}
\NormalTok{    dr\_terms, dyad\_effects }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_terms(dyad\_effects)}

    \CommentTok{\#\# Varying intercept and slope for D}
\NormalTok{    rf, dr\_raw, dr\_sigma, dr\_L }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_random\_effects(sender\_receiver.shape[}\DecValTok{0}\NormalTok{], cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{)}
\NormalTok{    dr }\OperatorTok{=}\NormalTok{ dr\_terms }\OperatorTok{+}\NormalTok{ rf}

\NormalTok{    lk(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, Poisson(jnp.exp( sender\_receiver }\OperatorTok{+}\NormalTok{ dr ), is\_sparse }\OperatorTok{=} \VariableTok{False}\NormalTok{), obs}\OperatorTok{=}\NormalTok{result\_outcomes) }\CommentTok{\# is\_sparse = True, if matrix have a lot of zeros, it can help to speed up computation.}

\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    idx }\OperatorTok{=}\NormalTok{ idx,}
\NormalTok{    result\_outcomes }\OperatorTok{=}\NormalTok{ bi.net.mat\_to\_edgl(data[}\StringTok{\textquotesingle{}outcomes\textquotesingle{}}\NormalTok{]), }
\NormalTok{    dyad\_effects }\OperatorTok{=}\NormalTok{ bi.net.prepare\_dyadic\_effect(kinship), }\CommentTok{\# Can be a jax array of multiple dimensions}
\NormalTok{    focal\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{],}
\NormalTok{    target\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{]}
\NormalTok{)}

\NormalTok{m.run(model2) }
\NormalTok{summary }\OperatorTok{=}\NormalTok{ m.summary()}
\NormalTok{summary.loc[[}\StringTok{\textquotesingle{}focal\_effects[0]\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}target\_effects[0]\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dyad\_effects[0]\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{2.\%20Multiple\%20continuous\%20Variables.qmd}{chapter 2:
  Multiple continuous Variables}.
\item
  We can apply interaction terms similarly as
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3: Interaction between continuous variables}.
\item
  Network links can be modeled using Bernoulli, Binomial, Poisson, or
  zero-inflated Poisson distributions. So, by replacing the Poisson
  distribution with a binomial distribution, we can model the existence
  or absence of link --- i.e., model binary networks.
\item
  If the network is undirected, then accounting for correlation between
  propensity to emit and receive links is not necessary, and the terms
  \(\lambda_i\), \(\pi_j\) , and \(\delta_{ij}\) are no longer required.
  (Is it correct?)
\item
  In the following chapters, we will see how to incorporate additional
  network effects into the model to account for network structural
  properties (e.g., clusters, assortativity, triadic closure, etc.).
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Network with block model}\label{network-with-block-model}

Within networks, nodes can belong to different categories, and these
categories can potentially affect the propensity for node interactions.
For example, nodes can have different sex categories, and the propensity
to interact with nodes of the same sex can be higher than with nodes of
different sexes. To model the propensity for interaction between nodes
based on the categories they belong to, we can use a stochastic block
model approach.

\section{Considerations}\label{considerations-20}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  We consider predefined groups here, with the goal of evaluating the
  propensity for interaction between nodes within each group.
\item
  In addition to the block(s) model being tested, we need to include a
  block where all individuals are considered as belonging to the same
  group (\texttt{Any} in the example). This allows us to assess whether
  interaction tendencies differ between groups or if the propensity to
  interact is uniform across all individuals.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-20}

Below is an example code snippet demonstrating a Bayesian network model
using the stochastic block model approach. The data is identical to the
\href{18.\%20Network\%20model.qmd}{Network model} example, with the
addition of covariates \emph{Any}, \emph{Merica}, and \emph{Quantum},
representing the block membership of each node.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ model3(idx, result\_outcomes, kinship, focal\_individual\_predictors, target\_individual\_predictors, Any, Merica, Quantum):}
\NormalTok{    N\_id }\OperatorTok{=}\NormalTok{ ids.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Block {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{    B\_any, b\_any, b\_ij\_any, b\_ii\_any }\OperatorTok{=}\NormalTok{ bi.net.block\_model(Any,}\DecValTok{1}\NormalTok{, name\_b\_ij }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ij\_Any\textquotesingle{}}\NormalTok{, name\_b\_ii }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ii\_Any\textquotesingle{}}\NormalTok{ )}
\NormalTok{    B\_Merica, b\_Merica, b\_ij\_Merica, b\_ii\_Merica }\OperatorTok{=}\NormalTok{ bi.net.block\_model(Merica,  }\DecValTok{3}\NormalTok{, name\_b\_ij }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ij\_Merica\textquotesingle{}}\NormalTok{, name\_b\_ii }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ii\_Merica\textquotesingle{}}\NormalTok{ )}
\NormalTok{    B\_Quantum, b\_Quantum, b\_ij\_Quantum, b\_ii\_Quantum }\OperatorTok{=}\NormalTok{ bi.net.block\_model(Quantum, }\DecValTok{2}\NormalTok{, name\_b\_ij }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ij\_Quantum\textquotesingle{}}\NormalTok{, name\_b\_ii }\OperatorTok{=} \StringTok{\textquotesingle{}b\_ii\_Quantum\textquotesingle{}}\NormalTok{ )}

    \CommentTok{\#\# SR {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{    sr\_terms, focal\_effects, target\_effects }\OperatorTok{=}\NormalTok{ bi.net.nodes\_terms(focal\_individual\_predictors, target\_individual\_predictors)}
\NormalTok{    sr\_rf, sr\_raw, sr\_sigma, sr\_L }\OperatorTok{=}\NormalTok{ bi.net.nodes\_random\_effects(sr\_terms.shape[}\DecValTok{0}\NormalTok{], cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{)}

\NormalTok{    sender }\OperatorTok{=}\NormalTok{ sr\_terms[idx[:,}\DecValTok{0}\NormalTok{],}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_terms[idx[:,}\DecValTok{1}\NormalTok{],}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_rf[idx[:,}\DecValTok{0}\NormalTok{],}\DecValTok{0}\NormalTok{]}
\NormalTok{    receiver }\OperatorTok{=}\NormalTok{  sr\_terms[idx[:,}\DecValTok{1}\NormalTok{],}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_terms[idx[:,}\DecValTok{0}\NormalTok{],}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ sr\_rf[idx[:,}\DecValTok{1}\NormalTok{],}\DecValTok{1}\NormalTok{]}
\NormalTok{    sender\_receiver }\OperatorTok{=}\NormalTok{ jnp.stack([sender, receiver], axis }\OperatorTok{=} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# Dyadic{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}  }
\NormalTok{    dr\_terms, dyad\_effects }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_terms(kinship[:,}\DecValTok{0}\NormalTok{], kinship[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{    rf, dr\_raw, dr\_sigma, dr\_L }\OperatorTok{=}\NormalTok{ bi.net.dyadic\_random\_effects(idx.shape[}\DecValTok{0}\NormalTok{], cholesky\_density }\OperatorTok{=} \DecValTok{2}\NormalTok{)}
\NormalTok{    dr }\OperatorTok{=}\NormalTok{ dr\_terms }\OperatorTok{+}\NormalTok{ rf}

\NormalTok{    lk(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, Poisson(jnp.exp(B\_any }\OperatorTok{+}\NormalTok{ B\_Merica }\OperatorTok{+}\NormalTok{ B\_Quantum }\OperatorTok{+}\NormalTok{ sender\_receiver }\OperatorTok{+}\NormalTok{ dr )), obs}\OperatorTok{=}\NormalTok{result\_outcomes)}

\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    idx }\OperatorTok{=}\NormalTok{ idx,}
\NormalTok{    Any }\OperatorTok{=}\NormalTok{ Any}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }
\NormalTok{    Merica }\OperatorTok{=}\NormalTok{ Merica}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }
\NormalTok{    Quantum }\OperatorTok{=}\NormalTok{ Quantum}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}
\NormalTok{    result\_outcomes }\OperatorTok{=}\NormalTok{ bi.net.mat\_to\_edgl(data[}\StringTok{\textquotesingle{}outcomes\textquotesingle{}}\NormalTok{]), }
\NormalTok{    kinship }\OperatorTok{=}\NormalTok{ bi.net.mat\_to\_edgl(kinship),}
\NormalTok{    focal\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{],}
\NormalTok{    target\_individual\_predictors }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}individual\_predictors\textquotesingle{}}\NormalTok{]}
\NormalTok{)}

\NormalTok{m.run(model3) }
\NormalTok{summary }\OperatorTok{=}\NormalTok{ m.summary()}
\NormalTok{summary.loc[[}\StringTok{\textquotesingle{}focal\_effects[0]\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}target\_effects[0]\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dyad\_effects[0]\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-21}

\subsection{\texorpdfstring{\emph{Main
Formula}}{Main Formula}}\label{main-formula-1}

The model's block structure can be represented by the following formula.
Note that the sender-receiver and dyadic effects are not represented
here, as they are already accounted for in the
\href{18.\%20Network\%20model.qmd}{Network model} chapter:

\[
G_{ij} \sim Poisson(Y_{ij})
\]

\[
log(Y_{ij}) =  B_{ij} + B_{ji}
\]

where:

\begin{itemize}
\tightlist
\item
  \(B_{ij}\) is the link probability between category \(i\) and \(j\).
\item
  \(B_{ji}\) is the link probability between category j to i.
\end{itemize}

\subsection{\texorpdfstring{\emph{Defining formula sub-equations and
prior
distributions}}{Defining formula sub-equations and prior distributions}}\label{defining-formula-sub-equations-and-prior-distributions-1}

To account for all link probabilities between categories, we can define
a square matrix \(B\) as follows: the off-diagonal elements represent
the link probabilities between categories \(i\) and \(j\), while the
diagonal elements represent the link probabilities between categories
\(i\) and \(j\).

\[
B_{i,j} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,j} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,j} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{i,1} & a_{i,2} & \cdots & a_{i,j} 
\end{bmatrix}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(B[i,j]\) is the link probability between category \(i\) and \(j\)
  when \(i \neq j\).
\item
  \(B[i,j]\) is the link probability within category \(i\) when
  \(i = j\).
\end{itemize}

As we concider link probability within categorioes to be higher to links
probabilites between categories we define different priors for the
diagonal and the off-diagonal. Priors should also depend on sample size,
N, so that the resultant network density approximates empirical
networks. Basic priors could be:

\[
\beta_{k \rightarrow k} \sim \text{Normal}\left(\text{Logit}\left(\frac{0.1}{\sqrt{N_k}}\right), 1.5\right)
\]

\[
\beta_{k \rightarrow \tilde{k}} \sim \text{Normal}\left(\text{Logit}\left(\frac{0.01}{0.5 \sqrt{N_k} + 0.5 \sqrt{N_{\tilde{k}}}}\right), 1.5\right)
\]

where :

\begin{itemize}
\tightlist
\item
  \(k \rightarrow k\) indicates a diagonal element.
\item
  \(k \rightarrow \tilde{k}\) indicates an off-diagonal element.
\end{itemize}

\section{Note(s)}\label{notes-10}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\item
  By defining this block model within our network model, we are
  estimating \phantomsection\label{assor}{{assortativity üõà}} and
  \phantomsection\label{disassor}{{disassortativity üõà}} for categorical
  variables.
\item
  Similarly, for continuous variables, we can generate a block model
  that includes all continuous variables.
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Network with data collection
biases}\label{network-with-data-collection-biases}

Data collection biases are a persistent issue in studies of social
networks. Two main types of biases can be considered:
\phantomsection\label{expoB}{{exposure biases üõà}} and
\phantomsection\label{censoB}{{censoring biases üõà}} .

To account for exposure biases, we can switch the network link
probability model from a \emph{Poisson} distribution to a
\emph{Binomial} distribution, as the binomial distribution allows us to
account for the number of trials for each data estimation.

To address censoring biases, we need to add an additional equation to
account for the probability of missing an interaction during observation
when modeling interaction between individual \emph{i} and \emph{j}.

\section{Considerations}\label{considerations-21}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Example 1}\label{example-1-1}

Below is an example code snippet demonstrating Bayesian network model
with send-receiver effect, dyadic effect and block model effect while
accounting for exposure biases:

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Example2}\label{example2}

Below is an example code snippet demonstrating Bayesian network model
with send-receiver effect, dyadic effect and block model effect while
accounting for exposure biases and cesnsoring biases:

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-22}

\subsection{\texorpdfstring{\emph{Main
Formula}}{Main Formula}}\label{main-formula-2}

\[
\begin{align}
    Y_{[i,j]} &\sim \mathrm{Binomial}\Big(E_{[i,j]}, Q_{[i,j]}  \Big)
\end{align}
\]

\[
Q_{[i,j]} \in \{0,1\} 
\]

Where:

\begin{itemize}
\tightlist
\item
  \(E_{[i,j]}\) is the number of trials for each observation (i.e., the
  sampling effort).
\item
  \(Q_{[i,j]}\) is the indicator of a true tie between \(i\) and \(j\).
\end{itemize}

\[
\begin{aligned}
Q_{[i,j]} \sim \begin{cases} 
0 & \text{when no interactions occurs or when i or j are not detectable} \\
\text{1}  & \text{when i and j are both detectable}
\end{cases}
\end{aligned}
\]

\[
\begin{align}
  Q_{[i,j]}= \phi_{[i,j]}\eta_{[i]}\eta_{[j]}
\end{align}
\]

Where: - \(\phi_{[i,j]}\) is the probability of a true tie between \(i\)
and \(j\). - \(\eta_{[i]}\) is the probability of individual \(i\) being
detectable. - \(\eta_{[j]}\) is the probability of individual \(j\)
being detectable.

\subsection{\texorpdfstring{\emph{Defining formula sub-equations and
prior
distributions}}{Defining formula sub-equations and prior distributions}}\label{defining-formula-sub-equations-and-prior-distributions-2}

We can let \(\eta_{[i]}\) depend on individual-specific covariates. To
model the probability of censoring, we can model \(1-\eta_{[i]}\): \[
\text{logit}(1-\eta_{[i]}) = \mu_\psi + \hat\psi_{[i]}  \sigma_\psi + \ldots 
\]

where - \(\mu_\psi\) is an intercept - \(\sigma_\psi\) is a scalar for
the variance of random effects -
\(\hat\psi_{[i]}\sim \text {Normal}(0,1)\), and the ellipsis signifies
any linear model of coefficients and individual-level covariates. For
example, if \(C\) is an animal-specific measure, like a binary variable
for cryptic coloration, then the ellipsis may be replaced with:
\(\kappa_{[5]}C_{[i]}\), to give the effects of coloration on censoring
probability.

\section{Note(s)}\label{notes-11}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  One major limitation of this model is the necessity of having an
  estimation of the censoring bias for each individual.
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Network metrics}\label{network-metrics}

\section{General Principles}\label{general-principles-19}

Network metrics are mathematical calculations to quantify specific
features of a network, including global, nodal and polyadic measures.
Contrary to other chapters we will present where a suite of most common
used network metrics and the corresponding BI functions under the class
\texttt{bi.net.} implemented with JAX and usable within any bi model.
This section is inspire by XXX and user whiling to dig further are
invited to read and site this paper.

\section{Nodal metrics}\label{nodal-metrics}

Nodal metrics* enable to assess nodes' social heterogeneity and to
understand the underlying mechanisms such as individual characteristics
(e.g.~ageing process), ecological factors (e.g.~demographic variation)
and evolutionary processes (e.g.~differences in social styles). Node
measures are calculated at an nodal level and assess in different ways
and with different meanings how an individual is connected. Connections
can be ego's* direct links only (e.g.~degree, strength), alters'* links
as well (e.g.~eigenvector, clustering coefficient) or even all the links
of the network (e.g.~betweenness). Node measures can also be used to
describe the overall network structure through distributions, means and
coefficients of variation.

\subsection{Degree and strength}\label{degree-and-strength}

The degree \texttt{bi.net.degree} measures the number of links of a
node. When computed on an undirected network, the degree represents the
number of alters of ego. When the network is directed\emph{, it
represents the number of either incoming} or outgoing* links of ego and
it is then called in-degree \texttt{bi.net.indegree} or out-degree
\texttt{bi.net.outdegree} respectively. Note that degree can also be
computed in directed networks, in this case it represents the sum of
incoming and outgoing links and not the number of alters.

\[
D_i = \sum_{j=1}^N a_{ij}
\]

Where \(a_{ij}\) is the value of the link between nodes \(i\) and \(j\).
Isolated node(s) can be considered as zero(s).

Strength (or weighted degree) \texttt{bi.net.strength} is the sum of
links' weights in a weighted network*. When the network comprises
directed links, then it is also possible to differentiate between
in-strength \texttt{bi.net.instrength} (the sum of weights of incoming
links) and \texttt{bi.net.outstrength} out-strength (the sum of weights
of outgoing links). While degree and strength can be considered
correlated, it may not always be the case as individuals can interact
frequently with few social partners or vice versa (Liao, Sosa, Wu, \&
Zhang, 2018). Therefore, it is necessary to test their correlation prior
to the analysis.

\[
S_i = \sum_{j=1}^N a_{ij} w_{ij}
\]

Where \(a_{ij}\) is the value of the link between nodes \(i\) and \(j\).
Isolated node(s) can be considered as zero(s).

\subsection{Eigenvector centrality}\label{eigenvector-centrality}

Eigenvector centrality \texttt{bi.net.eigenvector} is the first
non-negative eigenvector value obtained by transforming an adjacency
matrix linearly. It can be computed on weighted, binary\emph{, directed
or undirected networks. It measures the centrality} by examining the
connectedness of ego as well as that of its alters. Thus, a node's
eigenvector value can be linked either to its own degree or strength or
to the degrees or strengths of the nodes to which it is connected.
Eigenvector may be interpreted as the social support or social capital
of an individual (Brent, Semple, Dubuc, Heistermann, \& MacLarnon,
2011), that is the real or perceived availability of social resources.

\[
\lambda c = W c
\]

Where \(\lambda\) is the largest eigenvalue of the adjacency matrix
\(W\). Isolated node(s) can be considered as zero(s).

\subsection{Local clustering
coefficient}\label{local-clustering-coefficient}

The local clustering coefficient \texttt{bi.net.cc} measures the number
of closed triplets* over the total theoretical number of triplets
(i.e.~open and closed), where a triplet is an ensemble of three nodes
that are connected by either two (open triplet) or three (closed
triplet) edges. This measure aims to examine the links that may exist
between the alters of ego and measures the cohesion of the network. The
main topological effect of closed triplets is the clusterization of the
network, generating cohesive clusters, and is thus strongly related to
modularity (see corresponding section). The local clustering coefficient
can be computed in a binary network by measuring the proportion of links
between the nodes of an ego-network* divided by the number of potential
links between them. n weighted networks, several versions exist such as
those from Barrat, Barthelemy, Pastor-Satorras, and Vespignani (2004) or
Opsahl and Panzarasa (2009).

\subsubsection{Binary Local Clustering
Coefficient}\label{binary-local-clustering-coefficient}

\[
C_i^b = \frac{2L}{N_i (N_i - 1)}
\] Where \(L\) is the number of links in the ego-network of node \(i\).

\subsubsection{Barrat's Local Clustering
Coefficient}\label{barrats-local-clustering-coefficient}

\[
C_i^W = \frac{1}{S_i (D_i - 1)} \sum_{j \neq h \in N} \frac{(w_{ij} + w_{ih})}{2} a_{ij} a_{ih} a_{jh}
\]

Where \(S_i\) and \(D_i\) are the strength and the degree of node \(i\),
respectively. \(w_{ij}\) and \(w_{ih}\) are the weights of the links,
and \(a_{ij}\), \(a_{ih}\), \(a_{jh}\) are the links between the nodes.

\subsubsection{Opsahl's Local Clustering
Coefficient}\label{opsahls-local-clustering-coefficient}

\[
C^W(G) = \frac{\sum_{\tau_\Delta} w}{\sum_\tau w}
\] Where \$tau\_\Delta\$ represents closed triplets, and \(w\) is the
chosen weighting scheme (maximum, minimum, arithmetic, or geometric
mean).

\subsection{Betweenness}\label{betweenness}

Betweenness (WIP) is the number of times a node is included in the
shortest paths (geodesic distances) generated by every combination of
two nodes. The value of the betweenness informs on the theoretical role
of a node in the social transmission (information, disease, etc., see
Figure 1) as it indicates to what extent a node connects subgroups, as a
bridge, and then is likely to spread an entity across the whole network
(Newman, 2005).

\[
b = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
\]

Where \(\sigma_{st}\) is the total number of shortest paths from node
\(s\) to node \(t\), and \(\sigma_{st}(v)\) is the number of those paths
that pass through \(v\). As no paths go through isolated nodes, their
betweenness value can be considered as zero.

\section{Polyadic metrics}\label{polyadic-metrics}

Patterns of interactions (how and with whom individuals interact) can be
examined using specific network measures* that analyse local-scale
interactions within a network and make possible to test hypotheses about
the mechanisms underlying network connectivity. These types of measures
are generally used to test mechanistic biological questions, such as
what factors (e.g.~ecological as well as sociodemographic) affect
individuals' interactions/associations.

\subsection{Assortativity}\label{assortativity}

Assortativity (Newman, 2003) (WIP) is probably the most used measure to
study homophily (preferential associations or interactions among
individuals sharing the same characteristics; Lazarsfeld \& Merton,
1954). Assortativity values range from ‚àí1 (total disassortativity
i.e.~all the nodes associate or interact with those with the opposite
characteristic, such as males interacting exclusively with females) to 1
(total assortativity i.e.~all the nodes associate or interact with those
with the same characteristic such as males interacting only with males).
The assortativity coefficient measures the proportion of links between
and within clusters of nodes with same characteristics. Individuals'
characteristics can be continuous (e.g.~age, individual network measure,
personality) or categorical features (e.g.~sex, matriline belonging;
Figure 2). Assortativity does not consider directionality* and can be
measured in weighted (Leung \& Chau, 2007) or binary (Newman, 2003)
networks using categorical or continuous characteristics (Figure 2). The
use of one or other assortativity variant depends on the type of
characteristics being examined and, whenever possible, the weighted
version should be preferred since it is more reliable than the binary
version (Farine, 2014).

\subsubsection{Binary Assortativity}\label{binary-assortativity}

\[
r = \frac{\sum_i e_{ii} - \sum_i a_i b_i}{1 - \sum_i a_i b_i}
\]

Where \(e_{ii}\) is the proportion of specific links, \(a_i\) is the
proportion of outgoing links, and \(b_i\) is the proportion of incoming
links.

\subsubsection{Weighted Continuous
Assortativity}\label{weighted-continuous-assortativity}

\[
r = \frac{\sum_i e_{ii}^w - \sum_i a_i^w b_i^w}{1 - \sum_i a_i^w b_i^w}
\] Where \(e_{ii}^w\) is the proportion of weighted links, and
\(a_i^w\), \(b_i^w\) are the proportions of weighted outgoing and
incoming links.

\subsection{Transitive triplets}\label{transitive-triplets}

Transitive triplets (WIP) are closed triplets where the links among the
nodes follow a specific temporal pattern of creation, that is when the
establishment of links between nodes A and B and between nodes A and C
is followed by the establishment of a link between nodes B and C. This
network measure can be computed in directed, binary or weighted
networks. This type of connections can be studied over time based on the
creation of links. From a static perspective, directionality can be
considered by calculating the number of transitive triplets divided by
the number of potential transitive triplets, and weights can also be
considered by using Opsahls' variants, which are discussed in the
section on local clustering coefficient (Opsahl \& Panzarasa, 2009).
While transitivity is importantly related to the clustering coefficient
(the clustering coefficient includes transitive triplets), not all close
triplets are transitive. Transitive triplets are one of the 16 possible
configurations of a triplet considering open and closed triplets as well
as link directionality (i.e.~triad census).

\section{Global metrics}\label{global-metrics}

The structure of this section is based on the distinction between
network connectivity and social diffusion (information or disease
spread). However, the social diffusion section contains measures
specifically designed to study theoretical (i.e.~considering the
diffusion is perfectly related to network links and link weights) social
diffusion features based on the geodesic distances (see corresponding
section). Aspects of the structure and properties of a group
(e.g.~cohesion, sub-grouping) can be quantified using global network
measures\emph{. For instance, one may quantify properties such as
network resilience} (see Diameter), network clusterization* (see
Modularity) through network connectivity analysis, or network
transmission efficiency* (see Global efficiency) through network
theoretical social diffusion analysis.

\subsection{Density}\label{density}

The density \texttt{bi.net.density} is the ratio between existing links
and all potential links of a network. This measure is easy to interpret,
it assesses how a network is fully connected. Density does not consider
directionality neither link weights.

\[
D = \frac{2|L|}{|N|(|N| - 1)}
\]

Where \(L\) is the number of links and \(N\) is the number of nodes.
Isolated node(s) can be considered as zero(s).

\subsection{Geodesic Distance}\label{geodesic-distance}

Geodesic distance \texttt{bi.net.geodesic\_distance} is the shortest
path considering all potential dyads in a network. This measure thereby
evidences the fastest path of diffusion. Geodesic distance can be
calculated in binary, weighted*, directed or undirected networks. In
weighted networks, it can be normalized (by dividing all links by the
network weight means) and the strongest or the weakest links can be
considered as the fastest route between two nodes. This great number of
variants of geodesic distance can greatly affect the results and
interpretations. Researchers must thus have knowledge of the variants
and which one is the most appropriate according to their research
question (Opsahl, Agneessens, \& Skvoretz, 2010).

The computation uses algorithms like breadth-first search, depth-first
search, or Dijkstra's algorithm. None handle isolated nodes.

\subsection{Diameter}\label{diameter}

The diameter \texttt{bi.net.diameter} of a network represents the
longest path of the shortest paths in the network. Diameter is used in
ASNA to examine the aspects such as network cohesion, the rapidness of
information or disease transmission. While global efficiency measures
the theoretical social diffusion spread, diameter informs on the maximum
paths of diffusion to reach all nodes.

\subsection{Global efficiency}\label{global-efficiency}

Global efficiency (WIP) is the ratio between the number of individuals
and the number of connections multiplied by the network diameter. It
provides a quantitative measure of how efficiently information is
exchanged within the nodes of the network. As global efficiency gives a
probability of social diffusion, it may help better understand social
transmission phenomena in short-term and long-term (Migliano et al.,
2017). Pasquaretta et al.~(2014) found a positive correlation between
the neocortex ratio and the global efficiency in primate species with a
higher neocortex ratio. By drawing a parallel between cognitive
capacities and social network efficiency, this study evidenced that in
species with higher neocortex ratio, individuals may adjust their social
relationships in order to gain better access to social information and
thus optimize network efficiency. Alternatively, studies on epidemiology
in ant colonies showed that ants adapt their interaction rate to
decrease the network efficiency when infected by a pathogen (Stroeymeyt
et al., 2018).

\subsection{Modularity}\label{modularity}

Modularity (WIP) is a measure designed to quantify the degree to which a
network could be divided into different groups or clusters and its value
ranges from 0 to 1. Networks with high modularity have dense connections
within the modules but sparse connections between the modules.
Modularity can be computed in weighted, binary, directed or undirected
networks.

\[
Q = \sum_{s=1}^m \left[ \frac{l_s}{|E|} - \left(\frac{d_s}{2|E|}\right)^2 \right]
\]

Where \(l_s\) is the number of edges in the \(s\)-th community, and
\(d_s\) is the sum of the degrees of the nodes in the community.

\subsection{Global Clustering
Coefficient}\label{global-clustering-coefficient}

The global clustering coefficient (WIP), like the local clustering
coefficient, evaluates how well the alters of ego are interconnected and
measures the cohesion of the network. Its main topological effect is the
clusterization of the network, generating cohesive clusters, and is thus
strongly related to modularity. However, it becomes highly correlated
with density and less to modularity as the density grows. Several
variants of the global clustering coefficient can be found: (a) the
ratio of closed triplets to all triplets (open and closed), (b) the
binary local mean clustering coefficient that derives from the node
level (see Local clustering coefficient). The binary local mean
clustering coefficient allows to consider node heterogeneity and thus
should be preferred over the first variant. Weighted versions also exist
and are based on the same variants described in the section on the local
clustering coefficient and require the same considerations.

\[
C^b(G) = \frac{\sum \tau_\Delta}{\sum \tau}
\] Where \$tau\$ is the total number of triplets and \$tau\_\Delta\$
represents closed triplets.

\section{Reference(s)}\label{references-16}

https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13366

\bookmarksetup{startatroot}

\chapter{Network Based Diffusion
analysis}\label{network-based-diffusion-analysis}

\section{General Principles}\label{general-principles-20}

The principle idea behind Network Based Diffusion analysis (NBDA) is
that if social transmission is involved in the spread of a novel
behavior through a group, then that spread is expected to follow a
social network links. The basic model underlying NBDA states that at
time \(t\) an individual, \(i\), learns the behavior of interest with a
specific rate formula.

In principle NBDA can be consider as a survival analysis, so we have the
same concepts as in \href{12.\%20Survival\%20analysis.qmd}{chapter 12}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Where the \textbf{baseline hazard} (e.g.~the hazard when all
  covariates are zero) is the asocial hazard.
\item
  Where the \textbf{covariate} is the sum of links toward informed
  individuals (i.e.~individuals that acquired the behavior of interest
  at time \(t-1\)).
\item
  Thus the \textbf{Hazard Function} which account for the network links
  weights \textbf{covariate} can thus be consider as the social rate of
  learning the behavior.
\end{enumerate}

\section{Considerations}\label{considerations-22}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  There are two main NBDA variants: order-of-acquisition diffusion
  analysis (OADA), which takes as data the order in which individuals
  acquired the target behaviour, and time-of-acquisition diffusion
  analysis (TADA), which uses the times of acquisition of the target
  behaviour.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics{attachment:image.png}

}

\caption{image.png}

\end{figure}%

\end{tcolorbox}

\section{Example}\label{example-21}

Below is an example code snippet demonstrating Bayesian Multiplex
network model using the Bayesian Inference (BI) package:

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-23}

\subsection{\texorpdfstring{\emph{Formulation}}{Formulation}}\label{formulation-1}

There are two parameters of interest in the basic time of acquisition
diffusion analysis model: the rate of social transmission be-tween
individuals per unit of network connection,\emph{s}, and the baseline
rate of trait performance in the absence of social transmission,
\(Œª_0\).

\[
\lambda_i(t) = \lambda_0(t) (1- z_i(t))  \left[ s \sum_{j = 1}^{N} a_{ij} z_j (t_{-1}) + 1 \right] 
\]

Where:

\begin{itemize}
\item
  \(\lambda_i(t)\) is the rate at which individuals \emph{i} acquire the
  task solution at time \emph{t}.
\item
  \(\lambda_0(t)\) is a baseline acquisition function determining the
  distribution of latencies to acquisition in the absence of social
  transmission (that is, through asocial learning). It can be specify by
  an exponential or Weibull distrbution.
\item
  \(z_i(t)\) gives the status (1 = informed, 0 = na√Øve) of individual
  \emph{i} at time \emph{t}.
\item
  \(s\) is the regression coefficients capturing the effect of \(x\) on
  the hazard have an assigned a normal prior.
\item
  \((1- z_i(t))\) and \(z_j (-1)\) terms ensure that the task solution
  is only transmitted from informed to uninformed individuals:
\end{itemize}

\[
z_j(t) =  Y_i \sim \begin{cases} 
0, & \text{if j is naive} \\
1, & \text{if j is informed}
\end{cases}
\]

\section{Notes}\label{notes-12}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Reference(s)}\label{references-17}

https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13307

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{16}
\tightlist
\item
  (PDF) Quantifying diffusion in social networks: a Bayesian approach.
  Available from:
  https://www.researchgate.net/publication/270048687\_Quantifying\_diffusion\_in\_social\_networks\_a\_Bayesian\_approach
  {[}accessed Oct 24 2024{]}.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Multiplex network model}\label{multiplex-network-model}

\section{General Principles}\label{general-principles-21}

\section{Considerations}\label{considerations-23}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Example}\label{example-22}

Below is an example code snippet demonstrating Bayesian Multiplex
network model using the Bayesian Inference (BI) package:

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-24}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-13}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-10}

\section{Notes}\label{notes-13}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Reference(s)}\label{references-18}

\bookmarksetup{startatroot}

\chapter{Multiplex temporal network
model}\label{multiplex-temporal-network-model}

\section{General Principles}\label{general-principles-22}

\section{Considerations}\label{considerations-24}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Example}\label{example-23}

Below is an example code snippet demonstrating Bayesian Multiplex
temporal network model using the Bayesian Inference (BI) package:

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-25}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-14}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-11}

\section{Notes}\label{notes-14}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Reference(s)}\label{references-19}

\bookmarksetup{startatroot}

\chapter{Multilayer Networks}\label{multilayer-networks}

\section{General Principles}\label{general-principles-23}

A multilayer network is a broader term that refers to networks composed
of multiple layers, where each layer may have its own set of nodes and
edges, potentially representing different types of entities and
interactions. This means the nodes can be the same or different across
layers.

\section{Considerations}\label{considerations-25}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Example}\label{example-24}

Below is an example code snippet demonstrating Bayesian Multilayer
Networks model using the Bayesian Inference (BI) package:

\section{Python}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-26}

\subsection{\texorpdfstring{\emph{Frequentist
formulation}}{Frequentist formulation}}\label{frequentist-formulation-15}

\subsection{\texorpdfstring{\emph{Bayesian
formulation}}{Bayesian formulation}}\label{bayesian-formulation-12}

\section{Notes}\label{notes-15}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, toprule=.15mm, left=2mm, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, arc=.35mm, opacitybacktitle=0.6, toptitle=1mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacityback=0, leftrule=.75mm]

\end{tcolorbox}

\section{Reference(s)}\label{references-20}

\end{tcolorbox}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-mcelreath2018statistical}
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\end{CSLReferences}




\end{document}
