% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={BI documentation},
  pdfauthor={Sebastian Sosa},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{BI documentation}
\author{Sebastian Sosa}
\date{2024-09-09}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{}\label{section}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

\section{1.1 Model set-up}\label{model-set-up}

We define a likelihood (e.g., a mathematical formula that specifies the
plausibility of the data). The likelihood has parameters (e.g.,
adjustable inputs) for which we define priors (e.g., initial
plausibility assignment for each possible value of the parameter).
Considering a linear regression with an intercept (e.g., \(Œº\) value
when \(x\) is at zero, or at the mean if the data is centered), a slope
(e.g., \(Œº\) change value when \(x\) is incremented by one unit), and
assuming the data is centered ({ as we will always concider in the next
chapters}):

{ * Toolpit available for each lines of equation }

\href{bi/doc/0.\%20\%Introduction.md}{\[y \sim  Normal(Œº,œÉ)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œº \sim Œ± + Œ≤x
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œ± \sim Normal(0,1)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ Œ≤ \sim Normal(0,1)
\]}

\href{bi/doc/0.\%20\%Introduction.md}{\[ œÉ \sim Uniform(0,1)
\]}

\section{1.2 Model fitting}\label{model-fitting}

By using probability distributions for parameters, we can better tune
the model by describing parameters with `\emph{subequations}' and
accounting for \emph{correlated varying effects}, \emph{Gaussian
processes}, \emph{measurement error}, and \emph{missing data}.

In addition, we can use \emph{Bayesian updating} using the
\emph{Bayesian theorem} to `reshape' the prior distributions by
considering every possible combination of values for ¬µ and œÉ and scoring
each combination by its relative plausibility in light of the data.
These relative plausibilities are the posterior probabilities of each
combination of values ¬µ and œÉ: the \emph{posterior distributions}.
Various techniques can be used to approximate the mathematics that
follows from the definition of Bayes' theorem: grid approximation,
quadratic approximation, and Markov chain Monte Carlo (\emph{MCMC}).

\hyperref[]{\[\frac{likelihood*Priors}{average likelihood}\]}

\section{1.3 Model `diagnostic'}\label{model-diagnostic}

The posterior distribution can be described using percentile intervals
(\emph{PI}), the highest posterior density interval (\emph{HPDI}), and
point estimates. We can also sample the posterior distribution and
generate \emph{dummy data}, which can help check the model through
\emph{observations and p uncertainty propagation on the samples}. In
some aspects, it is the opposite of a null model as it represents an
expected model.

\section{1.4 Link functions}\label{link-functions}

We will see different families of regreessions that have different
distribtions. For the moment we just need to know that those different
distribtions required \_link function (for each specific family we will
discuss the corresponding link function):

\begin{figure}[H]

{\centering \includegraphics{image.png}

}

\caption{img}

\end{figure}%

\section{Vocabulary}\label{vocabulary}

This method evaluate if variable we want to predict -the dependent
variable (\emph{Y})- and the variable(s) that may affect(s)-independent
variables (\emph{Xs})- this dependent variable is

\section{Conciderations}\label{conciderations}

When implementing Bayesian linear regression with TensorFlow
Probability, it's important to consider the following: - Specifying
appropriate prior distributions for the model parameters. - Choosing an
appropriate likelihood function that captures the relationship between
the inputs and outputs. - Selecting an inference method to approximate
the posterior distribution over parameters, such as Markov chain Monte
Carlo (MCMC) or variational inference.

\bookmarksetup{startatroot}

\chapter{Linear Regression for continuous
vairable}\label{linear-regression-for-continuous-vairable}

\section{General Principles}\label{general-principles}

To study relationships between two continuous variables (e.g.~heigth and
weigth), we can use : \emph{Linear regression approach}. Basically, we
draw a line that cross the points clouds of the two tested variables.
For this we need to have: 1) an intercept \(\alpha\) which inform us
about the starting point of the line, 2) a coefficient \(\beta\) which
in inform us about the slope of the line and 3) a error term \(\sigma\)
which inform us about spread of points between the line. We can
interpret the intercept \(\alpha\) as the mean for of \emph{Y} for the
smaller value of \emph{X}, the coefficient \(\beta\) as how much
\emph{Y} increase for each increment of \emph{X}, and \(\sigma\) as the
error arround the prediction. So the coefficient \(\beta\) give the
strength of the relationship between \emph{X} and \emph{Y} and
\(\sigma\) the amount of error in the model.

\includegraphics{index_files/mediabag/1-WCcaObzvvVzcrg8CBi.webp}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Conciderations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  Bayesian linear regression considers uncertainty in the model
  parameters and provides a full posterior distribution over them. We
  thus need to decalre prior distribution for \(\alpha\) , \(\beta\) and
  \(\sigma^2\) .
\item
  Ussually, we use \emph{Normal} distribution for \(\alpha\) , \(\beta\)
  and an \emph{exponential} or \emph{Uniform} distributiuon for
  \(\sigma\).
\item
  As we concider that data is standardized (see introduction) we use a
  distribution of mean 0 and of standard deviation of 1.
\item
  \(\sigma\) is assumed to be normally distributed and is squared to
  force positive error and account for values bellow and above the line.
\item
  Gaussian regression deals directly with continuous outcomes,
  estimating a linear relationship between predictors and the outcome
  variable without needing a link function. This simplifies
  interpretation, as coefficients represent direct changes in the
  outcome variable.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example}

Below is an example code snippet demonstrating Bayesian linear
regression using Bayesian Inference (BI) package:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ .bi.main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Howell1.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df }\OperatorTok{=}\NormalTok{ m.df[m.df.age }\OperatorTok{\textgreater{}} \DecValTok{18}\NormalTok{]}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{])}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(height, weight):    }
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal( }\DecValTok{178}\NormalTok{, }\DecValTok{20}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.normal(  }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)   }
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.uniform( }\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    lk(}\StringTok{"Y"}\NormalTok{, Normal(alpha }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ weight , sigma), obs }\OperatorTok{=}\NormalTok{ height)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula}

The following equation allow us to draw a line and is ths one that is
most used in statistic clases: \[
Y = \alpha + \beta  X + \sigma
\]

Where: - \emph{Y} is the target variable. - \emph{X} is the input
variable. - \(\beta\) is the regression coefficient. - \(\alpha\) is the
intercept term. - \(\sigma\) is the error term.

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model}

We can express the Bayesian regression model accounting for prior
distribution as follows: \[
p(Y | \alpha, \beta, X) \sim Normal(\alpha + \beta   X, \sigma)
\]

\[
p(\alpha) \sim Normal(0, 1)
\]

\[
p(\beta) \sim Normal(0, 1)
\]

\[
p(\sigma) \sim Uniform(0, 50)
\]

Where: - \(p(Y |X, \alpha , \beta)\) is the likelihood function
(equivalent of the line equation). - \(p(\beta)\) and \(p(\alpha)\) are
the prior distributions for the regression coefficients and intercept,
respectivelly. - \(p(\sigma)\), controll the variance of the likelihood.

\section{Reference(s)}\label{references}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Multiple continuous variables
model}\label{multiple-continuous-variables-model}

\section{General Principles}\label{general-principles-1}

To study relationships between multiple continuous variables (e.g.,
effect of height and age on weight), we can use a Multiple Regression
approach. Essentially, we extend the
\href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Linear
Regression for continuous variable} by adding a regression coefficient
\(\beta\) for each continuous variables.

\includegraphics{index_files/mediabag/0-dJqdzk1aMo2OQR7O.pdf}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We need a regression coefficient \(\beta\) for each
  \href{2.\%20Multiple\%20continuous\%20Variables.qmd}{independent
  variables üõà}.
\item
  Model interpretation of the regression coefficients \(\beta\) is
  considered for a fixed value of the other dependent variables'
  regression coefficients ---i.e., for a given age, a variation of 1
  unit in height reflects the value of the regression coefficient
  \(\beta\) for height.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-1}

Below is an example code snippet demonstrating Bayesian multiple
regression using Bayesian Inference (BI) package:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Howell1.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df }\OperatorTok{=}\NormalTok{ m.df[m.df.age }\OperatorTok{\textgreater{}} \DecValTok{18}\NormalTok{]}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{])}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(height, weight, age):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)    }
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta1\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta2\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ bi.dist.uniform( }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}

\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(alpha }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ weight }\OperatorTok{+}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ age, sigma), obs}\OperatorTok{=}\NormalTok{height)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-1}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-1}

We model the relationship between the independent variables
\((X_1, X_2, ..., X_n)\) and the dependent variable \emph{Y} using the
following equation:

\[
ùëå = \alpha +\beta_1  ùëã_1 + \beta_2  ùëã_2 + ... + \beta_n  ùëã_ùëõ + \sigma
\]

Where:

\begin{itemize}
\tightlist
\item
  \(Y\) is the dependent variable.
\item
  \(\alpha\) is the intercept term.
\item
  \(X_1\), \(X_2\), \ldots, \(X_n\) are the independent variables.
\item
  \(\beta_1\), \(\beta_2\), \ldots, \(\beta_n\) are the regression
  coefficients.
\item
  \(sigma\) is the error term.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-1}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
ùëù(ùëå‚à£\alpha, \beta, X) \sim Normal(\alpha + \sum_k^n  \beta_k  X, œÉ¬≤)
\]

\[
p(\alpha) \sim Normal(0,1)
\]

\[
p(\beta_i) \sim Normal(0,1)
\]

\[
p(œÉ) \sim Uniform(0, 50)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y | ùëã, \alpha,\beta)\) is the likelihood function.
\item
  \(p(\alpha)\) is prior distributions for the intercept
\item
  \(p(\beta_k)\) are the prior distributions for the regression
  coefficients \emph{k} distinct regression coefficients.
\item
  \(p(\sigma)\) is the prior distribution for the standard deviation,
  ensuring - it is positive.
\end{itemize}

\section{Reference(s)}\label{references-1}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Interaction Term between Two Continuous
Variables}\label{interaction-term-between-two-continuous-variables}

\section{General Principles}\label{general-principles-2}

To study relationships between two independent continuous variables and
their interaction effect on a dependent variable (e.g., temperature and
humidity affecting energy consumption), we can use Regression Analysis
with Interaction Terms. In this approach, we extend the simple linear
regression model to include an interaction term (a multiplication)
between the two continuous variables.

Parallel lines indicate that there is no interaction effect while
different slopes suggest that one might be present. Below is the plot
for Food x Condiment. The crossed lines on the graph suggest that there
is an interaction effect, which the significant p-value for the
Food*Condiment term confirms. The graph shows that enjoyment levels are
higher for chocolate sauce when the food is ice cream. Conversely,
satisfaction levels are higher for mustard when the food is a hot dog.
If you put mustard on ice cream or chocolate sauce on hot dogs, you
won't be happy!

\includegraphics{index_files/mediabag/interactions_plot_ca.png}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  Model relationship between Y and R to vary as a function of A. you
  explicitly model the hypothesis that the slope between Y and R
  depends---is conditional---upon A.
\item
  For continuous interactions, the intercept becomes the grand mean of
  the outcome variable. This ease of interpretation alone is a good
  reason to center predictor variables.
\item
  Estimate interpretation is more difficult as estimate of
  non-interaction terms become expected change in Y when R increases by
  one unit and A is at its average value and estimate of interaction
  terms are expected change in the influence of A on Y when increasing R
  by one unit and expected change in the influence of R on Y when
  increasing A by one unit.
\item
  \href{3.\%20\%Interaction\%20\%between\%20\%continuous\%20\%variables.qmd}{Triptych
  üõà} plots are very handy for understanding the impact of interactions.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-2}

Below is an example code snippet demonstrating Bayesian regression with
an interaction term between two continuous variables with the Bayesian
Inference (BI) package:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/tulips.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.scale([}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{])}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}blooms\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}shade\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(blooms,shade, water):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta1\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta2\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta\_interaction\_ }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}beta\_interaction\_\textquotesingle{}}\NormalTok{)    }
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(a }\OperatorTok{+}\NormalTok{ beta1}\OperatorTok{*}\NormalTok{water }\OperatorTok{+}\NormalTok{ beta2}\OperatorTok{*}\NormalTok{shade }\OperatorTok{+}\NormalTok{ beta\_interaction\_}\OperatorTok{*}\NormalTok{water}\OperatorTok{*}\NormalTok{shade, sigma), obs}\OperatorTok{=}\NormalTok{blooms)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-2}

\section{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-2}

We model the relationship between the input features (X1 and X2) and the
target variable (Y) using the following equation: \[
ùëå = \alpha + \beta_1ùëã_1‚àó+\beta_2ùëã_2+\\beta_{interaction}ùëã_1ùëã_2 + $\sigma$
\]

Where:

\begin{itemize}
\tightlist
\item
  \(Y\) is the target variable.
\item
  \(\alpha\) is the intercept term.
\item
  \(X_1\) and \(X_2\) are the two independent continuous variables.
\item
  \(\beta_1\) and \(\beta_2\) are the regression coefficients for
  \(X_1\) and \(X_2\), respectively.
\item
  \(\beta_{interaction}\) is the regression coefficient for the
  interaction term \((X_1  X_2)\).
\item
  \(\sigma\) is the error term assumed to be normally distributed.
\end{itemize}

In this context, the interaction term \(X_1 * X_2\) captures the joint
effect of \(X_1\) and \(X_2\) on the target variable \(Y\).

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-2}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y‚à£X_1‚Äã ,X_2‚Äã , \beta_1, \beta_2, \beta_{interaction} ) \sim Normal(\alpha +  \beta_1  X_1‚Äã + \beta_2  X_2‚Äã‚Äã + \beta_{interaction}  X_1‚Äã X_2‚Äã ,  $\sigma$)
\]

\[
ùëù(\alpha) \sim Normal(0,1)
\]

\[
ùëù(\beta_1) \sim Normal(0,1)
\]

\[
ùëù(\beta_2) \sim Normal(0,1)
\]

\[
ùëù(\beta_{interaction}) \sim Normal(0,1)
\]

\[
ùëù(œÉ) \sim Exponential(1)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y | X_1‚Äã ,X_2‚Äã , \beta_1, \beta_2, \beta_{interaction})\) is the
  likelihood function.
\item
  \(p(\alpha)\) is the prior distribution for the intercept
\item
  \(p(\beta_1)\), \(p(\beta_2)\) and \(\beta_{interaction}\) are the
  prior distributions for the regression coefficients.
\item
  \(p(\sigma)\) is the prior distribution for the standard deviation,
  ensuring it is positive.
\end{itemize}

\section{Reference(s)}\label{references-2}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Regression for Categorical
Variables}\label{regression-for-categorical-variables}

\section{General Principles}\label{general-principles-3}

To study the relationship between a categorical independent variable and
a continuous dependent variable, we use \emph{Categorical model} wich
apply \emph{stratification}.

\emph{Stratification} concist in modeling how the different categories
of the independent variable affect the target continuous variable, by
performing a regression for each categories and asing a regression
coefficient for each categories. To realize the \emph{stratification},
categorical variables are often encoded using one-hot encoding or
converting categories to indeces.

\includegraphics{index_files/mediabag/tumblr_inline_o8j406.png}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  As we generate regression coefficients for each ( k ) category in the
  code, we need to specify a prior with a shape equal to the number of
  categories (see comments in the code).
\item
  To compare differences between categories, we need to compute the
  distribution of the differences between categories, known as the
  contrast distribution. \textbf{Never compare the confidence intervals
  or p-values directly.}
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-3}

Below is an example code snippet demonstrating Bayesian regression with
an independent categorical variable:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/milk.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.index([}\StringTok{"clade"}\NormalTok{])}
\NormalTok{m.scale([}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{])}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}kcal\_per\_g\textquotesingle{}}\NormalTok{, }\StringTok{"index\_clade"}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(kcal\_per\_g, index\_clade):    }
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,), name }\OperatorTok{=} \StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{) }\CommentTok{\# we specify a vector of length 4 as we have 4 categories}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ bi.dist.exponential( }\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(beta[index\_clade], s), obs}\OperatorTok{=}\NormalTok{ kcal\_per\_g)}


\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-3}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-3}

We model the relationship between the categorical input feature (X) and
the target variable (Y) using the following equation:

\[ùëå=\alpha + \beta_k X_i + \sigma\]

Where:

\begin{itemize}
\tightlist
\item
  \(Y\) is the target variable.
\item
  \(X\) is the encoded categorical input variable .
\item
  \(\alpha\) is the intercept term.
\item
  \(\beta_k\) are the regression coefficient for each \emph{k}
  categories.
\item
  \(X\) is the independet varible
\item
  \(\sigma\) is the error term .
\end{itemize}

We can interpret \(\beta_i\) as the effect of each category on \(Y\)
relative to the baseline (usually one of the categories or the
intercept).

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-3}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y|\alpha, \beta, X) \sim Normal(\alpha +  \beta_k X, \sigma)
\]

\[
ùëù(\alpha) \sim Normal(0,1)
\] \[
ùëù(\beta_k) \sim Normal(0,1)
\] \[
ùëù(ùúé) \sim Exponential(1)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(ùëå_i‚à£\alpha, ùëã_i, \beta_i)\) is the likelihood function.
\item
  \(p(\alpha)\) is prior distributions for the intercept
\item
  \(p(\beta_k)\) are \emph{k} prior distributions for \emph{k}
  regression coefficients.
\item
  \(p(\sigma)\) is the prior distribution for the standard deviation,
  ensuring it is positive.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Notes}, colframe=quarto-callout-note-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{2.\%20Multiple\%20continuous\%20Variables.qmd}{chapter 2:
  Multiple continuous Variables}.
\item
  We can apply interaction terms similarly as
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3: Interaction between continuous variables}.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-3}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Binomial model}\label{binomial-model}

\section{General Principles}\label{general-principles-4}

To model the relationship between a binary outcome -e.g.~such as
success/failure, yes/no, or 1/0.- variable and one or more independent
variables, we can use Binomial model.

\includegraphics{index_files/mediabag/xHlvv.png}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We have the firs \href{5.\%20Binomial\%20regression.qmd}{link
  function} \emph{logit}. The \emph{logit} link function in Bayesian
  binomial model converts the linear combination of predictor variables
  into probabilities, making it suitable for modeling binary outcomes.
  It helps estimate the relationship between predictors and the
  probability of success, ensuring results fall within the bounds of the
  binomial distribution.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-4}

Below is an example code snippet demonstrating Bayesian binomial
regression

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/chimpanzees.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(pulled\_left):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal( }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(logits}\OperatorTok{=}\NormalTok{ alpha[actor] }\OperatorTok{+}\NormalTok{ beta1[side] }\OperatorTok{+}\NormalTok{ beta2[cond]), obs}\OperatorTok{=}\NormalTok{pulled\_left)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, init\_strategy }\OperatorTok{=}\NormalTok{ numpyro.infer.initialization.init\_to\_mean()) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-4}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-4}

We model the relationship between the independent variable (\(X\)) and
the binary outcome variable (\(Y\)) using the following equation: \[
logit(Y)=\alpha + \beta   ùëã 
\]

Where:

\begin{itemize}
\tightlist
\item
  \(Y\) is the probability of success (or the probability of the binary
  outcome being 1).
\item
  \(X\), is an independent variables.
\item
  \(\beta\) is the regression coefficients.
\item
  \(\alpha\) is the intercept term.
\item
  \(\sigma\) is the error term.
\item
  \(logit(Y)\) is the log-odds of success, calculated as the log of the
  odds ratio of success. Through this link function, the relationship
  between the independent variables and the log-odds of success is
  modeled linearly, allowing us to interpret the effect of each
  independent variables on the log-odds of success.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian model
(WIP)}}{Bayesian model (WIP)}}\label{bayesian-model-wip}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[ 
Y \sim Binomial(ùëõ = 1, ùëù)
\]

\[
logit(p) \sim \alpha + \beta X
\] \[
ùëù(\alpha) \sim Normal(0,1)
\] \[
ùëù(\beta) \sim Normal(0,1)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y | ùëå‚à£\alpha, \beta, ùëã)\) is the likelihood function.
\item
  \(p(\beta)\) and \(p(\alpha)\) are the prior distributions for the
  regression coefficients and intercept, respectivelly.
\item
  \(n=1\) represents the number of trials in the binomial distribution
  (binary outcome).
\item
  \(logit(\alpha + \beta  ùëã )\) is the \emph{logit} link function that
  is equal to sigmoid function applied to the linear combination of
  predictors, mapping the log-odds to probabilities.
\end{itemize}

\section{Notes}\label{notes-1}

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as
  \href{/3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply caterogical variables similarly as
  \href{4.\%20Categorical\%20variable.qmd}{chapter 4}.
\item
  Below is an example code snippet demonstrating Bayesian binomial model
  for multiple caterogical variables :
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/chimpanzees.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df[}\StringTok{"side"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.prosoc\_left  }\CommentTok{\# right 0, left 1}
\NormalTok{m.df[}\StringTok{"cond"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.condition  }\CommentTok{\# no partner 0, partner 1}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{, }\StringTok{"actor"}\NormalTok{, }\StringTok{"side"}\NormalTok{, }\StringTok{"cond"}\NormalTok{])}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(pulled\_left):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{7}\NormalTok{,), }\StringTok{"alpha"}\NormalTok{) }\CommentTok{\# generating k intercept  \#(one for each actor)}
\NormalTok{    beta1 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{2}\NormalTok{,), }\StringTok{"beta"}\NormalTok{) }\CommentTok{\# generating k regression coefficient  for each k prosoc\_left)}
\NormalTok{    beta2 }\OperatorTok{=}\NormalTok{ bi.dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{2}\NormalTok{,), }\StringTok{"alpha"}\NormalTok{) }\CommentTok{\# generating k regression coefficient for each k condition)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(logits}\OperatorTok{=}\NormalTok{ alpha[actor] }\OperatorTok{+}\NormalTok{ beta1[side] }\OperatorTok{+}\NormalTok{ beta2[cond]), obs}\OperatorTok{=}\NormalTok{pulled\_left)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model, init\_strategy }\OperatorTok{=}\NormalTok{ numpyro.infer.initialization.init\_to\_mean()) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Reference(s)}\label{references-4}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Beta-Binomial model}\label{beta-binomial-model}

\section{General Principles}\label{general-principles-5}

To model the relationship between a binary outcome variable representing
success counts and one or more indepedent variables with
\href{7.\%20Poisson\%20model.qmd}{overdispersion üõà}, we can use
Beta-Binomial model.

We model the relationship between the predictor variables (X1, X2,
\ldots, Xn) and the probability of success (p) using the following
equation:

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{5.\%20Binomial\%20model.qmd}{Binomial regression}.
\item
  A beta-binomial model assumes that each binomial count observation has
  its own probability of a success. The model estimates the distribution
  of probabilities of success across cases, instead of a single
  probability of success.
\item
  A beta distribution has two parameters, an average probability
  \emph{p} and a shape parameter Œ∏.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-5}

Below is an example code snippet demonstrating Bayesian Beta-Binomial
regression:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ .bi.main }\ImportTok{import}\OperatorTok{*}\CommentTok{\#}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/UCBadmit.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df[}\StringTok{"gid"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (m.df[}\StringTok{"applicant.gender"}\NormalTok{] }\OperatorTok{!=} \StringTok{"male"}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}
\NormalTok{gid }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"gid"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values)}
\NormalTok{applications }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"applications"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values)}
\NormalTok{admit }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"admit"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values)}

\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    gid }\OperatorTok{=}\NormalTok{ gid,}
\NormalTok{    applications }\OperatorTok{=}\NormalTok{ applications,}
\NormalTok{    admit }\OperatorTok{=}\NormalTok{  admit}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(gid, applications, admit):}
\NormalTok{    phi }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{)}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal( }\FloatTok{0.}\NormalTok{, }\FloatTok{1.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    theta }\OperatorTok{=}\NormalTok{ phi }\OperatorTok{+} \DecValTok{2}
\NormalTok{    pbar }\OperatorTok{=}\NormalTok{ jax.nn.sigmoid(alpha[gid])}
\NormalTok{    concentration1 }\OperatorTok{=}\NormalTok{ pbar}\OperatorTok{*}\NormalTok{theta}
\NormalTok{    concentration0 }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ pbar) }\OperatorTok{*}\NormalTok{ theta}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, BetaBinomial(total\_count }\OperatorTok{=}\NormalTok{ applications, concentration1 }\OperatorTok{=}\NormalTok{ concentration1, concentration0 }\OperatorTok{=}\NormalTok{ concentration0), obs}\OperatorTok{=}\NormalTok{admit)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-5}

\section{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-5}

\[
logit(ùëù)= \alpha + \beta X
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p\) is the probability of success.
\item
  \(\alpha\) the intercept term.
\item
  \(\beta\) the regression term.
\item
  \(X\) is an independent variables.
\item
  \(\text{logit}(p)\) is the log-odds of success.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian model
(WIP)}}{Bayesian model (WIP)}}\label{bayesian-model-wip-1}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y|n, \overline{p}, \theta) \sim BetaBinomial(n, \overline{p}, \theta)
\]

\[
logit(\overline{p}) \sim \alpha + \beta X
\]

\[ùëù(\alpha) \sim Normal(0,1)\]

\[p(\theta) \sim HalfCaychy(0,1)\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y | n, \overline{p}, \theta)\) is the likelihood function.
\item
  \(n\) is the total count of trials.
\item
  \(\overline{p}\) is the average probability.
\item
  \(p(\theta)\) is the shape distribution term.
\item
  \(ùëù(\alpha)\) is the intercept term.
\item
  \(ùëù(\beta)\) is regression coefficient term.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Poisson model}\label{poisson-model}

\section{General Principles}\label{general-principles-6}

To model the relationship between a count outcome variable -e.g.~counts
of events occurring in a fixed interval of time or space- and one or
more independent variables, we can use Poisson model.

This is a special shape of the binomial distribution, it is useful
because it model binomial events for which the number of trials n is
unknown or uncountably large.

\includegraphics{index_files/mediabag/Comparison-of-linear.png}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  We have the second \href{6.\%20Poisson\%20regression.qmd}{link
  function}. The conventional link function for a Poisson model is the
  \emph{log} link (it ensures that \emph{Œª} i is always positive).
\item
  To invert the log link function and model linearly the relationship
  between the predictor variables and the log of the mean rate parameter
  we can apply the exponential function (see comment in code)
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-6}

Below is an example code snippet demonstrating Bayesian Poisson model

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/Kline.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.sale([}\StringTok{"P"}\NormalTok{]) }
\NormalTok{m.df[}\StringTok{"cid"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (m.df.contact }\OperatorTok{==} \StringTok{"high"}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}

\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}total\_tools\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}P\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cid\textquotesingle{}}\NormalTok{])}


\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(cid, P, total\_tools):}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, name}\OperatorTok{=}\StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Poisson(jnp.exp(alpha[cid] }\OperatorTok{+}\NormalTok{ beta[cid]}\OperatorTok{*}\NormalTok{P)), obs}\OperatorTok{=}\NormalTok{total\_tools) }\CommentTok{\# Exponential ensure non{-}negative values and invert the log link function}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-6}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-6}

We model the relationship between the predictor variable (\(X\)) and the
count outcome variable (\(Y\)) using the following equation:

\[
log(ùúÜ)=\alpha + \beta  X 
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\lambda\) is the mean rate parameter of the Poisson distribution
  (expected count).
\item
  \(X\) is the predictor variables.
\item
  \(\beta\) is the regression coefficients.
\item
  \(\alpha\) is the intercept term.
\item
  \(\log(\lambda)\) is the log of the mean rate parameter, ensuring it
  is positive.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian model
(WIP)}}{Bayesian model (WIP)}}\label{bayesian-model-wip-2}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y‚à£\alpha, \beta, X) \sim Poisson(Œª)
\]

\[
log(\lambda) \sim \alpha + \beta X
\] \[
p(\alpha) \sim Normal(0, 1)
\]

\[
p(\beta) \sim Normal(0, 1)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y |alpha, \beta, X)\) is the likelihood function.
\item
  \(p(\beta)\) and \(p(\alpha)\) are the prior distributions for the
  regression coefficients and intercept.
\item
  \(\lambda\) is the mean rate parameter of the Poisson distribution,
  modeled as the exponential function of the linear combination of
  predictors.
\end{itemize}

\section{Notes}\label{notes-2}

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply caterogical variables similarly as
  \href{4.\%20Categorical\%20variable.md}{chapter 4}.
\end{itemize}

\section{Reference(s)}\label{references-5}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Gamma-Poisson model}\label{gamma-poisson-model}

\section{General Principles}\label{general-principles-7}

To model the relationship between a count outcome variable and one or
more independent variables with
\href{7.\%20Poisson\%20model.qmd}{overdispersion üõà}, we can use
\emph{Negative Binomial model}.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{7.\%20Poisson\%20model.qmd}{Poisson model}.
\item
  Overdispersion is handled because the Negative-binomial model assumes
  that each Poisson count observation has its own rate. This is an
  additional parameter specified in the model (in the code, it is
  \texttt{\_log\_days\_}).
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\ImportTok{import}\NormalTok{ tensorflow\_probability.substrates.jax.distributions }\ImportTok{as}\NormalTok{ tfd}
\NormalTok{init\_key, sample\_key }\OperatorTok{=}\NormalTok{ random.split(random.PRNGKey(}\BuiltInTok{int}\NormalTok{(r.randint(}\DecValTok{0}\NormalTok{, }\DecValTok{10000000}\NormalTok{))))}
\NormalTok{init\_key }\OperatorTok{=}\NormalTok{ jnp.array(init\_key)}
\NormalTok{num\_days }\OperatorTok{=} \DecValTok{30}
\NormalTok{y }\OperatorTok{=}\NormalTok{ tfd.Poisson(rate}\OperatorTok{=}\FloatTok{1.5}\NormalTok{).sample(seed }\OperatorTok{=}\NormalTok{ init\_key, sample\_shape}\OperatorTok{=}\NormalTok{(num\_days,))}
\NormalTok{num\_weeks }\OperatorTok{=} \DecValTok{4}
\NormalTok{y\_new }\OperatorTok{=}\NormalTok{ tfd.Poisson(rate}\OperatorTok{=}\FloatTok{0.5} \OperatorTok{*} \DecValTok{7}\NormalTok{).sample(seed }\OperatorTok{=}\NormalTok{ init\_key, sample\_shape}\OperatorTok{=}\NormalTok{(num\_weeks,))}
\NormalTok{y\_all }\OperatorTok{=}\NormalTok{ np.concatenate([y, y\_new])}
\NormalTok{exposure }\OperatorTok{=}\NormalTok{ np.concatenate([np.repeat(}\DecValTok{1}\NormalTok{, }\DecValTok{30}\NormalTok{), np.repeat(}\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{)])}
\NormalTok{monastery }\OperatorTok{=}\NormalTok{ np.concatenate([np.repeat(}\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{), np.repeat(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)])}
\NormalTok{d }\OperatorTok{=}\NormalTok{ pd.DataFrame.from\_dict(}\BuiltInTok{dict}\NormalTok{(y}\OperatorTok{=}\NormalTok{y\_all, days}\OperatorTok{=}\NormalTok{exposure, monastery}\OperatorTok{=}\NormalTok{monastery))}
\NormalTok{d[}\StringTok{"log\_days"}\NormalTok{] }\OperatorTok{=}\NormalTok{ d.days.pipe(np.log)}

\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}

\CommentTok{\# import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    log\_days }\OperatorTok{=}\NormalTok{ jnp.array(d.log\_days.values), }\CommentTok{\# rate of each counts data}
\NormalTok{    monastery }\OperatorTok{=}\NormalTok{ jnp.array(d.monastery.values),}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ jnp.array(d.y.values)}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(log\_days, monastery, output):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    l }\OperatorTok{=}\NormalTok{ log\_days }\OperatorTok{+}\NormalTok{ a }\OperatorTok{+}\NormalTok{  b }\OperatorTok{*}\NormalTok{ monastery}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Poisson(rate }\OperatorTok{=}\NormalTok{ l), obs}\OperatorTok{=}\NormalTok{output)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-7}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-7}

We model the relationship between the independent variable \(X\) and the
count outcome variable \(Y\) using the following equation:

\[
log‚Å°(\lambda)= exp(rates + \alpha + \beta ùëã)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\lambda\) is the mean rate parameter of the negative binomial
  distribution (expected count).
\item
  \(X\) is the predictor variables.
\item
  \(\beta\) is the regression coefficients.
\item
  \(\alpha\) is the intercept term.
\item
  \(\log(\lambda)\) is the log of the mean rate parameter, ensuring it
  is positive.
\end{itemize}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-4}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
ùëù(ùëå‚à£\alpha, \beta, X) \sim Poisson(Œª)
\]

\[
log(Œª) \sim rates + \alpha + \beta X
\]

\[
ùëù(\alpha)\sim Normal(0,1)
\]

\[
ùëù(\beta) \sim Normal(0,1)
\]

Where:

\begin{itemize}
\tightlist
\item
  \(p(Y | \alpha, \beta, X)\) is the likelihood function.
\item
  \(p(\beta)\) and \(p(\alpha)\) are the prior distributions for the
  regression coefficients and intercept.
\item
  \(\lambda = rates +\alpha + \beta X\) is the mean rate parameter of
  the Poisson distribution assuming that each Poisson count observation
  has its own rate.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Notes}, colframe=quarto-callout-note-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply caterogical variables similarly as
  \href{4.\%20Categorical\%20variable.md}{chapter 4}.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-6}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Multinomial model}\label{multinomial-model}

\section{General Principles}\label{general-principles-8}

To model the relationship between a categorical outcome variable with
more than two categories and one or more independent variables, we can
use a \emph{Multinomial} distribution.

\includegraphics{index_files/mediabag/Multinomial-Logistic.jpg}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  One way to interpret a multinomial is to consider that we need to
  build ( K - 1 ) linear models, where ( K ) is the number of
  categories. Once we get the linear prediction for each category, we
  can convert these predictions to probabilities by building a
  \href{8.\%20Multinomial.md}{simplex üõà}. To do this, we convert the
  regression outputs using the softmax function (see ``nn.softmax'' line
  in the code).
\item
  The intercept captures the difference in the log-odds of the outcome
  categories, thus different categories need different intercepts.
\item
  On the other hand as we assume that the effect of each predictor on
  the outcome is consistent across all categories, the regression
  coefficients are shared across categories.
\item
  The relationship between the predictor variables and the log-odds of
  each category is modeled linearly, allowing us to interpret the effect
  of each predictor on the log-odds of each category.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-8}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Simulated data{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# simulate career choices among 500 individuals}
\NormalTok{N }\OperatorTok{=} \DecValTok{500}  \CommentTok{\# number of individuals}
\NormalTok{income }\OperatorTok{=}\NormalTok{ jnp.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{])  }\CommentTok{\# expected income of each career}
\NormalTok{score }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ income  }\CommentTok{\# scores for each career, based on income}

\CommentTok{\# next line converts scores to probabilities}
\NormalTok{p }\OperatorTok{=}\NormalTok{ jnp.array( jax.nn.softmax(score))}

\CommentTok{\# now simulate choice}
\CommentTok{\# outcome career holds event type values, not counts}
\NormalTok{career }\OperatorTok{=}\NormalTok{ bi.dist.categorical(p, shape }\OperatorTok{=}\NormalTok{ N, sample }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    income }\OperatorTok{=}\NormalTok{ income,}
\NormalTok{    career }\OperatorTok{=}\NormalTok{ career}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(income, career):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ dist.halfnormal(}\FloatTok{0.5}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    s\_1 }\OperatorTok{=}\NormalTok{ a[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    s\_2 }\OperatorTok{=}\NormalTok{ a[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ income[}\DecValTok{1}\NormalTok{]}
\NormalTok{    s\_3 }\OperatorTok{=}\NormalTok{ a[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ income[}\DecValTok{0}\NormalTok{]}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ jax.nn.softmax(jnp.stack([s\_1[}\DecValTok{0}\NormalTok{], s\_2[}\DecValTok{0}\NormalTok{], s\_3[}\DecValTok{0}\NormalTok{]]))}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Categorical(probs }\OperatorTok{=}\NormalTok{  p[career]), obs}\OperatorTok{=}\NormalTok{career)}

\CommentTok{\# Run sampler {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\NormalTok{m.run(model)  }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-8}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-8}

We model the relationship between the predictor variables (X1, X2,
\ldots, Xn) and the categorical outcome variable (Y) using the following
equation:

\$\$ logit(p\_ik) = log(\frac{p_ik}{p_iK}) = Œ≤\_k\^{}T X\_i +Œ±\_k k

\$\$

Where: - \(p_ik\) is the probability of the ùëñ-th observation being in
category ùëò.

\begin{itemize}
\item
  \(Œ≤_k\) are the regression coefficients for category ùëò.
\item
  \(Œ±_k\) is the intercept for category ùëò.
\item
  \(X_i\) is the vector of predictor variables for the ùëñ-th observation.
\item
  The reference category ùêæ is often chosen to simplify the model (Note
  that we didi not did it in the code).
\end{itemize}

We can express the Bayesian Multinomial model as follows:

If \(K‚ààN ,  N‚ààN , and  Œ∏‚ààK\)-simplex , then for \(y‚ààNK\) such that
\(‚àëKk=1yk=N\):

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-5}

In Bayesian multinomial modeling, the likelihood function of the data is
specified using a multinomial distribution. The multinomial distribution
models the counts of outcomes falling into different categories.For an
outcome variable ùë¶ with ùêæ categories, the multinomial likelihood
function is: \[
Multinomial(y|Œ∏)=\frac{N!}{‚àè^K_{k=1}yk!} ‚àè_{k = 1}^{K} \theta_{k}^{y_k}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(y=(y_1, y_2,‚Ä¶,y_K)\) represents the counts of observations in each
  of the ùêæ categories.
\item
  \(N\) is the total number of observations or trials.
\item
  \(Œ∏=(Œ∏_1,Œ∏_2,‚Ä¶,Œ∏_K)\) is a simplex of category probabilities, with
  \(ùúÉ_ùëò\) representing the probability of category ùëò.
\item
  \(\frac{N!}{‚àè^K_{k=1}yk!}\) is the multinomial coefficient accounts
  for the number of ways to arrange the observations into the
  categories. This coefficient ensures that the likelihood function
  properly accounts for the permutations of the counts across different
  categories. \#\# Reference(s) McElreath (2018)
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Dirichlet model}\label{dirichlet-model}

\section{General Principles}\label{general-principles-9}

To model the relationship between a categorical outcome variable with
more than two categories and one or more independent variables with
\href{7.\%20Poisson\%20model.qmd}{overdispersion üõà}, we can use a
\emph{Dirichlet} distribution.

\includegraphics{index_files/mediabag/Multinomial-Logistic.jpg}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{7.\%20Multinomial\%20model.qmd}{Multinomial model}.
\item
  One major difference from the multinomial model is that the Dirichlet
  model doesn't require a simplex.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Simulated data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ jax }\ImportTok{import}\NormalTok{ random}
\ImportTok{from}\NormalTok{ jax.nn }\ImportTok{import}\NormalTok{ softmax}
\ImportTok{import}\NormalTok{ jax.numpy }\ImportTok{as}\NormalTok{ jnp}
\ImportTok{import}\NormalTok{ numpyro }\ImportTok{as}\NormalTok{ numpyro}
\ImportTok{import}\NormalTok{ numpyro.distributions }\ImportTok{as}\NormalTok{ dist}
\ImportTok{from}\NormalTok{ numpyro.infer }\ImportTok{import}\NormalTok{ MCMC, NUTS, Predictive}

\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\# SIMULATING MULTINOMIAL DATA WITH SOFTMAX LINK FUNCTION \#\#\#\#\#\#\#\#\#\#\#}
\KeywordTok{def}\NormalTok{ mysoftmax(x):}
\NormalTok{    exp\_x }\OperatorTok{=}\NormalTok{ np.exp(x }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(x))}
    \ControlFlowTok{return}\NormalTok{ exp\_x }\OperatorTok{/}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(exp\_x, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\NormalTok{K }\OperatorTok{=} \DecValTok{3}
\NormalTok{N }\OperatorTok{=} \DecValTok{100}
\NormalTok{N\_obs }\OperatorTok{=} \DecValTok{2}
\NormalTok{sigma\_random }\OperatorTok{=} \FloatTok{0.6}


\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\# Fixed effect Sim \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\#a = np.random.normal(0, 1, K)}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]) }\CommentTok{\# Forcing a values}


\CommentTok{\# Factors{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{NY }\OperatorTok{=} \DecValTok{4}
\NormalTok{NV }\OperatorTok{=} \DecValTok{8}

\NormalTok{Y2 }\OperatorTok{=}\NormalTok{ np.full((NV, NY), np.nan) }
\NormalTok{means }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, NY)}
\NormalTok{offsets }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, NV)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(NV):}
  \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(NY):}
\NormalTok{    Y2[i,k] }\OperatorTok{=}\NormalTok{ means[k] }\OperatorTok{+}\NormalTok{ offsets[i]}

    
\NormalTok{b\_individual }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, (N, K))}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ b\_individual }\OperatorTok{+}\NormalTok{ a}


\CommentTok{\# Declare an empty Matrix to fill with data}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.empty((N }\OperatorTok{*}\NormalTok{ N\_obs, K))}

\CommentTok{\# Declare an empty vector to fill with IDs}
\BuiltInTok{id} \OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Loop over each individual}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
    \CommentTok{\# Simulate N\_obs draws from the multinomial}
\NormalTok{    Y[i}\OperatorTok{*}\NormalTok{N\_obs:(i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{N\_obs, :] }\OperatorTok{=}\NormalTok{ np.apply\_along\_axis(}\KeywordTok{lambda}\NormalTok{ x: np.random.multinomial(}\DecValTok{100}\NormalTok{, nn.softmax(x)), }\DecValTok{0}\NormalTok{, mu[i])}
    \CommentTok{\# Assign ID vector}
    \BuiltInTok{id} \OperatorTok{+=}\NormalTok{ [i] }\OperatorTok{*}\NormalTok{ N\_obs}


\NormalTok{N }\OperatorTok{=}\NormalTok{ N}\OperatorTok{*}\NormalTok{N\_obs}
\NormalTok{K }\OperatorTok{=}\NormalTok{ K}
\NormalTok{ni }\OperatorTok{=}\NormalTok{ N}
\NormalTok{y }\OperatorTok{=}\NormalTok{ jnp.array(Y, dtype}\OperatorTok{=}\NormalTok{jnp.int32).reshape(N, K)}
\NormalTok{i\_ID }\OperatorTok{=}\NormalTok{ jnp.array(}\BuiltInTok{id}\NormalTok{)}



\CommentTok{\# Deifne model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi()}
\KeywordTok{def}\NormalTok{ model(K, ni, y, i\_ID):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ normal(}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, [K], }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    Sigma\_individual }\OperatorTok{=}\NormalTok{ exponential(}\StringTok{\textquotesingle{}Sigma\_individual\textquotesingle{}}\NormalTok{, [ni], }\DecValTok{1}\NormalTok{ )}
\NormalTok{    L\_individual }\OperatorTok{=}\NormalTok{ lkjcholesky(}\StringTok{\textquotesingle{}L\_individual\textquotesingle{}}\NormalTok{, [], ni, }\DecValTok{1}\NormalTok{) }\CommentTok{\# Implies a uniform distribution over correlation matrices}
\NormalTok{    z\_individual }\OperatorTok{=}\NormalTok{ normal(}\StringTok{\textquotesingle{}z\_individual\textquotesingle{}}\NormalTok{, [ni,K], }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ random\_centered(Sigma\_individual, L\_individual, z\_individual)}
\NormalTok{    lk }\OperatorTok{=}\NormalTok{ jnp.exp(a }\OperatorTok{+}\NormalTok{ alpha[i\_ID])}
\NormalTok{    sample(}\StringTok{"y"}\NormalTok{, DirichletMultinomial(lk, }\BuiltInTok{int}\NormalTok{(}\DecValTok{100}\NormalTok{)), obs}\OperatorTok{=}\NormalTok{y)}

\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    K }\OperatorTok{=}\NormalTok{ K,}
\NormalTok{    ni }\OperatorTok{=}\NormalTok{ ni,}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ y,}
\NormalTok{    i\_ID }\OperatorTok{=}\NormalTok{ i\_ID}
\NormalTok{)}

\CommentTok{\# Run sampler {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\NormalTok{m.run(model)  }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-9}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-9}

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-6}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\section{Reference(s)}\label{references-7}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Zero inflated}\label{zero-inflated}

\section{General Principles}\label{general-principles-10}

Zero-Inflated Regression models are used when the outcome variable is a
count variable with an excess of zero counts. These models combine a
count model (e.g., Poisson or Negative Binomial) with a separate model
for predicting the probability of excess zeros.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

In Bayesian Zero-Inflated regression, we consider uncertainty in the
model parameters and provide a full posterior distribution over them. We
need to declare prior distributions for \emph{W}\{1\pi\}, W\_\{2\pi\},
\ldots, W\_\{n\pi\}\emph{, \emph{W}\{1\lambda\}, W}\{2\lambda\}, \ldots,
W\_\{n\lambda\}\emph{, \emph{b}\pi}, and \emph{b}\lambda\_.

\end{tcolorbox}

\section{Example}\label{example-10}

Below is an example code snippet demonstrating Bayesian Zero-Inflated
Poisson regression using TensorFlow Probability:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ jax.scipy.special }\ImportTok{import}\NormalTok{ expit}
\NormalTok{r.seed(}\DecValTok{42}\NormalTok{)}
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Simulated data{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{prob\_drink }\OperatorTok{=} \FloatTok{0.2}  \CommentTok{\# 20\% of days}
\NormalTok{rate\_work }\OperatorTok{=} \DecValTok{1}     \CommentTok{\# average 1 manuscript per day}

\CommentTok{\# sample one year of production}
\NormalTok{N }\OperatorTok{=} \DecValTok{365}

\NormalTok{np.random.seed(}\DecValTok{365}\NormalTok{)}
\NormalTok{drink }\OperatorTok{=}\NormalTok{ np.random.binomial(}\DecValTok{1}\NormalTok{, prob\_drink, N)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ drink) }\OperatorTok{*}\NormalTok{ np.random.poisson(rate\_work, N)}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ jnp.array(y)}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(y):}
\NormalTok{    al }\OperatorTok{=}\NormalTok{ dist.normal( }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}al\textquotesingle{}}\NormalTok{)}
\NormalTok{    ap }\OperatorTok{=}\NormalTok{ dist.normal( }\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{,name }\OperatorTok{=} \StringTok{\textquotesingle{}ap\textquotesingle{}}\NormalTok{)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ expit(ap)}
\NormalTok{    lambda\_ }\OperatorTok{=}\NormalTok{ jnp.exp(al)}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, ZeroInflatedPoisson(p, lambda\_), obs}\OperatorTok{=}\NormalTok{y)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-10}

\section{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-10}

We model the relationship between the predictor variables \emph{X} and
the count outcome variable \emph{Y} using two components: 1. A logistic
regression model to predict the probability of an excess zero. 2. A
count model (e.g., Poisson or Negative Binomial) to predict the count
outcome.

The overall model can be represented as follows:

\[
\begin{aligned}
& \text{logit}(\pi) = \alpha_\pi + \beta_\pi X \\
& \text{log}(\lambda) = \alpha_\lambda + \beta_\lambda X\\
& Y \sim \begin{cases} 
0 & \text{with probability } \pi \\
\text{CountModel}(\lambda) & \text{with probability } (1 - \pi) 
\end{cases}
\end{aligned}
\]

Where: - \(\pi\) is the probability of an excess zero. - \(\lambda\) is
the mean rate parameter of the count model. - \(\alpha_\pi\) and
\(\beta_\pi\) are respectivelly, the intercept and the regression
coefficient for the logistic model. - \(\alpha_\lambda\) and
\(\beta_\lambda\) are respectivelly, the regression coefficient for the
the count model. - \(X\) is the independent variables.

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-7}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
ùëù(ùëå‚à£\lambda) \sim  ZIPoisson(\pi,\lambda)
\]

\[
logit(\pi) = \alpha_\pi + \beta_\pi X
\]

\[
log(\lambda) = \alpha_\lambda + \beta_\lambda X
\]

\[
p(\alpha_\pi) \sim Normal(0,1)
\]

\[
p(\beta_\pi) \sim Normal(0,1)
\]

\[
p(\alpha_\lambda) \sim Normal(0,1)
\]

\[
p(\beta_\lambda) \sim Normal(0,1)
\]

Where: - \(\pi\) is the probability of an excess zero. - \(\lambda\) is
the mean rate parameter of the count model. - \(\alpha_\pi\) and
\(\beta_\pi\) are respectivelly, the intercept and the regression
coefficient for the logistic model. - \(\alpha_\lambda\) and
\(\beta_\lambda\) are respectivelly, the regression coefficient for the
the count model. - \(X\) is the independent variables.

\section{Reference(s)}\label{references-8}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Varying intercepts}\label{varying-intercepts}

\section{General Principles}\label{general-principles-11}

To model the relationship between predictor variables and an independent
variable while allowing for different intercepts across groups or
clusters, we can use a \emph{Varying Intercepts} model. This approach is
particularly useful when data is grouped (e.g., by subject, location, or
time period) and we expect the baseline level of the outcome to vary
across these groups.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{1.\%20Linear\%20Regression\%20for\%20continuous\%20variable.qmd}{Regression
  for continuous variable}.
\item
  The main idea of varying intercepts is to generate an intercept for
  each group, allowing each group to start at different levels. Thus,
  the intercept \$ \beta\$ is defined based on the \(k\) declared
  groups.
\item
  Each intercept have is own \emph{Normal distribution} -i.e.~a
  \href{12.\%20Varying\%20intercepts.qmd}{hyper-prior üõà}-. In the code
  below, the \emph{hyper-prior} is \texttt{\_a\_bar\_}.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-11}

Below is an example code snippet demonstrating Bayesian regression with
varying intercepts:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}

\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Import data {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.data(}\StringTok{\textquotesingle{}../data/reedfrogs.csv\textquotesingle{}}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{};\textquotesingle{}}\NormalTok{) }
\NormalTok{m.df[}\StringTok{"tank"}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.arange(m.df.shape[}\DecValTok{0}\NormalTok{])}
\NormalTok{tank }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"tank"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values)}
\NormalTok{density }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"density"}\NormalTok{].astype(}\StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{).values)}
\NormalTok{surv }\OperatorTok{=}\NormalTok{ jnp.array(m.df[}\StringTok{"surv"}\NormalTok{].astype(}\StringTok{\textquotesingle{}int32\textquotesingle{}}\NormalTok{).values)}
\NormalTok{m.data\_on\_model }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    tank }\OperatorTok{=}\NormalTok{ tank,}
\NormalTok{    surv }\OperatorTok{=}\NormalTok{ surv}
\NormalTok{)}

\CommentTok{\# Define model {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ model(tank, surv):}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential( }\DecValTok{1}\NormalTok{,  name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    a\_bar }\OperatorTok{=}\NormalTok{ dist.normal( }\FloatTok{0.}\NormalTok{, }\FloatTok{1.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}a\_bar\textquotesingle{}}\NormalTok{)}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ dist.normal( a\_bar, sigma, shape}\OperatorTok{=}\NormalTok{ [}\DecValTok{48}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ jnp.squeeze(alpha[tank])[}\DecValTok{0}\NormalTok{]}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Binomial(total\_count }\OperatorTok{=}\NormalTok{ density, logits }\OperatorTok{=}\NormalTok{ p), obs}\OperatorTok{=}\NormalTok{surv)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-11}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-11}

We model the relationship between the independent variables \emph{X} and
the outcome variable \emph{Y} with varying intercepts \(\alpha\) for
each group \emph{k} using the following equation:

\[
Y_{ik} = \alpha_k + \beta X_{ik} + \sigma
\]

Where: - \(Y_{ik}\) is the outcome variable for observation \emph{i} in
group \emph{k}. - \(\alpha_k\) is the varying intercept for group
\emph{k}. - \(X_{ik}\) is the independent variables for observation
\emph{i} in group \emph{k}. - \(\beta\) is the regression coefficients
term. - \(\sigma\) is the error term, typically assumed to be normally
distributed and positive.

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-8}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y_{ik} | \mu_{ik}, \sigma) = Normal(\mu_{ik}, \sigma) \\
\mu_{ik} = \alpha_j + \beta X_{ik} + \sigma \\
\alpha_k \sim \text{Normal}(\mu_{alpha_k}, \sigma_{alpha_k}) \\
p(\beta) \sim \text{Normal}(0, 1) \\
p(\sigma) \sim \text{Exponential}(1) \\
p(\mu_{alpha_k}) \sim \text{Normal}(0, 1) \\
p(\sigma_{alpha_k}) \sim \text{Exponential}(1) \\
\]

Where: - \(p(Y_{ij} | \mu_{ij}, \sigma)\) is the likelihood function for
the outcome variable. - \(\alpha_k\) is the varying intercepts across
groups. - \(\mu_alpha_k\) is the overall mean intercept. -
\(\sigma_alpha_k\) is the variance of the intercepts across groups. -
\(p(\beta)\) is the prior distributions for the regression coefficients.
- \(p(\sigma)\) is the prior distributions for the error term.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Notes}, colframe=quarto-callout-note-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}.
\item
  We can apply interaction terms similarly as
  \href{/3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply caterogical variables similarly as
  \href{4.\%20Categorical\%20variable.qmd}{chapter 4}.
\item
  We can apply varying intercepts with any distribution developped in
  previous chapters.
\end{itemize}

\end{tcolorbox}

\section{Reference(s)}\label{references-9}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{Varying slopes}\label{varying-slopes}

\section{General Principles}\label{general-principles-12}

To model the relationship between predictor variables and an independent
variable while allowing for varying effects across groups or clusters,
we use a \emph{Varying slopes} model.

This approach is useful when we expect the relationship between
predictors and the independent variable to differ across groups (e.g.,
different slopes for different subjects, locations, or time
periods).This allow every unit in the data to have its own unique
response to any treatment or exposure or event, while also improving
estimates via pooling.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Considerations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  We have the same considerations as for
  \href{12.\%20Varying\%20intercepts.qmd}{12. Varying interceps}.
\item
  The idea is pretty similar to categorical models, where a slope is
  specified for each category. However, here, we also estimate
  relationships between different groups. This leads to a different
  mathematical approach, as to model these relationships between groups,
  we model a \href{13.\%20Varying\%20slopes.qmd}{matrix of covariance
  üõà}.
\item
  The covariance matrix requiere a correlation matris distribution which
  is modeleld using a \(LKJcorr\) distribution that hold a parameter
  \(Œ∑\). \(Œ∑\) is ussually set to 2 to define a weakly informative prior
  that is skeptical of extreme correlations near ‚àí1 or 1. When we use
  LKJ- corr(1), the prior is flat over all valid correlation matrices.
  When the value is greater than 1, then extreme correlations are less
  likely.
\item
  The Half-Cauchy distribution is used when modeling the covariance
  matrix to specify strictly positive values for the diagonal of the
  covariance matrix, ensuring positive variances.
\end{itemize}

\end{tcolorbox}

\section{Example}\label{example-12}

Below is an example code snippet demonstrating Bayesian regression with
varying effects:

\subsection{Simulated data}\label{simulated-data}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}

\NormalTok{a }\OperatorTok{=} \FloatTok{3.5}  \CommentTok{\# average morning wait time}
\NormalTok{b }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{1}  \CommentTok{\# average difference afternoon wait time}
\NormalTok{sigma\_a }\OperatorTok{=} \DecValTok{1}  \CommentTok{\# std dev in intercepts}
\NormalTok{sigma\_b }\OperatorTok{=} \FloatTok{0.5}  \CommentTok{\# std dev in slopes}
\NormalTok{rho }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{0.7}  \CommentTok{\# correlation between intercepts and slopes}
\NormalTok{Mu }\OperatorTok{=}\NormalTok{ jnp.array([a, b])}
\NormalTok{cov\_ab }\OperatorTok{=}\NormalTok{ sigma\_a }\OperatorTok{*}\NormalTok{ sigma\_b }\OperatorTok{*}\NormalTok{ rho}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ jnp.array([[sigma\_a}\OperatorTok{**}\DecValTok{2}\NormalTok{, cov\_ab], [cov\_ab, sigma\_b}\OperatorTok{**}\DecValTok{2}\NormalTok{]])}
\NormalTok{jnp.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]).reshape(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{).T}
\NormalTok{sigmas }\OperatorTok{=}\NormalTok{ jnp.array([sigma\_a, sigma\_b])  }\CommentTok{\# standard deviations}
\NormalTok{Rho }\OperatorTok{=}\NormalTok{ jnp.array([[}\DecValTok{1}\NormalTok{, rho], [rho, }\DecValTok{1}\NormalTok{]])  }\CommentTok{\# correlation matrix}

\CommentTok{\# now matrix multiply to get covariance matrix}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ jnp.diag(sigmas) }\OperatorTok{@}\NormalTok{ Rho }\OperatorTok{@}\NormalTok{ jnp.diag(sigmas)}

\NormalTok{N\_cafes }\OperatorTok{=} \DecValTok{20}
\NormalTok{seed }\OperatorTok{=}\NormalTok{ random.PRNGKey(}\DecValTok{5}\NormalTok{)  }\CommentTok{\# used to replicate example}
\NormalTok{vary\_effects }\OperatorTok{=}\NormalTok{ bi.dist.multivariatenormal(Mu, Sigma, shape}\OperatorTok{=}\NormalTok{(N\_cafes,), sample }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{a\_cafe }\OperatorTok{=}\NormalTok{ vary\_effects[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{b\_cafe }\OperatorTok{=}\NormalTok{ vary\_effects[:, }\DecValTok{1}\NormalTok{]}

\NormalTok{seed }\OperatorTok{=}\NormalTok{ random.PRNGKey(}\DecValTok{22}\NormalTok{)}
\NormalTok{N\_visits }\OperatorTok{=} \DecValTok{10}
\NormalTok{afternoon }\OperatorTok{=}\NormalTok{ jnp.tile(jnp.arange(}\DecValTok{2}\NormalTok{), N\_visits }\OperatorTok{*}\NormalTok{ N\_cafes }\OperatorTok{//} \DecValTok{2}\NormalTok{)}
\NormalTok{cafe\_id }\OperatorTok{=}\NormalTok{ jnp.repeat(jnp.arange(N\_cafes), N\_visits)}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ a\_cafe[cafe\_id] }\OperatorTok{+}\NormalTok{ b\_cafe[cafe\_id] }\OperatorTok{*}\NormalTok{ afternoon}
\NormalTok{sigma }\OperatorTok{=} \FloatTok{0.5}  \CommentTok{\# std dev within cafes}
\NormalTok{wait }\OperatorTok{=}\NormalTok{ dist.normal(mu, sigma, sample }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{d }\OperatorTok{=}\NormalTok{ pd.DataFrame(}\BuiltInTok{dict}\NormalTok{(cafe}\OperatorTok{=}\NormalTok{cafe\_id, afternoon}\OperatorTok{=}\NormalTok{afternoon, wait}\OperatorTok{=}\NormalTok{wait))}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}cafe\_id\textquotesingle{}}\NormalTok{, }\StringTok{"afternoon"}\NormalTok{, }\StringTok{"wait"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Model definition}\label{model-definition}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ model(cafe, wait, N\_cafes):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ dist.normal(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma\_cafe }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{], name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\_cafe\textquotesingle{}}\NormalTok{)}
\NormalTok{    sigma }\OperatorTok{=}\NormalTok{ dist.exponential( }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{)}
\NormalTok{    Rho }\OperatorTok{=}\NormalTok{ dist.lkj(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}Rho\textquotesingle{}}\NormalTok{)}
\NormalTok{    cov }\OperatorTok{=}\NormalTok{ jnp.outer(sigma\_cafe, sigma\_cafe) }\OperatorTok{*}\NormalTok{ Rho}
\NormalTok{    a\_cafe\_b\_cafe }\OperatorTok{=}\NormalTok{ dist.multivariatenormal(jnp.stack([a, b]), cov, shape }\OperatorTok{=}\NormalTok{ [N\_cafes], name }\OperatorTok{=} \StringTok{\textquotesingle{}a\_cafe\textquotesingle{}}\NormalTok{)    }

\NormalTok{    a\_cafe, b\_cafe }\OperatorTok{=}\NormalTok{ a\_cafe\_b\_cafe[:, }\DecValTok{0}\NormalTok{], a\_cafe\_b\_cafe[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{    mu }\OperatorTok{=}\NormalTok{ a\_cafe[cafe] }\OperatorTok{+}\NormalTok{ b\_cafe[cafe] }\OperatorTok{*}\NormalTok{ afternoon}
\NormalTok{    lk(}\StringTok{"y"}\NormalTok{, Normal(mu, sigma), obs}\OperatorTok{=}\NormalTok{wait)}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mathematical Details}\label{mathematical-details-12}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-12}

\[
\left(\begin{array}{cc} 
\sigma_\alpha^2 & \sigma_\alpha \sigma_{\beta \rho }\\
\sigma_\alpha \sigma_{\beta \rho } & \sigma_\beta^2
\end{array}\right)
\]

where : - \(\sigma_\alpha^2\) is the variance of intercepts. -
\(\sigma_\beta^2\) is the covariance of intercepts \& slopes. -
\(\sigma_\alpha \sigma_{\beta \rho }\) is the covariance between
intercepts and slopes -i.e.~the product of the two standard deviations-.

\section{Mathematical Details}\label{mathematical-details-13}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-13}

We model the relationship between the independent variable \(X\) and the
outcome variable \emph{Y} with varying intercepts (\(\alpha\)) and
varying slopes (\(\beta\)) for each group (\emph{k}) using the following
equation:

\[
Y_{ik} = \alpha_k + \beta_k X_{ik} + \sigma
\]

Where: - \(Y_{ik}\) is the outcome variable for observation \emph{i} in
group \emph{k}. - \(X_{ik}\) is the independent variables for
observation \emph{i} in group \emph{k}. - \(\alpha_k\) is the varying
intercept for group \emph{k}. - \(\beta_k\) is the varying regression
coefficients for group \emph{k}. - \$\sigma \$ is the error term,
assumed to be strictly positive.

\subsection{\texorpdfstring{\emph{Bayesian
model}}{Bayesian model}}\label{bayesian-model-9}

We can express the Bayesian regression model accounting for prior
distribution as follows:

\[
p(Y_{ik} |\mu_{ik} , \sigma) \sim \text{Normal}(\mu_{ik} , \sigma)
\] \[
\mu_{ik} = \alpha_k + \beta_k X_{ik} + \sigma 
\] \[
\alpha_k \sim Normal(0,1) 
\] \[
\beta_k \sim Normal(0,1)
\] \[
\sigma \sim Exponential(0,1)
\]

The varying intercepts slopes (\(\alpha_k\)) and (\(\beta_k\)) are
modeled using a \emph{Multivariate Normal distribution}:

\[ 
\begin{pmatrix} 
\alpha_k \\ 
\beta_k 
\end{pmatrix} \sim \text{MultivariateNormal}\left( 
\begin{pmatrix} 
0 \\ 
0 
\end{pmatrix}, 
\begin{pmatrix} 
\sigma_\alpha^2 & \sigma_\pi \sigma_{\alpha\rho} \\ 
\sigma_\alpha \sigma_{\pi\rho} & \sigma_\pi 
\end{pmatrix} 
\right) 
\]

Where: - \(\left(\begin{array}{cc} 0 \\0\end{array}\right)\), is the
prior for average intercept. -
\(\left(\begin{array}{cc} \sigma_\alpha^2 & \sigma_\pi\sigma_{\alpha\rho} \\ \sigma_\alpha\sigma_{\pi\rho} & \sigma_\pi \end{array}\right)\)
is is the covariance matrix which specifies the variance and covariance
of \(\alpha_k\) and \(\beta_k\), where: - \(\sigma_\alpha^2\) The
variance of \(\alpha_k\). - \(\sigma_\pi^2\) The variance of
\(\beta_k\). - \(\sigma_\pi\sigma_{\alpha\rho}\) and
\(\sigma_\alpha\sigma_{\pi\rho}\) The covariance between \(\alpha_k\)
and \(\beta_k\)

For computational reasons, it is often better to implement a
\href{12.\%20Varying\%20intercepts.qmd}{centered version of the varying
intercept üõà} that is equivalent to the \emph{Multivariate Normal
distribution} approach:

\[
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
 \sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
\]

\begin{itemize}
\item
  Where:

  \begin{itemize}
  \tightlist
  \item
    \(\sigma_\alpha \sim Exponential(1)\) bewing the priorstddev
    amongintercepts.
  \item
    \(\sigma_\beta \sim Exponential(1)\) bewing the prior
    stddevamongslopes.
  \item
    \(L \sim LKJcorr(Œ∑)\) bewing the prior for the correlationmatrix.
  \end{itemize}
\end{itemize}

The full cetered version of the model is thus :

\[
p(Y_{i} |\mu_k , \sigma) \sim \text{Normal}(\mu_k , \sigma) \\
\]

\[
\mu_k =   \alpha_k + \beta_i X_i \\
\]

\[
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
 \sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
\]

\[
\alpha \sim Normal(0,1)
\] \[
\beta \sim Normal(0,1)
\] \[
\sigma_\alpha \sim Exponential(1)
\] \[
\sigma_\pi \sim Exponential(1)
\] \[
L \sim LKJcorr(2)
\]

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Notes}, colframe=quarto-callout-note-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\begin{itemize}
\tightlist
\item
  We can apply multivariate model similarly as
  \href{./2.\%20Multiple\%20Regression\%20for\%20Continuous\%20Variables.qmd}{chapter
  2}. In this case, we apply the same principle, but with a covariance
  matrix of a dimension equal to the number of varying slopes we define.
  For example, if we want to generate random slopes for \(i\) actors in
  a model with two independent variables \(X_1\) and \(X_2\), we can
  define the formula as follows:
\end{itemize}

\[
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) 
\]

\[
\mu_i =   \alpha_i + \beta_{1i} X_{1i}  + \beta_{1i} X_{2i} 
\]

\[ 
\begin{pmatrix} 
\alpha_{i}\\ 
\beta_{1i}\\ 
\beta_{2i} 
\end{pmatrix} 
\sim \begin{pmatrix} 
\sigma_{\alpha}\\ 
\sigma_{\pi}\\ 
\sigma_{\gamma} 
\end{pmatrix} \circ L \cdot \begin{pmatrix} 
\widehat{\alpha}_{k} \\ 
\widehat{\pi}_{k} \\ 
\widehat{\gamma}_{k} 
\end{pmatrix} 
\] Key Correc

\[
\sigma_{\alpha} \sim Exponential(1)
\] \[
\sigma_{\pi} \sim Exponential(1)
\] \[
\sigma_{\gamma} \sim Exponential(1) 
\] \[
L \sim LKJcorr(2)
\]

\begin{itemize}
\item
  We can apply interaction terms similarly as
  \href{/3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3}.
\item
  We can apply caterogical variables similarly as
  \href{4.\%20Categorical\%20variable.qmd}{chapter 4}.
\item
  We can apply varying slopes with any distribution presented in
  previous chapters.
\item
  For more than two varying effects we apply the same principel but with
  a covariance matrix for each varying effect that are summed to
  gernerat the varying intercept and slope. For exmaple, if we want to
  generate random slopes for \(i\) actors, and \(k\) groups we can
  define the formula as follow:
\end{itemize}

\[
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) \\
\]

\[
\mu_i =   \alpha_i + \beta_{i} X_i 
\] \[
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
\] \[
\beta_{i} = \beta + \beta_{actor[i]} + \beta_{group[i]} 
\]

\[
\alpha \sim Normal(0,1)
\] \[
\beta \sim Normal(0,1) 
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{\text{actor}} 
\end{pmatrix} 
\sim 
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} 
\end{pmatrix} \circ L_a \cdot \begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha a} \sim Exponential(1)
\] \[
\sigma_{\pi a} \sim Exponential(1)
\] \[
L_{a} \sim LKJcorr(2)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{\text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha g} \sim Exponential(1)
\] \[
\sigma_{\pi g} \sim Exponential(1) 
\] \[
L_{g} \sim LKJcorr(2)
\]

\begin{itemize}
\tightlist
\item
  Bellow the formula and the code snipset for a Binomial multivariate
  model with interaction between two independent variables \(X_1\) and
  \(X_2\) and multiples varying effects for each actor and each group:
\end{itemize}

\[
p(Y_{i} |n , p_i) \sim \text{Binomial}(n = 1, p_i) \\
\]

\[
logit{p_i}=   \alpha_i + (\beta_{1i}  + \beta_{2i} X_{2i})  X_{1i}
\] \[
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
\] \[
\beta_{1i} = \beta + \beta_{1 actor[i]} + \beta_{ group[i]}
\] \[
\beta_{2i} = \beta + \beta_{2 actor[i]} + \beta_{2 group[i]}
\]

\[
\alpha \sim Normal(0,1)
\] \[
\beta \sim Normal(0,1)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{1 \, \text{actor}} \\ 
\beta_{2 \, \text{actor}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} \\ 
\sigma_{\gamma a} 
\end{pmatrix} \circ L_a \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} \\ 
\widehat{\gamma}_{ka} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha a} \sim Exponential(1) 
\]

\[
\sigma_{\pi a} \sim Exponential(1) 
\] \[
\sigma_{\gamma a} \sim Exponential(1) 
\] \[
L_{a} \sim LKJcorr(2)
\]

\[ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{1 \, \text{group}} \\ 
\beta_{2 \, \text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} \\ 
\sigma_{\gamma g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} \\ 
\widehat{\gamma}_{kg} 
\end{pmatrix} 
\]

\[
\sigma_{\alpha g} \sim Exponential(1)
\] \[
\sigma_{\pi g} \sim Exponential(1) 
\] \[
\sigma_{\gamma g} \sim Exponential(1)
\] \[
L_{g} \sim LKJcorr(2)
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ main }\ImportTok{import}\OperatorTok{*}
\CommentTok{\# Setup device{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m }\OperatorTok{=}\NormalTok{ bi(platform}\OperatorTok{=}\StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Import data}
\NormalTok{m.read\_csv(}\StringTok{"../data/chimpanzees.csv"}\NormalTok{, sep}\OperatorTok{=}\StringTok{";"}\NormalTok{)}
\NormalTok{m.df[}\StringTok{"block\_id"}\NormalTok{] }\OperatorTok{=}\NormalTok{ m.df.block}
\NormalTok{m.df[}\StringTok{"treatment"}\NormalTok{] }\OperatorTok{=} \DecValTok{1} \OperatorTok{+}\NormalTok{ m.df.prosoc\_left }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ m.df.condition}
\NormalTok{m.data\_to\_model([}\StringTok{\textquotesingle{}pulled\_left\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}treatment\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}actor\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}block\_id\textquotesingle{}}\NormalTok{])}


\KeywordTok{def}\NormalTok{ model(tid, actor, block\_id, L}\OperatorTok{=}\VariableTok{None}\NormalTok{, link}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
    \CommentTok{\# fixed priors}
\NormalTok{    g }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}g\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    sigma\_actor }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{\textquotesingle{}sigma\_actor\textquotesingle{}}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    L\_Rho\_actor }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{"L\_Rho\_actor"}\NormalTok{)}
\NormalTok{    sigma\_block }\OperatorTok{=}\NormalTok{ dist.exponential(}\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"sigma\_block"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,))}
\NormalTok{    L\_Rho\_block }\OperatorTok{=}\NormalTok{ dist.lkjcholesky(}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, name }\OperatorTok{=} \StringTok{"L\_Rho\_block"}\NormalTok{)}

    \CommentTok{\# adaptive priors {-} non{-}centered}
\NormalTok{    z\_actor }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"z\_actor"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{))}
\NormalTok{    z\_block }\OperatorTok{=}\NormalTok{ dist.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, name }\OperatorTok{=} \StringTok{"z\_block"}\NormalTok{, shape }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{    alpha }\OperatorTok{=}\NormalTok{ deterministic(}
        \StringTok{"alpha"}\NormalTok{, ((sigma\_actor[..., }\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ L\_Rho\_actor) }\OperatorTok{@}\NormalTok{ z\_actor).T}
\NormalTok{    )}
\NormalTok{    beta }\OperatorTok{=}\NormalTok{ deterministic(}
        \StringTok{"beta"}\NormalTok{, ((sigma\_block[..., }\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ L\_Rho\_block) }\OperatorTok{@}\NormalTok{ z\_block).T}
\NormalTok{    )}

\NormalTok{    logit\_p }\OperatorTok{=}\NormalTok{ g[tid] }\OperatorTok{+}\NormalTok{ alpha[actor, tid] }\OperatorTok{+}\NormalTok{ beta[block\_id, tid]}
\NormalTok{    dist(}\StringTok{"L"}\NormalTok{, dist.Binomial(logits}\OperatorTok{=}\NormalTok{logit\_p), obs}\OperatorTok{=}\NormalTok{L)}

    \CommentTok{\# compute ordinary correlation matrixes from Cholesky factors}
    \ControlFlowTok{if}\NormalTok{ link:}
\NormalTok{        deterministic(}\StringTok{"Rho\_actor"}\NormalTok{, L\_Rho\_actor }\OperatorTok{@}\NormalTok{ L\_Rho\_actor.T)}
\NormalTok{        deterministic(}\StringTok{"Rho\_block"}\NormalTok{, L\_Rho\_block }\OperatorTok{@}\NormalTok{ L\_Rho\_block.T)}
\NormalTok{        deterministic(}\StringTok{"p"}\NormalTok{, expit(logit\_p))}

\CommentTok{\# Run mcmc {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.run(model) }

\CommentTok{\# Summary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{m.sampler.print\_summary(}\FloatTok{0.89}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Reference(s)}\label{references-10}

McElreath (2018)

\bookmarksetup{startatroot}

\chapter{}\label{section-1}

\bookmarksetup{startatroot}

\chapter{}\label{section-2}

\bookmarksetup{startatroot}

\chapter{}\label{section-3}

\bookmarksetup{startatroot}

\chapter{17. Modeling Network}\label{modeling-network}

A network represents the relationships (links) between entities (nodes).
These links can be weighted (weighted network) or unweighted (binary
network), directed (directed network) or undirected (undirected
network). Regardless of their type, networks generate links shared by
nodes, leading to data dependency when modeling the network. One
proposed solution is to model network links with random
\href{12.\%20Varying\%20intercepts.qmd}{intercepts} and
\href{13.\%20Varying\%20slopes.qmd}{effects}. By adding such parameters
to the model, we can account for the correlations between node link
relationships. The particularity here is that random intercepts and
effects are generated for both
\href{19.\%20\%20Network\%20model.qmd}{nodal effects üõà} and {[}dyadic
effects üõà{]}(19.\%20Network\%20model.qmd '' link-related categorical
(e.g., same group membership) or continuous (e.g., genetic distances)
characteristics between nodes''). For nodal characteristics, the random
intercepts and effects are identical to those described in previous
chapters and will therefore not be detailed further.

When modeling network data, the relationships between nodes can be
represented by weighted links. These weights might indicate the strength
of connections such as the number of interactions, shared attributes, or
other quantitative measures of connection strength between nodes.
However, in such cases as links are shared between nodes we obtain data
dependency and to account for such data structure we need to account for
it. One proposed solution is to generate random intercepts and effects.
Indeed, by adding such parameter in our model we can account for
correlation between

The simple model that can be built to model link weights between nodes
\emph{i} and \emph{j} can be defined as follows:

\[
G_{ij} \sim Poisson(Y_{ij}) \\
log(Y_{ij}) =  \alpha  + \lambda_i + \pi_j + \delta_{ij}
\]

where: - ( Y\_\{ij\} ) is the weight of links between \emph{i} and
\emph{j}. - ( \alpha ) is the intercept. - ( \lambda\_i ) is the
characteristic of node \emph{i} that affects node \emph{i}'s propensity
to emit a link toward node \emph{j}. - ( \pi*j ) is the characteristic
of node j* that affects node \emph{j}'s propensity to receive a link
from node \emph{i}. - ( \delta{ij} ) is a matrix of dyadic effects
governing reciprocity between dyads.

Note that any additional covariates can be added with a regression
coefficient to the model representing nodal characteristics (e.g., sex,
age) or dyadic characteristics (e.g., genetic distances).

To account for correlation between the propensity to emit and receive
links of a dyad, we can use \href{13.\%20Varying\%20slopes.md}{varying
slopes} as follows:

\[
\left(\begin{array}{cc} 
\lambda_i \\
\pi_j 
\end{array}\right) 
\sim 
MultivariateNormal\left(\begin{array}{cc} 
\left(\begin{array}{cc} 
0 \\
0
\end{array}\right),
\left(\begin{array}{cc} 
\sigma_\lambda^2 & \sigma_\pi \sigma_{\lambda \rho }\\
\sigma_\lambda \sigma_{\pi \rho } & \sigma_\pi^2
\end{array}\right)
\end{array}\right)
\]

This can be optimized for computation by:

\[
\left(\begin{array}{cc} 
\lambda_i \\
\pi_j 
\end{array}\right) 
\sim 
MultivariateNormal\left(\begin{array}{cc} 
\left(\begin{array}{cc} 
\sigma_\lambda \\
\sigma_\pi
\end{array}\right) \circ 
\left(\begin{array}{cc} 
L *
\left(\begin{array}{cc} 
\hat{\lambda}_i \\ \hat{\pi}_i
\end{array}\right)
\end{array}\right)
\end{array}\right)
\]

\[
\sigma_\alpha \sim Exponential(1)
\]

\[
\sigma_\pi \sim Exponential(1)
\]

\[
L \sim LKJ(2)
\]

Similarly, the dyad reciprocity effect is also modeled using a random
slope:

\[
\left(\begin{array}{cc} 
\delta_{ij} \\
\delta_{ji}
\end{array}\right) 
\sim 
MultivariateNormal\left(\begin{array}{cc} 
\left(\begin{array}{cc} 
\sigma_\delta \\
\sigma_\delta
\end{array}\right) \circ 
\left(\begin{array}{cc} 
L_\delta *
\left(\begin{array}{cc} 
\hat{\delta}_{ij} \\ \hat{\delta}_{ji}
\end{array}\right)
\end{array}\right)
\end{array}\right)
\]

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, arc=.35mm, titlerule=0mm, opacitybacktitle=0.6, bottomtitle=1mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Conciderations}, colframe=quarto-callout-caution-color-frame, breakable, coltitle=black, toptitle=1mm, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, colback=white]

\begin{itemize}
\item
  Network links can be modeled using Bernoulli, Binomial, Poisson, or
  zero-inflated Poisson distributions. So, by replacing the Poisson
  distribution with a binomial distribution, we can model the existence
  or absence of link weights --- i.e., model binary networks.
\item
  Note that if the network is undirected, then accounting for
  correlation between propensity to emit and receive links is not
  necessary, and the terms ( \lambda\_i ), ( \pi*j ), and (* \delta{ij}
  ) are no longer required. (Is it correct?)
\item
  In the following chapters, we will see how to incorporate additional
  network effects into the model to account for network structural
  properties (e.g., clusters, assortativity, triadic closure, etc.).
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Notes}\label{notes-6}

\begin{itemize}
\item
  We can apply multiple variables similarly as
  \href{2.\%20Multiple\%20continuous\%20Variables.qmd}{chapter 2:
  Multiple continuous Variables}.
\item
  We can apply interaction terms similarly as
  \href{3.\%20Interaction\%20between\%20continuous\%20variables.qmd}{chapter
  3: Interaction between continuous variables}.
\item
\end{itemize}

\bookmarksetup{startatroot}

\chapter{}\label{section-4}

\bookmarksetup{startatroot}

\chapter{}\label{section-5}

\bookmarksetup{startatroot}

\chapter{Multiplex Networks}\label{multiplex-networks}

\section{General Principles}\label{general-principles-13}

A multiplex network is a type of multilayer network where all layers
consist of the same set of nodes, but the links between them differ.
Each layer represents a different type of relationship or interaction
between the same entities (nodes).

\section{Considerations}\label{considerations-12}

\begin{itemize}
\tightlist
\item
  We have the same considerations as for
  \href{19.\%20Network\%20model.qmd}{Network model}. \#\# Example
\end{itemize}

\section{Mathematical Details}\label{mathematical-details-14}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-14}

\subsection{\texorpdfstring{\emph{Bayesian model
(WIP)}}{Bayesian model (WIP)}}\label{bayesian-model-wip-3}

\bookmarksetup{startatroot}

\chapter{Notes}\label{notes-7}

\begin{itemize}
\item
  We can apply sender-receiver model
  \href{19.1.\%20Sender\%20receiver\%20network\%20model.qmd}{19.1.
  Sender receiver network model.}.
\item
  We can apply block model
  \href{3.\%20Block\%20model\%20network.qmd}{19.2. Block model
  network.qmd}.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{}\label{section-6}

\bookmarksetup{startatroot}

\chapter{Multilayer Networks}\label{multilayer-networks}

\section{General Principles}\label{general-principles-14}

A multilayer network is a broader term that refers to networks composed
of multiple layers, where each layer may have its own set of nodes and
edges, potentially representing different types of entities and
interactions. This means the nodes can be the same or different across
layers.

\section{Considerations}\label{considerations-13}

\begin{itemize}
\tightlist
\item
  We have the same considerations as for
  \href{19.\%20Network\%20model.qmd}{Network model}. \#\# Example
\end{itemize}

\section{Mathematical Details}\label{mathematical-details-15}

\subsection{\texorpdfstring{\emph{Formula}}{Formula}}\label{formula-15}

\subsection{\texorpdfstring{\emph{Bayesian model
(WIP)}}{Bayesian model (WIP)}}\label{bayesian-model-wip-4}

\bookmarksetup{startatroot}

\chapter{Notes}\label{notes-8}

\begin{itemize}
\item
  We can apply sender-receiver model
  \href{19.1.\%20Sender\%20receiver\%20network\%20model.qmd}{19.1.
  Sender receiver network model.}.
\item
  We can apply block model
  \href{3.\%20Block\%20model\%20network.qmd}{19.2. Block model
  network.qmd}.
\end{itemize}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-mcelreath2018statistical}
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\end{CSLReferences}




\end{document}
