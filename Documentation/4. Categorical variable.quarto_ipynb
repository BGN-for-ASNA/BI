{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Regression with a Categorical Independent Variable\"\n",
        "description: \"Incorporating categorical predictors (i.e., factor variables) into a regression model using dummy variables.\"\n",
        "categories: [Regression, GLM]\n",
        "image: \"Figures/4.png\"\n",
        "order: 5\n",
        "---\n",
        "\n",
        "## General Principles\n",
        "To study the relationship between a categorical independent variable and a continuous dependent variable, we use a _Categorical model_ which applies _stratification_.\n",
        "\n",
        "_Stratification_ involves modeling how the *k* different categories of the independent variable affect the target continuous variable by performing a regression for each *k* category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using [<span style=\"color:#0D6EFD\">one-hot encoding ðŸ›ˆ</span>]{#ohe} or by converting categories to [<span style=\"color:#0D6EFD\">indices ðŸ›ˆ</span>]{#indices}.\n",
        "\n",
        "\n",
        "## Considerations\n",
        "::: callout-note\n",
        "- We have the same considerations as for [Regression for a Continuous Variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n",
        " \n",
        "- As we generate regression coefficients for each *k* category, we need to specify a prior with a shape equal to the number of categories *k* in the code (see comments in the code).\n",
        "  \n",
        "- To compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. **Never compare confidence intervals or p-values directly**.\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## Example\n",
        "Below is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (*kcal_per_g*), representing the caloric value of milk per gram, a categorical independent variable (*index_clade*), representing species clade membership, and a continuous independent variable (*mass*), representing the mass of individuals in the clade. The goal is to estimate the differences in milk calories between clades. This example is based on @mcelreath2018statistical.\n",
        "\n",
        "::: {.panel-tabset group=\"language\"}\n",
        "### Python"
      ],
      "id": "68efba54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from BI import bi\n",
        "\n",
        "# Setup device------------------------------------------------\n",
        "m = bi(platform='cpu')\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "from importlib.resources import files\n",
        "data_path = m.load.milk(only_path = True)\n",
        "m.data(data_path, sep=';') \n",
        "m.index([\"clade\"]) # Convert clade names into index\n",
        "m.scale(['kcal_per_g']) # Scale\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "def model(kcal_per_g, index_clade, mass):\n",
        "    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n",
        "    b = m.dist.normal(0, 0.5, shape=(4,), name = 'b')\n",
        "    s = m.dist.exponential( 1, name = 's')    \n",
        "    mu = a[index_clade]+b[index_clade]*mass\n",
        "    m.dist.normal(mu, s, obs=kcal_per_g)\n",
        "\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m.fit(model) # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary()"
      ],
      "id": "ad76d26b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### R\n",
        "```R\n",
        "library(BayesianInference)\n",
        "m=importBI(platform='cpu')\n",
        "\n",
        "# Load csv file\n",
        "m$data(m$load$milk(only_path = T), sep=';')\n",
        "m$scale(list('kcal.per.g')) # Manipulate\n",
        "m$index(list('clade')) # Scale\n",
        "m$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "model <- function(kcal_per_g, index_clade){\n",
        "  # Parameter prior distributions\n",
        "  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n",
        "  sigma = bi.dist.exponential(1, name = 's')\n",
        "  # Likelihood\n",
        "  bi.dist.normal(beta[index_clade], sigma, obs=kcal_per_g)\n",
        "}\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m$fit(model) # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m$summary() # Get posterior distributions\n",
        "```\n",
        "\n",
        "### Julia\n",
        "```julia\n",
        "using BayesianInference\n",
        "\n",
        "# Setup device------------------------------------------------\n",
        "m = importBI(platform=\"cpu\")\n",
        "\n",
        "# Import Data & Data Manipulation ------------------------------------------------\n",
        "# Import\n",
        "data_path = m.load.milk(only_path = true)\n",
        "m.data(data_path, sep=';')\n",
        "m.index(\"clade\") # Convert clade names into index\n",
        "m.scale([\"kcal_per_g\"]) # Scale\n",
        "\n",
        "# Define model ------------------------------------------------\n",
        "@BI function model(kcal_per_g, index_clade, mass)\n",
        "    a = m.dist.normal(0, 0.5, shape=(4,), name = \"a\") # shape based on the number of clades\n",
        "    b = m.dist.normal(0, 0.5, shape=(4,), name = \"b\")\n",
        "    s = m.dist.exponential( 1, name = 's')    \n",
        "    mu = a[index_clade]+b[index_clade]*mass\n",
        "    m.dist.normal(mu, s, obs=kcal_per_g)\n",
        "end\n",
        "\n",
        "# Run mcmc ------------------------------------------------\n",
        "m.fit(model)  # Optimize model parameters through MCMC sampling\n",
        "\n",
        "# Summary ------------------------------------------------\n",
        "m.summary() # Get posterior distributions\n",
        "```\n",
        ":::\n",
        "\n",
        "::: callout-caution\n",
        "For R users, when working with indices you have to ensure 1) that indices are intergers (i.e. ```as.integer(index_clade)```) and, 2) that indices start at 0 (i.e. ```as.integer(index_clade)-1```).\n",
        ":::\n",
        "\n",
        "## Mathematical Details\n",
        "### *Frequentist formulation*\n",
        "We model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n",
        "\n",
        "$$\n",
        "Y_i = \\alpha + \\beta_k X_i + \\sigma\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Y_i$ is the dependent variable for observation *i*. \n",
        "  \n",
        "- $\\alpha$ is the intercept term.\n",
        "  \n",
        "- $\\beta_k$ are the regression coefficients for each _k_ category.\n",
        "  \n",
        "- $X_i$ is the encoded categorical input variable for observation *i*. \n",
        "  \n",
        "- $\\sigma$ is the error term.\n",
        "\n",
        "We can interpret $\\beta_i$ as the effect of each category on $Y$ relative to the baseline (usually one of the categories or the intercept). \n",
        "\n",
        "### *Bayesian formulation*\n",
        "In the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:\n",
        "\n",
        "$$\n",
        "Y \\sim \\text{Normal}(\\alpha +  \\beta_K X, \\sigma)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha \\sim \\text{Normal}(0,1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_K \\sim \\text{Normal}(0,1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma \\sim \\text{Exponential}(1)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Y_i$ is the dependent variable for observation *i*.\n",
        "  \n",
        "- $\\alpha$ is the intercept term, which in this case has a unit-normal prior.\n",
        "  \n",
        "- $\\beta_K$ are slope coefficients for the _K_ distinct independent variables categories, which also have unit-normal priors.\n",
        "  \n",
        "- $X_i$ is the encoded categorical input variable for observation *i*. \n",
        "  \n",
        "- $\\sigma$ is a standard deviation parameter, which here has a Exponential prior that constrains it to be positive.\n",
        "\n",
        "## Notes\n",
        "::: callout-note\n",
        "\n",
        "- We can apply multiple variables similarly to [Chapter 2: Multiple Continuous Variables](2.&#32;Multiple&#32;continuous&#32;Variables.qmd).\n",
        "\n",
        "- We can apply interaction terms similarly to [Chapter 3: Interaction between Continuous Variables](3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).\n",
        ":::\n",
        "\n",
        "## Reference(s)\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "c708a4bb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/sosa/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}