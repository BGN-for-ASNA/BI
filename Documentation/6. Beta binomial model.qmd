# Beta-Binomial model
## General Principles
To model the relationship between a binary outcome variable representing success counts and one or more indepedent variables with [overdispersion üõà](7.&#32;Poisson&#32;model.qmd "Overdispersion refers to a situation in statistical modeling where the variability of the data exceeds what is expected under the assumed model. In this case, by the binomial distribution."), we can use Beta-Binomial model.

We model the relationship between the predictor variables (X1, X2, ..., Xn) and the probability of success (p) using the following equation:

::: callout-caution 
## Considerations
- We have the same considerations as for [Binomial regression](5.&#32;Binomial&#32;model.qmd).

- A beta-binomial model assumes that each binomial count observation has its own probability of a success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success.
-  A beta distribution has two parameters, an average probability  _p_ and a shape parameter Œ∏.
:::


## Example
Below is an example code snippet demonstrating Bayesian Beta-Binomial regression:

```python
from .bi.main import*#
# Setup device------------------------------------------------
m = bi()

# Import data ------------------------------------------------
m.data('../data/UCBadmit.csv', sep=';') 
m.df["gid"] = (m.df["applicant.gender"] != "male").astype(int)
gid = jnp.array(m.df["gid"].astype('int32').values)
applications = jnp.array(m.df["applications"].astype('float32').values)
admit = jnp.array(m.df["admit"].astype('float32').values)

m.data_on_model = dict(
    gid = gid,
    applications = applications,
    admit =  admit
)

# Define model ------------------------------------------------
def model(gid, applications, admit):
    phi = dist.exponential(1, shape=[1], name = 'phi')
    alpha = dist.normal( 0., 1.5, shape=[2], name = 'alpha')
    theta = phi + 2
    pbar = jax.nn.sigmoid(alpha[gid])
    concentration1 = pbar*theta
    concentration0 = (1 - pbar) * theta
    lk("y", BetaBinomial(total_count = applications, concentration1 = concentration1, concentration0 = concentration0), obs=admit)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

## Mathematical Details
## *Formula*

$$
logit(ùëù)= \alpha + \beta X
$$

Where:

- $p$ is the probability of success.
- $\alpha$ the intercept term.
- $\beta$ the regression term.
- $X$ is an independent variables.
- $\text{logit}(p)$ is the log-odds of success.

### *Bayesian model (WIP)*
We can express the Bayesian regression model accounting for prior distribution as follows:

$$
p(Y|n, \overline{p}, \theta) \sim BetaBinomial(n, \overline{p}, \theta)
$$

$$
logit(\overline{p}) \sim \alpha + \beta X
$$

$$ùëù(\alpha) \sim Normal(0,1)$$

$$p(\theta) \sim HalfCaychy(0,1)$$


Where:

- $p(Y | n, \overline{p}, \theta)$ is the likelihood function.
- $n$ is the total count of trials.
- $\overline{p}$ is the average probability.
- $p(\theta)$ is the shape distribution term.
- $ùëù(\alpha)$ is the intercept term.
- $ùëù(\beta)$ is regression coefficient term.


