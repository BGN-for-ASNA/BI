{
  "hash": "534ea343e035a3a479a86c53ca7665a9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Binomial Model\"\ndescription: \"Modeling binary outcomes (e.g., successes/failures) across multiple independent trials.\"\ncategories: [Classification, Regression, GLM]\nimage: \"Figures/5.png\"\norder: 6\n---\n\n## General Principles\n\nTo model the relationship between a binary dependent variable—e.g., counts of successes/failures, yes/no, or 1/0—and one or more independent variables, we can use a *Binomial model*.\n\n## Considerations\n\n::: callout-note\n-   We have the same considerations as for [Regression for continuous variable](1.%20Linear%20Regression%20for%20continuous%20variable.qmd).\n\n-   This is the first model for which we need a link function: e.g., the *logit* function. The *logit* link function converts the linear combination of predictor variables into probabilities, making it suitable for modeling the probability of binary outcomes. It helps to estimate the relationship between predictors and the probability of success, ensuring that model predictions fall within the bounds of the binomial distribution's success parameter $\\in(0,1)$.\n:::\n\n## Example\n\nBelow is an example code snippet that demonstrates Bayesian binomial regression using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (*pulled_left*), which represents which lever each chimpanzee pulled in an experimental setup. The goal is to evaluate the probability of pulling the left side. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {#b0e2a860 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi\n\n# setup platform------------------------------------------------\nm = bi(platform='cpu')\n# import data ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'chimpanzees.csv'\nm.data(data_path, sep=';')\nm.data_to_model(['pulled_left'])\n\n# Define model ------------------------------------------------\ndef model(pulled_left):\n    a = m.dist.normal( 0, 10, shape=(1,), name = 'a')\n    m.dist.binomial(total_count = 1, logits=a[0], obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model, num_samples=500)\n\n# Diagnostic ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<15:42,  1.06it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  18%|█▊        | 177/1000 [00:01<00:03, 232.32it/s, 3 steps of size 8.56e-01. acc. prob=0.78]\rwarmup:  35%|███▍      | 349/1000 [00:01<00:01, 467.31it/s, 1 steps of size 1.55e+00. acc. prob=0.79]\rsample:  53%|█████▎    | 530/1000 [00:01<00:00, 714.36it/s, 1 steps of size 1.10e+00. acc. prob=0.94]\rsample:  72%|███████▏  | 715/1000 [00:01<00:00, 950.69it/s, 3 steps of size 1.10e+00. acc. prob=0.93]\rsample:  89%|████████▉ | 893/1000 [00:01<00:00, 1140.43it/s, 1 steps of size 1.10e+00. acc. prob=0.93]\rsample: 100%|██████████| 1000/1000 [00:01<00:00, 664.63it/s, 3 steps of size 1.10e+00. acc. prob=0.93]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>0.33</td>\n      <td>0.09</td>\n      <td>0.2</td>\n      <td>0.47</td>\n      <td>0.01</td>\n      <td>0.0</td>\n      <td>167.61</td>\n      <td>173.42</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### R\n\n``` r\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(pulled_left){\n  # Parameters priors distributions\n  alpha = bi.dist.normal( 0, 10, name = 'alpha')\n  # Likelihood\n  bi.dist.binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n```\n:::\n\n## Mathematical Details\n### *Bayesian formulation*\n\nWe can express the Bayesian Binomial regression model including prior distributions as follows:\n\n$$\nY_i \\sim \\text{Binomial}(N_i, p_i)\n$$\n\n$$\nlogit(p_i) = \\alpha + \\beta X_i\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta \\sim \\text{Normal}(0,1)\n$$\n\nWhere:\n\n-   $Y_i$ is the count of successes for observation *i* (often a binary 0 or 1).\n   \n-   $N_i$ is the count of trials for observation *i* (1 in the case of binary outcomes, as in the example for `total_count` above).\n\n-   $p_i$ is the probability of success (0 \\< $p_i$ \\< 1) for observation *i*, the probability of a success.\n\n-   $logit(p_i)$ is the log-odds of success, calculated as the log of the odds ratio of success. Through this link function, the relationship between the independent variables and the log-odds of success is modeled linearly, allowing us to interpret the effect of each independent variable on the log-odds of success for observation *i*.\n\n-   $\\beta$ and $\\alpha$ are the regression coefficient and intercept, respectively.\n\n## Notes\n\n::: callout-note\n-   We can apply multiple variables similarly to [chapter 2](2.%20Multiple%20continuous%20variables.qmd).\n\n-   We can apply interaction terms similarly to [chapter 3](3.%20Interaction%20between%20continuous%20variables.qmd).\n\n-   We can apply categorical variables similarly to [chapter 4](4.%20Categorical%20variable.qmd).\n\n-   Below is an example code snippet demonstrating a Bayesian binomial model for multiple categorical variables using the Bayesian Inference (BI) package. The data consist of one binary dependent variable (*pulled_left*), which represents which side individuals pulled, and three independent variables (*actor*, *side*, *cond*). The goal is to evaluate, for each individual, the probability of pulling the left side, accounting for whether the individual is left-handed or right-handed, as well as for the different conditions.\n\n``` python\nfrom BI import bi\nm = bi(platform='cpu')\nm.data('../resources/data/chimpanzees.csv', sep=';')\nm.df['treatment'] =  m.df.prosoc_left + 2 * m.df.condition\nm.df['actor'] = m.df['actor'] - 1\n\nm.data_to_model(['actor', 'treatment', 'pulled_left'])\n\ndef model(actor, treatment, pulled_left):\n    a = m.dist.normal(0, 1.5, shape = (7,), name='a')\n    b = m.dist.normal(0, 0.5, shape = (4,), name='b')\n    p = a[actor] + b[treatment]\n    m.dist.binomial(total_count = 1, logits=p, obs=pulled_left)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n# Diagnostic ------------------------------------------------\nm.summary()\n```\n\n``` r\nlibrary(BayesianInference)\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/chimpanzees.csv\", sep = ''), sep=';')\nm$data_to_model(list('pulled_left')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(pulled_left){\n  # Parameters priors distributions\n  a = bi.dist.normal( 0, 1.5, name = 'a')\n  b = bi.dist.normal( 0, 0.5, name = 'b')\n  p = a[actor] + b[treatment]\n  # Likelihood\n  m$binomial(total_count = 1, logits = alpha, obs=pulled_left)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n```\n:::\n\n## Reference(s)\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "5. Binomial model_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}