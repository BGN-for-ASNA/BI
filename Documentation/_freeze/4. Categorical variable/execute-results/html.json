{
  "hash": "251ff0c9dfd43da63b35962114b75f37",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression with a Categorical Independent Variable\"\ndescription: \"Incorporating categorical predictors (i.e., factor variables) into a regression model using dummy variables.\"\ncategories: [Regression, GLM]\nimage: \"Figures/4.png\"\norder: 5\n---\n\n\n## General Principles\nTo study the relationship between a categorical independent variable and a continuous dependent variable, we use a _Categorical model_ which applies _stratification_.\n\n_Stratification_ involves modeling how the *k* different categories of the independent variable affect the target continuous variable by performing a regression for each *k* category and assigning a regression coefficient for each category. To implement stratification, categorical variables are often encoded using [<span style=\"color:#0D6EFD\">one-hot encoding ðŸ›ˆ</span>]{#ohe} or by converting categories to [<span style=\"color:#0D6EFD\">indices ðŸ›ˆ</span>]{#indices}.\n\n\n## Considerations\n::: callout-note\n- We have the same considerations as for [Regression for a Continuous Variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n \n- As we generate regression coefficients for each *k* category, we need to specify a prior with a shape equal to the number of categories *k* in the code (see comments in the code).\n  \n- To compare differences between categories, we need to compute the distribution of the differences between categories, known as the contrast distribution. **Never compare confidence intervals or p-values directly**.\n\n\n:::\n\n## Example\nBelow is an example of code that demonstrates Bayesian regression with an independent categorical variable using the Bayesian Inference (BI) package. The data consist of one continuous dependent variable (*kcal_per_g*), representing the caloric value of milk per gram, a categorical independent variable (*index_clade*), representing species clade membership, and a continuous independent variable (*mass*), representing the mass of individuals in the clade. The goal is to estimate the differences in milk calories between clades. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {#5e557c08 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.milk(only_path = True)\nm.data(data_path, sep=';') \nm.index([\"clade\"]) # Convert clade names into index\nm.scale(['kcal_per_g']) # Scale\n\n# Define model ------------------------------------------------\ndef model(kcal_per_g, index_clade, mass):\n    a = m.dist.normal(0, 0.5, shape=(4,), name = 'a') # shape based on the number of clades\n    b = m.dist.normal(0, 0.5, shape=(4,), name = 'b')\n    s = m.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.dist.normal(mu, s, obs=kcal_per_g)\n\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<07:03,  2.36it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   9%|â–‰         | 90/1000 [00:00<00:04, 225.50it/s, 127 steps of size 9.63e-03. acc. prob=0.76]\rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:00<00:00, 802.39it/s, 7 steps of size 4.43e-01. acc. prob=0.78] \rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [00:00<00:00, 1417.22it/s, 7 steps of size 3.48e-01. acc. prob=0.93]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [00:00<00:00, 1843.36it/s, 15 steps of size 3.48e-01. acc. prob=0.93]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1168.63it/s, 15 steps of size 3.48e-01. acc. prob=0.93]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>-0.32</td>\n      <td>0.35</td>\n      <td>-0.79</td>\n      <td>0.34</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>430.93</td>\n      <td>365.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[1]</th>\n      <td>0.59</td>\n      <td>0.30</td>\n      <td>0.10</td>\n      <td>1.06</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>412.79</td>\n      <td>283.95</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[2]</th>\n      <td>0.31</td>\n      <td>0.37</td>\n      <td>-0.28</td>\n      <td>0.87</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>404.55</td>\n      <td>368.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[3]</th>\n      <td>-0.17</td>\n      <td>0.49</td>\n      <td>-0.92</td>\n      <td>0.64</td>\n      <td>0.03</td>\n      <td>0.02</td>\n      <td>376.94</td>\n      <td>232.82</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>-0.00</td>\n      <td>0.01</td>\n      <td>-0.02</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>456.57</td>\n      <td>350.35</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[1]</th>\n      <td>-0.17</td>\n      <td>0.12</td>\n      <td>-0.36</td>\n      <td>0.04</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>356.03</td>\n      <td>342.61</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[2]</th>\n      <td>0.08</td>\n      <td>0.07</td>\n      <td>-0.02</td>\n      <td>0.19</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>371.61</td>\n      <td>388.81</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[3]</th>\n      <td>-0.27</td>\n      <td>0.27</td>\n      <td>-0.72</td>\n      <td>0.13</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>352.82</td>\n      <td>217.58</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>s</th>\n      <td>0.80</td>\n      <td>0.12</td>\n      <td>0.59</td>\n      <td>0.97</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>386.61</td>\n      <td>438.95</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(m$load$milk(only_path = T), sep=';')\nm$scale(list('kcal.per.g')) # Manipulate\nm$index(list('clade')) # Scale\nm$data_to_model(list('kcal_per_g', 'index_clade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(kcal_per_g, index_clade){\n  # Parameter prior distributions\n  beta = bi.dist.normal( 0, 0.5, name = 'beta', shape = c(4))  # shape based on the number of clades\n  sigma = bi.dist.exponential(1, name = 's')\n  # Likelihood\n  bi.dist.normal(beta[index_clade], sigma, obs=kcal_per_g)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n```\n\n### Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.milk(only_path = true)\nm.data(data_path, sep=';')\nm.index(\"clade\") # Convert clade names into index\nm.scale([\"kcal_per_g\"]) # Scale\n\n# Define model ------------------------------------------------\n@BI function model(kcal_per_g, index_clade, mass)\n    a = m.dist.normal(0, 0.5, shape=(4,), name = \"a\") # shape based on the number of clades\n    b = m.dist.normal(0, 0.5, shape=(4,), name = \"b\")\n    s = m.dist.exponential( 1, name = 's')    \n    mu = a[index_clade]+b[index_clade]*mass\n    m.dist.normal(mu, s, obs=kcal_per_g)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n::: callout-caution\nFor R users, when working with indices you have to ensure 1) that indices are intergers (i.e. ```as.integer(index_clade)```) and, 2) that indices start at 0 (i.e. ```as.integer(index_clade)-1```).\n:::\n\n## Mathematical Details\n### *Frequentist formulation*\nWe model the relationship between the categorical input feature (X) and the target variable (Y) using the following equation:\n\n$$\nY_i = \\alpha + \\beta_k X_i + \\sigma\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*. \n  \n- $\\alpha$ is the intercept term.\n  \n- $\\beta_k$ are the regression coefficients for each _k_ category.\n  \n- $X_i$ is the encoded categorical input variable for observation *i*. \n  \n- $\\sigma$ is the error term.\n\nWe can interpret $\\beta_i$ as the effect of each category on $Y$ relative to the baseline (usually one of the categories or the intercept). \n\n### *Bayesian formulation*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:\n\n$$\nY \\sim \\text{Normal}(\\alpha +  \\beta_K X, \\sigma)\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta_K \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\sigma \\sim \\text{Exponential}(1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*.\n  \n- $\\alpha$ is the intercept term, which in this case has a unit-normal prior.\n  \n- $\\beta_K$ are slope coefficients for the _K_ distinct independent variables categories, which also have unit-normal priors.\n  \n- $X_i$ is the encoded categorical input variable for observation *i*. \n  \n- $\\sigma$ is a standard deviation parameter, which here has a Exponential prior that constrains it to be positive.\n\n## Notes\n::: callout-note\n\n- We can apply multiple variables similarly to [Chapter 2: Multiple Continuous Variables](2.&#32;Multiple&#32;continuous&#32;Variables.qmd).\n\n- We can apply interaction terms similarly to [Chapter 3: Interaction between Continuous Variables](3.&#32;Interaction&#32;between&#32;continuous&#32;variables.qmd).\n:::\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "4. Categorical variable_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}