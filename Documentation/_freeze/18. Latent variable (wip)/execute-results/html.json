{
  "hash": "b5c5eaa1c205337f218945d93d29159f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ðŸš§ Latent Variable Models ðŸš§\"\ndescription: \"Models that relate a set of observable variables to a set of unobserved (latent) variables.\"\ncategories: [Structural Equation Modeling, Factor Analysis]\nimage: \"Figures/25.png\"\norder: 21\n---\n\n\n\n\n\n\n## General Principles\n\nIn some scenarios, the observed data does not directly reflect the underlying structure or factors influencing the outcome. Instead, _latent variables_â€”variables that are not directly observed but are inferred from the dataâ€”can help model this hidden structure. These latent variables capture unobserved factors that affect the relationship between predictors (_X_) and the outcome (_Y_).\n\nWe model the relationship between the predictor variables (_X_) and the outcome variable (_Y_) with a latent variable (_Z_) as follows:\n\n$$\nY = f(X, Z) + \\epsilon\n$$\n\nWhere:\n- _Y_ is the observed outcome variable.\n- _X_ is the observed predictor variable(s).\n- _Z_ is the latent (unobserved) variable, which we aim to infer.\n- _f(X, Z)_ is the function that relates _X_ and _Z_ to _Y_.\n- _\\epsilon_ is the error term, typically assumed to be normally distributed with mean 0 and variance _\\sigma^2_.\n\nThe latent variable _Z_ can represent various phenomena, such as group-level effects, time-varying trends, or individual-level factors, that are not captured by the observed predictors alone.\n\n## Considerations\n\nIn Bayesian regression with latent variables, we consider the uncertainty in both the observed and latent variables. We declare prior distributions for the latent variables, in addition to the usual priors for regression coefficients and intercepts. These latent variables are often modeled using Gaussian distributions (_Normal_ priors) or more flexible distributions such as _Multivariate Normal_ for correlations among the latent variables.\n\nThe goal is to infer the posterior distribution over both the parameters and the latent variables, given the observed data.\n\n## Example\n\nBelow is an example code snippet demonstrating Bayesian regression with latent variables using TensorFlow Probability:\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#07b22d65 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\n\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Data Simulation ------------------------------------------------\nNY = 4  # Number of dependent variables or outcomes (e.g., dimensions for latent variables)\nNV = 8  # Number of observations or individual-level data points (e.g., subjects)\nN = 100\nK = 5\na = 0.5\n# Generate the means and offsets for the data\n# means: Generate random normal means for each of the NY outcomes\n# offsets: Generate random normal offsets for each of the NV observations\nmeans = m.dist.normal(0, 1, shape=(NY,), sample=True, seed=10)\noffsets = m.dist.normal(0, 1, shape=(NV, 1), sample=True, seed=20)\n\nY2 = offsets + means\n\n# Simulate individual-level random effects (e.g., random slopes or intercepts)\n# b_individual: A matrix of size (N, K) where N is the number of individuals and K is the number of covariates\nb_individual = m.dist.normal(0, 1, shape=(N, K), sample=True, seed=0)\n\n# mu: Add an additional effect 'a' to the individual-level random effects 'b_individual'\n# 'a' could represent a population-level effect or a baseline\nmu = b_individual + a\n\n# Convert Y2 to a JAX array for further computation in a JAX-based framework\nY2 = jnp.array(Y2)\n\n\n# Set data ------------------------------------------------\ndat = dict(\n    NY = NY,\n    NV = NV,\n    Y2 = Y2\n)\nm.data_on_model = dat\n\n# Define model ------------------------------------------------\ndef model(NY, NV, Y2):\n    means = m.dist.normal(0, 1, shape=(NY,), name='means')\n    offset = m.dist.normal(0, 1, shape=(NV, 1), name='offset')\n    sigma = m.dist.exponential(1, shape=(NY,), name='sigma')\n    tmp = jnp.tile(means, (NV, 1)).reshape(NV, NY)\n    mu_l = tmp + offset\n    m.dist.normal(mu_l, jnp.tile(sigma, [NV, 1]), obs=Y2)\n\n# Run sampler ------------------------------------------------\nm.fit(model)\n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<07:28,  2.23it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   7%|â–‹         | 70/1000 [00:00<00:05, 166.60it/s, 1023 steps of size 3.02e-04. acc. prob=0.73]\rwarmup:  11%|â–ˆ         | 110/1000 [00:00<00:04, 202.34it/s, 1023 steps of size 1.38e-03. acc. prob=0.74]\rwarmup:  14%|â–ˆâ–        | 144/1000 [00:00<00:04, 212.95it/s, 1023 steps of size 5.97e-04. acc. prob=0.76]\rwarmup:  17%|â–ˆâ–‹        | 174/1000 [00:00<00:03, 231.78it/s, 1023 steps of size 1.60e-03. acc. prob=0.76]\rwarmup:  21%|â–ˆâ–ˆ        | 207/1000 [00:01<00:03, 255.72it/s, 789 steps of size 5.90e-04. acc. prob=0.76] \rwarmup:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:01<00:03, 247.57it/s, 239 steps of size 5.12e-04. acc. prob=0.77]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:01<00:02, 250.26it/s, 1023 steps of size 5.50e-04. acc. prob=0.77]\rwarmup:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:01<00:02, 239.40it/s, 1023 steps of size 2.56e-04. acc. prob=0.77]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:01<00:02, 240.95it/s, 1023 steps of size 3.97e-04. acc. prob=0.77]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:01<00:02, 238.81it/s, 1023 steps of size 3.34e-04. acc. prob=0.77]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:01<00:02, 238.18it/s, 1023 steps of size 2.95e-04. acc. prob=0.77]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [00:01<00:02, 223.37it/s, 1023 steps of size 1.59e-04. acc. prob=0.77]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [00:01<00:02, 223.51it/s, 1023 steps of size 9.98e-05. acc. prob=0.77]\rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [00:02<00:02, 222.25it/s, 1023 steps of size 1.74e-04. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [00:02<00:02, 214.86it/s, 1023 steps of size 1.17e-04. acc. prob=0.78]\rwarmup:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [00:02<00:02, 223.70it/s, 1023 steps of size 3.72e-04. acc. prob=0.78]\rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [00:02<00:02, 217.14it/s, 1023 steps of size 1.66e-04. acc. prob=0.93]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [00:02<00:02, 215.75it/s, 1023 steps of size 1.66e-04. acc. prob=0.91]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [00:02<00:02, 213.70it/s, 1023 steps of size 1.66e-04. acc. prob=0.81]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [00:02<00:01, 212.61it/s, 1023 steps of size 1.66e-04. acc. prob=0.70]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [00:02<00:01, 210.20it/s, 1023 steps of size 1.66e-04. acc. prob=0.66]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [00:02<00:00, 390.37it/s, 17 steps of size 1.66e-04. acc. prob=0.43]  \rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [00:03<00:00, 394.36it/s, 1023 steps of size 1.66e-04. acc. prob=0.38]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [00:03<00:00, 309.99it/s, 1023 steps of size 1.66e-04. acc. prob=0.42]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [00:03<00:00, 283.05it/s, 1023 steps of size 1.66e-04. acc. prob=0.43]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [00:03<00:00, 254.83it/s, 1023 steps of size 1.66e-04. acc. prob=0.43]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [00:03<00:00, 226.35it/s, 538 steps of size 1.66e-04. acc. prob=0.44] \rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [00:03<00:00, 257.81it/s, 413 steps of size 1.66e-04. acc. prob=0.42]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [00:03<00:00, 255.73it/s, 967 steps of size 1.66e-04. acc. prob=0.42]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [00:04<00:00, 243.98it/s, 1023 steps of size 1.66e-04. acc. prob=0.41]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [00:04<00:00, 226.91it/s, 7 steps of size 1.66e-04. acc. prob=0.40]   \rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:04<00:00, 234.85it/s, 1023 steps of size 1.66e-04. acc. prob=0.40]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>means[0]</th>\n      <td>-1.45</td>\n      <td>0.0</td>\n      <td>-1.45</td>\n      <td>-1.45</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>means[1]</th>\n      <td>-0.73</td>\n      <td>0.0</td>\n      <td>-0.73</td>\n      <td>-0.72</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>means[2]</th>\n      <td>1.24</td>\n      <td>0.0</td>\n      <td>1.24</td>\n      <td>1.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>means[3]</th>\n      <td>-0.60</td>\n      <td>0.0</td>\n      <td>-0.61</td>\n      <td>-0.60</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.34</td>\n      <td>15.50</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[0, 0]</th>\n      <td>-0.64</td>\n      <td>0.0</td>\n      <td>-0.64</td>\n      <td>-0.64</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[1, 0]</th>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>0.75</td>\n      <td>0.76</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.34</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[2, 0]</th>\n      <td>-1.14</td>\n      <td>0.0</td>\n      <td>-1.14</td>\n      <td>-1.13</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[3, 0]</th>\n      <td>0.10</td>\n      <td>0.0</td>\n      <td>0.09</td>\n      <td>0.10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[4, 0]</th>\n      <td>-0.29</td>\n      <td>0.0</td>\n      <td>-0.30</td>\n      <td>-0.29</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[5, 0]</th>\n      <td>-0.80</td>\n      <td>0.0</td>\n      <td>-0.81</td>\n      <td>-0.80</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.34</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[6, 0]</th>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.17</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>offset[7, 0]</th>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.35</td>\n      <td>14.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma[0]</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.41</td>\n      <td>10.90</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma[1]</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.57</td>\n      <td>10.86</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma[2]</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.85</td>\n      <td>12.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma[3]</th>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.64</td>\n      <td>11.56</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## R\n``` R\n```\n![](travaux-routiers.png){fig-align=\"center\"}\n:::\n\n## Mathematical Details\n\n![](travaux-routiers.png){fig-align=\"center\"}\n<!---\nWe can express the Bayesian latent variable model using probability distributions as follows:\n\n$$\n\\begin{aligned}\n& p(Y | X, Z, W, \\sigma) = \\text{Normal}(X \\cdot W + Z, \\sigma^2) \\\\\n& p(Z) = \\text{Normal}(0, \\tau^2) \\\\\n& p(W) = \\text{Normal}(0, \\alpha^2) \\\\\n\\end{aligned}\n$$\n\nWhere:\n- _p(Y | X, Z, W, \\sigma)_ is the likelihood function for the observed outcome variable, which depends on both the observed predictor _X_ and the latent variable _Z_.\n- _p(Z)_ is the prior distribution for the latent variable _Z_, often modeled as _Normal_ with a mean of 0 and variance _\\tau^2_.\n- _p(W)_ is the prior distribution for the regression coefficient(s) _W_, typically assumed to follow a _Normal_ distribution with mean 0 and variance _\\alpha^2_.\n\nThe latent variable _Z_ introduces additional flexibility to the model, capturing unobserved influences on the outcome _Y_.\n\n## Interpretation of Latent Variables\n\n- **Latent Variable (_Z_)**: Represents hidden factors not captured by the observed variables, allowing the model to explain more of the variance in the outcome. For instance, in a psychological model, _Z_ might represent a latent trait such as intelligence or anxiety that influences the outcome.\n  \n- **Posterior Inference**: The posterior distribution of the latent variable _Z_ can give insights into how much the unobserved factors contribute to the outcome.\n\n## Use Cases\n- **Latent Factors in Psychometrics**: In psychometric models, latent variables represent traits or abilities that are not directly observed, such as cognitive ability or personality traits.\n- **Time-Varying Effects**: Latent variables can represent unobserved time trends or individual-specific effects in time-series or longitudinal models.\n- **Mixed Models**: In hierarchical or mixed models, latent variables can represent group-specific intercepts or slopes.\n\n-->\n\n",
    "supporting": [
      "18. Latent variable (wip)_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}