{
  "hash": "a0683edca696e7b815af5a0e5929fa36",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Interaction Terms in Regression\"\ndescription: \"Modeling how the effect of one independent variable on the outcome changes based on the value of another independent variable.\"\ncategories: [Regression]\nimage: \"Figures/3.png\"\norder: 4\n---\n\n\n\n\n\n\n\n## General Principles\nIf you have a case where you believe the effect of one independent variable depends on the value of another independent variable, you can use regression analysis with interaction terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two independent variables (see [note](#Notes) on how this multiplication arises).\n\n## Considerations\n::: callout-note\n- We have the same assumptions as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n\n- We wish to model the relationship between a dependent variable, *Y*, and an independent variable,  $X_1$, whose effect varies as a function of a second independent variable $X_2$. To do this, we explicitly model the hypothesis that the slope between *Y* and $X_1$ depends on (i.e., is conditional on) $X_2$.\n\n- For continuous interactions with normalized data, the intercept becomes the [<span style=\"color:#0D6EFD\">grand mean ðŸ›ˆ</span>]{#grandMean} of the outcome variable.\n\n- The interpretation of slopes estimates is more complex. The coefficient for a non-interaction term reflects the expected change in *Y* when $X_1$ increases by one unit, holding $X_2$ constant at its average value. The coefficient for the interaction term represents how the effect of $X_1$ on *Y* changes depending on the value of $X_2$, and vice versa, showing how the relationship between the two variables influences the outcome *Y*.\n\n- [<span style=\"color:#0D6EFD\">Triptych ðŸ›ˆ</span>]{#triptych} plots are very handy for understanding the impact of interactions, especially when more than two interactions are present.\n\n:::\n\n## Example\nBelow is example code demonstrating Bayesian regression with an interaction term between two continuous variables using the Bayesian Inference (BI) package. The data consist of three continuous variables (temperature, humidity, energy consumption), and the goal is to estimate the effect of the interaction between temperature and humidity on energy consumption. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {#5403c57f .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.tulips(only_path = True)\nm.data(data_path, sep=';')\nm.scale(['blooms', 'water', 'shade']) # Normalize\n\n\n# Define model ------------------------------------------------\ndef model(blooms,shade, water):\n    sigma = m.dist.exponential(1, name = 'sigma', shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = 'bws', shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = 'bs', shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = 'bw', shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = 'a', shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.dist.normal(mu, sigma, obs=blooms)\n\n# Run mcmc ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<09:23,  1.77it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:00<00:00, 744.76it/s, 7 steps of size 5.88e-01. acc. prob=0.79]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [00:00<00:00, 1410.55it/s, 7 steps of size 6.86e-01. acc. prob=0.89]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1193.24it/s, 7 steps of size 6.86e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>0.09</td>\n      <td>0.10</td>\n      <td>-0.05</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>560.88</td>\n      <td>176.01</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bs[0]</th>\n      <td>-0.31</td>\n      <td>0.11</td>\n      <td>-0.49</td>\n      <td>-0.13</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>634.25</td>\n      <td>265.47</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bw[0]</th>\n      <td>0.56</td>\n      <td>0.10</td>\n      <td>0.39</td>\n      <td>0.72</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>510.54</td>\n      <td>317.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bws[0]</th>\n      <td>-0.32</td>\n      <td>0.11</td>\n      <td>-0.53</td>\n      <td>-0.17</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>504.61</td>\n      <td>382.08</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma[0]</th>\n      <td>0.57</td>\n      <td>0.09</td>\n      <td>0.42</td>\n      <td>0.70</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>484.71</td>\n      <td>399.50</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\nm$data(m$load$tulips(only_path = T), sep = ''), sep=';')\nm$scale(list('blooms', 'water', 'shade')) # Normalize\nm$data_to_model(list('blooms', 'water', 'shade')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(blooms, water,shade){\n  # Parameter prior distributions\n  alpha = bi.dist.normal( 0.5, 0.25, name = 'a')\n  beta1 = bi.dist.normal( 0,  0.25, name = 'b1')\n  beta2 = bi.dist.normal(  0,  0.25, name = 'b2')\n  beta_interaction_ = bi.dist.normal(  0, 0.25, name = 'bint')\n  sigma = bi.dist.normal(0, 50, name = 's')\n  # Likelihood\n  m$normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma, obs=blooms)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n```\n\n### Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.tulips(only_path = true)\nm.data(data_path, sep=';')\nm.scale([\"blooms\", \"water\", \"shade\"]) # Normalize\n# Define model ------------------------------------------------\n@BI function model(blooms,shade, water)\n    sigma = m.dist.exponential(1, name = \"sigma\", shape = (1,))\n    bws = m.dist.normal(0, 0.25, name = \"bws\", shape = (1,))\n    bs = m.dist.normal(0, 0.25, name = \"bs\", shape = (1,))\n    bw = m.dist.normal(0, 0.25, name = \"bw\", shape = (1,))\n    a = m.dist.normal(0.5, 0.25, name = \"a\", shape = (1,))\n    mu = a + bw*water + bs*shade + bws*water*shade\n    m.dist.normal(mu, sigma, obs=blooms)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\n## *Frequentist formulation*\nWe model the relationship between the input features ($X_1$ and $X_2$) and the target variable ($Y$) using the following equation:\n$$\nð‘Œ_i = \\alpha + \\beta_1 ð‘‹_{[1,i]} + \\beta_2 ð‘‹_{[2,i]} + \\beta_3 ð‘‹_{[1,i]} ð‘‹_{[2,i]} + \\epsilon_i\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*.\n\n- $\\alpha$ is the intercept term.\n\n- $X_{[1,i]}$ and $X_{[2,i]}$ are the values of the two independent variables for observation *i*.\n\n- $\\beta_1$ and $\\beta_2$ are the coefficients for $X_{1}$ and $X_{2}$, respectively, when the other variable has value 0.\n\n- $\\beta_3$ is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\n\n- $\\epsilon_i$ is the error term, assumed to be independent and normally distributed.\n\n\n### *Bayesian formulation*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model as follows:\n\n$$\nY_i \\sim \\text{Normal}(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta_1 \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta_2 \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta_{3} \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\sigma \\sim \\text{Exponential}(1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*.\n\n- $\\alpha$ is the intercept term, which in this case has a unit-normal prior.\n\n- $\\beta_1$ and $\\beta_2$ are the coefficients for $X_{1}$ and $X_{2}$, respectively, when the other variable has value 0.\n\n- $\\beta_3$ is the coefficient which controls the extent to which the coefficient on one variable depends on the value of the other.\n\n- $X_{[1,i]}$ and $X_{[2,i]}$ are the two values of the independent continuous variables for observation *i*.\n\n- $\\sigma$ is a standard deviation parameter, which here has an Exponential prior that constrains it to be positive.\n\n## Notes{#Notes}\n::: callout-note\nThe interaction term equation:\n$$\nY_i \\sim Normal(\\alpha + \\beta_1 X_{[1,i]} + \\beta_2 X_{[2,i]} + \\beta_{3} X_{[1,i]} X_{[2,i]}, \\sigma)\n$$\n\n\ncan be re-written as:\n$$\nY_i \\sim Normal(\\alpha + (\\beta_1 + \\beta_{3} X_{[2,i]}) X_{[1,i]} + \\beta_2 X_{[2,i]}, \\sigma)\n$$\n\nsimply by factoring the terms with $X_{[1,i]}$ in them. The result is that the coefficient on $X_{[1,i]}$ is written specifically as a linear regression model of $X_{[2,i]}$.\n:::\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "3. Interaction between continuous variables_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}