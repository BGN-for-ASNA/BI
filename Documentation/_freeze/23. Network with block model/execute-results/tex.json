{
  "hash": "4b5e44d6dfb36f48113af417bc27d721",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Stochastic Block Models\"\ndescription: \"A generative model for random graphs used to find communities of nodes with similar connection patterns.\"\ncategories: [Network Analysis, Graph Theory, Community Detection]\nimage: \"Figures/23.png\"\norder: 26\n---\n\nWithin networks, nodes can belong to different categories, and these categories can potentially affect the propensity for node interactions. For example, nodes can have different sex categories, and the propensity to interact with nodes of the same sex can be higher than with nodes of different sexes. To model the propensity for interaction between nodes based on the categories they belong to, we can use a stochastic block model approach.\n\n## Considerations\n::: callout-caution\n- We consider predefined groups here, with the goal of evaluating the propensity for interaction between nodes within each group.\n- In addition to the block model(s) being tested, we need to include a block where all individuals are considered as belonging to the same group (```Any``` in the example). This allows us to assess whether interaction tendencies differ between groups or if the propensity to interact is uniform across all individuals.\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian network model using the stochastic block model approach. The data is identical to the [Network model](20.&#32;Network&#32;model.qmd) example, with the addition of covariates *Any*, *Merica*, and *Quantum*, representing the block membership of each node. This example is based on @ross2024modelling.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Setup device------------------------------------------------\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu', rand_seed = False)\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = True)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = True)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\ncategory = m.dist.categorical(jnp.array([.25,.25,.25,.25]), sample = True, shape  = (N,))\nN_grp, N_by_grp = jnp.unique(category, return_counts=True)\nN_grp = N_grp.shape[0]\n\ndef sim_network(kinship, individual_predictor,category):\n  # Intercept\n  B_intercept = m.net.block_model(jnp.full((N,),0), 1, N, sample = True)\n  B_category = m.net.block_model(category, N_grp, N_by_grp, sample = True)\n\n  # SR\n  sr = m.net.sender_receiver(\n    individual_predictor, \n    individual_predictor, \n    s_mu = 0.4, r_mu = -0.4, sample = True)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = True)\n\n\n\n  return m.dist.bernoulli(\n    logits = B_intercept + B_category + sr + DR, \n    sample = True\n    )\n\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor, category)\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = dict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor, \n    category = category\n)\n\n\ndef model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors,category):\n    N_id = focal_individual_predictors.shape[0]\n\n    # Block ---------------------------------------\n    B_intercept = m.net.block_model(jnp.full((N_id,),0), 1, N_id, name = \"B_intercept\")\n    B_category = m.net.block_model(category, N_grp, N_by_grp, name = \"B_category\")\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors, \n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n    m.dist.bernoulli(logits = B_intercept + B_category + sr + dr, obs=network)\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1)\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:04<1:07:10,  4.03s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|          | 9/1000 [00:04<05:33,  2.97it/s, 63 steps of size 1.41e-01. acc. prob=0.65] \rwarmup:   2%|â–         | 21/1000 [00:04<01:56,  8.40it/s, 31 steps of size 1.57e-01. acc. prob=0.73]\rwarmup:   3%|â–Ž         | 32/1000 [00:04<01:06, 14.63it/s, 63 steps of size 1.07e-01. acc. prob=0.75]\rwarmup:   4%|â–         | 44/1000 [00:04<00:41, 23.16it/s, 31 steps of size 9.55e-02. acc. prob=0.76]\rwarmup:   6%|â–Œ         | 55/1000 [00:04<00:29, 32.12it/s, 15 steps of size 9.77e-02. acc. prob=0.76]\rwarmup:   7%|â–‹         | 67/1000 [00:04<00:21, 42.46it/s, 127 steps of size 6.33e-02. acc. prob=0.76]\rwarmup:   8%|â–Š         | 78/1000 [00:04<00:18, 49.83it/s, 31 steps of size 1.47e-01. acc. prob=0.77] \rwarmup:   9%|â–‰         | 93/1000 [00:04<00:13, 65.47it/s, 98 steps of size 6.42e-02. acc. prob=0.77]\rwarmup:  10%|â–ˆ         | 104/1000 [00:05<00:12, 71.49it/s, 127 steps of size 5.66e-02. acc. prob=0.77]\rwarmup:  12%|â–ˆâ–        | 115/1000 [00:05<00:11, 76.97it/s, 63 steps of size 1.15e-01. acc. prob=0.77] \rwarmup:  13%|â–ˆâ–Ž        | 126/1000 [00:05<00:10, 83.73it/s, 31 steps of size 1.66e-01. acc. prob=0.78]\rwarmup:  14%|â–ˆâ–        | 138/1000 [00:05<00:09, 91.17it/s, 63 steps of size 1.34e-01. acc. prob=0.78]\rwarmup:  15%|â–ˆâ–        | 149/1000 [00:05<00:09, 92.73it/s, 63 steps of size 1.34e-01. acc. prob=0.78]\rwarmup:  16%|â–ˆâ–Œ        | 160/1000 [00:05<00:08, 96.94it/s, 31 steps of size 1.71e-01. acc. prob=0.78]\rwarmup:  17%|â–ˆâ–‹        | 172/1000 [00:05<00:08, 97.95it/s, 127 steps of size 8.50e-02. acc. prob=0.78]\rwarmup:  18%|â–ˆâ–Š        | 183/1000 [00:05<00:08, 91.09it/s, 31 steps of size 2.16e-01. acc. prob=0.78] \rwarmup:  20%|â–ˆâ–‰        | 196/1000 [00:05<00:08, 100.25it/s, 31 steps of size 5.59e-02. acc. prob=0.78]\rwarmup:  21%|â–ˆâ–ˆ        | 207/1000 [00:06<00:07, 100.73it/s, 31 steps of size 2.09e-01. acc. prob=0.78]\rwarmup:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:06<00:07, 109.70it/s, 31 steps of size 2.06e-01. acc. prob=0.78]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:06<00:06, 111.45it/s, 63 steps of size 1.08e-01. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:06<00:07, 105.86it/s, 63 steps of size 1.15e-01. acc. prob=0.78]\rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:06<00:06, 112.01it/s, 15 steps of size 1.17e-01. acc. prob=0.78]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:06<00:07, 100.40it/s, 63 steps of size 1.46e-01. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:06<00:07, 102.26it/s, 31 steps of size 1.54e-01. acc. prob=0.78]\rwarmup:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:06<00:06, 106.34it/s, 63 steps of size 9.74e-02. acc. prob=0.78]\rwarmup:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:06<00:06, 108.94it/s, 127 steps of size 7.28e-02. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:07<00:06, 105.66it/s, 31 steps of size 1.81e-01. acc. prob=0.78] \rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:07<00:06, 103.87it/s, 31 steps of size 1.55e-01. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:07<00:06, 105.52it/s, 63 steps of size 1.06e-01. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:07<00:05, 109.48it/s, 63 steps of size 8.35e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:07<00:05, 111.52it/s, 31 steps of size 1.85e-01. acc. prob=0.78]\rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:07<00:05, 114.92it/s, 31 steps of size 1.54e-01. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [00:07<00:05, 120.46it/s, 31 steps of size 1.38e-01. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [00:07<00:04, 122.45it/s, 31 steps of size 1.51e-01. acc. prob=0.79]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [00:07<00:04, 123.97it/s, 31 steps of size 2.18e-01. acc. prob=0.79]\rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [00:08<00:04, 129.81it/s, 31 steps of size 1.86e-01. acc. prob=0.79]\rwarmup:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [00:08<00:04, 130.30it/s, 31 steps of size 1.44e-01. acc. prob=0.79]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [00:08<00:04, 119.65it/s, 31 steps of size 1.08e-01. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [00:08<00:04, 120.26it/s, 31 steps of size 1.99e-01. acc. prob=0.79]\rwarmup:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [00:08<00:04, 115.48it/s, 55 steps of size 6.30e-02. acc. prob=0.78]\rwarmup:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [00:08<00:04, 112.13it/s, 31 steps of size 1.21e-01. acc. prob=0.79]\rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [00:08<00:04, 120.09it/s, 31 steps of size 1.28e-01. acc. prob=0.92]\rsample:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [00:08<00:03, 124.13it/s, 31 steps of size 1.28e-01. acc. prob=0.92]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [00:08<00:03, 121.31it/s, 31 steps of size 1.28e-01. acc. prob=0.91]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [00:09<00:03, 126.07it/s, 31 steps of size 1.28e-01. acc. prob=0.91]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [00:09<00:03, 131.76it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [00:09<00:03, 134.60it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [00:09<00:02, 135.60it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [00:09<00:02, 137.34it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [00:12<00:22, 16.73it/s, 31 steps of size 1.28e-01. acc. prob=0.90] \rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [00:12<00:15, 23.02it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [00:12<00:11, 30.37it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [00:12<00:08, 40.11it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [00:12<00:06, 50.52it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [00:12<00:04, 62.99it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [00:12<00:03, 74.96it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [00:12<00:03, 86.38it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [00:12<00:02, 98.34it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [00:13<00:02, 106.90it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [00:13<00:01, 115.43it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [00:13<00:01, 120.38it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [00:13<00:01, 125.10it/s, 31 steps of size 1.28e-01. acc. prob=0.89]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [00:13<00:01, 130.09it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [00:13<00:01, 133.74it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [00:13<00:01, 135.68it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [00:13<00:01, 136.48it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [00:13<00:00, 136.01it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [00:13<00:00, 136.20it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [00:14<00:00, 136.92it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [00:14<00:00, 137.48it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [00:14<00:00, 138.42it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [00:14<00:00, 138.37it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [00:14<00:00, 139.86it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [00:14<00:00, 139.97it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [00:14<00:00, 139.26it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:14<00:00, 67.83it/s, 31 steps of size 1.28e-01. acc. prob=0.90]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n/home/sosa/work/.venv/lib/python3.12/site-packages/arviz/stats/diagnostics.py:991: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\n/home/sosa/work/.venv/lib/python3.12/site-packages/arviz/stats/diagnostics.py:991: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>b_B_category[0, 0]</th>\n      <td>-2.51</td>\n      <td>1.99</td>\n      <td>-5.39</td>\n      <td>1.06</td>\n      <td>0.09</td>\n      <td>0.07</td>\n      <td>498.22</td>\n      <td>330.41</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b_B_category[0, 1]</th>\n      <td>-6.86</td>\n      <td>2.39</td>\n      <td>-10.46</td>\n      <td>-3.16</td>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>861.69</td>\n      <td>514.39</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b_B_category[0, 2]</th>\n      <td>-6.91</td>\n      <td>2.28</td>\n      <td>-10.06</td>\n      <td>-2.99</td>\n      <td>0.10</td>\n      <td>0.11</td>\n      <td>568.97</td>\n      <td>311.32</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b_B_category[0, 3]</th>\n      <td>-6.45</td>\n      <td>2.32</td>\n      <td>-9.66</td>\n      <td>-2.64</td>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>822.69</td>\n      <td>398.72</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b_B_category[1, 0]</th>\n      <td>-7.04</td>\n      <td>2.45</td>\n      <td>-10.76</td>\n      <td>-3.22</td>\n      <td>0.10</td>\n      <td>0.10</td>\n      <td>564.03</td>\n      <td>393.66</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>sr_rf[48, 1]</th>\n      <td>-0.04</td>\n      <td>1.23</td>\n      <td>-1.98</td>\n      <td>1.49</td>\n      <td>0.05</td>\n      <td>0.11</td>\n      <td>549.16</td>\n      <td>423.28</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sr_rf[49, 0]</th>\n      <td>-0.13</td>\n      <td>1.07</td>\n      <td>-1.45</td>\n      <td>1.39</td>\n      <td>0.04</td>\n      <td>0.08</td>\n      <td>740.57</td>\n      <td>454.12</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sr_rf[49, 1]</th>\n      <td>0.75</td>\n      <td>1.26</td>\n      <td>-0.84</td>\n      <td>2.69</td>\n      <td>0.06</td>\n      <td>0.08</td>\n      <td>377.84</td>\n      <td>371.34</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sr_sigma[0]</th>\n      <td>0.91</td>\n      <td>0.69</td>\n      <td>0.01</td>\n      <td>1.86</td>\n      <td>0.03</td>\n      <td>0.02</td>\n      <td>321.37</td>\n      <td>310.17</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sr_sigma[1]</th>\n      <td>0.99</td>\n      <td>0.77</td>\n      <td>0.01</td>\n      <td>2.04</td>\n      <td>0.04</td>\n      <td>0.03</td>\n      <td>232.24</td>\n      <td>138.28</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5131 rows Ã— 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n## R\n```R\n![](travaux-routiers.png){fig-align=\"center\"}\n```\n\n## Julia\n```julia\n# Setup device------------------------------------------------\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\", rand_seed = false)\n\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = true)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = true)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\ncategory = m.dist.categorical(jnp.array([.25,.25,.25,.25]), sample = true, shape  = (N,))\nN_grp, N_by_grp = jnp.unique(category, return_counts=true)\nN_grp = N_grp.shape[0]\n\n\nfunction sim_network(kinship, individual_predictor)\n  # Intercept\n  alpha = m.dist.normal(0,1, sample = true)\n\n  # SR\n  sr = m.net.sender_receiver(individual_predictor, individual_predictor, s_mu = 0.4, r_mu = -0.4, sample = true)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = true)\n\n  return m.dist.bernoulli(logits = alpha + sr + DR, sample = true)\n\nend\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor)\n\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = pydict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor, \n    category = category\n)\n\n\n@BI function model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors,category) \n    N_id = focal_individual_predictors.shape[0]\n\n    # Block ---------------------------------------\n    B_intercept = m.net.block_model(jnp.full((N_id,),0), 1, N_id, name = \"B_intercept\")\n    B_category = m.net.block_model(category, N_grp, N_by_grp, name = \"B_category\")\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors, \n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n    m.dist.bernoulli(logits = B_intercept + B_category + sr + dr, obs=network)\nend \n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1)\nm.summary()\n```\n:::\n\n## Mathematical Details\n\n### *Main Formula*\nThe model's block structure can be represented by the following formula. Note that the sender-receiver and dyadic effects are not represented here, as they are already accounted for in the [Network model](20.&#32;Network&#32;model.qmd) chapter:\n\n$$\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n$$ \n\n$$\n\\log(Y_{ij}) = B_{k(i), k(j)} \n$$\n\n\nwhere:\n\n- $B$ is a matrix of intercept parameters unique to the interaction of categories. For example, if there are three groups, then $B$ will be a 3x3 matrix where each element give the rate an individual in group $k$ interacting with an individual in group $l$.\n  \n- We use the function $k$, to return the group identity (i.e., the block) of individual *i*.\n\n\n### *Defining formula sub-equations and prior distributions*\nTo account for all link rates between categories, we can define a square matrix $B$ as follows: the off-diagonal elements represent the link rates between categories $i$ and $j$, while the diagonal elements represent the link rates within category $i$.\n\n$$\nB_{i,j} = \n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,j} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,j} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\na_{i,1} & a_{i,2} & \\cdots & a_{i,j} \n\\end{bmatrix}\n$$\n\n\nAs we consider the link probability within categories to be higher than the link probabilities between categories, we define different priors for the diagonal and the off-diagonal. Priors should also depend on sample size, N, so that the resultant network density approximates empirical networks.\nBasic priors could be:\n\n\n$$\na_{k \\rightarrow k} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.1}{\\sqrt{N_k}}\\right), 1.5\\right)\n$$\n\n$$\na_{k \\rightarrow \\tilde{k}} \\sim \\text{Normal}\\left(\\text{Logit}\\left(\\frac{0.01}{0.5 \\sqrt{N_k} + 0.5 \\sqrt{N_{\\tilde{k}}}}\\right), 1.5\\right)\n$$\n\nwhere:\n\n-   $k \\rightarrow k$ indicates a diagonal element.\n-   $k \\rightarrow \\tilde{k}$ indicates an off-diagonal element.\n\n## Note(s)\n::: callout-note\n- By defining this block model within our network model, we are estimating [<span style=\"color:#0D6EFD\">assortativity ðŸ›ˆ</span>]{#assor} and [<span style=\"color:#0D6EFD\">disassortativity ðŸ›ˆ</span>]{#disassor} for categorical variables.\n  \n- Similarly, for continuous variables, we can generate a block model that includes all continuous variables.\n<!--Correct? -->\n:::\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "23. Network with block model_files/figure-pdf"
    ],
    "filters": []
  }
}