{
<<<<<<< HEAD
  "hash": "0702739ac422be62eb03359db5bfa6c6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Model\"\ndescription: \"Modeling the counts of outcomes across multiple categorical trials.\"\ncategories: [Regression, GLM, Classification]\nimage: \"Figures/9.png\"\norder: 12\n---\n\n\n\n\n\n\n## General Principles\nTo model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Multinomial_ model.\n\n## Considerations\n::: callout-note\n- We have the same considerations as for the [Categorical model](9.&#32;Categorical&#32;model.qmd).\n\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#26ff5a90 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = [0]\n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<06:56,  2.40it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  16%|â–ˆâ–Œ        | 159/1000 [00:00<00:02, 404.50it/s, 1 steps of size 1.28e-01. acc. prob=0.77]\rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [00:00<00:00, 1237.97it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\rsample:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [00:00<00:00, 1830.41it/s, 3 steps of size 7.96e-01. acc. prob=0.90]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1315.70it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>0.00</td>\n      <td>0.97</td>\n      <td>-1.60</td>\n      <td>1.51</td>\n      <td>0.05</td>\n      <td>0.04</td>\n      <td>428.89</td>\n      <td>395.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[1]</th>\n      <td>82.06</td>\n      <td>1.02</td>\n      <td>80.40</td>\n      <td>83.58</td>\n      <td>0.05</td>\n      <td>0.05</td>\n      <td>472.75</td>\n      <td>340.24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>40.96</td>\n      <td>0.50</td>\n      <td>40.12</td>\n      <td>41.65</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>616.30</td>\n      <td>368.44</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## R\n\n```R\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys <- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel <- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n  \n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n  \n  # Likelihood\n  m$dist$multinomial(probs=p[career], obs=career)\n}\n\n\n# Run sampler ------------------------------------------------ \nm$fit(model)  \n\n# Summary ------------------------------------------------\nm$summary()\n```\n\n## Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_multinomial(only_path = true)\nm.data(data_path, sep=',')\n\n# Define model ------------------------------------------------\n@BI function model(income, career)\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    # âš ï¸  Use jnp.array to create a Python object, so [0] indexing works\n    s_3 = jnp.array([0.0]) \n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $K$ categories, the *Dirichlet* likelihood function is:\n\n$$\nY_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n$$\n\nWhere:\n\n- $Y_i$ is the outcome (i.e. the vector of frequencies for each  $k$ categories) for observation *i*.\n  \n- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n  \n- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n  \n- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n\n\n## Reference(s)\n\n",
=======
  "hash": "04486aec6d5679d715e01922cf2c7927",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Model\"\ndescription: \"Modeling the counts of outcomes across multiple categorical trials.\"\ncategories: [Regression, GLM, Classification]\nimage: \"Figures/9.png\"\norder: 12\n---\n\n## General Principles\nTo model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Multinomial_ model.\n\n## Considerations\n::: callout-note\n- We have the same considerations as for the [Categorical model](9.&#32;Categorical&#32;model.qmd).\n\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#160715d8 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = [0]\n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<18:23,  1.10s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   7%|â–‹         | 70/1000 [00:01<00:11, 79.55it/s, 143 steps of size 1.21e-02. acc. prob=0.75]\rwarmup:  13%|â–ˆâ–Ž        | 127/1000 [00:01<00:05, 148.29it/s, 31 steps of size 2.59e-01. acc. prob=0.77]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:01<00:02, 355.15it/s, 7 steps of size 7.73e-01. acc. prob=0.78] \rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [00:01<00:01, 551.97it/s, 3 steps of size 6.93e-01. acc. prob=0.79]\rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [00:01<00:00, 740.27it/s, 3 steps of size 7.96e-01. acc. prob=0.90]\rsample:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [00:01<00:00, 886.29it/s, 3 steps of size 7.96e-01. acc. prob=0.90]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [00:01<00:00, 1010.16it/s, 3 steps of size 7.96e-01. acc. prob=0.90]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [00:01<00:00, 1128.46it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 516.94it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>0.00</td>\n      <td>0.97</td>\n      <td>-1.60</td>\n      <td>1.51</td>\n      <td>0.05</td>\n      <td>0.04</td>\n      <td>428.89</td>\n      <td>395.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[1]</th>\n      <td>82.06</td>\n      <td>1.02</td>\n      <td>80.40</td>\n      <td>83.58</td>\n      <td>0.05</td>\n      <td>0.05</td>\n      <td>472.75</td>\n      <td>340.24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>40.96</td>\n      <td>0.50</td>\n      <td>40.12</td>\n      <td>41.65</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>616.30</td>\n      <td>368.44</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## R\n\n```R\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys <- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel <- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n  \n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n  \n  # Likelihood\n  m$dist$multinomial(probs=p[career], obs=career)\n}\n\n\n# Run sampler ------------------------------------------------ \nm$fit(model)  \n\n# Summary ------------------------------------------------\nm$summary()\n```\n\n## Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_multinomial(only_path = true)\nm.data(data_path, sep=',')\n\n# Define model ------------------------------------------------\n@BI function model(income, career)\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    # âš ï¸  Use jnp.array to create a Python object, so [0] indexing works\n    s_3 = jnp.array([0.0]) \n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $K$ categories, the *Dirichlet* likelihood function is:\n\n$$\nY_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n$$\n\nWhere:\n\n- $Y_i$ is the outcome (i.e. the vector of frequencies for each  $k$ categories) for observation *i*.\n  \n- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n  \n- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n  \n- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n\n\n## Reference(s)\n\n",
>>>>>>> 9ba7a667d655f408e7b5a9c3897d8b55113721d1
    "supporting": [
      "10. Multinomial model_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}