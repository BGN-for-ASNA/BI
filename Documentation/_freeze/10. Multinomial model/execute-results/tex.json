{
  "hash": "d4e3d87dab023e2ad906f5a66298f78e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Model\"\ndescription: \"Modeling the counts of outcomes across multiple categorical trials.\"\ncategories: [Regression, GLM, Classification]\nimage: \"Figures/9.png\"\norder: 12\n---\n\n\n\n\n\n\n## General Principles\nTo model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Multinomial_ model.\n\n## Considerations\n::: callout-note\n- We have the same considerations as for the [Categorical model](9.&#32;Categorical&#32;model.qmd).\n\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian multinomial model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(income, career):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n    s_1 = alpha[0] + beta * income[0]\n    s_2 = alpha[1] + beta * income[1]\n    s_3 = [0]\n    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    # Likelihood\n    m.dist.multinomial(probs = p[career], obs=career)\n    \n# Run sampler ------------------------------------------------ \nm.fit(model)  \n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<07:44,  2.15it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  10%|â–‰         | 96/1000 [00:00<00:04, 224.84it/s, 139 steps of size 2.40e-02. acc. prob=0.77]\rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:00<00:00, 750.77it/s, 1 steps of size 2.78e-01. acc. prob=0.78] \rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [00:00<00:00, 1245.72it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [00:00<00:00, 1667.30it/s, 3 steps of size 7.96e-01. acc. prob=0.90]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1085.81it/s, 7 steps of size 7.96e-01. acc. prob=0.90]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>0.00</td>\n      <td>0.97</td>\n      <td>-1.60</td>\n      <td>1.51</td>\n      <td>0.05</td>\n      <td>0.04</td>\n      <td>428.89</td>\n      <td>395.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[1]</th>\n      <td>82.06</td>\n      <td>1.02</td>\n      <td>80.40</td>\n      <td>83.58</td>\n      <td>0.05</td>\n      <td>0.05</td>\n      <td>472.75</td>\n      <td>340.24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>40.96</td>\n      <td>0.50</td>\n      <td>40.12</td>\n      <td>41.65</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>616.30</td>\n      <td>368.44</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## R\n\n```R\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys <- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel <- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n  \n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n  \n  # Likelihood\n  m$dist$multinomial(probs=p[career], obs=career)\n}\n\n\n# Run sampler ------------------------------------------------ \nm$fit(model)  \n\n# Summary ------------------------------------------------\nm$summary()\n```\n:::\n\n## Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $K$ categories, the *Dirichlet* likelihood function is:\n\n$$\nY_i \\sim \\text{Multinomial}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n$$\n\nWhere:\n\n- $Y_i$ is the outcome (i.e. the vector of frequencies for each  $k$ categories) for observation *i*.\n  \n- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n  \n- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n  \n- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n\n\n## Reference(s)\n\n",
    "supporting": [
      "10. Multinomial model_files/figure-pdf"
    ],
    "filters": []
  }
}