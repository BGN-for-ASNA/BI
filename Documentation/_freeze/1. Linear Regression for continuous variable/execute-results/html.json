{
  "hash": "f43b565df73a592d0042f94d1fba3f5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Univariate Linear Regression\"\ndescription: \"An introduction to linear regression models.\"\ncategories: [Regression]\nimage: \"Figures/regression_random_sampling.gif\"\norder: 2\nreading-time: true\n---\n<!--\n\n-->\n\n## General Principles\n\nTo study relationships between a continuous independent variable and a continuous dependent variable (e.g., height and weight), we can use linear regression. Essentially, we draw a line that passes through the point cloud of the two variables being tested. For this, we need to have:\n\n1) An intercept $\\alpha$, which represents the origin of the line,i.e., the expected value of the dependent variable (height) when the independent variable (weight) is equal to zero.\n\n2) A coefficient $\\beta$, which informs us about the slope of the line. In other words, it tells us how much Y (height) increases for each increment of the independent variable (weight).\n\n3) A standard deviation term $\\sigma$, which informs us about the spread of points around the line, i.e., the variance around the prediction.\n\n\n## Considerations\n\n::: callout-note\n-   Bayesian models allow us to update our understanding of parameters conditional on an observed data set. This allows us to consider [<span style=\"color:#0D6EFD\">model parameter uncertainty ðŸ›ˆ</span>]{#uncertainty}, which quantifies our confidence or uncertainty in the parameters in the form of a [<span style=\"color:#0D6EFD\">posterior distribution ðŸ›ˆ</span>]{#posterior}. Therefore, we need to declare [<span style=\"color:#0D6EFD\">prior distributions ðŸ›ˆ</span>]{#prior} for each model parameter, in this case for: $\\alpha$, $\\beta$, and $\\sigma$.\n\n- Prior distributions are built following these considerations:\n\n  - As the data are [<span style=\"color:#0D6EFD\">normalizedðŸ›ˆ</span>]{#scaled} (see introduction), we can use a Normal distribution for $\\alpha$ and $\\beta$, with a mean of 0 and a standard deviation of 1. This tends to be a weakly regularizing prior, and weaker priors like a $Normal(0,10)$ are also possible.\n\n  - Since $\\sigma$ must be strictly positive, we must use a distribution with support on the positive reals, such as the *Exponential* or *Folded-Normal* distribution.\n\n- Gaussian regression deals directly with continuous outcomes, estimating a linear relationship between predictors and the outcome variable without depending on a non linear [<span style=\"color:#0D6EFD\">link function ðŸ›ˆ</span>]{#linkF} (see introduction). This simplifies interpretation, as coefficients represent direct changes in the outcome variable.\n\n\n:::\n\n## Example\n\nBelow is an example code snippet demonstrating *Bayesian linear regression* using the Bayesian Inference (**BI**) package. Data consist of two continuous variables (height and weight), and the goal is to estimate the effect of weight on height. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\njax.local_device_count 16\n```\n\n\n:::\n\n```{.python .cell-code}\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.howell1(only_path = True)\nm.data(data_path, sep=';') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      height     weight   age  male\n0    151.765  47.825606  63.0     1\n1    139.700  36.485807  63.0     0\n2    136.525  31.864838  65.0     0\n3    156.845  53.041914  41.0     1\n4    145.415  41.276872  51.0     0\n..       ...        ...   ...   ...\n539  145.415  31.127751  17.0     1\n540  162.560  52.163080  31.0     1\n541  156.210  54.062497  21.0     0\n542   71.120   8.051258   0.0     1\n543  158.750  52.531624  68.0     1\n\n[544 rows x 4 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\nm.df = m.df[m.df.age > 18] # Subset data to adults\nm.scale(['weight']) # Normalize\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      height    weight   age  male\n0    151.765  0.430669  63.0     1\n1    139.700 -1.326018  63.0     0\n2    136.525 -2.041868  65.0     0\n3    156.845  1.238745  41.0     1\n4    145.415 -0.583818  51.0     0\n..       ...       ...   ...   ...\n534  162.560  0.307701  27.0     0\n537  142.875 -1.672963  31.0     0\n540  162.560  1.102602  31.0     1\n541  156.210  1.396847  21.0     0\n543  158.750  1.159694  68.0     1\n\n[346 rows x 4 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Define model ------------------------------------------------\ndef model(weight, height):    \n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.log_normal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.dist.normal(a + b * weight , s, obs = height) \n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0%|          | 0/1000 [00:00<?, ?it/s]\nwarmup:   0%|          | 1/1000 [00:02<37:44,  2.27s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\nwarmup:   9%|8         | 87/1000 [00:02<00:17, 51.39it/s, 11 steps of size 4.25e-02. acc. prob=0.77]\nwarmup:  18%|#8        | 183/1000 [00:02<00:06, 122.24it/s, 3 steps of size 1.66e+00. acc. prob=0.78]\nwarmup:  30%|##9       | 297/1000 [00:02<00:03, 224.28it/s, 3 steps of size 8.50e-01. acc. prob=0.78]\nwarmup:  40%|####      | 405/1000 [00:02<00:01, 331.40it/s, 3 steps of size 9.55e-01. acc. prob=0.79]\nsample:  51%|#####     | 507/1000 [00:02<00:01, 435.38it/s, 7 steps of size 7.35e-01. acc. prob=0.94]\nsample:  61%|######1   | 611/1000 [00:02<00:00, 542.54it/s, 3 steps of size 7.35e-01. acc. prob=0.92]\nsample:  71%|#######1  | 711/1000 [00:02<00:00, 631.34it/s, 3 steps of size 7.35e-01. acc. prob=0.93]\nsample:  82%|########2 | 822/1000 [00:03<00:00, 737.53it/s, 3 steps of size 7.35e-01. acc. prob=0.93]\nsample:  92%|#########2| 925/1000 [00:03<00:00, 741.07it/s, 7 steps of size 7.35e-01. acc. prob=0.93]\nsample: 100%|##########| 1000/1000 [00:03<00:00, 293.32it/s, 7 steps of size 7.35e-01. acc. prob=0.93]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mean    sd  hdi_5.5%  hdi_94.5%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\na  154.64  0.26    154.27     155.14  ...     0.01    422.23    407.54    NaN\nb    5.82  0.29      5.40       6.31  ...     0.01    376.62    356.33    NaN\ns    5.15  0.20      4.81       5.47  ...     0.01    446.91    333.97    NaN\n\n[3 rows x 9 columns]\n```\n\n\n:::\n:::\n\n\n## R\n\n``` r\nlibrary(BayesianInference)\nm <- importBI(platform = \"cpu\")\n\n# Load csv file\nm$data(m$load$howell1(only_path = T), sep = \";\")\n\n# Filter data frame\nm$df <- m$df[m$df$age > 18, ] # Subset data to adults\n\n# Scale\nm$scale(list(\"weight\")) # Normalize\n\n# Convert data to JAX arrays\nm$data_to_model(list(\"weight\", \"height\"))\n\n# Define model ------------------------------------------------\nmodel <- function(height, weight) {\n    # Parameter prior distributions\n    s <- bi.dist.uniform(0, 50, name = \"s\")\n    a <- bi.dist.normal(178, 20, name = \"a\")\n    b <- bi.dist.normal(0, 1, name = \"b\")\n\n    # Likelihood\n    bi.dist.normal(a + b * weight, s, obs = height)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary()\n``` \n\n## Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.howell1(only_path = true)\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age > 18] # Subset data to adults\nm.scale([\"weight\"]) # Normalize\n\n# Define model ------------------------------------------------\n@BI function model(weight, height)\n    # Priors\n    a = m.dist.normal(178, 20, name = 'a') \n    b = m.dist.log_normal(0, 1, name = 'b') \n    s = m.dist.uniform(0, 50, name = 's') \n    m.dist.normal(a + b * weight , s, obs = height) \nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\n\n### *Frequentist Formulation*\n\nThe following equation describe the frequentist formulation of linear regression:\n\n$$\nY_i = \\alpha + \\beta  X_i + \\epsilon_i\n$$\n\nWhere:\n\n-   $Y_i$ is the dependent variable for observation *i*.\n\n-   $\\alpha$ is the intercept term.\n\n-   $\\beta$ is the regression coefficient.\n\n-   $X_i$ is the input variable for observation *i*.\n\n-   $\\epsilon_i$ is the error term for observation *i*, and the vector of the error terms, $\\epsilon$, are assumed to be independent and identically distributed.\n\n### *Bayesian Formulation*\n\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express a Bayesian version of this regression model using the following model:\n\n$$\nY_i \\sim \\text{Normal}(\\alpha + \\beta   X_i, \\sigma)\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0, 1)\n$$\n\n$$\n\\beta \\sim \\text{Normal}(0, 1)\n$$\n\n$$\n\\sigma \\sim \\text{Uniform}(0, 50)\n$$\n\nWhere:\n\n-   $Y_i$ is the dependent variable for observation *i*.\n\n-   $\\alpha$ and $\\beta$ are the intercept and regression coefficient, respectively.\n\n-   $X_i$ is the independent variable for observation *i*.\n\n-   $\\sigma$ is the standard deviation of the Normal distribution, which describes the variance in the relationship between the dependent variable $Y$ and the independent variable $X$.\n\n\n## Notes\n::: callout-note\nWe observe a difference between the *Frequentist* and the *Bayesian* formulation regarding the error term. Indeed, in the *Frequentist* formulation, the error term $\\epsilon$ represents residual fluctuations around the predicted values. This assumption leads to point estimates for $\\alpha$ and $\\beta$. In contrast, the *Bayesian* formulation treats $\\sigma$ as a parameter with its own prior distribution. This allows us to incorporate our uncertainty about the error term into the model.\n:::\n\n## Reference(s)\n\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}