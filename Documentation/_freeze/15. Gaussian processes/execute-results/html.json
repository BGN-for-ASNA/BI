{
  "hash": "4d9d5590b170ddf7acd5ac30c8311ca3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gaussian Processes\"\ndescription: \"A Bayesian approach to regression and classification that defines a distribution over functions.\"\ncategories: [Regression, Non-parametric]\nimage: \"Figures/15.png\"\norder: 18\n---\n\n\n\n\n\n\n## General Principles\nThrough varying intercepts and slopes, we have seen how to quantify some of the unique features that generate variation across clusters and covariance among the observations within each cluster. But through the covariance matrix that is used to account for correlation between clusters, we are inherently assuming linear relationships between clusters. What if we want to model the relationship between two variables that are not linearly related? In this case, we can use a Gaussian Process (GP) to model the relationship between two variables. \n<!---\nBasically, a GP is a varying-slope model with a covariance matrix where each element of the matrix is a [<span style=\"color:#0D6EFD\">kernel function ðŸ›ˆ</span>]{#kernel}.\n-->\n\n## Considerations\n::: callout-caution\n- To capture complex, non-linear relationships in data where the underlying function is smooth but has an unknown functional form, GPs use a [<span style=\"color:#0D6EFD\">kernel ðŸ›ˆ</span>]{#kernel}.\n- The choice of kernel hyperparameters can significantly impact results; thus, GPs require choosing an appropriate kernel function that captures the expected behavior of your data.\n- Through kernel definition, we can incorporate domain knowledge.\n- They scale poorly with dataset size (O(nÂ³) complexity) due to matrix operations; thus, memory requirements can be substantial for large datasets, which has led to neural networks being used instead to resolve large non-linear problems.\n<!--- Check scale of the GP -->\n:::\n\n## Example\nBelow is an example code snippet demonstrating Gaussian Process regression using the Bayesian Inference (BI) package. Data consist of a continuous dependent variable (*total_tools*), representing the number of tools invented in the islands, and a continuous independent variable (*population*), representing the population of the islands. The goal is to estimate the effect of population on the total tools. We use the distance matrix of the islands for the kernel function in order to capture the spatial dependence of the relationship. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#f977e539 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport pandas as pd\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.kline2(only_path=True)\nm.data(data_path, sep=';') \n\n\ndata_path2 = files('BI.Resources') / 'islandsDistMatrix.csv'\nislandsDistMatrix = pd.read_csv(data_path2, index_col=0)\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix.values # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    # non-centered Gaussian Process prior\n    etasq = m.dist.exponential(2, name = 'etasq')\n    rhosq = m.dist.exponential(0.5, name = 'rhosq')\n    SIGMA = etasq * jnp.exp(-rhosq * jnp.square(Dmat))\n    SIGMA = SIGMA.at[jnp.diag_indices(Dmat.shape[0])].add(etasq)\n    k = m.dist.multivariate_normal(0, SIGMA, name = 'k')\n\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<11:41,  1.43it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   4%|â–         | 43/1000 [00:00<00:13, 71.39it/s, 383 steps of size 1.21e-02. acc. prob=0.74]\rwarmup:   8%|â–Š         | 77/1000 [00:00<00:07, 123.09it/s, 127 steps of size 1.45e-02. acc. prob=0.76]\rwarmup:  11%|â–ˆ         | 112/1000 [00:01<00:05, 171.41it/s, 255 steps of size 6.22e-02. acc. prob=0.77]\rwarmup:  14%|â–ˆâ–        | 145/1000 [00:01<00:04, 207.19it/s, 127 steps of size 9.25e-02. acc. prob=0.77]\rwarmup:  18%|â–ˆâ–Š        | 177/1000 [00:01<00:03, 222.27it/s, 6 steps of size 8.92e-03. acc. prob=0.77]  \rwarmup:  21%|â–ˆâ–ˆ        | 207/1000 [00:01<00:03, 220.45it/s, 255 steps of size 3.96e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:01<00:03, 248.30it/s, 191 steps of size 6.20e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:01<00:02, 275.94it/s, 511 steps of size 2.24e-02. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:01<00:02, 294.31it/s, 511 steps of size 1.65e-02. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:01<00:02, 303.67it/s, 255 steps of size 2.41e-02. acc. prob=0.78]\rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [00:01<00:01, 330.66it/s, 127 steps of size 2.83e-02. acc. prob=0.78]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [00:01<00:01, 333.90it/s, 63 steps of size 1.98e-02. acc. prob=0.78] \rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [00:02<00:01, 350.73it/s, 255 steps of size 2.05e-02. acc. prob=0.78]\rwarmup:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [00:02<00:01, 338.54it/s, 9 steps of size 1.89e-02. acc. prob=0.78]  \rsample:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [00:02<00:01, 333.04it/s, 127 steps of size 2.85e-02. acc. prob=0.93]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [00:02<00:01, 339.29it/s, 127 steps of size 2.85e-02. acc. prob=0.92]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [00:02<00:01, 333.38it/s, 127 steps of size 2.85e-02. acc. prob=0.92]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [00:02<00:01, 320.39it/s, 63 steps of size 2.85e-02. acc. prob=0.93] \rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [00:02<00:01, 325.67it/s, 127 steps of size 2.85e-02. acc. prob=0.93]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [00:02<00:00, 318.72it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [00:02<00:00, 298.84it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [00:03<00:00, 311.64it/s, 63 steps of size 2.85e-02. acc. prob=0.94] \rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [00:03<00:00, 313.58it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [00:03<00:00, 314.26it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [00:03<00:00, 311.80it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [00:03<00:00, 321.80it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [00:03<00:00, 342.14it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [00:03<00:00, 334.86it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 265.13it/s, 127 steps of size 2.85e-02. acc. prob=0.94]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a</th>\n      <td>1.39</td>\n      <td>1.01</td>\n      <td>0.17</td>\n      <td>2.81</td>\n      <td>0.07</td>\n      <td>0.05</td>\n      <td>133.98</td>\n      <td>214.31</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b</th>\n      <td>0.29</td>\n      <td>0.08</td>\n      <td>0.17</td>\n      <td>0.41</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>184.17</td>\n      <td>119.24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>etasq</th>\n      <td>0.08</td>\n      <td>0.07</td>\n      <td>0.01</td>\n      <td>0.15</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>136.57</td>\n      <td>149.24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>g</th>\n      <td>0.64</td>\n      <td>0.59</td>\n      <td>0.02</td>\n      <td>1.31</td>\n      <td>0.03</td>\n      <td>0.05</td>\n      <td>194.77</td>\n      <td>235.45</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[0]</th>\n      <td>-0.21</td>\n      <td>0.30</td>\n      <td>-0.64</td>\n      <td>0.24</td>\n      <td>0.02</td>\n      <td>0.03</td>\n      <td>170.75</td>\n      <td>120.39</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[1]</th>\n      <td>0.03</td>\n      <td>0.28</td>\n      <td>-0.34</td>\n      <td>0.48</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>166.01</td>\n      <td>144.33</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[2]</th>\n      <td>-0.07</td>\n      <td>0.27</td>\n      <td>-0.40</td>\n      <td>0.39</td>\n      <td>0.03</td>\n      <td>0.03</td>\n      <td>115.89</td>\n      <td>108.80</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[3]</th>\n      <td>0.33</td>\n      <td>0.23</td>\n      <td>-0.01</td>\n      <td>0.74</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>136.59</td>\n      <td>96.46</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[4]</th>\n      <td>0.02</td>\n      <td>0.24</td>\n      <td>-0.34</td>\n      <td>0.33</td>\n      <td>0.02</td>\n      <td>0.03</td>\n      <td>118.65</td>\n      <td>96.09</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[5]</th>\n      <td>-0.41</td>\n      <td>0.28</td>\n      <td>-0.78</td>\n      <td>-0.00</td>\n      <td>0.03</td>\n      <td>0.03</td>\n      <td>120.78</td>\n      <td>94.91</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[6]</th>\n      <td>0.11</td>\n      <td>0.23</td>\n      <td>-0.23</td>\n      <td>0.42</td>\n      <td>0.02</td>\n      <td>0.03</td>\n      <td>105.71</td>\n      <td>78.35</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[7]</th>\n      <td>-0.24</td>\n      <td>0.25</td>\n      <td>-0.64</td>\n      <td>0.11</td>\n      <td>0.03</td>\n      <td>0.03</td>\n      <td>109.64</td>\n      <td>71.99</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[8]</th>\n      <td>0.24</td>\n      <td>0.24</td>\n      <td>-0.07</td>\n      <td>0.61</td>\n      <td>0.03</td>\n      <td>0.03</td>\n      <td>97.87</td>\n      <td>68.36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k[9]</th>\n      <td>-0.23</td>\n      <td>0.34</td>\n      <td>-0.71</td>\n      <td>0.26</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>103.11</td>\n      <td>73.45</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>rhosq</th>\n      <td>1.69</td>\n      <td>1.65</td>\n      <td>0.01</td>\n      <td>3.62</td>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>246.84</td>\n      <td>201.20</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Python (Build in function)\n\n::: {#14087e7a .cell execution_count=2}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport pandas as pd\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.kline2(only_path=True)\nm.data(data_path, sep=';') \n\nislandsDistMatrix = m.load.islands_dist_matrix(frame = False)['data']\n\nm.data_to_model(['total_tools', 'population'])\nm.data_on_model[\"society\"] = jnp.arange(0,10)# index observations\nm.data_on_model[\"Dmat\"] = islandsDistMatrix # Distance matrix\n\n\ndef model(Dmat, population, society, total_tools):\n    a = m.dist.exponential(1, name = 'a')\n    b = m.dist.exponential(1, name = 'b')\n    g = m.dist.exponential(1, name = 'g')\n\n    k = m.gaussian.gaussian_process(Dmat)\n\n    lambda_ = a * population**b / g * jnp.exp(k[society])\n\n    m.dist.poisson(lambda_, obs=total_tools)\n\n# Run sampler ------------------------------------------------\nm.fit(model) \nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<09:37,  1.73it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   5%|â–Œ         | 53/1000 [00:00<00:09, 104.41it/s, 255 steps of size 1.01e-02. acc. prob=0.74]\rwarmup:   9%|â–‰         | 88/1000 [00:00<00:05, 157.55it/s, 127 steps of size 2.12e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–        | 122/1000 [00:00<00:04, 200.45it/s, 63 steps of size 9.73e-02. acc. prob=0.77]\rwarmup:  16%|â–ˆâ–Œ        | 157/1000 [00:00<00:03, 237.27it/s, 127 steps of size 4.46e-02. acc. prob=0.77]\rwarmup:  19%|â–ˆâ–‰        | 191/1000 [00:01<00:03, 252.53it/s, 63 steps of size 8.04e-02. acc. prob=0.78] \rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:01<00:02, 285.17it/s, 127 steps of size 2.93e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:01<00:02, 333.55it/s, 255 steps of size 4.56e-02. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:01<00:01, 360.21it/s, 63 steps of size 6.18e-02. acc. prob=0.78] \rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:01<00:01, 370.31it/s, 191 steps of size 2.64e-02. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [00:01<00:01, 372.82it/s, 23 steps of size 1.74e-02. acc. prob=0.78] \rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [00:01<00:01, 325.72it/s, 127 steps of size 3.82e-02. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [00:01<00:01, 296.91it/s, 127 steps of size 3.69e-02. acc. prob=0.78]\rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [00:02<00:01, 306.27it/s, 127 steps of size 3.82e-02. acc. prob=0.94]\rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [00:02<00:01, 341.14it/s, 63 steps of size 3.82e-02. acc. prob=0.91] \rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [00:02<00:01, 364.29it/s, 127 steps of size 3.82e-02. acc. prob=0.91]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [00:02<00:01, 337.58it/s, 63 steps of size 3.82e-02. acc. prob=0.92] \rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [00:02<00:01, 328.53it/s, 63 steps of size 3.82e-02. acc. prob=0.92]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [00:02<00:00, 346.50it/s, 63 steps of size 3.82e-02. acc. prob=0.92]\rsample:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [00:02<00:00, 370.93it/s, 255 steps of size 3.82e-02. acc. prob=0.92]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [00:02<00:00, 393.18it/s, 63 steps of size 3.82e-02. acc. prob=0.91] \rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [00:02<00:00, 389.39it/s, 127 steps of size 3.82e-02. acc. prob=0.91]\rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [00:02<00:00, 398.15it/s, 95 steps of size 3.82e-02. acc. prob=0.91] \rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [00:03<00:00, 412.32it/s, 63 steps of size 3.82e-02. acc. prob=0.91]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [00:03<00:00, 436.67it/s, 63 steps of size 3.82e-02. acc. prob=0.92]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 309.22it/s, 63 steps of size 3.82e-02. acc. prob=0.91]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a</th>\n      <td>1.42</td>\n      <td>1.11</td>\n      <td>0.07</td>\n      <td>2.72</td>\n      <td>0.07</td>\n      <td>0.09</td>\n      <td>162.72</td>\n      <td>159.21</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b</th>\n      <td>0.28</td>\n      <td>0.08</td>\n      <td>0.17</td>\n      <td>0.44</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>80.44</td>\n      <td>174.63</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>etasq</th>\n      <td>0.20</td>\n      <td>0.18</td>\n      <td>0.01</td>\n      <td>0.39</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>145.06</td>\n      <td>171.68</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>g</th>\n      <td>0.65</td>\n      <td>0.66</td>\n      <td>0.02</td>\n      <td>1.35</td>\n      <td>0.07</td>\n      <td>0.06</td>\n      <td>70.85</td>\n      <td>107.84</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[0]</th>\n      <td>-0.17</td>\n      <td>0.33</td>\n      <td>-0.74</td>\n      <td>0.27</td>\n      <td>0.04</td>\n      <td>0.03</td>\n      <td>54.29</td>\n      <td>92.78</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[1]</th>\n      <td>-0.02</td>\n      <td>0.32</td>\n      <td>-0.54</td>\n      <td>0.48</td>\n      <td>0.05</td>\n      <td>0.03</td>\n      <td>48.22</td>\n      <td>79.88</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[2]</th>\n      <td>-0.07</td>\n      <td>0.32</td>\n      <td>-0.48</td>\n      <td>0.50</td>\n      <td>0.05</td>\n      <td>0.04</td>\n      <td>51.01</td>\n      <td>55.60</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[3]</th>\n      <td>0.35</td>\n      <td>0.28</td>\n      <td>-0.10</td>\n      <td>0.78</td>\n      <td>0.04</td>\n      <td>0.03</td>\n      <td>51.18</td>\n      <td>83.79</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[4]</th>\n      <td>0.07</td>\n      <td>0.29</td>\n      <td>-0.35</td>\n      <td>0.50</td>\n      <td>0.04</td>\n      <td>0.03</td>\n      <td>53.67</td>\n      <td>57.59</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[5]</th>\n      <td>-0.40</td>\n      <td>0.29</td>\n      <td>-0.80</td>\n      <td>0.05</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>59.51</td>\n      <td>68.94</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[6]</th>\n      <td>0.12</td>\n      <td>0.29</td>\n      <td>-0.21</td>\n      <td>0.60</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>51.47</td>\n      <td>63.64</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[7]</th>\n      <td>-0.22</td>\n      <td>0.29</td>\n      <td>-0.59</td>\n      <td>0.27</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>55.68</td>\n      <td>76.35</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[8]</th>\n      <td>0.26</td>\n      <td>0.28</td>\n      <td>-0.17</td>\n      <td>0.67</td>\n      <td>0.04</td>\n      <td>0.03</td>\n      <td>57.48</td>\n      <td>50.38</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>kernel[9]</th>\n      <td>-0.19</td>\n      <td>0.38</td>\n      <td>-0.76</td>\n      <td>0.39</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>81.79</td>\n      <td>95.49</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>rhosq</th>\n      <td>1.38</td>\n      <td>1.73</td>\n      <td>0.03</td>\n      <td>3.37</td>\n      <td>0.12</td>\n      <td>0.13</td>\n      <td>134.65</td>\n      <td>216.24</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## R\n```r\nlibrary(BayesianInference)\njnp = reticulate::import('jax.numpy')\npd = reticulate::import('pandas')\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$kline2(only_path=T), sep=';')\nislandsDistMatrix = m$load$islands_dist_matrix(frame = FALSE)$data\nm$data_to_model(list('total_tools', 'population'))\nm$data_on_model$society = jnp$arange(0,10, dtype='int64')\nm$data_on_model$Dmat = jnp$array(islandsDistMatrix)\n\n\n# Define model ------------------------------------------------\nmodel <- function(Dmat, population, society, total_tools){\n  a = bi.dist.exponential(1, name = 'a')\n  b = bi.dist.exponential(1, name = 'b')\n  g = bi.dist.exponential(1, name = 'g')\n  \n  # non-centered Gaussian Process prior\n  etasq = bi.dist.exponential(2, name = 'etasq')\n  rhosq = bi.dist.exponential(0.5, name = 'rhosq')\n  k = m$gaussian$gaussian_process(Dmat, etasq, rhosq, 0.01)\n  \n  lambda_ = a * population**b / g * jnp$exp(k[society])\n  m$dist$poisson(lambda_, obs=total_tools)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n\n```\n:::\n\n## Mathematical Details\n### *Formula*\n\n\nThe following equation allows us to evaluate the relationship between the dependent variable $Y$ distributed normal, and the independent variable $X$ while incorporating a GP for the effect of variable $Q$:\n\n$$\nY_{[i]} \\sim \\text{Normal}( \\alpha + \\beta  X_{[i]} + \\gamma_{[Q_{[i]}]}, \\sigma)\n$$\n\nwhere:\n\n- $Y_{[i]}$ is the i-th value for the dependent variable $Y$.\n\n- $\\alpha$ is the intercept term.\n\n- $\\beta$ is the regression coefficient term.\n\n- $X_{[i]}$ is the i-th value for the independent variable $X$.\n\n- $Q_{[i]}$ is an integer-valued independent variable (e.g., year-of-birth, age, year) for observation $i$.\n\n- $\\gamma$ is a vector output from a Gaussian process:\n\n$$\n\\gamma\n\\sim \\text{MVNormal} \\left(\nZ,\n\\varsigma\\Omega\\varsigma\n\\right)\n$$\n\nwhere:\n\n- $Z$ represents the mean vector of the multivariate normal distribution and set to [<span style=\"color:#0D6EFD\">zero ðŸ›ˆ</span>]{#kernelMean0}.\n\n- $\\varsigma$ is a diagonal matrix of standard deviations. \n\n- $\\Omega$ is a correlation matrix. \n\n\n- Multiple kernel functions for $\\Omega$ exist and will be discussed in the [Note(s)](#notes) section. But the most common one is the quadratic kernel:\n\n$$\n\\Omega_{[i,j]} = \\eta \\exp(-\\phi^2 D_{[i,j]}^2) \n$$\n\nWhere:\n\n- $\\eta$ is the maximal correlation.\n  \n- $\\phi$ determines the rate of decline.\n  \n- $D_{[i,j]}$ is the distance between the $i$-th and $j$-th categories.\n  \n\n\n### *Bayesian model*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express a Bayesian version of this GP using the following model:\n\n$$\nY_i = \\alpha + \\beta  X_i + \\gamma_{Z_i}\n$$\n\n$$\n\\gamma \\sim \\text{MVNormal} \\left(\n\\begin{pmatrix}\n    0 \\\\\n    \\vdots \\\\\n    0\n\\end{pmatrix},\nK\n\\right)\n$$\n\n$$\nK_{ij} = \\eta^2 \\exp(-p^2D_{ij}^2) + \\delta_{ij} \\sigma^2 \n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\eta^2 \\sim \\text{HalfCauchy}(0,1)\n$$\n\n$$\np^2 \\sim \\text{HalfCauchy}(0,1)\n$$\n\nwhere:\n\n- $Y_i$ is the i-th value for the dependent variable $Y$.\n\n- $\\alpha$ is the intercept term with a prior of $\\text{Normal}(0,1)$.\n\n- $\\beta$ is the regression coefficient term with a prior of $\\text{Normal}(0,1)$.\n\n- $X_i$ is the i-th value for the independent variable $X$.\n\n- $\\gamma_{Z_i}$ is the Gaussian process i-th value for the independent variable $Z$.\n\n- $\\gamma$ is the latent function modeled by the GP.\n\n- $K_{ij}$ is the kernel function evaluated at the corresponding points, $K_{ij} = k(Z_i, Z_j)$, with priors of HalfCauchy(0,1) for $\\eta^2$ and $p^2$ to ensure positive values.\n\n\n## Notes{#notes}\n::: callout-note\n\nCommon kernel functions include:\n\n- *Radial Basis Function* (RBF) or Squared Exponential Kernel:\n$$k(x,x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)$$\n\n\n- *Rational Quadratic Kernel*, this kernel is equivalent to adding together many RBF kernels with different length scales:\n$$k(x,x') = \\sigma^2 \\left(1 + \\frac{||x-x'||^2}{2l^2}\\right)^{-\\alpha}$$\n\n- *Periodic kernel* allows for modeling functions that repeat themselves exactly:\n$$k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)$$\n\n- *Locally Periodic Kernel*:\n\n$$k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right) \\exp\\left(-\\frac{||x-x'||^2}{2l^2}\\right)$$ \n\n- Any slope or intercept in your model can be defined using a Gaussian Process.\n\n:::\n\n\n\n## Reference(s)\n::: {#refs}\n:::\n\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/\n\n",
    "supporting": [
      "15. Gaussian processes_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}