{
  "hash": "8bc43f85cbf482bc3d0ecbcdb20f6d6e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gaussian Mixture Models\"\ndescription: \"A probabilistic model for representing normally distributed subpopulations, often used for clustering.\"\ncategories: [Clustering, Unsupervised Learning]\nimage: \"Figures/19.png\"\norder: 23\n---\n\n## General Principles\n\nTo discover group structures or clusters in data, we can use a **Gaussian Mixture Model (GMM)**. This is a parametric clustering method. A GMM assumes that the data is generated from a mixture of a **pre-specified number (`K`)** of different Gaussian distributions. The model's goal is to figure out:\n\n1.  **The properties of each of the `K` clusters**: For each of the `K` clusters, it estimates its center (mean $\\mu$) and its shape/spread (covariance $\\Sigma$).\n2.  **The mixture weights**: It estimates the proportion of the data that belongs to each cluster.\n3.  **The assignment of each data point**: It determines the probability of each data point belonging to each of the $K$ clusters.\n\n## Considerations\n\n::: callout-caution\n-   A GMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters, *except for the number of clusters, $K$*, which must be fixed in advance.\n\n-   The key parameters and their priors are:\n\n    -   **Number of Clusters $K$**: This is a **fixed hyperparameter** that you must choose before running the model. Choosing the right `K` often involves running the model multiple times and using model comparison criteria (like cross-validation, AIC, or BIC).\n    -   **Cluster Weights `w`**: These are the probabilities of drawing a data point from any given cluster. Since there are a fixed number `K` of them and they must sum to 1, they are typically given a `Dirichlet` prior. A symmetric `Dirichlet` prior (e.g., `Dirichlet(1, 1, ..., 1)`) represents an initial belief that all clusters are equally likely.\n    -   **Cluster Parameters ($\\mu$, $\\Sigma$): Each of the `K` clusters has a mean $\\mu$ and a covariance matrix $\\Sigma$. We place priors on these to define our beliefs about their plausible values.\n\n-   Like the DPMM, the model is often implemented in its marginalized form . Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n  \n- To increase accuracy we run a k-means algorithm to initialize the cluster mean priors.\n:::\n\n## Example\n\nBelow is an example of a GMM implemented in BI. The goal is to cluster a synthetic dataset into a pre-specified K=4 groups.\n\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nfrom sklearn.datasets import make_blobs\n\nm = bi()\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n\n#  The model\ndef gmm(data, K, initial_means): # Here K is the *exact* number of clusters\n    D = data.shape[1]  # Number of features\n    alpha_prior = 0.5 * jnp.ones(K)\n    w = m.dist.dirichlet(concentration=alpha_prior, name='weights') \n\n    with m.dist.plate(\"components\", K): # Use fixed K\n        mu = m.dist.multivariate_normal(loc=initial_means, covariance_matrix=0.1*jnp.eye(D), name='mu')        \n        sigma = m.dist.half_cauchy(1, shape=(D,), event=1, name='sigma')\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name='Lcorr')\n\n        scale_tril = sigma[..., None] * Lcorr\n\n    m.dist.mixture_same_family(\n        mixing_distribution=m.dist.categorical(probs=w, create_obj=True),\n        component_distribution=m.dist.multivariate_normal(loc=mu, scale_tril=scale_tril, create_obj=True),\n        name=\"obs\",\n        obs=data\n    )\n\n# Kmeans clustering do initiate the means\nm.ml.KMEANS(data, n_clusters=8)\nm.data_on_model = {\"data\": data,\"K\": 8 }\nm.data_on_model['initial_means'] = m.ml.results['centroids']\n\n\nm.fit(gmm) # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:04<1:22:50,  4.98s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   2%|â–         | 24/1000 [00:05<02:27,  6.60it/s, 31 steps of size 3.67e-02. acc. prob=0.72]\rwarmup:   4%|â–Ž         | 37/1000 [00:05<01:26, 11.09it/s, 31 steps of size 5.77e-02. acc. prob=0.75]\rwarmup:   6%|â–Œ         | 55/1000 [00:05<00:48, 19.64it/s, 15 steps of size 5.47e-02. acc. prob=0.76]\rwarmup:   7%|â–‹         | 72/1000 [00:05<00:31, 29.76it/s, 7 steps of size 4.78e-02. acc. prob=0.76] \rwarmup:   9%|â–‰         | 94/1000 [00:05<00:19, 46.46it/s, 7 steps of size 2.43e-02. acc. prob=0.77]\rwarmup:  11%|â–ˆ         | 111/1000 [00:05<00:15, 59.04it/s, 15 steps of size 5.94e-01. acc. prob=0.77]\rwarmup:  13%|â–ˆâ–Ž        | 131/1000 [00:05<00:11, 77.83it/s, 15 steps of size 5.63e-01. acc. prob=0.78]\rwarmup:  15%|â–ˆâ–Œ        | 151/1000 [00:05<00:08, 97.24it/s, 7 steps of size 5.61e+00. acc. prob=0.78] \rwarmup:  17%|â–ˆâ–‹        | 169/1000 [00:05<00:07, 107.88it/s, 7 steps of size 7.50e-01. acc. prob=0.78]\rwarmup:  19%|â–ˆâ–Š        | 186/1000 [00:06<00:06, 117.90it/s, 15 steps of size 4.28e-01. acc. prob=0.78]\rwarmup:  20%|â–ˆâ–ˆ        | 203/1000 [00:06<00:06, 129.10it/s, 15 steps of size 4.00e-01. acc. prob=0.78]\rwarmup:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:06<00:05, 135.47it/s, 15 steps of size 4.38e-01. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:06<00:05, 150.73it/s, 15 steps of size 6.55e-01. acc. prob=0.78]\rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:06<00:04, 161.46it/s, 15 steps of size 2.40e-01. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:06<00:04, 146.47it/s, 15 steps of size 4.64e-01. acc. prob=0.78]\rwarmup:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:06<00:04, 149.63it/s, 15 steps of size 4.13e-01. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:06<00:04, 149.81it/s, 7 steps of size 1.02e+00. acc. prob=0.78] \rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:06<00:04, 160.28it/s, 15 steps of size 4.61e-01. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:07<00:03, 178.72it/s, 15 steps of size 3.73e-01. acc. prob=0.78]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:07<00:03, 186.15it/s, 15 steps of size 4.87e-01. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [00:07<00:03, 178.01it/s, 7 steps of size 6.59e-01. acc. prob=0.79] \rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [00:07<00:03, 174.99it/s, 7 steps of size 3.83e-01. acc. prob=0.79]\rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [00:07<00:02, 189.70it/s, 15 steps of size 5.15e-01. acc. prob=0.79]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [00:07<00:02, 205.94it/s, 15 steps of size 5.06e-01. acc. prob=0.79]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [00:07<00:02, 201.47it/s, 7 steps of size 5.55e-01. acc. prob=0.79] \rsample:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [00:07<00:02, 176.35it/s, 7 steps of size 4.07e-01. acc. prob=0.95]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [00:08<00:02, 161.81it/s, 15 steps of size 4.07e-01. acc. prob=0.87]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [00:08<00:03, 148.39it/s, 15 steps of size 4.07e-01. acc. prob=0.89]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [00:08<00:02, 150.02it/s, 7 steps of size 4.07e-01. acc. prob=0.89] \rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [00:08<00:02, 159.40it/s, 7 steps of size 4.07e-01. acc. prob=0.90]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [00:08<00:02, 172.66it/s, 15 steps of size 4.07e-01. acc. prob=0.89]\rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [00:08<00:02, 172.54it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [00:08<00:02, 166.12it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [00:08<00:02, 168.73it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [00:08<00:01, 167.18it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [00:09<00:01, 169.12it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [00:09<00:01, 156.65it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [00:09<00:01, 154.99it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [00:09<00:01, 160.59it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [00:09<00:01, 177.84it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [00:09<00:01, 182.80it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [00:09<00:01, 187.12it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [00:09<00:00, 182.37it/s, 15 steps of size 4.07e-01. acc. prob=0.89]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [00:09<00:00, 173.91it/s, 15 steps of size 4.07e-01. acc. prob=0.89]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [00:10<00:00, 164.11it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [00:10<00:00, 157.15it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [00:10<00:00, 164.33it/s, 7 steps of size 4.07e-01. acc. prob=0.90] \rsample:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [00:10<00:00, 161.39it/s, 7 steps of size 4.07e-01. acc. prob=0.90]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [00:10<00:00, 160.06it/s, 7 steps of size 4.07e-01. acc. prob=0.90]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [00:10<00:00, 168.20it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [00:10<00:00, 181.87it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [00:10<00:00, 180.68it/s, 15 steps of size 4.07e-01. acc. prob=0.89]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:10<00:00, 92.04it/s, 15 steps of size 4.07e-01. acc. prob=0.90]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nâš ï¸This function is still in development. Use it with caution. âš ï¸\nâš ï¸This function is still in development. Use it with caution. âš ï¸\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](20. GMM_files/figure-pdf/cell-2-output-4.png){fig-pos='H'}\n:::\n:::\n\n\n## R\n\n![](travaux-routiers.png){fig-align=\"center\"}\n\n\n## Julia\n```julia\n@BI function gmm(data, K, initial_means)\n    D = data.shape[1] \n    alpha_prior = 0.5 * jnp.ones(K)\n    w = m.dist.dirichlet(concentration=alpha_prior, name=\"weights\") \n\n    # We capture the output of the pywith block\n    # The block returns a tuple (mu, scale_tril)\n    mu, scale_tril = pywith(m.dist.plate(\"components\", K)) do _\n        mu_inner = m.dist.multivariate_normal(\n            loc=initial_means, \n            covariance_matrix=0.1*jnp.eye(D), \n            name=\"mu\"\n        )        \n        \n        sigma = m.dist.half_cauchy(1, shape=(D,), event=1, name=\"sigma\")\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name=\"Lcorr\")\n\n        # FIX: Use expand_dims instead of slicing\n        scale_tril_inner = jnp.expand_dims(sigma, -1) * Lcorr\n        \n        # Return them so they are available outside\n        (mu_inner, scale_tril_inner)\n    end\n\n    m.dist.mixture_same_family(\n        mixing_distribution=m.dist.categorical(probs=w, create_obj=true),\n        component_distribution=m.dist.multivariate_normal(loc=mu, scale_tril=scale_tril, create_obj=true),\n        name=\"obs\",\n        obs=data\n    )\nend\n\n# Run\nm.fit(gmm)\nm.summary()\n```\n\n:::\n## Mathematical Details\n\n\nThis section describes the generative process for a GMM. \n\n\n$$\n\\begin{pmatrix}\nY_{i,1} \\\\\n\\vdots \\\\\nY_{i,D}\n\\end{pmatrix}\n\\sim \n\\text{MVN}\\left(\n\\begin{pmatrix}\n\\mu_{z_i,1} \\\\\n\\vdots \\\\\n\\mu_{z_i,D}\n\\end{pmatrix},\n\\Sigma_{z_i}\n\\right)\n$$\n\n$$\n\\begin{pmatrix}\n\\mu_{k,1} \\\\\n\\vdots \\\\\n\\mu_{k,D}\n\\end{pmatrix}\n\\sim \n\\text{MVN}\\left(\n\\begin{pmatrix}\nA_{k,1} \\\\\n\\vdots \\\\\nA_{k,D}\n\\end{pmatrix},\nB\n\\right)\n$$\n\n$$\n\\Sigma_k = \\text{Diag}(\\sigma_k) \\Omega_k  \\text{Diag}(\\sigma_k)\n$$\n\n$$\n\\sigma_{[k,d]} \\sim \\text{HalfCauchy}(1) \n$$\n\n$$\n\\Omega_k \\sim \\text{LKJ}(2) \n$$\n\n$$\nz_{i} \\sim \\text{Categorical}(\\pi) \n$$\n\n$$\n\\pi \\sim \\text{Dirichlet}(0.5, \\dots, 0.5)\n$$\n\n\nWhere : \n\n*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n*   $\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix}$ is a prior for the $k$-th mean vector as derived by a *KMEANS* clustering algorithm. \n*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n*  $\\text{Diag}(\\sigma_k)$ is a diagonal matrix whose diagonal entries are the standard deviations:\n  $$\n  \\text{Diag}(\\sigma_k) =\n  \\begin{pmatrix}\n  \\sigma_{[k,1]} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{[k,2]} &        & \\vdots \\\\\n  \\vdots &        & \\ddots & 0 \\\\\n  0 & \\cdots & 0 & \\sigma_{[k,D]}\n  \\end{pmatrix}.\n  $$\n\n*   $\\sigma_{k}$ is a $D$-vector of standard deviations for the $k$-th cluster where each element, $d$, has a half-cauchy prior.\n*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n*   $\\pi$ is a vector of $K$ cluster weights.\n\n\nWhere : \n\n*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n\n*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n\n*   $\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix}$ is a prior for the $k$-th mean vector as derived by a *KMEANS* clustering algorithm. \n\n*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\n*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n\n*   $\\sigma_k$ is a diagonal matrix of standard deviations for the $k$-th cluster.\n\n*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n\n*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n\n*   $\\pi$ is a vector of $K$ cluster weights.\n\n\n## Notes\n\n::: callout-note\nThe primary challenge of the GMM compared to the DPMM is the need to **manually specify the number of clusters `K`**. If the chosen `K` is too small, the model may merge distinct clusters. If `K` is too large, it may split natural clusters into meaningless sub-groups. Therefore, applying a GMM often involves an outer loop of model selection where one fits the model for a range of `K` values and uses a scoring metric to select the best one.\n:::\n\n## Reference(s)\nC. M. Bishop (2006). *Pattern Recognition and Machine Learning*. Springer. (Chapter 9)\n\n",
    "supporting": [
      "20. GMM_files/figure-pdf"
    ],
    "filters": []
  }
}