{
  "hash": "378f50167555dba1316f80f10b88763a",
  "result": {
    "engine": "jupyter",
<<<<<<< HEAD
    "markdown": "---\ntitle: \"Gamma-Poisson Model\"\ndescription: \"An extension of the Poisson model for count data that exhibits overdispersion.\"\ncategories: [Regression, Count Data, GLM]\nimage: \"Figures/8.png\"\norder: 10\n---\n\n\n\n\n\n\n## General Principles \nTo model the relationship between a count outcome variable and one or more independent variables with [<span style=\"color:#0D6EFD\">overdispersion ðŸ›ˆ</span>]{#overdispersion}, we can use the _Negative Binomial model_. \n\n## Considerations\n::: callout-caution \n- We have the same considerations as for the [Poisson model](7.&#32;Poisson&#32;model.qmd).\n  \n- Overdispersion is handled because the Gamma-Poisson model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is `log_days`).\n:::\n \n## Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package. \n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {#89a36518 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\n\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path =  m.load.sim_gamma_poisson(only_path=True)\nm.data(data_path, sep=',') \n\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    phi = m.dist.exponential(1, name = 'phi', shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n    m.dist.poisson(rate = Lambda, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<08:59,  1.85it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|          | 10/1000 [00:00<00:51, 19.36it/s, 511 steps of size 9.70e-03. acc. prob=0.58]\rwarmup:   2%|â–         | 16/1000 [00:00<00:35, 27.82it/s, 255 steps of size 1.31e-02. acc. prob=0.67]\rwarmup:   2%|â–         | 22/1000 [00:00<00:27, 35.14it/s, 1023 steps of size 4.12e-03. acc. prob=0.68]\rwarmup:   3%|â–Ž         | 28/1000 [00:01<00:27, 35.24it/s, 255 steps of size 9.52e-03. acc. prob=0.71] \rwarmup:   3%|â–Ž         | 33/1000 [00:01<00:26, 36.25it/s, 511 steps of size 5.53e-03. acc. prob=0.71]\rwarmup:   4%|â–         | 40/1000 [00:01<00:22, 42.43it/s, 511 steps of size 4.30e-03. acc. prob=0.72]\rwarmup:   5%|â–         | 48/1000 [00:01<00:19, 48.20it/s, 511 steps of size 2.94e-03. acc. prob=0.73]\rwarmup:   5%|â–Œ         | 54/1000 [00:01<00:19, 48.24it/s, 511 steps of size 5.01e-03. acc. prob=0.74]\rwarmup:   6%|â–‹         | 63/1000 [00:01<00:16, 56.38it/s, 255 steps of size 8.47e-03. acc. prob=0.75]\rwarmup:   7%|â–‹         | 72/1000 [00:01<00:14, 63.94it/s, 127 steps of size 5.29e-03. acc. prob=0.75]\rwarmup:   8%|â–Š         | 79/1000 [00:01<00:14, 62.68it/s, 511 steps of size 3.58e-03. acc. prob=0.75]\rwarmup:   9%|â–Š         | 86/1000 [00:02<00:14, 60.98it/s, 255 steps of size 7.11e-03. acc. prob=0.76]\rwarmup:   9%|â–‰         | 93/1000 [00:02<00:14, 60.69it/s, 50 steps of size 2.57e-03. acc. prob=0.75] \rwarmup:  10%|â–ˆ         | 100/1000 [00:02<00:14, 60.57it/s, 255 steps of size 5.12e-03. acc. prob=0.76]\rwarmup:  11%|â–ˆ         | 112/1000 [00:02<00:11, 75.98it/s, 63 steps of size 1.46e-01. acc. prob=0.77] \rwarmup:  14%|â–ˆâ–        | 142/1000 [00:02<00:06, 135.96it/s, 63 steps of size 9.22e-02. acc. prob=0.77]\rwarmup:  17%|â–ˆâ–‹        | 167/1000 [00:02<00:04, 167.02it/s, 63 steps of size 1.64e-01. acc. prob=0.77]\rwarmup:  20%|â–ˆâ–‰        | 199/1000 [00:02<00:03, 208.75it/s, 31 steps of size 2.34e-01. acc. prob=0.78]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:02<00:03, 236.97it/s, 31 steps of size 1.89e-01. acc. prob=0.78]\rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:02<00:02, 260.19it/s, 63 steps of size 1.34e-01. acc. prob=0.78]\rwarmup:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:02<00:02, 262.04it/s, 63 steps of size 1.10e-01. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:03<00:02, 281.51it/s, 31 steps of size 1.76e-01. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:03<00:02, 296.92it/s, 31 steps of size 1.71e-01. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [00:03<00:02, 300.47it/s, 63 steps of size 1.47e-01. acc. prob=0.78]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [00:03<00:01, 299.20it/s, 31 steps of size 2.15e-01. acc. prob=0.78]\rwarmup:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [00:03<00:01, 310.13it/s, 31 steps of size 1.44e-01. acc. prob=0.78]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [00:03<00:01, 297.83it/s, 31 steps of size 1.59e-01. acc. prob=0.78]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [00:03<00:01, 295.84it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [00:03<00:01, 301.76it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [00:03<00:01, 303.03it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [00:03<00:01, 303.21it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [00:04<00:01, 306.29it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [00:04<00:01, 310.29it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [00:04<00:00, 314.70it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [00:04<00:00, 312.11it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [00:04<00:00, 295.18it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [00:04<00:00, 293.14it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [00:04<00:00, 299.57it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [00:04<00:00, 304.97it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [00:04<00:00, 311.05it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [00:05<00:00, 310.00it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [00:05<00:00, 306.70it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [00:05<00:00, 308.06it/s, 31 steps of size 1.31e-01. acc. prob=0.89]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:05<00:00, 189.86it/s, 31 steps of size 1.31e-01. acc. prob=0.88]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Lambda[0]</th>\n      <td>1.56</td>\n      <td>0.38</td>\n      <td>0.97</td>\n      <td>2.11</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>793.43</td>\n      <td>367.44</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[1]</th>\n      <td>1.51</td>\n      <td>0.36</td>\n      <td>1.02</td>\n      <td>2.12</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>877.99</td>\n      <td>293.31</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[2]</th>\n      <td>1.58</td>\n      <td>0.38</td>\n      <td>0.93</td>\n      <td>2.12</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>741.78</td>\n      <td>372.31</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3]</th>\n      <td>1.40</td>\n      <td>0.34</td>\n      <td>0.93</td>\n      <td>1.96</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>937.17</td>\n      <td>277.70</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[4]</th>\n      <td>1.47</td>\n      <td>0.37</td>\n      <td>0.89</td>\n      <td>2.00</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>507.60</td>\n      <td>428.36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Lambda[3398]</th>\n      <td>3.53</td>\n      <td>0.77</td>\n      <td>2.43</td>\n      <td>4.67</td>\n      <td>0.03</td>\n      <td>0.04</td>\n      <td>914.23</td>\n      <td>399.13</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3399]</th>\n      <td>3.68</td>\n      <td>0.79</td>\n      <td>2.56</td>\n      <td>5.06</td>\n      <td>0.03</td>\n      <td>0.06</td>\n      <td>727.90</td>\n      <td>235.65</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[0]</th>\n      <td>-0.42</td>\n      <td>0.01</td>\n      <td>-0.45</td>\n      <td>-0.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>40.06</td>\n      <td>101.97</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>-2.75</td>\n      <td>0.03</td>\n      <td>-2.81</td>\n      <td>-2.71</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>57.05</td>\n      <td>188.62</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>phi[0]</th>\n      <td>17.63</td>\n      <td>1.91</td>\n      <td>14.32</td>\n      <td>20.23</td>\n      <td>0.56</td>\n      <td>0.18</td>\n      <td>12.25</td>\n      <td>44.88</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3403 rows Ã— 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$sim_gamma_poisson(only_path=T), sep = '')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  phi = bi.dist.exponential(1, name='phi', shape=c(1))\n  mu = jnp$exp(log_days + alpha + beta * monastery)\n  Lambda =  bi.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n  # Likelihood\n  bi.dist.poisson(rate=Lambda, obs=y)\n}\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n```\n\n### Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_gamma_poisson(only_path = true)\nm.data(data_path, sep=',')\n\n# Define model ------------------------------------------------\n@BI function model(log_days, monastery, y)\n    a = m.dist.normal(0, 1, name = \"a\", shape=(1,))\n    b = m.dist.normal(0, 1, name = \"b\", shape=(1,))\n    phi = m.dist.exponential(1, name = \"phi\", shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = \"Lambda\")\n    m.dist.poisson(rate = Lambda, obs=y)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\n### *Bayesian model*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda_i)\n$$\n\n$$\n\\lambda_i \\sim \\text{Gamma}(\\mu_i \\phi, \\phi)\n$$\n\n$$\n\\log(\\mu_i) = \\text{rates}_i + \\alpha + \\beta X_i\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\phi \\sim \\text{Exponential}(1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*. \n  \n- $\\lambda_i$ is the rate parameter of the Poisson distribution for observation *i*, assuming that each Poisson count observation has its own $rate_i$.\n  \n- $\\mu_i$ is the mean rate parameter.\n- \n- $\\phi$ controls the level of overdispersion in the rates.\n  \n- $\\alpha$ is the intercept term.\n  \n- $\\beta$ is the regression coefficient.\n  \n- $X_i$ is the value of the predictor variable for observation *i*.\n\n\n## Notes\n\n::: callout-note \n- We can apply multiple variables similarly as in [chapter 2](2.&#32;Multiple&#32;continuous&#32;variables.qmd).\n\n- We can apply interaction terms similarly as in [chapter 3](3.&#32;Interaction&#32;between&#32;Continuous&#32;Variables.qmd).\n\n- We can apply categorical variables similarly as in [chapter 4](4.&#32;Categorical&#32;variable.qmd).\n:::\n\n## Reference(s)\n\n",
=======
    "markdown": "---\ntitle: \"Gamma-Poisson Model\"\ndescription: \"An extension of the Poisson model for count data that exhibits overdispersion.\"\ncategories: [Regression, Count Data, GLM]\nimage: \"Figures/8.png\"\norder: 10\n---\n\n## General Principles \nTo model the relationship between a count outcome variable and one or more independent variables with [<span style=\"color:#0D6EFD\">overdispersion ðŸ›ˆ</span>]{#overdispersion}, we can use the _Negative Binomial model_. \n\n## Considerations\n::: callout-caution \n- We have the same considerations as for the [Poisson model](7.&#32;Poisson&#32;model.qmd).\n  \n- Overdispersion is handled because the Gamma-Poisson model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is `log_days`).\n:::\n \n## Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package. \n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {#158348b7 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\n\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path =  m.load.sim_gamma_poisson(only_path=True)\nm.data(data_path, sep=',') \n\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    phi = m.dist.exponential(1, name = 'phi', shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n    m.dist.poisson(rate = Lambda, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<23:14,  1.40s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|          | 6/1000 [00:01<03:09,  5.24it/s, 511 steps of size 1.14e-02. acc. prob=0.43]\rwarmup:   1%|          | 11/1000 [00:01<01:41,  9.79it/s, 511 steps of size 5.90e-03. acc. prob=0.59]\rwarmup:   2%|â–         | 16/1000 [00:01<01:15, 13.05it/s, 1023 steps of size 4.14e-03. acc. prob=0.64]\rwarmup:   2%|â–         | 19/1000 [00:02<01:06, 14.75it/s, 127 steps of size 2.52e-02. acc. prob=0.70] \rwarmup:   2%|â–         | 22/1000 [00:02<01:05, 14.82it/s, 511 steps of size 8.18e-03. acc. prob=0.69]\rwarmup:   2%|â–Ž         | 25/1000 [00:02<01:03, 15.44it/s, 1023 steps of size 3.45e-03. acc. prob=0.69]\rwarmup:   3%|â–Ž         | 28/1000 [00:02<00:55, 17.38it/s, 127 steps of size 1.13e-02. acc. prob=0.71] \rwarmup:   3%|â–Ž         | 33/1000 [00:02<00:50, 19.00it/s, 1023 steps of size 3.34e-03. acc. prob=0.71]\rwarmup:   4%|â–Ž         | 36/1000 [00:02<00:47, 20.13it/s, 127 steps of size 6.20e-03. acc. prob=0.72] \rwarmup:   4%|â–         | 40/1000 [00:02<00:41, 22.95it/s, 511 steps of size 4.09e-03. acc. prob=0.72]\rwarmup:   4%|â–         | 44/1000 [00:03<00:36, 26.00it/s, 255 steps of size 6.81e-03. acc. prob=0.73]\rwarmup:   5%|â–         | 48/1000 [00:03<00:34, 27.55it/s, 511 steps of size 3.45e-03. acc. prob=0.73]\rwarmup:   5%|â–Œ         | 51/1000 [00:03<00:35, 26.85it/s, 127 steps of size 7.90e-03. acc. prob=0.74]\rwarmup:   6%|â–Œ         | 57/1000 [00:03<00:28, 32.99it/s, 255 steps of size 1.09e-02. acc. prob=0.75]\rwarmup:   6%|â–Œ         | 61/1000 [00:03<00:29, 31.30it/s, 255 steps of size 9.98e-03. acc. prob=0.75]\rwarmup:   7%|â–‹         | 66/1000 [00:03<00:26, 35.72it/s, 3 steps of size 2.28e-03. acc. prob=0.74]  \rwarmup:   7%|â–‹         | 70/1000 [00:03<00:28, 32.25it/s, 127 steps of size 7.37e-03. acc. prob=0.75]\rwarmup:   7%|â–‹         | 74/1000 [00:03<00:28, 32.47it/s, 255 steps of size 4.66e-03. acc. prob=0.75]\rwarmup:   8%|â–Š         | 78/1000 [00:04<00:28, 32.56it/s, 511 steps of size 3.81e-03. acc. prob=0.75]\rwarmup:   8%|â–Š         | 83/1000 [00:04<00:26, 34.92it/s, 255 steps of size 5.87e-03. acc. prob=0.75]\rwarmup:   9%|â–‰         | 88/1000 [00:04<00:25, 36.24it/s, 255 steps of size 5.61e-03. acc. prob=0.76]\rwarmup:   9%|â–‰         | 92/1000 [00:04<00:28, 31.40it/s, 255 steps of size 8.23e-03. acc. prob=0.76]\rwarmup:  10%|â–‰         | 96/1000 [00:04<00:28, 31.73it/s, 255 steps of size 5.14e-03. acc. prob=0.76]\rwarmup:  10%|â–ˆ         | 100/1000 [00:04<00:27, 33.21it/s, 127 steps of size 1.01e-02. acc. prob=0.76]\rwarmup:  11%|â–ˆ         | 107/1000 [00:04<00:21, 41.37it/s, 127 steps of size 7.50e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–        | 121/1000 [00:04<00:13, 66.17it/s, 31 steps of size 2.47e-01. acc. prob=0.77] \rwarmup:  14%|â–ˆâ–Ž        | 137/1000 [00:05<00:09, 90.97it/s, 63 steps of size 9.63e-02. acc. prob=0.77]\rwarmup:  15%|â–ˆâ–Œ        | 152/1000 [00:05<00:07, 106.83it/s, 1 steps of size 3.70e-01. acc. prob=0.77]\rwarmup:  17%|â–ˆâ–‹        | 169/1000 [00:05<00:06, 124.47it/s, 63 steps of size 1.21e-01. acc. prob=0.77]\rwarmup:  18%|â–ˆâ–Š        | 185/1000 [00:05<00:06, 133.70it/s, 63 steps of size 1.41e-01. acc. prob=0.78]\rwarmup:  20%|â–ˆâ–ˆ        | 201/1000 [00:05<00:05, 140.32it/s, 31 steps of size 1.42e-01. acc. prob=0.78]\rwarmup:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:05<00:05, 147.72it/s, 31 steps of size 1.64e-01. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:05<00:04, 153.60it/s, 63 steps of size 1.38e-01. acc. prob=0.78]\rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:05<00:04, 162.53it/s, 63 steps of size 1.29e-01. acc. prob=0.78]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:05<00:04, 155.39it/s, 15 steps of size 3.96e-01. acc. prob=0.78]\rwarmup:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:06<00:04, 152.52it/s, 31 steps of size 1.53e-01. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:06<00:04, 166.23it/s, 31 steps of size 1.15e-01. acc. prob=0.78]\rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:06<00:04, 166.73it/s, 31 steps of size 1.77e-01. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:06<00:03, 165.18it/s, 31 steps of size 8.23e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:06<00:04, 159.33it/s, 31 steps of size 1.34e-01. acc. prob=0.78]\rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:06<00:03, 162.19it/s, 31 steps of size 1.85e-01. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [00:06<00:03, 163.78it/s, 31 steps of size 1.88e-01. acc. prob=0.78]\rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [00:06<00:03, 167.00it/s, 31 steps of size 1.71e-01. acc. prob=0.78]\rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [00:06<00:03, 163.72it/s, 31 steps of size 8.71e-02. acc. prob=0.78]\rwarmup:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [00:06<00:03, 157.95it/s, 31 steps of size 2.10e-01. acc. prob=0.79]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [00:07<00:03, 141.83it/s, 63 steps of size 1.54e-01. acc. prob=0.78]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [00:07<00:04, 130.61it/s, 127 steps of size 9.02e-02. acc. prob=0.78]\rwarmup:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [00:07<00:04, 125.05it/s, 31 steps of size 1.67e-01. acc. prob=0.78] \rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [00:07<00:03, 130.60it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [00:07<00:03, 138.78it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [00:07<00:03, 143.16it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [00:07<00:03, 146.90it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [00:07<00:02, 149.84it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [00:08<00:02, 148.14it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [00:08<00:02, 148.45it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [00:08<00:02, 147.17it/s, 31 steps of size 1.33e-01. acc. prob=0.88]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [00:08<00:02, 148.14it/s, 31 steps of size 1.33e-01. acc. prob=0.88]\rsample:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [00:08<00:02, 149.88it/s, 31 steps of size 1.33e-01. acc. prob=0.88]\rsample:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [00:08<00:02, 150.29it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [00:08<00:02, 151.83it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [00:08<00:01, 152.75it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [00:08<00:01, 154.13it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [00:08<00:01, 153.94it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [00:09<00:01, 153.57it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [00:09<00:01, 154.41it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [00:09<00:01, 151.51it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [00:09<00:01, 152.97it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [00:09<00:01, 152.69it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [00:09<00:01, 152.66it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [00:09<00:01, 152.96it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [00:09<00:00, 147.99it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [00:09<00:00, 149.93it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [00:10<00:00, 150.06it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [00:10<00:00, 150.95it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [00:10<00:00, 150.37it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [00:10<00:00, 148.06it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [00:10<00:00, 145.21it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [00:10<00:00, 146.03it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [00:10<00:00, 149.36it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [00:12<00:00, 24.45it/s, 31 steps of size 1.33e-01. acc. prob=0.89] \rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:12<00:00, 79.79it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Lambda[0]</th>\n      <td>1.39</td>\n      <td>0.36</td>\n      <td>0.83</td>\n      <td>1.91</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>537.43</td>\n      <td>299.02</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[1]</th>\n      <td>1.76</td>\n      <td>0.40</td>\n      <td>1.13</td>\n      <td>2.34</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>713.23</td>\n      <td>285.08</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[2]</th>\n      <td>1.57</td>\n      <td>0.42</td>\n      <td>0.92</td>\n      <td>2.17</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>826.04</td>\n      <td>397.99</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3]</th>\n      <td>1.47</td>\n      <td>0.36</td>\n      <td>0.86</td>\n      <td>1.97</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>850.98</td>\n      <td>414.15</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[4]</th>\n      <td>1.53</td>\n      <td>0.38</td>\n      <td>0.95</td>\n      <td>2.14</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>630.33</td>\n      <td>428.36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Lambda[3398]</th>\n      <td>3.04</td>\n      <td>0.77</td>\n      <td>1.93</td>\n      <td>4.19</td>\n      <td>0.03</td>\n      <td>0.04</td>\n      <td>854.46</td>\n      <td>465.41</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3399]</th>\n      <td>2.86</td>\n      <td>0.73</td>\n      <td>1.76</td>\n      <td>3.95</td>\n      <td>0.03</td>\n      <td>0.04</td>\n      <td>341.11</td>\n      <td>200.88</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[0]</th>\n      <td>-0.42</td>\n      <td>0.01</td>\n      <td>-0.44</td>\n      <td>-0.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>51.13</td>\n      <td>111.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>-2.78</td>\n      <td>0.03</td>\n      <td>-2.83</td>\n      <td>-2.73</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>63.60</td>\n      <td>188.73</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>phi[0]</th>\n      <td>16.10</td>\n      <td>1.92</td>\n      <td>13.30</td>\n      <td>19.15</td>\n      <td>0.55</td>\n      <td>0.14</td>\n      <td>12.83</td>\n      <td>101.44</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3403 rows Ã— 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(m$load$sim_gamma_poisson(only_path=T), sep = '')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  phi = bi.dist.exponential(1, name='phi', shape=c(1))\n  mu = jnp$exp(log_days + alpha + beta * monastery)\n  Lambda =  bi.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n  # Likelihood\n  bi.dist.poisson(rate=Lambda, obs=y)\n}\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n```\n\n### Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_gamma_poisson(only_path = true)\nm.data(data_path, sep=',')\n\n# Define model ------------------------------------------------\n@BI function model(log_days, monastery, y)\n    a = m.dist.normal(0, 1, name = \"a\", shape=(1,))\n    b = m.dist.normal(0, 1, name = \"b\", shape=(1,))\n    phi = m.dist.exponential(1, name = \"phi\", shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = \"Lambda\")\n    m.dist.poisson(rate = Lambda, obs=y)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details\n### *Bayesian model*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda_i)\n$$\n\n$$\n\\lambda_i \\sim \\text{Gamma}(\\mu_i \\phi, \\phi)\n$$\n\n$$\n\\log(\\mu_i) = \\text{rates}_i + \\alpha + \\beta X_i\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\phi \\sim \\text{Exponential}(1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*. \n  \n- $\\lambda_i$ is the rate parameter of the Poisson distribution for observation *i*, assuming that each Poisson count observation has its own $rate_i$.\n  \n- $\\mu_i$ is the mean rate parameter.\n- \n- $\\phi$ controls the level of overdispersion in the rates.\n  \n- $\\alpha$ is the intercept term.\n  \n- $\\beta$ is the regression coefficient.\n  \n- $X_i$ is the value of the predictor variable for observation *i*.\n\n\n## Notes\n\n::: callout-note \n- We can apply multiple variables similarly as in [chapter 2](2.&#32;Multiple&#32;continuous&#32;variables.qmd).\n\n- We can apply interaction terms similarly as in [chapter 3](3.&#32;Interaction&#32;between&#32;Continuous&#32;Variables.qmd).\n\n- We can apply categorical variables similarly as in [chapter 4](4.&#32;Categorical&#32;variable.qmd).\n:::\n\n## Reference(s)\n\n",
>>>>>>> 9ba7a667d655f408e7b5a9c3897d8b55113721d1
    "supporting": [
      "8. Gamma-Poisson_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}