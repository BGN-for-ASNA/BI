{
  "hash": "746d740843610b4a3e0f18bc03ea9844",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gamma-Poisson Model\"\ndescription: \"An extension of the Poisson model for count data that exhibits overdispersion.\"\ncategories: [Regression, Count Data, GLM]\nimage: \"Figures/8.png\"\norder: 10\n---\n\n## General Principles \nTo model the relationship between a count outcome variable and one or more independent variables with [<span style=\"color:#0D6EFD\">overdispersion ðŸ›ˆ</span>]{#overdispersion}, we can use the _Negative Binomial model_. \n\n## Considerations\n::: callout-caution \n- We have the same considerations as for the [Poisson model](7.&#32;Poisson&#32;model.qmd).\n  \n- Overdispersion is handled because the Gamma-Poisson model assumes that each Poisson count observation has its own rate. This is an additional parameter specified in the model (in the code, it is `log_days`).\n:::\n \n## Example\nBelow is an example code snippet demonstrating a Bayesian Gamma-Poisson model using the Bayesian Inference (BI) package. \n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\n\n# Setup device ------------------------------------------------\nm = bi(platform='cpu') # Import\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = files('BI.resources.data') / 'Sim dat Gamma poisson.csv'\nm.data(data_path, sep=',') \nm.data_to_model(['log_days', 'monastery', 'y']) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\ndef model(log_days, monastery, y):\n    a = m.dist.normal(0, 1, name = 'a', shape=(1,))\n    b = m.dist.normal(0, 1, name = 'b', shape=(1,))\n    phi = m.dist.exponential(1, name = 'phi', shape=(1,))\n    mu = jnp.exp(log_days + a + b * monastery)\n    Lambda =  m.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n    m.dist.poisson(rate = Lambda, obs=y)\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<24:25,  1.47s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|          | 6/1000 [00:01<03:18,  5.01it/s, 511 steps of size 1.14e-02. acc. prob=0.43]\rwarmup:   1%|          | 11/1000 [00:01<01:45,  9.39it/s, 511 steps of size 5.90e-03. acc. prob=0.59]\rwarmup:   2%|â–         | 16/1000 [00:01<01:17, 12.68it/s, 1023 steps of size 4.14e-03. acc. prob=0.64]\rwarmup:   2%|â–         | 19/1000 [00:02<01:06, 14.68it/s, 127 steps of size 2.52e-02. acc. prob=0.70] \rwarmup:   2%|â–         | 22/1000 [00:02<01:06, 14.81it/s, 511 steps of size 8.18e-03. acc. prob=0.69]\rwarmup:   2%|â–Ž         | 25/1000 [00:02<01:03, 15.29it/s, 1023 steps of size 3.45e-03. acc. prob=0.69]\rwarmup:   3%|â–Ž         | 28/1000 [00:02<00:56, 17.26it/s, 127 steps of size 1.13e-02. acc. prob=0.71] \rwarmup:   3%|â–Ž         | 33/1000 [00:02<00:50, 19.07it/s, 1023 steps of size 3.34e-03. acc. prob=0.71]\rwarmup:   4%|â–Ž         | 36/1000 [00:02<00:47, 20.33it/s, 127 steps of size 6.20e-03. acc. prob=0.72] \rwarmup:   4%|â–         | 40/1000 [00:03<00:41, 23.23it/s, 511 steps of size 4.09e-03. acc. prob=0.72]\rwarmup:   4%|â–         | 44/1000 [00:03<00:36, 26.31it/s, 255 steps of size 6.81e-03. acc. prob=0.73]\rwarmup:   5%|â–         | 48/1000 [00:03<00:34, 27.99it/s, 511 steps of size 3.45e-03. acc. prob=0.73]\rwarmup:   5%|â–Œ         | 52/1000 [00:03<00:33, 28.21it/s, 127 steps of size 9.51e-03. acc. prob=0.74]\rwarmup:   6%|â–Œ         | 57/1000 [00:03<00:29, 32.44it/s, 255 steps of size 1.09e-02. acc. prob=0.75]\rwarmup:   6%|â–Œ         | 61/1000 [00:03<00:30, 31.10it/s, 255 steps of size 9.98e-03. acc. prob=0.75]\rwarmup:   7%|â–‹         | 66/1000 [00:03<00:26, 35.61it/s, 3 steps of size 2.28e-03. acc. prob=0.74]  \rwarmup:   7%|â–‹         | 70/1000 [00:03<00:28, 32.10it/s, 127 steps of size 7.37e-03. acc. prob=0.75]\rwarmup:   7%|â–‹         | 74/1000 [00:04<00:28, 32.12it/s, 255 steps of size 4.66e-03. acc. prob=0.75]\rwarmup:   8%|â–Š         | 78/1000 [00:04<00:28, 32.33it/s, 511 steps of size 3.81e-03. acc. prob=0.75]\rwarmup:   8%|â–Š         | 83/1000 [00:04<00:26, 34.87it/s, 255 steps of size 5.87e-03. acc. prob=0.75]\rwarmup:   9%|â–‰         | 88/1000 [00:04<00:24, 36.58it/s, 255 steps of size 5.61e-03. acc. prob=0.76]\rwarmup:   9%|â–‰         | 92/1000 [00:04<00:28, 31.86it/s, 255 steps of size 8.23e-03. acc. prob=0.76]\rwarmup:  10%|â–‰         | 96/1000 [00:04<00:28, 32.04it/s, 255 steps of size 5.14e-03. acc. prob=0.76]\rwarmup:  10%|â–ˆ         | 100/1000 [00:04<00:26, 33.35it/s, 127 steps of size 1.01e-02. acc. prob=0.76]\rwarmup:  11%|â–ˆ         | 107/1000 [00:04<00:21, 41.75it/s, 127 steps of size 7.50e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–        | 121/1000 [00:05<00:13, 66.53it/s, 31 steps of size 2.47e-01. acc. prob=0.77] \rwarmup:  14%|â–ˆâ–Ž        | 137/1000 [00:05<00:09, 91.01it/s, 63 steps of size 9.63e-02. acc. prob=0.77]\rwarmup:  15%|â–ˆâ–Œ        | 151/1000 [00:05<00:08, 104.06it/s, 31 steps of size 2.31e+00. acc. prob=0.78]\rwarmup:  17%|â–ˆâ–‹        | 169/1000 [00:05<00:06, 124.35it/s, 63 steps of size 1.21e-01. acc. prob=0.77]\rwarmup:  18%|â–ˆâ–Š        | 185/1000 [00:05<00:06, 132.32it/s, 63 steps of size 1.41e-01. acc. prob=0.78]\rwarmup:  20%|â–ˆâ–ˆ        | 201/1000 [00:05<00:05, 137.98it/s, 31 steps of size 1.42e-01. acc. prob=0.78]\rwarmup:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:05<00:05, 142.96it/s, 63 steps of size 1.32e-01. acc. prob=0.78]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:05<00:05, 150.44it/s, 15 steps of size 1.22e-01. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:05<00:04, 160.33it/s, 15 steps of size 7.93e-02. acc. prob=0.78]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:05<00:04, 148.40it/s, 31 steps of size 1.51e-01. acc. prob=0.78]\rwarmup:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:06<00:04, 148.73it/s, 63 steps of size 8.72e-02. acc. prob=0.78]\rwarmup:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:06<00:04, 158.78it/s, 15 steps of size 1.35e-01. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:06<00:04, 159.62it/s, 31 steps of size 1.37e-01. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:06<00:04, 156.94it/s, 15 steps of size 1.54e-01. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:06<00:04, 154.56it/s, 31 steps of size 1.32e-01. acc. prob=0.78]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:06<00:04, 156.50it/s, 31 steps of size 1.78e-01. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:06<00:03, 157.79it/s, 31 steps of size 1.48e-01. acc. prob=0.78]\rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [00:06<00:03, 160.75it/s, 31 steps of size 1.88e-01. acc. prob=0.78]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [00:06<00:03, 163.03it/s, 63 steps of size 1.12e-01. acc. prob=0.78]\rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [00:07<00:03, 161.75it/s, 31 steps of size 1.11e-01. acc. prob=0.78]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [00:07<00:03, 163.73it/s, 12 steps of size 3.40e-02. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [00:07<00:03, 143.41it/s, 63 steps of size 1.48e-01. acc. prob=0.78]\rwarmup:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [00:07<00:03, 134.72it/s, 31 steps of size 1.41e-01. acc. prob=0.78]\rsample:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [00:07<00:03, 137.47it/s, 31 steps of size 1.33e-01. acc. prob=0.95]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [00:07<00:03, 143.96it/s, 31 steps of size 1.33e-01. acc. prob=0.91]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [00:07<00:03, 149.51it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [00:07<00:02, 153.60it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [00:07<00:02, 157.00it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [00:08<00:02, 158.15it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [00:08<00:02, 156.77it/s, 31 steps of size 1.33e-01. acc. prob=0.87]\rsample:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [00:08<00:02, 157.29it/s, 31 steps of size 1.33e-01. acc. prob=0.88]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [00:08<00:02, 159.00it/s, 31 steps of size 1.33e-01. acc. prob=0.88]\rsample:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [00:08<00:02, 160.44it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [00:08<00:02, 159.42it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [00:08<00:01, 161.48it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [00:08<00:01, 162.11it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [00:08<00:01, 162.47it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [00:08<00:01, 159.44it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [00:09<00:01, 158.69it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [00:09<00:01, 159.56it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [00:09<00:01, 156.37it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [00:09<00:01, 157.55it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [00:09<00:01, 160.93it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [00:09<00:00, 163.94it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [00:09<00:00, 163.92it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [00:09<00:00, 165.33it/s, 31 steps of size 1.33e-01. acc. prob=0.90]\rsample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [00:09<00:00, 162.77it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [00:10<00:00, 163.73it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [00:10<00:00, 162.84it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [00:10<00:00, 162.60it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [00:10<00:00, 162.13it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [00:10<00:00, 157.13it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [00:10<00:00, 149.00it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:10<00:00, 93.96it/s, 31 steps of size 1.33e-01. acc. prob=0.89]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Lambda[0]</th>\n      <td>1.39</td>\n      <td>0.36</td>\n      <td>0.83</td>\n      <td>1.91</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>537.43</td>\n      <td>299.02</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[1]</th>\n      <td>1.76</td>\n      <td>0.40</td>\n      <td>1.13</td>\n      <td>2.34</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>713.23</td>\n      <td>285.08</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[2]</th>\n      <td>1.57</td>\n      <td>0.42</td>\n      <td>0.92</td>\n      <td>2.17</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>826.04</td>\n      <td>397.99</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3]</th>\n      <td>1.47</td>\n      <td>0.36</td>\n      <td>0.86</td>\n      <td>1.97</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>850.98</td>\n      <td>414.15</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[4]</th>\n      <td>1.53</td>\n      <td>0.38</td>\n      <td>0.95</td>\n      <td>2.14</td>\n      <td>0.01</td>\n      <td>0.02</td>\n      <td>630.33</td>\n      <td>428.36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Lambda[3398]</th>\n      <td>3.04</td>\n      <td>0.77</td>\n      <td>1.93</td>\n      <td>4.19</td>\n      <td>0.03</td>\n      <td>0.04</td>\n      <td>854.46</td>\n      <td>465.41</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Lambda[3399]</th>\n      <td>2.86</td>\n      <td>0.73</td>\n      <td>1.76</td>\n      <td>3.95</td>\n      <td>0.03</td>\n      <td>0.04</td>\n      <td>341.11</td>\n      <td>200.88</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[0]</th>\n      <td>-0.42</td>\n      <td>0.01</td>\n      <td>-0.44</td>\n      <td>-0.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>51.13</td>\n      <td>111.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>-2.78</td>\n      <td>0.03</td>\n      <td>-2.83</td>\n      <td>-2.73</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>63.60</td>\n      <td>188.73</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>phi[0]</th>\n      <td>16.10</td>\n      <td>1.92</td>\n      <td>13.30</td>\n      <td>19.15</td>\n      <td>0.55</td>\n      <td>0.14</td>\n      <td>12.83</td>\n      <td>101.44</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3403 rows Ã— 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\n\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nm$data(paste(system.file(package = \"BayesianInference\"),\"/data/Sim dat Gamma poisson.csv\", sep = ''), sep=',')\nm$data_to_model(list('log_days', 'monastery', 'y' )) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(log_days, monastery, y){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape=c(1))\n  beta = bi.dist.normal(0, 1, name='beta', shape=c(1))\n  phi = bi.dist.exponential(1, name='phi', shape=c(1))\n  mu = jnp$exp(log_days + alpha + beta * monastery)\n  Lambda =  bi.dist.gamma(rate = mu*phi, concentration = phi, name = 'Lambda')\n  # Likelihood\n  bi.dist.poisson(rate=Lambda, obs=y)\n}\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n\n\n```\n:::\n\n## Mathematical Details\n### *Bayesian model*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distributions as follows:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda_i)\n$$\n\n$$\n\\lambda_i \\sim \\text{Gamma}(\\mu_i \\phi, \\phi)\n$$\n\n$$\n\\log(\\mu_i) = \\text{rates}_i + \\alpha + \\beta X_i\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\phi \\sim \\text{Exponential}(1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*. \n  \n- $\\lambda_i$ is the rate parameter of the Poisson distribution for observation *i*, assuming that each Poisson count observation has its own $rate_i$.\n  \n- $\\mu_i$ is the mean rate parameter.\n- \n- $\\phi$ controls the level of overdispersion in the rates.\n  \n- $\\alpha$ is the intercept term.\n  \n- $\\beta$ is the regression coefficient.\n  \n- $X_i$ is the value of the predictor variable for observation *i*.\n\n\n## Notes\n\n::: callout-note \n- We can apply multiple variables similarly as in [chapter 2](2.&#32;Multiple&#32;continuous&#32;variables.qmd).\n\n- We can apply interaction terms similarly as in [chapter 3](3.&#32;Interaction&#32;between&#32;Continuous&#32;Variables.qmd).\n\n- We can apply categorical variables similarly as in [chapter 4](4.&#32;Categorical&#32;variable.qmd).\n:::\n\n## Reference(s)\n\n",
    "supporting": [
      "8. Gamma-Poisson_files/figure-pdf"
    ],
    "filters": []
  }
}