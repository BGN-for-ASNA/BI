{
  "hash": "e8c092f45fda45902952fd903f8e670c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multivariate Linear Regression\"\ndescription: \"Extending linear regression to model a continuous outcome using multiple predictor variables.\"\nimage: \"Figures/multiple_regression_3D.gif\"\ncategories: [Regression]\norder: 3\n---\n\n\n\n\n\n\n\n## General Principles\nTo study relationships between multiple continuous independent variables (e.g., the effect of weight and age on height), we can use a multiple regression approach. Essentially, we extend [Linear Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd) by adding a regression coefficient $\\beta_x$ for each continuous variable (e.g., $\\beta_{weight}$ and $\\beta_{age}$).\n\n\n## Considerations\n::: callout-note \n- We have the same considerations as for the [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n\n- The model interpretation of the regression coefficients $\\beta_x$ is considered for fixed values of the other independent variable(s)' regression coefficients‚Äîi.e., for a given age, $\\beta_{weight}$ represents the expected change in the dependent variable (height) for each one-unit increase in weight, holding all other variables (e.g., age) constant.\n\n:::\n\n## Example\nBelow is example code demonstrating Bayesian multiple linear regression using the Bayesian Inference (BI) package. Data consist of three continuous variables (*height*, *weight*, *age*), and the goal is to estimate the effect of *weight* and *age* on *height*. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nfrom importlib.resources import files\n# Import\ndata_path = m.load.howell1(only_path = True)\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age > 18] # Subset data to adults\nm.scale(['weight', 'age']) # Normalize\n\n# Define model ------------------------------------------------\ndef model(height, weight, age):\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = 'alpha')    \n    beta1 = m.dist.normal(0, 0.5, name = 'beta1')\n    beta2 = m.dist.normal(0, 0.5, name = 'beta2')\n    sigma = m.dist.uniform(0, 50, name = 'sigma')\n    # Likelihood\n    m.dist.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\n\n# Run MCMC ------------------------------------------------\nm.fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<08:50,  1.88it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  28%|‚ñà‚ñà‚ñä       | 275/1000 [00:00<00:01, 581.63it/s, 31 steps of size 2.48e-01. acc. prob=0.78]\rsample:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/1000 [00:00<00:00, 1070.96it/s, 3 steps of size 6.01e-01. acc. prob=0.92]\rsample:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/1000 [00:00<00:00, 1443.65it/s, 7 steps of size 6.01e-01. acc. prob=0.92]\rsample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1099.04it/s, 3 steps of size 6.01e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alpha</th>\n      <td>5.20</td>\n      <td>0.49</td>\n      <td>4.46</td>\n      <td>6.06</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>469.94</td>\n      <td>349.45</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>beta1</th>\n      <td>0.20</td>\n      <td>0.51</td>\n      <td>-0.60</td>\n      <td>1.03</td>\n      <td>0.02</td>\n      <td>0.03</td>\n      <td>570.81</td>\n      <td>264.55</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>beta2</th>\n      <td>-0.02</td>\n      <td>0.49</td>\n      <td>-0.89</td>\n      <td>0.69</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>576.00</td>\n      <td>338.15</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>sigma</th>\n      <td>49.98</td>\n      <td>0.02</td>\n      <td>49.96</td>\n      <td>50.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>579.62</td>\n      <td>271.13</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\nm$data(m$load$howell1(only_path = T), sep=';')# Import\nm$df = m$df[m$df$age > 18,] # Subset data to adults\nm$scale(list('weight', 'age')) # Normalize\nm$data_to_model(list('weight', 'height', 'age')) # Send to model (convert to jax array)\n\n# Define model ------------------------------------------------\nmodel <- function(height, weight, age){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 0.5, name = 'a')\n  beta1 = bi.dist.normal(0, 0.5, name = 'b1')\n  beta2 = bi.dist.normal(0, 0.5, name = 'b2')   \n  sigma = bi.dist.uniform(0, 50, name = 's')\n  # Likelihood\n  bi.dist.normal(alpha + beta1 * weight + beta2 * age, sigma, obs=height)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distributions\n\n```\n\n### Julia\n```Julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.howell1(only_path = true)\nm.data(data_path, sep=';') \nm.df = m.df[m.df.age > 18] # Subset data to adults\nm.scale([\"weight\", \"age\"]) # Normalize\n\n# Define model ------------------------------------------------\n@BI function model(height, weight, age)\n    # Parameter prior distributions\n    alpha = m.dist.normal(0, 0.5, name = \"alpha\")    \n    beta1 = m.dist.normal(0, 0.5, name = \"beta1\")\n    beta2 = m.dist.normal(0, 0.5, name = \"beta2\")\n    sigma = m.dist.uniform(0, 50, name = \"sigma\")\n    # Likelihood\n    m.dist.normal(alpha + beta1 * weight + beta2 * age, sigma, obs = height)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n::: callout-caution\nFor R users, if you create the regression coefficient in a single call:\n\n```python\nbetas = bi.dist.normal(0, 0.5, name = 'regression_coefficients', shape = (2,))\n```\n\nyou need to index them starting by 0:\n\n```python\n m$normal(alpha + betas[0] * weight + betas[1] * age, sigma, obs=height)\n``` \n:::\n\n## Mathematical Details\n### *Frequentist formulation*\nWe model the relationship between the independent variables $(X_{1i}, X_{2i}, ..., X_{[K,i]})$ and the dependent variable _Y_ using the following equation:\n\n$$\nùëå_i = \\alpha +\\beta_1  ùëã_{[1,i]} + \\beta_2  ùëã_{[2,i]} + ... + \\beta_n  ùëã_{[K,i]} + \\epsilon_i\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*.\n  \n- $\\alpha$ is the intercept term.\n  \n- $X_{[1,i]}$, $X_{[2,i]}$, ..., $X_{[K,i]}$ are the values of the independent variables for observation *i*.\n  \n- $\\beta_1$, $\\beta_2$, ..., $\\beta_K$ are the regression coefficients.\n  \n- $\\epsilon_i$ is the error term for observation *i*, and the vector of the error terms, $\\epsilon$, are assumed to be independent and identically distributed.\n  \n\n### *Bayesian formulation*\nIn the Bayesian formulation, we define each parameter with [<span style=\"color:#0D6EFD\">priors üõà</span>]{#prior}. We can express the Bayesian model as follows:\n\n$$\nùëå_i \\sim \\text{Normal}(\\alpha + \\sum_{k=1}^K  \\beta_k  X_{[K,i]}, œÉ)\n$$\n\n$$\n\\alpha \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\beta_k \\sim \\text{Normal}(0,1)\n$$\n\n$$\nœÉ \\sim \\text{Uniform}(0, 50)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent variable for observation *i*. \n  \n- $\\alpha$ is the intercept term, which in this case has a unit-normal prior.\n  \n- $\\beta_k$ are slope coefficients for the _K_ distinct independent variables, which also have unit-normal priors.\n  \n- $X_{[1,i]}$, $X_{[2,i]}$, ..., $X_{[K,i]}$ are the values of the independent variables for observation *i*.\n  \n- $\\sigma$ is a standard deviation parameter, which here has a Uniform prior that constrains it to be positive.\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "2. Multiple continuous Variables_files/figure-pdf"
    ],
    "filters": []
  }
}