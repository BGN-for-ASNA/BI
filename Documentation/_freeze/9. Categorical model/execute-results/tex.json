{
  "hash": "085b69043cd44ce5229f643d1c53bc35",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Categorical Model\"\ndescription: \"Modeling a categorical dependent variable with more than two nominal outcomes.\"\ncategories: [Classification, Regression, GLM]\nimage: \"Figures/9.png\"\norder: 11\n---\n\n## General Principles\nTo model the relationship between a outcome variable in which each observation is a single choice from a set of more than two categories and one or more independent variables, we can use a *Categorical* model. \n\n\n## Considerations\n\n::: callout-caution \n- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).\n  \n- One way to interpret a *Categorical* model is to consider that we need to build $K - 1$ linear models, where $K$ is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}. To do this, we convert the regression outputs using the [<span style=\"color:#0D6EFD\">softmax function ðŸ›ˆ</span>]{#softmax} (see the \"jax.nn.softmax\" line in the code). \n\n- The intercept $\\alpha$ captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.\n  \n- On the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients $\\beta$ are shared across categories.\n\n- The relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category.\n\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian Categorical model using the Bayesian Inference (BI) package. This example is based on @mcelreath2018statistical.\n\n::: {.panel-tabset group=\"language\"}\n### Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nimport jax\n# Setup device ------------------------------------------------\nm = bi('cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\nfrom importlib.resources import files\ndata_path = m.load.sim_multinomial(only_path=True)\nm.data(data_path, sep=',') \n\n# Define model ------------------------------------------------\ndef model(career, income):\n    a = m.dist.normal(0, 1, shape=(2,), name = 'a')\n    b = m.dist.half_normal(0.5, shape=(1,), name = 'b')\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    s_3 = [0] #pivot\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    m.dist.categorical(probs=p, obs=career)\n\n# Run sampler ------------------------------------------------ \nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<19:11,  1.15s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:  12%|â–ˆâ–        | 115/1000 [00:01<00:07, 126.37it/s, 31 steps of size 1.76e-01. acc. prob=0.77]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:01<00:02, 266.74it/s, 27 steps of size 1.88e-01. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:01<00:01, 419.64it/s, 5 steps of size 3.23e-01. acc. prob=0.78] \rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [00:01<00:00, 562.43it/s, 7 steps of size 6.42e-01. acc. prob=0.79]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [00:01<00:00, 674.09it/s, 15 steps of size 1.86e-01. acc. prob=0.94]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [00:01<00:00, 797.54it/s, 15 steps of size 1.86e-01. acc. prob=0.94]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [00:01<00:00, 891.52it/s, 11 steps of size 1.86e-01. acc. prob=0.91]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [00:01<00:00, 951.89it/s, 3 steps of size 1.86e-01. acc. prob=0.92] \rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 495.00it/s, 7 steps of size 1.86e-01. acc. prob=0.92]\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_5.5%</th>\n      <th>hdi_94.5%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a[0]</th>\n      <td>-2.09</td>\n      <td>0.27</td>\n      <td>-2.43</td>\n      <td>-1.67</td>\n      <td>0.03</td>\n      <td>0.03</td>\n      <td>85.36</td>\n      <td>78.74</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>a[1]</th>\n      <td>-1.56</td>\n      <td>0.16</td>\n      <td>-1.80</td>\n      <td>-1.30</td>\n      <td>0.02</td>\n      <td>0.01</td>\n      <td>101.63</td>\n      <td>28.52</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>b[0]</th>\n      <td>0.05</td>\n      <td>0.05</td>\n      <td>0.00</td>\n      <td>0.12</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>66.53</td>\n      <td>81.49</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### R\n```R\nlibrary(BayesianInference)\njax = reticulate::import('jax')\n\n# setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# import data ------------------------------------------------\nm$data(m$load$sim_multinomial(only_path=T), sep=',')\nkeys <- c(\"income\", \"career\")\nincome = unique(m$df$income)\nincome = income[order(income)]\nvalues <- list(jnp$array(as.integer(income)),jnp$array( as.integer(m$df$career)))\nm$data_on_model = py_dict(keys, values, convert = TRUE)\n\n# Define model ------------------------------------------------\nmodel <- function(income, career){\n  # Parameter prior distributions\n  alpha = bi.dist.normal(0, 1, name='alpha', shape = c(2))\n  beta = bi.dist.normal(0.5, name='beta')\n  \n  s_1 = alpha[0] + beta * income[0]\n  s_2 = alpha[1] + beta * income[1]\n  s_3 = 0 # reference category\n\n  p = jax$nn$softmax(jnp$stack(list(s_1, s_2, s_3)))\n\n  # Likelihood\n  m$dist$categorical(probs=p[career], obs=career)\n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm$summary() # Get posterior distribution\n\n```\n\n### Julia\n```julia\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Import Data & Data Manipulation ------------------------------------------------\n# Import\ndata_path = m.load.sim_multinomial(only_path = true)\nm.data(data_path, sep=',')\n\n# Define model ------------------------------------------------\n@BI function model(career, income)\n    a = m.dist.normal(0, 1, shape=(2,), name = \"a\")\n    b = m.dist.half_normal(0.5, shape=(1,), name = \"b\")\n    \n    # indexing works now because of the package update\n    s_1 = a[0] + b * income[0]\n    s_2 = a[1] + b * income[1]\n    \n    # âš ï¸ Use jnp.array to create a Python object, so [0] indexing works\n    s_3 = jnp.array([0.0]) \n    \n    # Now s_3[0] is valid because it calls Python's __getitem__(0)\n    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n    \n    m.dist.categorical(probs=p, obs=career)\nend\n\n# Run mcmc ------------------------------------------------\nm.fit(model)  # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nm.summary() # Get posterior distributions\n```\n:::\n\n## Mathematical Details \nWe can model a *Categorical* model using a $Categorical distribution$. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable \nð‘¦ with ð¾ categories, the multinomial likelihood function is:\n\n$$\nY_i \\sim \\text{Categorical}(\\theta_i) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n$$\n\nWhere:\n\n- $Y_i$ is the dependent categorical variable for observation *i* indicating the category of the observation.\n  \n- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n  \n- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex ðŸ›ˆ</span>]{#simplex}.\n  \n- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "9. Categorical model_files/figure-pdf"
    ],
    "filters": []
  }
}