{
  "hash": "cf1d89776b9ad88dbfbe61a6d7c60380",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dirichlet Process Mixture Models ðŸš§\"\ndescription: \"A non-parametric Bayesian clustering method that automatically determines the number of clusters.\"\ncategories: [Clustering, Unsupervised Learning, Non-parametric]\nimage: \"Figures/20.png\"\norder: 24\n---\n\n\n\n\n\n\n\n\n## General Principles\n\nTo discover group structures or clusters in data without pre-specifying the number of groups, we can use a **Dirichlet Process Mixture Model (DPMM)**. This is a unsupervised clustering method. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\n1.  **How many clusters (`K`) exist**: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\n2.  **The properties of each cluster**: For each inferred cluster, it estimates its center and its shape/spread.\n3.  **The assignment of each data point**: It determines the probability of each data point belonging to each cluster.\n\n## Considerations\n\n::: callout-caution\n-   A DPMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation called the [Stick-Breaking Process ðŸ›ˆ]{style=\"color:#0D6EFD\"}.\n\n-   The key parameters and their priors are:\n\n    - **Concentration** $\\alpha$: This single parameter controls the tendency to create new clusters. A low `Î±` favors fewer, larger clusters, while a high `Î±` allows for many smaller clusters. We typically place a `Gamma` prior on $\\alpha$ to learn its value from the data.\n- \n    - **Cluster Weights `w`**: Generated via the Stick-Breaking process from $\\alpha$. These are the probabilities of drawing a data point from any given cluster.\n  \n    - **Cluster Parameters (**$\\mu$, $\\sigma$): Each potential cluster has a mean $\\mu$ and a covariance matrix $\\sigma$. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, [14](14.%20Varying%20slopes.qmd)). Howver, if the data is one-dimensional, we use a univariate normal distribution.\n\n-   The model is often implemented in its [marginalized form ðŸ›ˆ]{style=\"color:#0D6EFD\"}. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n:::\n\n## Example\n\nBelow is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp\nfrom BI.Models.DPMM import mix_weights\nfrom sklearn.datasets import make_blobs\nimport numpyro\n\nm = bi()\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\n\n#  The model\ndef dpmm(data, T=10):\n    N, D = data.shape  # Number of features\n    data_mean = jnp.mean(data, axis=0)\n    data_std = jnp.std(data, axis=0)*2\n\n    # 1) stick-breaking weights\n    alpha = m.dist.gamma(1.0, 10.0,name='alpha')\n\n    with m.dist.plate(\"beta_plate\", T - 1):\n        beta = m.dist.beta(1, alpha)\n\n    w = numpyro.deterministic(\"w\",mix_weights(beta))\n\n    # 2) component parameters\n    with m.dist.plate(\"components\", T):\n        mu = m.dist.multivariate_normal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n        sigma = m.dist.log_normal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n\n        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n\n    # 3) Latent cluster assignments for each data point\n    with m.dist.plate(\"data\", N):\n        # Sample the assignment for each data point\n        z = m.dist.categorical(w, name = 'z') # shape (N,)  \n\n        # Sample the data point from the assigned component\n        m.dist.multivariate_normal(loc=mu[z], scale_tril=scale_tril[z],\n            obs=data, name = 'Y'\n        )  \n\nm.data_on_model = dict(data=data)\nm.fit(dpmm)  # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/sosa/work/BI/BI/Main/main.py:244: FutureWarning:\n\nSome algorithms will automatically enumerate the discrete latent site z of your model. In the future, enumerated sites need to be marked with `infer={'enumerate': 'parallel'}`.\n\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:00<15:47,  1.05it/s, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   2%|â–         | 17/1000 [00:01<00:45, 21.82it/s, 255 steps of size 2.92e-02. acc. prob=0.69]\rwarmup:   3%|â–Ž         | 27/1000 [00:01<00:44, 21.97it/s, 255 steps of size 2.70e-02. acc. prob=0.72]\rwarmup:   3%|â–Ž         | 34/1000 [00:01<00:40, 24.05it/s, 87 steps of size 6.93e-02. acc. prob=0.74] \rwarmup:   4%|â–         | 41/1000 [00:01<00:32, 29.20it/s, 511 steps of size 1.18e-02. acc. prob=0.73]\rwarmup:   5%|â–         | 47/1000 [00:01<00:28, 33.42it/s, 31 steps of size 2.64e-02. acc. prob=0.75] \rwarmup:   5%|â–Œ         | 53/1000 [00:02<00:25, 37.37it/s, 255 steps of size 3.67e-02. acc. prob=0.75]\rwarmup:   6%|â–Œ         | 59/1000 [00:02<00:25, 36.82it/s, 191 steps of size 3.85e-02. acc. prob=0.76]\rwarmup:   6%|â–‹         | 64/1000 [00:02<00:26, 35.39it/s, 127 steps of size 6.28e-02. acc. prob=0.76]\rwarmup:   7%|â–‹         | 71/1000 [00:02<00:21, 42.46it/s, 15 steps of size 7.27e-02. acc. prob=0.77] \rwarmup:  10%|â–ˆ         | 100/1000 [00:02<00:09, 98.03it/s, 47 steps of size 5.46e-02. acc. prob=0.77]\rwarmup:  12%|â–ˆâ–        | 122/1000 [00:02<00:07, 124.14it/s, 127 steps of size 7.93e-02. acc. prob=0.77]\rwarmup:  14%|â–ˆâ–        | 139/1000 [00:02<00:06, 134.23it/s, 63 steps of size 1.66e-01. acc. prob=0.78] \rwarmup:  16%|â–ˆâ–‹        | 163/1000 [00:02<00:05, 160.40it/s, 127 steps of size 4.42e-02. acc. prob=0.77]\rwarmup:  18%|â–ˆâ–Š        | 185/1000 [00:03<00:04, 175.67it/s, 15 steps of size 3.58e-01. acc. prob=0.78] \rwarmup:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:03<00:03, 205.50it/s, 47 steps of size 1.16e-01. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:03<00:03, 242.16it/s, 15 steps of size 1.67e-01. acc. prob=0.78]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:03<00:04, 152.93it/s, 31 steps of size 1.90e-01. acc. prob=0.78]\rwarmup:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:03<00:05, 128.38it/s, 191 steps of size 3.70e-02. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:03<00:05, 129.32it/s, 15 steps of size 1.84e-01. acc. prob=0.78] \rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:04<00:04, 141.90it/s, 63 steps of size 1.11e-01. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:04<00:05, 125.54it/s, 127 steps of size 7.23e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:04<00:04, 130.49it/s, 63 steps of size 1.12e-01. acc. prob=0.78] \rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [00:04<00:04, 137.06it/s, 31 steps of size 1.07e-01. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [00:04<00:04, 125.24it/s, 63 steps of size 7.86e-02. acc. prob=0.78]\rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [00:04<00:05, 116.00it/s, 127 steps of size 4.86e-02. acc. prob=0.78]\rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [00:04<00:04, 135.04it/s, 31 steps of size 7.65e-02. acc. prob=0.78] \rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [00:04<00:04, 133.63it/s, 63 steps of size 4.43e-02. acc. prob=0.78]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [00:05<00:04, 116.01it/s, 63 steps of size 1.11e-01. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [00:05<00:06, 81.14it/s, 255 steps of size 6.92e-02. acc. prob=0.78]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [00:05<00:06, 75.64it/s, 63 steps of size 9.80e-02. acc. prob=0.78] \rwarmup:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [00:05<00:07, 71.20it/s, 127 steps of size 5.11e-02. acc. prob=0.78]\rwarmup:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [00:05<00:07, 70.51it/s, 63 steps of size 5.44e-02. acc. prob=0.78] \rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [00:05<00:06, 76.17it/s, 63 steps of size 5.44e-02. acc. prob=0.92]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [00:06<00:05, 83.54it/s, 63 steps of size 5.44e-02. acc. prob=0.93]\rsample:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [00:06<00:05, 86.36it/s, 127 steps of size 5.44e-02. acc. prob=0.94]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [00:06<00:05, 87.76it/s, 63 steps of size 5.44e-02. acc. prob=0.93] \rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [00:06<00:05, 79.53it/s, 63 steps of size 5.44e-02. acc. prob=0.92]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [00:06<00:05, 77.16it/s, 63 steps of size 5.44e-02. acc. prob=0.91]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [00:06<00:06, 70.76it/s, 127 steps of size 5.44e-02. acc. prob=0.91]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [00:06<00:05, 74.86it/s, 63 steps of size 5.44e-02. acc. prob=0.89] \rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [00:06<00:05, 72.67it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [00:07<00:05, 72.70it/s, 127 steps of size 5.44e-02. acc. prob=0.83]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [00:07<00:05, 69.84it/s, 63 steps of size 5.44e-02. acc. prob=0.81] \rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [00:07<00:05, 66.18it/s, 575 steps of size 5.44e-02. acc. prob=0.79]\rsample:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [00:07<00:05, 70.79it/s, 63 steps of size 5.44e-02. acc. prob=0.78] \rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [00:07<00:04, 78.25it/s, 63 steps of size 5.44e-02. acc. prob=0.79]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [00:07<00:04, 88.60it/s, 63 steps of size 5.44e-02. acc. prob=0.81]\rsample:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [00:07<00:03, 93.37it/s, 127 steps of size 5.44e-02. acc. prob=0.81]\rsample:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [00:07<00:03, 94.56it/s, 63 steps of size 5.44e-02. acc. prob=0.81] \rsample:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [00:07<00:03, 99.16it/s, 63 steps of size 5.44e-02. acc. prob=0.82]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [00:08<00:03, 101.68it/s, 63 steps of size 5.44e-02. acc. prob=0.82]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [00:08<00:02, 105.92it/s, 63 steps of size 5.44e-02. acc. prob=0.83]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [00:08<00:02, 107.87it/s, 63 steps of size 5.44e-02. acc. prob=0.84]\rsample:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [00:08<00:02, 109.23it/s, 63 steps of size 5.44e-02. acc. prob=0.84]\rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [00:08<00:02, 109.65it/s, 63 steps of size 5.44e-02. acc. prob=0.85]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [00:08<00:02, 106.86it/s, 63 steps of size 5.44e-02. acc. prob=0.85]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [00:08<00:02, 103.11it/s, 63 steps of size 5.44e-02. acc. prob=0.85]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [00:08<00:02, 97.72it/s, 63 steps of size 5.44e-02. acc. prob=0.86] \rsample:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [00:08<00:02, 101.51it/s, 63 steps of size 5.44e-02. acc. prob=0.85]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [00:09<00:02, 93.90it/s, 63 steps of size 5.44e-02. acc. prob=0.85] \rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [00:09<00:02, 98.90it/s, 63 steps of size 5.44e-02. acc. prob=0.85]\rsample:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [00:09<00:01, 101.46it/s, 127 steps of size 5.44e-02. acc. prob=0.86]\rsample:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [00:09<00:01, 105.53it/s, 63 steps of size 5.44e-02. acc. prob=0.86] \rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [00:09<00:01, 105.40it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [00:09<00:01, 105.17it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [00:09<00:01, 107.06it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [00:09<00:01, 110.05it/s, 63 steps of size 5.44e-02. acc. prob=0.87]\rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [00:09<00:01, 108.72it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [00:10<00:00, 108.22it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [00:10<00:00, 108.51it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [00:10<00:00, 100.97it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [00:10<00:00, 102.59it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [00:10<00:00, 103.73it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [00:10<00:00, 104.40it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [00:10<00:00, 105.97it/s, 63 steps of size 5.44e-02. acc. prob=0.86]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [00:10<00:00, 106.68it/s, 63 steps of size 5.44e-02. acc. prob=0.87]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [00:10<00:00, 105.89it/s, 63 steps of size 5.44e-02. acc. prob=0.87]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [00:10<00:00, 101.59it/s, 63 steps of size 5.44e-02. acc. prob=0.87]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:11<00:00, 90.47it/s, 127 steps of size 5.44e-02. acc. prob=0.87]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nâš ï¸This function is still in development. Use it with caution. âš ï¸\nâš ï¸This function is still in development. Use it with caution. âš ï¸\nModel found 8 clusters.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](21. DPMM_files/figure-pdf/cell-2-output-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n## R\n\n``` r\n\n```\n![](travaux-routiers.png){fig-align=\"center\"}\n\n:::\n\n## Mathematical Details\n\nThe process involves two steps: first, assigning the data point to a cluster, and second, drawing it from that cluster's specific distribution. We use a truncation level $K$ as a finite approximation for the infinite number of possible clusters in a true Dirichlet Process.\n\n$$\n\\begin{aligned}\n\\begin{pmatrix}\nY_{i,1} \\\\\n\\vdots \\\\\nY_{i,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\n\\mu_{z_i,1} \\\\\n\\vdots \\\\\n\\mu_{z_i,D}\n\\end{pmatrix},\n\\,\n\\Sigma_{z_i}\n\\right) \\\\\n\\\\\n\\begin{pmatrix}\n\\mu_{k,1} \\\\\n\\vdots \\\\\n\\mu_{k,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\nA{k,1} \\\\\n\\vdots \\\\\nA{k,D}\n\\end{pmatrix},\n\\, B\n\\right) \\\\\n\\\\\n\\Sigma_k &= \\sigma_k \\Omega_k \\sigma_k \\\\\n\\\\\n\\sigma_k &\\sim \\text{HalfCauchy}(1) \\\\\n\\\\\n\\Omega_k &\\sim \\text{LKJ}(2) \\\\\n\\\\\nz_{i} &\\sim \\text{Categorical}(\\pi) \\\\\n\\\\\n\\pi  &= \\text{StickBreaking}(\\beta_1, ..., \\beta_{K}) \\\\\n\\\\\n\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n\\\\\n\\alpha &\\sim \\text{Gamma}(1, 10) \\\\\n\\end{aligned}\n$$\n\nWhere : \n\n*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n\n*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n\n*   $\\begin{pmatrix} A_{[k,1]} \\\\ \\vdots \\\\ A_{[k,D]} \\end{pmatrix}$ is a prior for the $k$-th mean vector as derived by a *KMEANS* clustering algorithm. \n\n*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\n*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n\n*   $\\sigma_k$ is a diagonal matrix of standard deviations for the $k$-th cluster.\n\n*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n\n*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n\n*   $\\pi$ is a vector of $K$ cluster weights.\n  \n*   $\\beta_k$: The set of $K$ Beta-distributed random variables used in the stick-breaking process to construct the mixture weights.\n\n*   $\\alpha$: The concentration parameter, controlling the effective number of clusters. \n\n\n## Notes\n\n::: callout-note\nThe primary advantage of the DPMM over methods like K-Means or a GMM is the **automatic inference of the number of clusters**. Instead of running the model multiple times with different values of `K` and comparing them, the DPMM explores different numbers of clusters as part of its fitting process. The posterior distribution of the weights `w` reveals which components are \"active\" (have significant weight) and thus gives a probabilistic estimate of the number of clusters supported by the data.\n:::\n\n## Reference(s)\n\n@gershman2012tutorial\n\n",
    "supporting": [
      "21. DPMM_files/figure-pdf"
    ],
    "filters": []
  }
}