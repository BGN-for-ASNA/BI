{
  "hash": "2cd70ce47f141f5275b3e446ff4fc76e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dirichlet Process Mixture Models\"\ndescription: \"A non-parametric Bayesian clustering method that automatically determines the number of clusters.\"\ncategories: [Clustering, Unsupervised Learning, Non-parametric]\nimage: \"Figures/20.png\"\norder: 24\n---\n\n\n\n\n\n\n## General Principles\n\nTo discover group structures or clusters in data without pre-specifying the number of groups, we can use a **Dirichlet Process Mixture Model (DPMM)** @gershman2012tutorial. This is a [<span style=\"color:#0D6EFD\"> unsupervised clustering method ðŸ›ˆ</span>]{#unsupervised}. Essentially, the model assumes the data is generated from a collection of different Gaussian distributions, and it simultaneously tries to figure out:\n\n1.  **How many clusters (`K`) exist**: Unlike algorithms like K-Means, the DPMM infers the most probable number of clusters directly from the data.\n2.  **The properties of each cluster**: For each inferred cluster, it estimates its location and its spread.\n3.  **The assignment of each data point**: It determines the probability of each data point belonging to each cluster.\n\n## Considerations\n\n::: callout-caution\n-   A DPMM is a Bayesian model ðŸ›ˆ that considers uncertainty in all its parameters. The core idea is to use the Dirichlet Process prior that allows for a potentially infinite number of clusters. In practice, we use a finite approximation where we cap the maximum number of clusters at $K$ and use the [<span style=\"color:#0D6EFD\"> Stick-Breaking Process ðŸ›ˆ</span>]{#stickProcess}.\n\n-   The key parameters and their priors are:\n\n    - **Concentration** $\\alpha$: This single parameter controls the tendency to create new clusters. A low `Î±` favors fewer, larger clusters, while a high `Î±` allows for many smaller clusters. We typically place a `Gamma` prior on $\\alpha$ to learn its value from the data.\n- \n    - **Cluster Weights `w`**: Generated via the Stick-Breaking process from $\\alpha$. These are the probabilities of drawing a data point from any given cluster.\n  \n    - **Cluster Parameters (**$\\mu$, $\\Sigma$): Each potential cluster has a mean $\\mu$ and a covariance matrix $\\Sigma$. If the data have multiple dimensions, we use a multivariate normal distribution (see chapter, [14](14.%20Varying%20slopes.qmd)). However, if the data is one-dimensional, we use a univariate normal distribution.\n\n-   The model is often implemented in its [marginalized form ðŸ›ˆ]{style=\"color:#0D6EFD\"}. Instead of explicitly assigning each data point to a cluster, we integrate out this choice. This creates a smoother probability surface for the inference algorithm to explore, leading to much more efficient computation.\n:::\n\n## Example\n\nBelow is an example of a DPMM implemented in BI. The goal is to cluster a synthetic dataset into its underlying groups. The code first generates data with 4 distinct centers and then applies the DPMM to recover these clusters.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi, jnp \nfrom sklearn.datasets import make_blobs\nimport numpyro\n\nm = bi(rand_seed = False)\n\n# Generate synthetic data\ndata, true_labels = make_blobs(\n    n_samples=500, centers=8, cluster_std=0.8,\n    center_box=(-10,10), random_state=101\n)\ndata_mean = jnp.mean(data, axis=0)\ndata_std = jnp.std(data, axis=0)*2\n\n#  The model\ndef dpmm(data, K, data_mean, data_std):\n    N, D = data.shape  # Number of features\n\n\n    # 1) stick-breaking weights\n    alpha = m.dist.gamma(1.0, 10.0,name='alpha')\n\n    with m.dist.plate(\"beta_plate\", K - 1):\n        beta = m.dist.beta(1, alpha, name = \"beta\")\n\n    w = numpyro.deterministic(\"w\",m.models.dpmm.mix_weights(beta))\n\n    # 2) component parameters\n    with m.dist.plate(\"components\", K):\n        mu = m.dist.multivariate_normal(loc=data_mean, covariance_matrix=data_std*jnp.eye(D),name='mu')# shape (T, D)        \n        sigma = m.dist.log_normal(0.0, 1.0,shape=(D,),event=1,name='sigma')# shape (T, D)\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0,name='Lcorr')# shape (T, D, D)\n\n        scale_tril = sigma[..., None] * Lcorr  # shape (T, D, D)\n\n    # 3) Latent cluster assignments for each data point\n    m.dist.mixture_same_family(\n        mixing_distribution=m.dist.categorical(probs=w, create_obj=True),\n        component_distribution=m.dist.multivariate_normal(\n            loc=mu, \n            scale_tril=scale_tril, \n            create_obj=True\n        ),\n        obs=data\n    )\n\nm.data_on_model = dict(data=data,K = 10, data_mean=data_mean, data_std=data_std)\nm.fit(dpmm)  # Optimize model parameters through MCMC sampling\nm.plot(X=data,sampler=m.sampler) # Prebuild plot function for GMM\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<17:29,  1.05s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|â–         | 14/1000 [00:01<01:00, 16.30it/s, 511 steps of size 1.05e-02. acc. prob=0.64]\rwarmup:   2%|â–         | 22/1000 [00:01<00:47, 20.69it/s, 255 steps of size 1.28e-02. acc. prob=0.70]\rwarmup:   3%|â–Ž         | 28/1000 [00:01<00:50, 19.38it/s, 1023 steps of size 2.71e-02. acc. prob=0.72]\rwarmup:   4%|â–         | 40/1000 [00:01<00:29, 32.54it/s, 12 steps of size 1.33e-02. acc. prob=0.73]  \rwarmup:   5%|â–         | 47/1000 [00:02<00:27, 34.18it/s, 63 steps of size 3.37e-02. acc. prob=0.75]\rwarmup:   6%|â–Œ         | 56/1000 [00:02<00:21, 43.30it/s, 63 steps of size 4.24e-02. acc. prob=0.76]\rwarmup:   6%|â–‹         | 63/1000 [00:02<00:25, 36.17it/s, 1023 steps of size 2.39e-02. acc. prob=0.76]\rwarmup:   7%|â–‹         | 69/1000 [00:02<00:26, 35.17it/s, 63 steps of size 3.38e-02. acc. prob=0.76]  \rwarmup:   8%|â–Š         | 75/1000 [00:02<00:24, 38.45it/s, 255 steps of size 2.96e-02. acc. prob=0.76]\rwarmup:   8%|â–Š         | 80/1000 [00:02<00:22, 40.10it/s, 175 steps of size 6.51e-02. acc. prob=0.77]\rwarmup:   8%|â–Š         | 85/1000 [00:02<00:22, 40.42it/s, 511 steps of size 3.17e-02. acc. prob=0.77]\rwarmup:   9%|â–‰         | 92/1000 [00:03<00:19, 45.47it/s, 191 steps of size 6.86e-02. acc. prob=0.77]\rwarmup:  10%|â–‰         | 99/1000 [00:03<00:19, 46.90it/s, 511 steps of size 2.27e-02. acc. prob=0.77]\rwarmup:  10%|â–ˆ         | 105/1000 [00:03<00:31, 28.28it/s, 255 steps of size 5.21e-02. acc. prob=0.76]\rwarmup:  11%|â–ˆ         | 109/1000 [00:03<00:33, 26.62it/s, 127 steps of size 6.96e-02. acc. prob=0.76]\rwarmup:  11%|â–ˆâ–        | 113/1000 [00:04<00:36, 24.17it/s, 511 steps of size 1.72e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–        | 117/1000 [00:04<00:33, 26.16it/s, 255 steps of size 5.17e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–        | 121/1000 [00:04<00:30, 28.62it/s, 127 steps of size 3.11e-02. acc. prob=0.76]\rwarmup:  12%|â–ˆâ–Ž        | 125/1000 [00:04<00:34, 25.43it/s, 1023 steps of size 1.66e-02. acc. prob=0.76]\rwarmup:  13%|â–ˆâ–Ž        | 128/1000 [00:04<00:37, 23.49it/s, 127 steps of size 7.11e-02. acc. prob=0.77] \rwarmup:  13%|â–ˆâ–Ž        | 132/1000 [00:04<00:33, 26.16it/s, 127 steps of size 2.77e-02. acc. prob=0.77]\rwarmup:  14%|â–ˆâ–Ž        | 135/1000 [00:05<00:49, 17.52it/s, 1023 steps of size 2.60e-02. acc. prob=0.77]\rwarmup:  14%|â–ˆâ–        | 141/1000 [00:05<00:36, 23.68it/s, 255 steps of size 4.77e-02. acc. prob=0.77] \rwarmup:  15%|â–ˆâ–        | 146/1000 [00:05<00:32, 26.29it/s, 511 steps of size 3.00e-02. acc. prob=0.77]\rwarmup:  15%|â–ˆâ–Œ        | 153/1000 [00:05<00:24, 34.54it/s, 63 steps of size 5.40e-02. acc. prob=0.77] \rwarmup:  16%|â–ˆâ–Œ        | 158/1000 [00:05<00:22, 36.68it/s, 63 steps of size 4.42e-02. acc. prob=0.77]\rwarmup:  16%|â–ˆâ–‹        | 163/1000 [00:05<00:22, 36.83it/s, 255 steps of size 2.21e-02. acc. prob=0.77]\rwarmup:  17%|â–ˆâ–‹        | 169/1000 [00:05<00:25, 32.07it/s, 1023 steps of size 1.72e-02. acc. prob=0.77]\rwarmup:  17%|â–ˆâ–‹        | 173/1000 [00:06<00:25, 32.87it/s, 48 steps of size 1.07e-02. acc. prob=0.77]  \rwarmup:  18%|â–ˆâ–Š        | 177/1000 [00:06<00:33, 24.55it/s, 191 steps of size 2.37e-02. acc. prob=0.77]\rwarmup:  18%|â–ˆâ–Š        | 183/1000 [00:06<00:27, 29.41it/s, 255 steps of size 2.27e-02. acc. prob=0.77]\rwarmup:  19%|â–ˆâ–‰        | 188/1000 [00:06<00:27, 29.54it/s, 511 steps of size 1.22e-02. acc. prob=0.77]\rwarmup:  19%|â–ˆâ–‰        | 192/1000 [00:06<00:27, 29.77it/s, 127 steps of size 1.29e-02. acc. prob=0.77]\rwarmup:  20%|â–ˆâ–‰        | 196/1000 [00:07<00:39, 20.47it/s, 511 steps of size 3.14e-02. acc. prob=0.77]\rwarmup:  20%|â–ˆâ–‰        | 199/1000 [00:07<00:49, 16.11it/s, 1023 steps of size 2.44e-02. acc. prob=0.77]\rwarmup:  20%|â–ˆâ–ˆ        | 202/1000 [00:07<00:57, 13.93it/s, 127 steps of size 3.16e-02. acc. prob=0.77] \rwarmup:  20%|â–ˆâ–ˆ        | 204/1000 [00:07<00:56, 13.99it/s, 255 steps of size 6.68e-02. acc. prob=0.78]\rwarmup:  21%|â–ˆâ–ˆ        | 206/1000 [00:08<00:58, 13.50it/s, 1023 steps of size 1.75e-02. acc. prob=0.77]\rwarmup:  21%|â–ˆâ–ˆ        | 208/1000 [00:08<01:08, 11.48it/s, 1023 steps of size 4.27e-02. acc. prob=0.77]\rwarmup:  21%|â–ˆâ–ˆ        | 210/1000 [00:08<01:14, 10.66it/s, 1023 steps of size 1.42e-02. acc. prob=0.77]\rwarmup:  21%|â–ˆâ–ˆ        | 212/1000 [00:08<01:21,  9.73it/s, 1023 steps of size 2.81e-02. acc. prob=0.77]\rwarmup:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:09<01:26,  9.12it/s, 1023 steps of size 7.69e-03. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:09<01:28,  8.89it/s, 1023 steps of size 1.18e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:09<01:32,  8.51it/s, 1023 steps of size 1.78e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:09<01:34,  8.25it/s, 1023 steps of size 2.76e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:09<01:37,  8.00it/s, 1023 steps of size 9.07e-03. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:09<01:38,  7.90it/s, 1023 steps of size 1.28e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:09<01:40,  7.80it/s, 1023 steps of size 1.97e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:10<01:40,  7.78it/s, 1023 steps of size 9.69e-03. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:10<01:40,  7.77it/s, 1023 steps of size 1.49e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:10<01:39,  7.78it/s, 1023 steps of size 2.17e-02. acc. prob=0.77]\rwarmup:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:10<01:41,  7.64it/s, 1023 steps of size 3.22e-02. acc. prob=0.78]\rwarmup:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:10<01:41,  7.63it/s, 1023 steps of size 4.61e-02. acc. prob=0.78]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:10<01:41,  7.63it/s, 1023 steps of size 1.27e-02. acc. prob=0.77]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:10<01:42,  7.53it/s, 1023 steps of size 1.93e-02. acc. prob=0.77]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:10<01:25,  9.03it/s, 1023 steps of size 2.21e-02. acc. prob=0.77]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:11<00:57, 13.31it/s, 511 steps of size 3.07e-02. acc. prob=0.78] \rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:11<00:58, 13.00it/s, 255 steps of size 3.19e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:11<01:11, 10.64it/s, 1023 steps of size 2.39e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:11<01:09, 10.93it/s, 511 steps of size 3.71e-02. acc. prob=0.78] \rwarmup:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:11<01:18,  9.66it/s, 1023 steps of size 2.40e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:12<01:25,  8.85it/s, 1023 steps of size 3.36e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:12<01:30,  8.34it/s, 1023 steps of size 4.27e-02. acc. prob=0.78]\rwarmup:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:12<01:34,  7.98it/s, 1023 steps of size 3.97e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:12<01:22,  9.17it/s, 1023 steps of size 3.07e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:12<01:26,  8.69it/s, 1023 steps of size 2.61e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:12<01:30,  8.29it/s, 1023 steps of size 3.80e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:13<01:33,  8.00it/s, 1023 steps of size 4.34e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:13<01:35,  7.82it/s, 1023 steps of size 6.00e-02. acc. prob=0.78]\rwarmup:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:13<00:51, 14.61it/s, 511 steps of size 1.68e-02. acc. prob=0.77] \rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:13<00:39, 18.82it/s, 511 steps of size 1.65e-02. acc. prob=0.77]\rwarmup:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:13<00:36, 20.37it/s, 1023 steps of size 1.04e-02. acc. prob=0.77]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:13<00:35, 20.91it/s, 127 steps of size 2.96e-02. acc. prob=0.78] \rwarmup:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:13<00:32, 22.73it/s, 511 steps of size 1.14e-02. acc. prob=0.78]\rwarmup:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:14<00:31, 23.00it/s, 127 steps of size 2.62e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:14<00:27, 26.77it/s, 127 steps of size 2.88e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:14<00:27, 25.94it/s, 511 steps of size 1.65e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:14<00:23, 30.98it/s, 255 steps of size 3.05e-02. acc. prob=0.78]\rwarmup:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:14<00:18, 38.01it/s, 47 steps of size 1.20e-02. acc. prob=0.78] \rwarmup:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:14<00:17, 40.80it/s, 31 steps of size 1.13e-02. acc. prob=0.78]\rwarmup:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:14<00:18, 38.19it/s, 255 steps of size 2.79e-02. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:14<00:16, 42.03it/s, 127 steps of size 3.26e-02. acc. prob=0.78]\rwarmup:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:15<00:15, 44.66it/s, 255 steps of size 1.80e-02. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:15<00:15, 45.31it/s, 255 steps of size 3.09e-02. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:15<00:14, 46.10it/s, 127 steps of size 3.43e-02. acc. prob=0.78]\rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:15<00:16, 40.95it/s, 127 steps of size 3.14e-02. acc. prob=0.78]\rwarmup:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:15<00:16, 40.89it/s, 255 steps of size 2.30e-02. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:15<00:15, 42.90it/s, 127 steps of size 4.95e-02. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:15<00:15, 43.11it/s, 127 steps of size 3.95e-02. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:15<00:14, 44.15it/s, 127 steps of size 4.05e-02. acc. prob=0.78]\rwarmup:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:15<00:15, 41.72it/s, 255 steps of size 2.33e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:16<00:15, 42.04it/s, 127 steps of size 4.17e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:16<00:14, 42.91it/s, 255 steps of size 2.54e-02. acc. prob=0.78]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:16<00:19, 33.14it/s, 511 steps of size 1.39e-02. acc. prob=0.78]\rwarmup:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:16<00:17, 36.57it/s, 127 steps of size 1.87e-02. acc. prob=0.78]\rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:16<00:16, 37.87it/s, 255 steps of size 1.84e-02. acc. prob=0.78]\rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [00:16<00:15, 40.75it/s, 127 steps of size 4.04e-02. acc. prob=0.78]\rwarmup:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:16<00:15, 40.21it/s, 127 steps of size 3.31e-02. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [00:17<00:13, 44.57it/s, 127 steps of size 1.52e-02. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [00:17<00:13, 43.98it/s, 127 steps of size 3.87e-02. acc. prob=0.78]\rwarmup:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [00:17<00:14, 41.72it/s, 255 steps of size 3.32e-02. acc. prob=0.78]\rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [00:17<00:12, 46.09it/s, 127 steps of size 1.46e-02. acc. prob=0.78]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [00:17<00:13, 42.43it/s, 127 steps of size 4.14e-02. acc. prob=0.78]\rwarmup:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [00:17<00:13, 42.21it/s, 127 steps of size 3.46e-02. acc. prob=0.78]\rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [00:17<00:13, 43.32it/s, 127 steps of size 3.41e-02. acc. prob=0.78]\rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [00:17<00:12, 44.21it/s, 127 steps of size 3.54e-02. acc. prob=0.78]\rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [00:17<00:12, 45.63it/s, 255 steps of size 2.07e-02. acc. prob=0.78]\rwarmup:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [00:18<00:12, 46.31it/s, 127 steps of size 3.32e-02. acc. prob=0.78]\rwarmup:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [00:18<00:11, 48.35it/s, 127 steps of size 3.13e-02. acc. prob=0.78]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [00:18<00:10, 53.98it/s, 63 steps of size 6.09e-02. acc. prob=0.78] \rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [00:18<00:10, 50.14it/s, 127 steps of size 6.29e-02. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [00:18<00:10, 51.17it/s, 127 steps of size 5.57e-02. acc. prob=0.78]\rwarmup:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [00:18<00:10, 51.50it/s, 127 steps of size 6.25e-02. acc. prob=0.78]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [00:18<00:10, 52.04it/s, 63 steps of size 6.72e-02. acc. prob=0.78] \rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [00:18<00:10, 49.31it/s, 127 steps of size 2.98e-02. acc. prob=0.78]\rwarmup:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [00:19<00:08, 62.13it/s, 63 steps of size 6.72e-02. acc. prob=0.78] \rsample:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [00:19<00:07, 66.92it/s, 63 steps of size 4.60e-02. acc. prob=0.42]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [00:19<00:05, 81.30it/s, 63 steps of size 4.60e-02. acc. prob=0.85]\rsample:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [00:19<00:05, 83.68it/s, 63 steps of size 4.60e-02. acc. prob=0.87]\rsample:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [00:19<00:05, 84.42it/s, 63 steps of size 4.60e-02. acc. prob=0.87]\rsample:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [00:19<00:05, 79.87it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [00:19<00:06, 73.30it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [00:19<00:06, 64.37it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [00:19<00:07, 59.38it/s, 191 steps of size 4.60e-02. acc. prob=0.88]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [00:20<00:07, 57.68it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [00:20<00:06, 66.02it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [00:20<00:05, 71.66it/s, 127 steps of size 4.60e-02. acc. prob=0.86]\rsample:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [00:20<00:05, 70.94it/s, 63 steps of size 4.60e-02. acc. prob=0.86] \rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [00:20<00:05, 70.87it/s, 63 steps of size 4.60e-02. acc. prob=0.86]\rsample:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [00:20<00:05, 67.69it/s, 127 steps of size 4.60e-02. acc. prob=0.86]\rsample:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [00:20<00:06, 54.32it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [00:21<00:07, 51.04it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [00:21<00:06, 54.22it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [00:21<00:06, 57.03it/s, 63 steps of size 4.60e-02. acc. prob=0.87] \rsample:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [00:21<00:05, 66.04it/s, 63 steps of size 4.60e-02. acc. prob=0.88]\rsample:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [00:21<00:04, 74.61it/s, 63 steps of size 4.60e-02. acc. prob=0.88]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [00:21<00:04, 75.68it/s, 63 steps of size 4.60e-02. acc. prob=0.88]\rsample:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [00:21<00:04, 76.61it/s, 191 steps of size 4.60e-02. acc. prob=0.88]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [00:21<00:04, 77.35it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [00:21<00:03, 82.13it/s, 63 steps of size 4.60e-02. acc. prob=0.89] \rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [00:22<00:03, 84.13it/s, 127 steps of size 4.60e-02. acc. prob=0.89]\rsample:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [00:22<00:03, 71.79it/s, 255 steps of size 4.60e-02. acc. prob=0.88]\rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [00:22<00:03, 69.68it/s, 63 steps of size 4.60e-02. acc. prob=0.89] \rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [00:22<00:03, 71.20it/s, 63 steps of size 4.60e-02. acc. prob=0.87]\rsample:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [00:22<00:04, 57.63it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [00:22<00:06, 39.93it/s, 383 steps of size 4.60e-02. acc. prob=0.87]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [00:23<00:07, 33.09it/s, 895 steps of size 4.60e-02. acc. prob=0.87]\rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [00:23<00:07, 32.98it/s, 63 steps of size 4.60e-02. acc. prob=0.87] \rsample:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [00:23<00:07, 31.66it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [00:23<00:07, 30.07it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [00:23<00:07, 30.95it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [00:23<00:07, 31.28it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [00:24<00:06, 35.93it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [00:24<00:05, 40.76it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [00:24<00:04, 47.86it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [00:24<00:03, 51.38it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [00:24<00:03, 54.01it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [00:24<00:03, 58.04it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [00:24<00:03, 53.54it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [00:24<00:03, 53.36it/s, 255 steps of size 4.60e-02. acc. prob=0.87]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [00:25<00:03, 52.30it/s, 191 steps of size 4.60e-02. acc. prob=0.87]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [00:25<00:02, 55.78it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [00:25<00:02, 67.04it/s, 63 steps of size 4.60e-02. acc. prob=0.88] \rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [00:25<00:02, 66.72it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [00:25<00:01, 72.59it/s, 63 steps of size 4.60e-02. acc. prob=0.88] \rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [00:25<00:01, 79.45it/s, 127 steps of size 4.60e-02. acc. prob=0.87]\rsample:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [00:25<00:01, 75.29it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [00:25<00:01, 75.13it/s, 63 steps of size 4.60e-02. acc. prob=0.87] \rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [00:25<00:01, 60.76it/s, 511 steps of size 4.60e-02. acc. prob=0.88]\rsample:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [00:26<00:01, 62.15it/s, 63 steps of size 4.60e-02. acc. prob=0.88] \rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [00:26<00:01, 60.28it/s, 63 steps of size 4.60e-02. acc. prob=0.88]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [00:26<00:01, 62.12it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [00:26<00:01, 62.81it/s, 63 steps of size 4.60e-02. acc. prob=0.88] \rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [00:26<00:00, 67.29it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [00:26<00:00, 70.41it/s, 127 steps of size 4.60e-02. acc. prob=0.88]\rsample:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [00:26<00:00, 70.77it/s, 63 steps of size 4.60e-02. acc. prob=0.88] \rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [00:26<00:00, 61.99it/s, 127 steps of size 4.60e-02. acc. prob=0.86]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [00:27<00:00, 68.28it/s, 127 steps of size 4.60e-02. acc. prob=0.86]\rsample:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [00:27<00:00, 73.67it/s, 63 steps of size 4.60e-02. acc. prob=0.86] \rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [00:27<00:00, 73.24it/s, 255 steps of size 4.60e-02. acc. prob=0.86]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:27<00:00, 36.50it/s, 63 steps of size 4.60e-02. acc. prob=0.87]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nModel found 8 clusters.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](21. DPMM_files/figure-pdf/cell-2-output-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n## R\n\n``` r\n\n```\n![](travaux-routiers.png){fig-align=\"center\"}\n\n## Julia\n```julia\nusing BayesianInference\nusing PythonCall\nnumpyro = pyimport(\"numpyro\")\n\nm = importBI(rand_seed = false)\n\n# 1. Generate Data\nsk_datasets = pyimport(\"sklearn.datasets\")\noutput = sk_datasets.make_blobs(n_samples=500, centers=8, cluster_std=0.8, center_box=(-10, 10), random_state=101)\ndata = output[0]\ndata_mean = jnp.mean(data, axis=0)\ndata_std = jnp.std(data, axis=0) * 2\nm.data_on_model = pydict(data=data, K=10, data_mean = data_mean, data_std = data_std)\n\n\n@BI function dpmm(data, K, data_mean , data_std)\n    N, D = data.shape \n\n    alpha = m.dist.gamma(1.0, 10.0, name=\"alpha\")\n\n    beta = pywith(m.dist.plate(\"beta_plate\", K - 1)) do _\n        m.dist.beta(1, alpha, name = \"beta\")\n    end\n\n    w = numpyro.deterministic(\"w\", m.models.dpmm.mix_weights(beta))\n\n    mu, scale_tril = pywith(m.dist.plate(\"components\", K)) do _\n        mu_val = m.dist.multivariate_normal(\n            loc=data_mean, \n            covariance_matrix=data_std * jnp.eye(D),\n            name=\"mu\"\n        )\n        \n        sigma = m.dist.log_normal(0.0, 1.0, shape=(D,), event=1, name=\"sigma\")\n        Lcorr = m.dist.lkj_cholesky(dimension=D, concentration=1.0, name=\"Lcorr\")\n        scale_tril_inner = jnp.expand_dims(sigma, -1) * Lcorr\n        (mu_val, scale_tril_inner)\n    end\n    \n    m.dist.mixture_same_family(\n        mixing_distribution=m.dist.categorical(probs=w, create_obj=true),\n        component_distribution=m.dist.multivariate_normal(\n            loc=mu, \n            scale_tril=scale_tril, \n            create_obj=true\n        ),\n        obs=data\n    )\nend\n\n# 4. Run\n\nm.fit(dpmm) \n\n@pyplot m.models.dpmm.plot_dpmm(m.data_on_model[\"data\"], m.sampler)\n```\n:::\n\n## Mathematical Details\n\nThe process involves two keys submodels. The first, aims to identify the location and scale of $K$ potential clusters. The second, aims to identify which cluster is most likely to have generated a given data point. \n\n$$\n\\begin{aligned}\n\\begin{pmatrix}\nY_{i,1} \\\\\n\\vdots \\\\\nY_{i,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\n\\mu_{z_i,1} \\\\\n\\vdots \\\\\n\\mu_{z_i,D}\n\\end{pmatrix},\n\\,\n\\Sigma_{z_i}\n\\right) \\\\\n\\\\\n\\begin{pmatrix}\n\\mu_{k,1} \\\\\n\\vdots \\\\\n\\mu_{k,D}\n\\end{pmatrix}\n&\\sim\n\\text{MVN}\\!\\left(\n\\begin{pmatrix}\nA_{1} \\\\\n\\vdots \\\\\nA_{D}\n\\end{pmatrix},\n\\, B\n\\right) \\\\\n\\\\\n\\Sigma_k &= \\text{Diag}(\\sigma_k) \\Omega_k  \\text{Diag}(\\sigma_k) \\\\\n\\\\\n\\sigma_{[k,d]} &\\sim \\text{HalfCauchy}(1) \\\\\n\\\\\n\\Omega_k &\\sim \\text{LKJ}(2) \\\\\n\\\\\nz_{i} &\\sim \\text{Categorical}(\\pi) \\\\\n\\\\\n\\pi_{i}(\\beta_{1:K})  &=  \\beta_i \\prod_{j<K} (1-\\beta_j) \\\\\n\\\\\n\\beta_k &\\sim \\text{Beta}(1, \\alpha) \\\\\n\\\\\n\\alpha &\\sim \\text{Gamma}(1, 10) \\\\\n\\end{aligned}\n$$\n\nWhere : \n\n*   $\\begin{pmatrix} Y_{[i,1]} \\\\ \\vdots \\\\ Y_{[i,D]} \\end{pmatrix}$ is the $i$-th observation of a D-dimensional data array.\n\n*   $\\begin{pmatrix}\\mu_{[k,1]} \\\\ \\vdots \\\\ \\mu_{[k,D]}\\end{pmatrix}$ is the $k$-th parameter vector of dimension D.\n\n*   $\\begin{pmatrix} A_{1} \\\\ \\vdots \\\\ A_{D} \\end{pmatrix}$ is a prior for the  mean vector as derived from mean of the raw data. \n\n*   $B$ is the prior covariance of the cluster means, and is setup as a diagonal matrix with 0.1 along the diagonal.\n\n*   $\\Sigma_k$ is the DxD covariance matrix of the $k$-th cluster (it is composed from $\\sigma_k$ and $\\Omega_k$).\n\n*  $\\text{Diag}(\\sigma_k)$ is a diagonal matrix whose diagonal entries are the standard deviations:\n  $$\n  \\text{Diag}(\\sigma_k) =\n  \\begin{pmatrix}\n  \\sigma_{[k,1]} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{[k,2]} &        & \\vdots \\\\\n  \\vdots &        & \\ddots & 0 \\\\\n  0 & \\cdots & 0 & \\sigma_{[k,D]}\n  \\end{pmatrix}.\n  $$\n\n*   $\\sigma_{k}$ is a $D$-vector of standard deviations for the $k$-th cluster where each element, $d$, has a half-cauchy prior.\n\n*   $\\Omega_k$ is a correlation matrix for the $k$-th cluster.\n\n*   $z_i$ is a latent variable that maps observation $i$ to cluster $k$.\n\n*   $\\pi$ is a vector of $K$ cluster weights, some of which may be close to zero if the predicted number of clusters is less than the maximum number of clusters.\n  \n*   $\\beta_k$: The set of $K$ Beta-distributed random variables used in the stick-breaking process to construct the mixture weights.\n\n*   $\\alpha$: The concentration parameter, controlling the effective number of clusters. \n\n\n## Notes\n\n::: callout-note\n* The primary advantage of the DPMM is the **automatic inference of the number of clusters**. The posterior distribution of the weights `w` reveals which components are \"active\", giving a probabilistic estimate of `K`.\n\n* Prior $\\alpha$ strongly influence the predicted number of clusters. Below are examples of this relationship:\n\n| Shape | Rate | Behavior |\n|:---:|:---:|:---:|\n| 1     | 15    | Forces very few clusters |\n| 5     | 1     | Encourages many small clusters |\n| 10    | 2     | Same mean, less variance |\n| 2     | 0.5   | Moderately many clusters |\n| 15    | 1     | Explosive prior cluster count |\n: Impact of Gamma Prior Hyperparameters on Cluster Counts {tbl-colwidths=\"[10,10,15,25,40]\"}\n:::\n\n## Reference(s)\n\nhttps://en.wikipedia.org/wiki/Dirichlet_process\nhttps://pyro.ai/examples/dirichlet_process_mixture.html\n\n",
    "supporting": [
      "21. DPMM_files/figure-pdf"
    ],
    "filters": []
  }
}