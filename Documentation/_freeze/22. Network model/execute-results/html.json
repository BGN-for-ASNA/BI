{
  "hash": "3b80f8044a471fa7312c61a55929c14b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Network Models\"\ndescription: \"Modeling relationships and interactions between entities as a graph of nodes and edges.\"\ncategories: [Network Analysis, Graph Theory]\nimage: \"Figures/22.png\"\norder: 25\n---\n\n\n\n\n\n\nA network represents the relationships (links) between entities (nodes). These links can be weighted (weighted network) or unweighted (binary network), directed (directed network) or undirected (undirected network). Regardless of their type, networks generate links shared by nodes, leading to data dependency when modeling the network. One proposed solution is to model network links with random [intercepts](13.%20Varying%20intercepts.qmd) and [slopes](14.%20Varying%20slopes.qmd). By adding such parameters to the model, we can account for the correlations between node link relationships.\n\n## Considerations\n::: callout-caution\n-   The particularity here is that varying intercepts and slopes are generated for both [<span style=\"color:#0D6EFD\">nodal effects ðŸ›ˆ</span>]{#NodeF} and [<span style=\"color:#0D6EFD\">dyadic effects ðŸ›ˆ</span>]{#DyadicF}. These varying intercepts and slopes are identical to those described in previous chapters and will therefore not be detailed further. Only the random-centered version of the varying slopes will be described here.\n:::\n\n## Example\nBelow is an example code snippet demonstrating a Bayesian network model with a sender-receiver effect. This example is based on @ross2024modelling.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#11c2e319 .cell execution_count=1}\n``` {.python .cell-code}\n# Setup device------------------------------------------------\nfrom BI import bi, jnp\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = True)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = True)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\ndef sim_network(kinship, individual_predictor):\n  # Intercept\n  alpha = m.dist.normal(0,1, sample = True)\n\n  # SR\n  sr = m.net.sender_receiver(individual_predictor, individual_predictor, s_mu = 0.4, r_mu = -0.4, sample = True)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = True)\n\n  return m.dist.bernoulli(logits = alpha + sr + DR, sample = True)\n\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor)\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = dict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor\n)\n\n\ndef model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors):\n    N_id = network.shape[0]\n\n    # Block ---------------------------------------\n    alpha = m.dist.normal(0,1, sample = True)\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors,\n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n\n    m.dist.bernoulli(logits = alpha + sr + dr, obs=network)\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/1000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/1000 [00:01<20:22,  1.22s/it, 1 steps of size 2.34e+00. acc. prob=0.00]\rwarmup:   1%|          | 10/1000 [00:01<01:38, 10.08it/s, 511 steps of size 1.25e-02. acc. prob=0.59]\rwarmup:   2%|â–         | 16/1000 [00:01<01:02, 15.67it/s, 511 steps of size 2.08e-02. acc. prob=0.67]\rwarmup:   2%|â–         | 24/1000 [00:01<00:38, 25.12it/s, 127 steps of size 5.01e-02. acc. prob=0.72]\rwarmup:   3%|â–Ž         | 34/1000 [00:01<00:25, 37.53it/s, 255 steps of size 3.06e-02. acc. prob=0.74]\rwarmup:   4%|â–         | 42/1000 [00:01<00:22, 43.01it/s, 511 steps of size 1.22e-02. acc. prob=0.74]\rwarmup:   5%|â–Œ         | 50/1000 [00:01<00:18, 50.67it/s, 511 steps of size 9.93e-03. acc. prob=0.74]\rwarmup:   6%|â–Œ         | 58/1000 [00:02<00:17, 53.46it/s, 127 steps of size 3.73e-02. acc. prob=0.76]\rwarmup:   6%|â–‹         | 65/1000 [00:02<00:16, 55.98it/s, 127 steps of size 3.43e-02. acc. prob=0.76]\rwarmup:   7%|â–‹         | 72/1000 [00:02<00:16, 55.02it/s, 127 steps of size 1.31e-02. acc. prob=0.76]\rwarmup:   8%|â–Š         | 80/1000 [00:02<00:15, 58.21it/s, 511 steps of size 1.76e-02. acc. prob=0.76]\rwarmup:   9%|â–‰         | 88/1000 [00:02<00:14, 63.04it/s, 127 steps of size 4.08e-02. acc. prob=0.77]\rwarmup:  10%|â–‰         | 97/1000 [00:02<00:13, 68.39it/s, 255 steps of size 3.27e-02. acc. prob=0.77]\rwarmup:  11%|â–ˆ         | 112/1000 [00:02<00:10, 88.69it/s, 127 steps of size 8.87e-02. acc. prob=0.77]\rwarmup:  13%|â–ˆâ–Ž        | 131/1000 [00:02<00:07, 114.90it/s, 127 steps of size 5.36e-02. acc. prob=0.77]\rwarmup:  14%|â–ˆâ–        | 144/1000 [00:02<00:07, 117.95it/s, 255 steps of size 3.91e-02. acc. prob=0.77]\rwarmup:  16%|â–ˆâ–Œ        | 157/1000 [00:03<00:06, 120.53it/s, 63 steps of size 1.21e-01. acc. prob=0.77] \rwarmup:  18%|â–ˆâ–Š        | 175/1000 [00:03<00:06, 137.48it/s, 31 steps of size 2.52e-01. acc. prob=0.78]\rwarmup:  20%|â–ˆâ–ˆ        | 204/1000 [00:03<00:04, 181.17it/s, 31 steps of size 1.24e-01. acc. prob=0.78]\rwarmup:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:03<00:03, 215.59it/s, 31 steps of size 1.77e-01. acc. prob=0.78]\rwarmup:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:03<00:03, 224.07it/s, 127 steps of size 4.67e-02. acc. prob=0.78]\rwarmup:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:03<00:04, 168.74it/s, 63 steps of size 1.06e-01. acc. prob=0.78] \rwarmup:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:03<00:04, 154.79it/s, 63 steps of size 2.25e-02. acc. prob=0.78]\rwarmup:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:03<00:04, 146.99it/s, 127 steps of size 4.21e-02. acc. prob=0.78]\rwarmup:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:04<00:04, 140.45it/s, 127 steps of size 4.74e-02. acc. prob=0.78]\rwarmup:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:04<00:04, 156.25it/s, 63 steps of size 6.74e-02. acc. prob=0.78] \rwarmup:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:04<00:04, 151.74it/s, 127 steps of size 5.04e-02. acc. prob=0.78]\rwarmup:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [00:04<00:03, 195.12it/s, 63 steps of size 1.30e-01. acc. prob=0.78] \rwarmup:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [00:04<00:02, 211.78it/s, 31 steps of size 1.72e-01. acc. prob=0.79]\rwarmup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [00:04<00:02, 223.16it/s, 63 steps of size 1.10e-01. acc. prob=0.78]\rwarmup:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [00:04<00:02, 205.52it/s, 63 steps of size 1.23e-01. acc. prob=0.78]\rsample:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [00:04<00:02, 195.53it/s, 63 steps of size 7.62e-02. acc. prob=0.90]\rsample:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [00:04<00:02, 197.98it/s, 63 steps of size 7.62e-02. acc. prob=0.90]\rsample:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [00:05<00:02, 195.92it/s, 63 steps of size 7.62e-02. acc. prob=0.90]\rsample:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [00:05<00:02, 196.75it/s, 63 steps of size 7.62e-02. acc. prob=0.92]\rsample:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [00:05<00:02, 195.38it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [00:05<00:01, 200.49it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [00:05<00:01, 203.06it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [00:05<00:01, 202.11it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [00:05<00:01, 197.94it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [00:05<00:01, 197.86it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [00:05<00:01, 199.28it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [00:06<00:01, 198.33it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [00:06<00:01, 190.33it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [00:06<00:01, 186.39it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [00:06<00:01, 185.75it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [00:06<00:00, 191.72it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [00:06<00:00, 193.41it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [00:06<00:00, 197.46it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [00:06<00:00, 202.08it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [00:06<00:00, 201.42it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [00:06<00:00, 202.50it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [00:07<00:00, 207.50it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [00:07<00:00, 211.53it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [00:07<00:00, 213.86it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\rsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:07<00:00, 136.00it/s, 63 steps of size 7.62e-02. acc. prob=0.93]\n```\n:::\n:::\n\n\n## R\n```r\nlibrary(BayesianInference)\n# Setup platform------------------------------------------------\nm=importBI(platform='cpu')\n\n# Import data ------------------------------------------------\nload(paste(system.file(package = \"BayesianInference\"),'/data/STRAND sim sr only.Rdata', sep = ''))\n\nm$data_on_model = list()\nm$data_on_model$N_id = length(ids)\nm$data_on_model$network = m$net$mat_to_edgl(model_dat$outcomes[,,1])\nm$data_on_model$N_dyads = m$net$mat_to_edgl(model_dat$outcomes[,,1])$shape[[1]]\nm$data_on_model$focal_individual_predictors = jnp$array(model_dat$individual_predictors)\nm$data_on_model$target_individual_predictors =jnp$array(model_dat$individual_predictors)\n\n# Define model ------------------------------------------------\nmodel <- function( N_id, N_dyads, network, focal_individual_predictors, target_individual_predictors){\n\n  \n  ## Block ---------------------------------------\n  B = bi.dist.normal(0, 2.5, shape=c(1), name = 'block')\n  \n  #SR ---------------------------------------\n  sr =  m$net$sender_receiver(focal_individual_predictors,target_individual_predictors)\n  \n  ### Dyadic--------------------------------------  \n  #dr, dr_raw, dr_sigma, dr_L = m.net.dyadic_random_effects(idx.shape[0], cholesky_density = 2)# shape = n dyads\n  dr = m$net$dyadic_effect(shape = c(N_dyads))\n  \n  ## SR ---------------------------------------                                                      \n  m$dist$poisson(jnp$exp(B + sr + dr), obs=network)  \n}\n\n# Run MCMC ------------------------------------------------\nm$fit(model) # Optimize model parameters through MCMC sampling\n\n# Summary ------------------------------------------------\nsummary =m$summary()\n\n```\n\n## Julia\n```julia\n# Setup device------------------------------------------------\nusing BayesianInference\n\n# Setup device------------------------------------------------\nm = importBI(platform=\"cpu\")\n\n# Simulate data ------------------------------------------------\nN = 50\nindividual_predictor = m.dist.normal(0,1, shape = (N,1), sample = true)\n\nkinship = m.dist.bernoulli(0.3, shape = (N,N), sample = true)\nkinship = kinship.at[jnp.diag_indices(N)].set(0)\n\nfunction sim_network(kinship, individual_predictor)\n  # Intercept\n  alpha = m.dist.normal(0,1, sample = true)\n\n  # SR\n  sr = m.net.sender_receiver(individual_predictor, individual_predictor, s_mu = 0.4, r_mu = -0.4, sample = true)\n\n  # D\n  DR = m.net.dyadic_effect(kinship, d_sd=2.5, sample = true)\n\n  return m.dist.bernoulli(logits = alpha + sr + DR, sample = true)\n\nend\n\nnetwork = sim_network(m.net.mat_to_edgl(kinship), individual_predictor)\n\n\n# Predictive model ------------------------------------------------\n\nm.data_on_model = pydict(\n    network = network, \n    dyadic_predictors = m.net.mat_to_edgl(kinship),\n    focal_individual_predictors = individual_predictor,\n    target_individual_predictors = individual_predictor\n)\n\n\n@BI function model(network, dyadic_predictors, focal_individual_predictors, target_individual_predictors)\n    N_id = network.shape[0]\n\n    # Block ---------------------------------------\n    alpha = m.dist.normal(0,1, name = \"alpha\")\n\n    ## SR shape =  N individuals---------------------------------------\n    sr =  m.net.sender_receiver(\n      focal_individual_predictors,\n      target_individual_predictors,\n      s_mu = 0.4, r_mu = -0.4\n    )\n\n    # Dyadic shape = N dyads--------------------------------------  \n    dr = m.net.dyadic_effect(dyadic_predictors, d_sd=2.5) # Diadic effect intercept only \n\n    m.dist.bernoulli(logits = alpha + sr + dr, obs=network)\nend\n\nm.fit(model, num_samples = 500, num_warmup = 500, num_chains = 1, thinning = 1)\n\n\n```\n:::\n\n:::::: {.callout-caution}\nEvent if you don't have dyadic effect, or block model effect, they need to be define to create intercepts (means) for those effects\n:::\n\n## Mathematical Details\n\n### *Main Formula*\n\nThe simple model that can be built to model link weights between nodes *i* and *j* can be defined using a Poisson distribution:\n\n$$\nG_{ij} \\sim \\text{Poisson}(Y_{ij})\n$$ \n\n$$\nlog(Y_{ij}) = \\alpha +  \\lambda_i + \\pi_j + \\delta_{ij}  + \\beta_1 X_i + \\beta_2 X_j + \\beta_3 Q_{ij}\n$$\n\n\nwhere:\n\n- $Y_{ij}$ is the weight of the link between *i* and *j*.\n\n  \n- $\\lambda_i$ is the [<span style=\"color:#0D6EFD\">sender random effect ðŸ›ˆ</span>]{#senderF}.\n  \n- $\\pi_j$ is the [<span style=\"color:#0D6EFD\">receiver random effect ðŸ›ˆ</span>]{#receiverF}.\n  \n- $\\delta_{ij}$ is the [<span style=\"color:#0D6EFD\">dyadic random effect ðŸ›ˆ</span>]{#DyadicF2}.\n\n- $\\beta_1$ is the effect of an individuals *i* level feature on the emission of a link (i.e., out-strength).\n\n- $\\beta_2$ is the effect of an individuals *j* level feature on the receiving  a link (i.e., in-strength).\n\n- $\\beta_3$ is the effect of an dyadic characteristic between *i* and *j* on the likelihood of a tie.\n\n### *Defining formula sub-equations and prior distributions*\n\nThe sender and receiver random effects are similar to those described in [chapter 13: Varying intercepts](13.%20Varying%20intercepts.qmd), but they are defined here using a joint prior so as to estimate the correlation within individuals to emit and receive a link:\n\n$$\n\\left(\\begin{array}{cc} \n\\lambda_i \\\\\n\\pi_i\n\\end{array}\\right) \n=\n\\begin{array}{cc} \n\\left(\\begin{array}{cc} \n\\sigma_\\lambda \\\\\n\\sigma_\\pi\n\\end{array}\\right) \\circ \n\\left(\\begin{array}{cc} \nL \n\\left(\\begin{array}{cc} \n\\hat{\\lambda}_i \\\\ \\hat{\\pi}_i\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n$$\n\n\n$$\n\\sigma_\\lambda \\sim \\text{Exponential}(1)\n$$\n\n$$\n\\sigma_\\pi \\sim \\text{Exponential}(1)\n$$\n\n$$\nL \\sim \\text{LKJ}(2)\n$$\n\n$$\n\\hat{\\lambda}_i \\sim \\text{Normal}(0,1)\n$$\n\n$$\n\\hat{\\pi}_i \\sim \\text{Normal}(0,1)\n$$\n\nSimilarly, for each dyad we can define a joint prior to estimate correlation between *i*--*j* links and *j*--*i* links:\n\n$$\n\\left(\\begin{array}{cc} \n\\delta_{ij} \\\\\n\\delta_{ji}\n\\end{array}\\right) \n=\n\\begin{array}{cc} \n\\left(\\begin{array}{cc} \n\\sigma_\\delta \\\\\n\\sigma_\\delta\n\\end{array}\\right) \\circ \n\\left(\\begin{array}{cc} \nL_\\delta \n\\left(\\begin{array}{cc} \n\\hat{\\delta}_{ij} \\\\ \\hat{\\delta}_{ji}\n\\end{array}\\right)\n\\end{array}\\right)\n\\end{array}\n$$\n\n$$\n\\sigma_\\delta \\sim \\text{Exponential}(1)\n$$\n\n\n\n$$\nL_\\delta \\sim \\text{LKJ}(2)\n$$\n\n$$\n\\hat{\\delta}_{ij}  \\sim \\text{Normal}(0,1)\n$$\n\n\n\n## Note(s)\n::: callout-note\n-  Note that any additional covariates can be summed with a regression coefficient to $\\lambda_i$, $\\pi_j$ and $\\delta_{ij}$. Of course, for $\\lambda_i$ and $\\pi_j$, as they represent nodal effects, these covariates need to be nodal characteristics (e.g., sex, age), whereas for $\\delta_{ij}$, as it represents dyadic effects, these covariates need to be dyadic characteristics (e.g., genetic distances). Considering the previous example, given a vector of nodal characteristics, *individual_predictors*, and a matrix of dyadic characteristics, *kinship*, we can incorporate these covariates into the sender-receiver and dyadic effects, respectively.\n-  \n\n-   We can apply multiple variables as in [chapter 2: Multiple Continuous Variables](2.%20Multiple%20continuous%20Variables.qmd).\n\n-   We can apply interaction terms as in [chapter 3: Interaction Between Continuous Variables](3.%20Interaction%20between%20continuous%20variables.qmd).\n\n-   Network links can be modeled using Bernoulli (for proportions), Binomial (for unweighted network), Poisson  or zero-inflated Poisson distributions (for count). In BI, you just need to set the correct likelihood distributions. For example, if you want to model the number of interactions between nodes, you can use the Poisson distribution. If you want to model the existence or absence of a link, you can use the Bernoulli distribution.\n\n-   If the network is undirected, then accounting for the correlation between the propensity to emit and receive links is not necessary, and the terms $\\lambda_i$, $\\pi_j$, and $\\delta_{ij}$ are no longer required. (Is it correct?)\n\n- To account for exposure on a poisson model treat exposure as a nodal characteristic with its own parameter effect (i.e., regression coefficient). Their is several function that will help you to convert vectors or matrices in edge list format to have compatible data structure for the model (see API reference for bi.net.vec_to_edgl and bi.net.mat_to_edgl). F\n- \n\n-   In the following chapters, we will see how to incorporate additional network effects into the model to account for network structural properties (e.g., clusters, assortativity, triadic closure, etc.).\n:::\n\n## Reference(s)\n::: {#refs}\n:::\n\n",
    "supporting": [
      "22. Network model_files"
    ],
    "filters": [],
    "includes": {}
  }
}