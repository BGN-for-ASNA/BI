{
  "hash": "ed4805cd328509eb1717b07c63676790",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Bayesian Neural Networks\"\ndescription: \"Neural networks that incorporate Bayesian inference to quantify uncertainty in their predictions.\"\ncategories: [Neural Networks,  Deep Learning]\nimage: \"Figures/24.png\"\norder: 30\n---\n\n\n\n\n\n\n## General Principles\n\nTo model complex, non-linear relationships between variables, we can utilize various approaches, including [splines](https://en.wikipedia.org/wiki/Spline_(mathematics)), [polynomials](https://en.wikipedia.org/wiki/Polynomial),\n[Gaussian processes](15.&#32;Gaussian&#32;processes.qmd), and neural networks. Here, we focus on the *Bayesian Neural Network (BNN)*. Think of a neural network as a highly flexible function composed of interconnected layers of [<span style=\"color:#0D6EFD\">neurons üõà</span>]{#Neurons}. Each connection possesses a *weight*, and each neuron has a *bias*. These *weights* and *biases* act as a vast set of adjustable knobs. In a standard neural network, the goal is to find the single best setting for these knobs to map inputs to outputs. Conversely, a BNN learns **distributions** over its *weights* and *biases* rather than single optimal values. This allows the model to capture not only the patterns in the data but also its own uncertainty regarding those patterns.\n\nTo construct a *BNN*, we must define:\n\n1)  **A Network Architecture**: This specifies the structural design‚Äîthe number of layers, the number of neurons per [<span style=\"color:#0D6EFD\">layer üõà</span>]{#Layers}, and the activation functions (e.g., ReLU, tanh) that introduce non-linearity. This defines the arrangement of our \"knobs.\"\n\n2)  **Priors for Arrays of Weights and Biases**: In simple models like linear regression, we define a prior for each individual parameter (e.g., the slope $\\beta$). However, neural networks often contain thousands or millions of parameters, making it impractical to define a unique prior for each one. Instead, we define priors that act as templates for entire **arrays of parameters**. For instance, we might specify that all weights in a given layer are drawn from the same `Normal(0, 1)` distribution. This allows us to efficiently quantify our beliefs about the entire set of network parameters.\n\n3)  **An Output Distribution (Likelihood)**: This defines the probability of the observed data given the network's predictions. For continuous variables (regression), this is typically a *Normal distribution* with a variance term $\\sigma$, which quantifies the noise of the data around the model's predictions.\n\n## Considerations\n\n::: callout-caution\n-   **Uncertainty**: Like all Bayesian models, BNNs explicitly account for [<span style=\"color:#0D6EFD\">model parameter uncertainty üõà</span>]{#uncertainty}. Here, the parameters are the network's  **weights (W)** and **biases (b)**. We quantify uncertainty through their [<span style=\"color:#0D6EFD\">posterior distribution üõà</span>]{#posterior}. Consequently, we must declare [<span style=\"color:#0D6EFD\">prior distributions üõà</span>]{#prior} for all *weights* and *biases*, as well as for the output variance $\\sigma$.\n\n-   **Interpretation**: Unlike in a linear regression where the coefficient $\\beta$ has a direct interpretation (e.g., the change in $Y$ given a unit increase in $X$), the individual *weights* and *biases* in a *BNN* are not directly interpretable. A single weight's influence is entangled with thousands of others through non-linear functions. Therefore, BNNs should be viewed as powerful **predictive tools** rather than explanatory ones. They excel at learning complex topologies and quantifying predictive uncertainty, but if the goal is to isolate the effect of a specific variable, a simpler model is often more appropriate.\n\n-   **Prior distributions**: are built following these considerations:\n\n    -   As the data is typically [<span style=\"color:#0D6EFD\">scaled üõà</span>]{#scaled} (see introduction), we can use a standard Normal distribution (mean 0, standard deviation 1) as a weakly-informative prior for all weights and biases. This acts as a form of regularization.\n\n    -   Since the output variance $\\sigma$ must be positive, we can use a positively-defined distribution, such as the Exponential or Half-Normal.\n\n-   BNNs can be used for both *regression* and *classification*. The final layer's activation and the chosen likelihood distribution depend on the task. For binary classification, a *sigmoid* activation is paired with a Bernoulli likelihood, which requires a [<span style=\"color:#0D6EFD\">link function üõà</span>]{#linkF} (logit) to connect the linear output of the network to the probability space [0, 1]. For regression, the identity activation is often used with a Gaussian likelihood.\n\n-   **Computational Scalability**: Strictly applying Bayes' theorem to update the posterior distribution becomes computationally intractable for very large neural networks. Calculating the model evidence (the denominator in Bayes' rule) requires integrating over a parameter space that may contain millions of dimensions. Furthermore, the posterior landscape in deep networks is often highly non-convex and multimodal. Consequently, sampling algorithms (such as MCMC) may suffer from poor mixing or become \"stuck\" in local modes, failing to explore the full posterior. In such high-dimensional scenarios, practitioners often must rely on approximate methods, such as \n[<span style=\"color:#0D6EFD\">Variational Inference  üõà</span>]{#VI}\nor [<span style=\"color:#0D6EFD\">Stochastic Gradient MCMC üõà</span>]{#SG-MCMC}, to ensure convergence. \n\n\n:::\n\n## Example\n\nBelow is an example code snippet demonstrating a *Bayesian Neural Network* for regression using the Bayesian Inference (BI) package. Simulated data consist of two continuous variables ($Y$ and $X$), and the goal is to predict $X$ from $Y$ using a non-linear model.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n::: {#ca39c298 .cell execution_count=1}\n``` {.python .cell-code}\nfrom BI import bi\nimport json\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Setup device------------------------------------------------\nm = bi(platform='cpu')\n\n# Import Data & Data Manipulation ------------------------------------------------\n\nwith open('BNN.json', 'r', encoding='utf-8') as file:\n    # Load the JSON data into a Python dictionary\n    data = json.load(file)\n# X is already scaled\nX = jnp.array(data['X']) # Note X shape = (N,2) where first column is the intercept and second column is the predictor\nY = jnp.array(data['Y']) # Note Y shape = (N,1) where N is the number of observations\n\nm.data_on_model = dict(X = X, Y = Y)\n# Define model ------------------------------------------------\ndef model(X, Y,  D_H=5, D_Y=1):  \n    N, D_X = X.shape\n    \n    # First hidden layer: Transforms input to N √ó D_H (hidden units)\n    w1 = m.bnn.layer_linear(\n        X, \n        dist=m.dist.normal(\n            0, 1,  name='w1',shape=(D_X,D_H)\n            ),\n        activation='tanh'\n        )\n\n    # sample final layer of weights and neural network output\n    # Final layer (z3) computes linear combination of second hidden layer\n    w2 = m.bnn.layer_linear(\n        X=w1, \n        dist=m.dist.normal(0, 1,  name='w2',shape=(D_H,D_Y))\n        )\n\n    sigma = m.dist.exponential(1, name='sigma')\n\n    m.dist.normal(w2, sigma, obs=Y,name='Y')\n\n# Run mcmc ------------------------------------------------\nm.fit(model, num_samples=500, progress_bar=False)   # Approximate posterior distributions for weights, biases, and sigma\n\n# Predictions from the model ------------------------------------------------\npred = m.sample(samples = 500)['Y']\npred = pred[..., 0]\nmean_prediction = jnp.mean(pred, axis=0)\npercentiles = jnp.percentile(pred, jnp.array([5.0, 95.0]), axis=0)\n# make plots\nfig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n# plot training data\nax.plot(X[:, 1], Y[:, 0], \"kx\")\n# plot 90% confidence level of predictions\nax.fill_between(\n    X[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n)\n# plot mean prediction\nax.plot(X[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\nax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\njax.local_device_count 32\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n‚ö†Ô∏èThis function is still in development. Use it with caution. ‚ö†Ô∏è\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n[Text(0.5, 0, 'X'),\n Text(0, 0.5, 'Y'),\n Text(0.5, 1.0, 'Mean predictions with 90% CI')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](27. BNN_files/figure-html/cell-2-output-3.png){width=779 height=587}\n:::\n:::\n\n\n## R\n\n```r\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n\n# Load csv file\n# Import data ------------------------------------------------\npath = normalizePath(paste(system.file(package = \"BayesianInference\"),\"/data/BNN.json\", sep = ''))\ndata <- fromJSON(path)\nm$data_on_model = list()\nm$data_on_model$X = jnp$array(data$X)\nm$data_on_model$Y = jnp$array(data$Y)\n\n# Define model ------------------------------------------------\nmodel <- function(X, Y, D_X = 2, D_H=5L, D_Y=1L){\n\n  w1 <- m$bnn$layer_linear(X, dist=bi.dist.normal(0, 1,  name='w1',shape=c(D_X,D_H)), activation='tanh')\n  \n  w2 <- m$bnn$layer_linear(\n    w1, \n    dist=bi.dist.normal(0, 1,  name='w2',shape=c(D_H,D_Y)),\n    activation='tanh'\n  )\n  \n  # Prior for the output standard deviation\n  s = bi.dist.exponential(1, name = 's')\n  \n  # Likelihood\n  bi.dist.normal(w2, s, obs = Y)\n}\n\n# Run mcmc ------------------------------------------------\nm$fit(model) # Approximate posterior distributions\n```\n:::\n\n\n## Mathematical Details\nIn the Bayesian formulation, we place [<span style=\"color:#0D6EFD\">priors üõà</span>]{#prior} on all weights and biases and define a likelihood for the output. For a regression task with a one-hidden-layer BNN with $J$ neurons per hidden layer and a $5$-vector of predictors and a $2$-vector of outcomes we can run the model as below.\n\nFor a given individual $i$ assuming you have data on their height $H_i$, weight $W_i$, age $A_i$, and strength $S_i$, the the covariate vector (including the intercept of 1) can be written as:\n$$\nX_i = [1, H_i, W_i, A_i, S_i]\n$$\n\nWe can map this into a hidden layer as follows. Let $\\Theta_1$ be a $J \\times 5$ matrix of weights and $b_1$ be a $5$-vector of biases, then the first hidden layer can be written as:\n$$\n\\Omega_{i,1} = f(X_i \\Theta_1 + b_1)\n$$\n\nNext, we can had an arbitrary number of hidden layers. In this case, we will include only a single additional layer, $\\Omega_{i,2}$. In this case, $\\Theta_2$ is an $J \\times J$ matrix of weights and $b_2$ is a $J$-vector of biases. This second hidden layer can be written as:\n\n$$\n\\Omega_{i,2}  = f(\\Omega_{i,1} \\Theta_2 + b_2)\n$$\n\nFinally we can map to the output layer. In this case, $\\Theta_3$ is a $J \\times 2$ matrix of weights and $b_3$ is a $2$-vector of biases. This leads to a final output layer mapping to the dimensions of the observed outcome (in this a $2$-vector):\n\n$$\n\\Omega_{i,3}  = f(\\Omega_{i,2} \\Theta_3 + b_3)\n$$\n\nFinally, we can model the observed outcomes using $\\Omega_{i,3}$ as the mean vector of a normal distribution, with a standard deviation of $\\sigma$. This is the likelihood function for the output layer:\n\n$$\n\\mu = \\Omega_{i,3}\n$$\n$$\nY_{i,1} \\sim \\text{Normal}(\\mu_{1}, \\sigma_1)\n$$\n$$\nY_{i,2} \\sim \\text{Normal}(\\mu_{2}, \\sigma_2) \n$$\n\nWith priors for $\\sigma$:\n$$\n\\sigma_1 \\sim \\text{Exponential}(1)\n$$\n$$\n\\sigma_2 \\sim \\text{Exponential}(1)\n$$\n\n## Notes\n::: callout-note\n- The primary difference between a *Frequentist* and *Bayesian* neural network lies in how parameters are treated. In the frequentist approach, weights and biases are point estimates found by minimizing a loss function (e.g., via gradient descent). Techniques like  [<span style=\"color:#0D6EFD\">Dropout üõà</span>]{#Dropout}or [<span style=\"color:#0D6EFD\">L2 regularization üõà</span>]{#L2} are often used to prevent [<span style=\"color:#0D6EFD\">overfitting üõà</span>]{#overfitting}, which can be interpreted as approximations to a Bayesian treatment. In contrast, the *Bayesian* formulation does not seek a single best set of weights. Instead, it uses methods like MCMC or Variational Inference to approximate the entire posterior distribution for every *weight* and *bias*. This provides a principled and direct way to quantify model uncertainty.\n- While present an example of non-linear regression, the Bayesian Neural Network can be used for linear regressions as well (keeping in mind that interpretation of the weights are impossible).\n\n:::\n\n## Reference(s)\n\n@phan2019composable\n\n",
    "supporting": [
      "27. BNN_files"
    ],
    "filters": [],
    "includes": {}
  }
}