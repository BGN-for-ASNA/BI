{
  "hash": "0bdbfe941ac2aaa22a5623096d03c8ca",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"üöß Dirichlet Model\"\ndescription: \"Modeling uncertainty about the probabilities of categories themselves.\"\ncategories: [Regression, GLM, Classification]\nimage: \"Figures/10.png\"\norder: 13\n---\n\n\n\n\n## General Principles\nTo model the relationship between a vector outcome variable in which each element of the vector is a frequency from a set of more than two categories and one or more independent variables, we can use a _Dirichlet_ model.\n\n## Considerations\n::: callout-note\n- We have the same considerations as for the [Multinomial model](9.&#32;Multinomial&#32;model.qmd).\n\n:::\n\n## Example\n::: {.panel-tabset group=\"language\"}\n## Python\n![](travaux-routiers.png){fig-align=\"center\"}\n\n<!---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#from BI import bi, jnp\n#import jax\n## Setup device ------------------------------------------------\n#m = bi('cpu')\n#\n## Import Data & Data Manipulation ------------------------------------------------\n## Import\n#from importlib.resources import files\n#data_path = files('BI.resources.data') / 'Sim data multinomial.csv'\n#m.data(data_path, sep=',') \n#\n## Define model ------------------------------------------------\n#def model(income, career):\n#    # Parameter prior distributions\n#    alpha = m.dist.normal(0, 1, shape=(2,), name='a')\n#    beta = m.dist.half_normal(0.5, shape=(1,), name='b')\n#    s_1 = alpha[0] + beta * income[0]\n#    s_2 = alpha[1] + beta * income[1]\n#    s_3 = [0]\n#    p = jnp.exp(jnp.stack([s_1[0], s_2[0], s_3[0]]))\n#    # Likelihood\n#    m.dist.dirichlet(p[career], obs=career)\n#\n## Run sampler ------------------------------------------------ \n#m.fit(model)  \n#\n## Summary ------------------------------------------------\n#m.summary()\n```\n:::\n\n\n--->\n## R\n![](travaux-routiers.png){fig-align=\"center\"}\n<!---\n```R\nlibrary(BayesianInference)\nm=importBI(platform='cpu')\n```\n--->\n:::\n\n## Mathematical Details\nWe can model a vector of frequencies using a Dirichlet distribution. For an outcome variable $Y_i$ with $ùêæ$ categories, the *Dirichlet* likelihood function is:\n\n$$\nY_i \\sim \\text{Dirichlet}(\\theta_i  \\kappa) \\\\\n\\theta_i = \\text{Softmax}(\\phi_i) \\\\\n\\phi_{[i,1]} = \\alpha_1 + \\beta_1 X_i \\\\\n\\phi_{[i,2]} = \\alpha_2 + \\beta_2 X_i \\\\\n... \\\\\n\\phi_{[i,k]} = 0 \\\\\n\\kappa \\sim \\text{Exponential}(1) \\\\\n\\alpha_{k} \\sim \\text{Normal}(0,1) \\\\\n\\beta_{k} \\sim \\text{Normal}(0.1)\n$$\n\nWhere:\n\n- $Y_i$ is the outcome [<span style=\"color:#0D6EFD\">simplex üõà</span>]{#simplex} for observation *i*.\n\n- $\\kappa$ is the concentration parameter, it controls the prior weight on each category.\n  \n- $\\theta_i$ is a vector unique to each observation, *i*, which gives the probability of observing *i* in category *k*. \n  \n- $\\phi_i$ give the linear model for each of the $k$ categories. Note that we use the softmax function to ensure that that the probabilities $\\theta_i$ form a [<span style=\"color:#0D6EFD\">simplex üõà</span>]{#simplex}.\n  \n- Each element of $\\phi_i$ is obtained by applying a linear regression model with its own respective intercept $\\alpha_k$ and slope coefficient $\\beta_k$. To ensure the model is identifiable, one category, *K*, is arbitrarily chosen as a reference or baseline category. The linear predictor for this reference category is set to zero. The coefficients for the other categories then represent the change in the log-odds of being in that category versus the reference category.\n\n\n## Reference(s)\n\n",
    "supporting": [
      "10. Dirichlet model_files/figure-pdf"
    ],
    "filters": []
  }
}